[
  {
    "objectID": "index_dashboards.html",
    "href": "index_dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "No matching items\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "[M_Study_3주차과제1] Softmax로 MNIST다루기\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\nMStudy\n\n\nMNIST\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-15\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_3주차과제2] Neural Network로 MNIST다루기\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\nMStudy\n\n\nMNIST\n\n\nTensorflow\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-15\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\ngithub\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-10\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_3주차] Multi-class Classification / Artificial Neural Network\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\nMStudy\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-09\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-2(github온라인db)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\ngithub\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-08\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-06\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_2주차] Multiple Regression / Logistic Regression\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\nMStudy\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-02\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_1주차] Tensorflow / Linear Regression\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\nMStudy\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-05-26\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-21\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-19\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 가장 비싼 상품 구하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-18\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 평균 일일 대여 요금 구하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-17\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n[한빛앤] GitHub Copilot 세미나 정리\n\n\n\nCopilot\n\n\nCodeium\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-09-13\n\n\n\n\n\n\n\n\n\n\n\n\n[Pycon2023] 짠내나는 데이터 다루기 세션 정리\n\n\n\n파이썬\n\n\n파이콘\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-08-13\n\n\n\n\n\n\n\n\n\n\n\n\n[Scikit-learn] Kaggle 집값예측 실습\n\n\n\n파이썬\n\n\n머신러닝\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-05-06\n\n\n\n\n\n\n\n\n\n\n\n\n[Pytorch] MNIST 실습\n\n\n\n파이썬\n\n\n머신러닝\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-02-19\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2022-11-13\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] 공공데이터포털 건강검진정보 활용\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2022-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "index_miniprojects.html",
    "href": "index_miniprojects.html",
    "title": "Mini Projects",
    "section": "",
    "text": "[Python] Github, API활용한 공공데이터 저장소 만들기\n\n\n\nPython\n\n\nrequests\n\n\njson\n\n\nsubprocess\n\n\npandas\n\n\n\n[Python] requests(공공데이터API호출용), json(json규격 자료저장), subprocess(github push자동화), pandas(csv저장)\n\n\n\nKibok Park\n\n\n2024-06-10\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool\n\n\n\nPython\n\n\nSAP Scripting\n\n\n\nPython, win32를 활용한 SAP Scripting\n\n\n\nKibok Park\n\n\n2024-03-06\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] COO발급관리용 Tool\n\n\n\nPython\n\n\nwin11toast\n\n\nsqlite3\n\n\nstreamlit\n\n\npandas\n\n\nselenium\n\n\n\n[Python] selenium(웹스크레핑), sqlite3(db), win11toast(알림), streamlit(UI)\n\n\n\nKibok Park\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] LocalL/C 관리용 Tool\n\n\n\nPython\n\n\nSAP Scripting\n\n\nStreamlit\n\n\nsqlite3\n\n\nBeautifulSoup\n\n\n\nPython, Streamlit을 활용한 업무자동화\n\n\n\nKibok Park\n\n\n2024-01-22\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록\n\n\n\nPython\n\n\nre\n\n\nxlwings\n\n\npandas\n\n\npdfminer\n\n\n\n[Python] re(regex), xlwings(암호화 Excel리딩), pdfminer(pdf리딩), pywin32(outlook)\n\n\n\nKibok Park\n\n\n2023-12-15\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] Peak타임 대응용 수출계약서pdf tabula리딩\n\n\n\nPython\n\n\ntabula\n\n\nxlwings\n\n\npathlib\n\n\npandas\n\n\n\n[Python] tabula(pdf리딩[표 형태], xlwings(암호화 Excel리딩)\n\n\n\nKibok Park\n\n\n2023-11-02\n\n\n\n\n\n\n\n\nNo matching items\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kibok Park",
    "section": "",
    "text": "I’m Kibok Park. Welcome to my blog. I’m not an expert in my favorites yet, but I’m working on becoming one. I hope you to have nice time with my blog.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "about.html#hello",
    "href": "about.html#hello",
    "title": "Kibok Park",
    "section": "",
    "text": "I’m Kibok Park. Welcome to my blog. I’m not an expert in my favorites yet, but I’m working on becoming one. I hope you to have nice time with my blog."
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Kibok Park",
    "section": "Topics",
    "text": "Topics\nPython, Quantative trading"
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "",
    "text": "공부할 겸 혼자서 구글링하면서 용어 정리해보는 포스팅입니다\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#프롤로그",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#프롤로그",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "프롤로그",
    "text": "프롤로그\n\n경제적 해자(enonomic moat) : 성 외곽에서 성을 보호해주는 해자처럼 경쟁우위를 갖게 해주는 요소 (a business’s ability to maintain a competitive edge over its competitors)\n자본배분(capital allocation) : 경제적 자원들을 효율증대/이익극대화를 위해 분배/투자 하는 것 (distributing and investing a company’s financial resources in ways that will increase its efficiency, and maximize its profits) &gt; 자본배분을 알면 워런버핏 사례의 캐피털시티를 이해할 수 있다\n플로트(float, 책임준비금) : 보험회사가 보험금을 지급하기 위해 적립시키는 돈(은행과 달리 자산운용준칙에 따라 자율적으로 사용 가능)\n\n플로트라는 단어의 다른 뜻 : 일반 대중이 거래할 수 있게 발행한 보통주 (the regular shares a company has issued to the public that are available for investors to trade) &gt; 플로트를 알면 워런버핏 사례의 가이코를 이해할 수 있다 (내셔널 인뎀너티와 블루칩스탬프의 플로트를 활용한 투자법 등)\n\n재보험 : 보험계약상 책임의 전부 또는 일부를 다른 보험자에게 인수시킴 (위험의 분산과 인수능력의 극대화를 위해 필요, 위험 대비 자산이 충분치 않은 경우 재보험으로 보험금액의 전부 인수가능)\n영업이익(operating income/earnings) : 기업의 핵심사업(영업활동)으로 얻은 이익 (a measure of the amount of profit realized from a business’s core operations)\n연간 영업활동 순이익(operating earnings) : 영업활동에 필요한 차입금 등 부채의 이자비용도 차감한 이익(영업활동에서 발생한 순이익)\n\n일반 회계기준(GAAP, Generally Accepted Accounting Principles, GAAP)\n\n일반적으로 인정된 회계원칙. 회계규정 자체 또는 회계실무 지침 등 광범위하게 인정되는 회계기준 &gt; GAAP관련 추가로 알아보기\n\n\n**IFRS(International Financial Reporting Standards) : 국제회계기준위원회에서 공표한 회계기준. IFRS를 공부하면 IFRS를 차용한 국가의 회사 재무제표는 같은 형식으로 이해 가능\n\n규칙기반의 GAAP vs 원칙기반의 IFRS\n\nIFRS는 원칙에 따라 작성해 형태 차이가 거의 없으나, GAAP는 세부적인 사안에 대해 자세히 기술(IFRS도 주석 부분에 기술)\n\n\n내용을 세세하게 다루거나, 공정가치를 계산하지않아 IFRS보다 현재가치를 크게 하고 싶을때 GAAP사용\n\n\n\nNon-GAAP\n\n반복적으로 발생하지 않는 1회적 비용은 제외하고 회계처리. 비용이 줄어 순수익이 늘어남\n\n\n미국의 경우 Non-GAAP로 다양한 비용을 제외시켜 표시하는 것이 불법은 아님(기업에 유리) 참고한 링크"
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-섬유공장",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-섬유공장",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "1장. 섬유공장",
    "text": "1장. 섬유공장\n\n\n총자산이익률(ROA, Return on assets) : 순이익 / 총자산(자기자본 + 타인자본). 얼마나 순이익이 창출되는지 판단할 수 있음. ROA의 높고 낮음에 따라 주가의 높낮이도 따를 가능성이 큼 산업의 성숙기[성장/성숙/사양 등]에 따라 ROA는 달라질 수 있음\n자기자본이익률(ROE, Return on equity) : 순이익 / 자본총계(자산-부채) ROA가 높은 경우 적정 수준으로 부채확대를 통해 총자산 자체를 늘리면 ROE를 높일 수 있음 참고한 링크 참고한 링크\n\n\n\n운전자본(Working Capital, =유동자본?) : 회사를 운영하는데 들어가는 돈(매출채권, 선급금, 재고자산 등 유동자산과 매입채무, 선수금 등 유동부채가 해당함)\n순운전자본(Net Working Capital, 운전자본-총부채) : ?? (클럽 설명) 벤자민 그레이엄이 ’현명한 투자자’에서 주창한 투자 전략 즉시 현금화할 수 있는 자산과 비교함으로써 보수적인 기준에서 주가의 저(고)평가 여부를 판단하는 척도로 사용\n장부가치\n주가순자산배수(PBR, price to book value)\n주주환원\n자사주 매입\n감가상각비\n자산 대체원가\n무형자산상각비 차감 전 이익(EBITDA)\n투하자본이익률(ROIC, return on invested capital)\n배당의 이중과세 : 연간이익에 대한 기업의 법인세 + 배당금에 대한 주주의 소득세\n자사주 매입을 통한 주주의 세금 이연(은 주주이익에 도움)"
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-투자-1962-1965년",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-투자-1962-1965년",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "2장. 투자: 1962-1965년",
    "text": "2장. 투자: 1962-1965년\n\n무한책임 파트너(general partner) : 펀드 운용에 관한 무한책임을 지는 출자자, 보통 운용자를 말함\n헤지펀드 :\n환매 :\n이월결손금\n손익계산서\n유동성소요\n**내재 사업가치(intrinsic business value)\n유한책임조합"
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-전환-1965-1967년",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-전환-1965-1967년",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "3장. 전환: 1965-1967년",
    "text": "3장. 전환: 1965-1967년\n\n자본배분\n투하자본이익률\n매출원가\n특별항목 (’특별항목 반영 전 순이익’에서 사용된 의미, ’1964년 특별항목은 유휴설비비용 22만 달러를 포함했다’와 같이 사용)\n유휴설비비용(idle plant expense): 비영업용 제조설비의 유지보수 및 감가상각비\n**기대손실\n손상차손 write-down\n유형자산 손상차손\n이익잉여금\n총포괄손익\n총창출자본(total capital generated)"
  },
  {
    "objectID": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-인수-1967-1969년",
    "href": "posts/yyyymmdd-bk-BerkshireHathaway-1/index.html#장.-인수-1967-1969년",
    "title": "[버크셔 해서웨이의 재탄생 읽고] 용어 정리",
    "section": "4장. 인수: 1967-1969년",
    "text": "4장. 인수: 1967-1969년\n\n완전 소유 기업(wholly-owned-subsidiary) : 1개 기업이 단독 투자하여 100% 소유 지문을 가지는것\n상환청구권 :\n\n\n플로트(책임준비금) : + 지급준비금(lost reserve) + 손해사정비 준비금(lost adjustment expense reserve) + 미경과보험료 적립금(unearned premium reserve) - 대리점 미수금(agents’ balance) - 선급 신계약비(prepaid acquisition cost) - 출재보험 준비금(deferred charges applicable to assumed reinsurance)"
  },
  {
    "objectID": "posts/vba-noname-yyyymmdd/index.html",
    "href": "posts/vba-noname-yyyymmdd/index.html",
    "title": "NH증권 직무인터뷰를 읽고(트레이딩&빅데이터)",
    "section": "",
    "text": "관심이 생겨서 관련 공고 등을 보고 있는데, 직무이해 겸 모르는 단어를 정리해봅니다.\n\n\n\n\n\n\n참고한 공고 (클릭해서 펼치기)\n\n\n\n\n\nNH투자증권 채용공고-Trading\nNH투자증권 채용공고-빅데이터 분석\nNH투자증권 직무인터뷰\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/vba-noname-yyyymmdd/index.html#참고한-공고-인터뷰-자료",
    "href": "posts/vba-noname-yyyymmdd/index.html#참고한-공고-인터뷰-자료",
    "title": "NH증권 직무인터뷰를 읽고(트레이딩&빅데이터)",
    "section": "",
    "text": "관심이 생겨서 관련 공고 등을 보고 있는데, 직무이해 겸 모르는 단어를 정리해봅니다.\n\n\n\n\n\n\n참고한 공고 (클릭해서 펼치기)\n\n\n\n\n\nNH투자증권 채용공고-Trading\nNH투자증권 채용공고-빅데이터 분석\nNH투자증권 직무인터뷰"
  },
  {
    "objectID": "posts/vba-noname-yyyymmdd/index.html#트레이딩-직무-단어정리",
    "href": "posts/vba-noname-yyyymmdd/index.html#트레이딩-직무-단어정리",
    "title": "NH증권 직무인터뷰를 읽고(트레이딩&빅데이터)",
    "section": "트레이딩 직무 단어정리",
    "text": "트레이딩 직무 단어정리\n\n프랍 트레이딩(Proprietary Trading) : 고객이 아닌 금융회사의 돈으로 주식이나 파생상품 등 금융상품을 거래하는 것 (=자기계정거래, 자기계좌거래, 고유계정거래, 고유계좌거래)\n\n\n프랍트레이딩은 통상 선물·옵션 등 파생상품 부문, 일반적 주식투자 같은 방향성 매매, 알고리즘에 따라 투자하는 시스템 매매, 채권투자, 부동산이나 인프라 등의 대체투자 등으로 나뉜다. 보통 자기매매 비중이 높은 중소형 증권사에는 많게는 50여명에 이르는 프랍트레이더들이 있다. 이 가운데 80% 이상이 파생 트레이더다.중소형 증권사들의 파생상품투자 비중이 높은 것은 비용이 싸기 때문이다. 증권사의 경우 선물·옵션 거래시 사후 증거금 제도가 적용된다. 장중 거래에는 증거금이 따로 필요 없고 장 마감 후 포지션 규모에 따라 증거금을 마련하면 된다. 따라서 장중에는 활발히 거래하고 장 마감 직전 파생상품을 보유하지 않았으면 증거금 부담은 없다. 하루 단위로 청산하는 경우 사실상 금융비용은 없다.\n상기내용 참고한 서울경제 기사링크 거래증거금vs위탁증거금 내용 참고할 수 있는 블로그\n\n\n장내거래 vs 장외거래 : 정규시장 외 체결되는 거래\n매크로 지식 : Macro economics지식 (↔︎ Micro economics)\n서킷브레이커(일시매매정지, Trading Curb) : 가격 변동이 지나친 경우 일시적으로 거래를 중단하는 것\n틱 : 체결량에 따른 단위 (5틱 = 체결 5회 기준)\n셀사이드(Sell-side|은행, 증권사 등) : 금융 서비스 등을 제공하여 수익 창출\n바이사이드(Buy-side|자산운용사, 연기금, 보험사, 헤지펀드 등) : 투자를 하여 수익 창출\n\n\n참고글:셀사이드와 바이사이드의 금융공학\n\n\n하우스 : 증권사, 금융사 등 단체를 칭함"
  },
  {
    "objectID": "posts/vba-noname-yyyymmdd/index.html#빅데이터-직무-단어정리",
    "href": "posts/vba-noname-yyyymmdd/index.html#빅데이터-직무-단어정리",
    "title": "NH증권 직무인터뷰를 읽고(트레이딩&빅데이터)",
    "section": "빅데이터 직무 단어정리",
    "text": "빅데이터 직무 단어정리\n\nMTS : Mobile Trading System (HTS : Home Trading System)\n원장 : 증권사가 고객계좌나 거래내역, 매매 등을 관리하는 프로그램"
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html",
    "href": "posts/coach-ml-20240505/index.html",
    "title": "[Pytorch] MNIST 실습",
    "section": "",
    "text": "파이토치로 MNIST 머신러닝 실습해본 코드 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "href": "posts/coach-ml-20240505/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "title": "[Pytorch] MNIST 실습",
    "section": "Pytorch활용한 MNIST 데이터셋 로딩",
    "text": "Pytorch활용한 MNIST 데이터셋 로딩\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nroot = './data'\nmnist_train = dset.MNIST (root=root, train=True, transform=transforms.ToTensor(), download=True )\nmnist_test = dset.MNIST (root=root, train=False, transform=transforms.ToTensor(), download=True)\n\n# Train용 / Test용 데이터셋\ntrain_loader = DataLoader(mnist_train, batch_size=10, shuffle=True)\ntest_loader = DataLoader(mnist_test, batch_size=10, shuffle=True)\n\n\nFigure 1"
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html#학습준비가중치-초기화-등",
    "href": "posts/coach-ml-20240505/index.html#학습준비가중치-초기화-등",
    "title": "[Pytorch] MNIST 실습",
    "section": "학습준비(가중치 초기화 등)",
    "text": "학습준비(가중치 초기화 등)\n\nMNIST의 크기 : 28 * 28\nLoss : Cross Entropy\nOptimizer - SGD(Stochastic Gradient Descent)\nLearning rate = 0.1\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 모델 구현 (28*28 = 784 / 0~9라서 10개 / 가중치 사용하므로 bias)\nlinear = torch.nn.Linear(784, 10, bias=True).to(device) \n\n# weight init 가중치 초기화\ntorch.nn.init.normal_(linear.weight)\n\n# Loss fn - Cross Entropy Loss\ncriterion = torch.nn.CrossEntropyLoss().to(device)\n\n# optimizer - SGD\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html#모델-학습",
    "href": "posts/coach-ml-20240505/index.html#모델-학습",
    "title": "[Pytorch] MNIST 실습",
    "section": "모델 학습",
    "text": "모델 학습\n\ntraining_epochs = 20 # training 반복 횟수\n\nfor epoch in range(training_epochs):\n  for i, (imgs, labels) in enumerate(train_loader):\n    labels = labels.to(device)\n    imgs = imgs.view(-1, 28 * 28).to(device)\n\n    outputs = linear(imgs) \n    loss = criterion(outputs, labels) \n\n    optimizer.zero_grad()# optimzier zero grad\n\n    loss.backward() # loss backward\n    optimizer.step() # optimzier step\n\n    _,argmax = torch.max(outputs, 1)\n    accuracy = (labels == argmax).float().mean()\n\n  if (i+1) % 100 == 0:\n    print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(\n    epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))\n\nEpoch [1/20], Step [6000/6000], Loss:  0.0273, Accuracy:  100.00%\nEpoch [2/20], Step [6000/6000], Loss:  0.0762, Accuracy:  100.00%\nEpoch [3/20], Step [6000/6000], Loss:  0.5928, Accuracy:  80.00%\nEpoch [4/20], Step [6000/6000], Loss:  0.2854, Accuracy:  90.00%\nEpoch [5/20], Step [6000/6000], Loss:  0.1373, Accuracy:  90.00%\nEpoch [6/20], Step [6000/6000], Loss:  0.0668, Accuracy:  100.00%\nEpoch [7/20], Step [6000/6000], Loss:  0.0253, Accuracy:  100.00%\nEpoch [8/20], Step [6000/6000], Loss:  0.0542, Accuracy:  100.00%\nEpoch [9/20], Step [6000/6000], Loss:  0.9203, Accuracy:  80.00%\nEpoch [10/20], Step [6000/6000], Loss:  0.1244, Accuracy:  90.00%\nEpoch [11/20], Step [6000/6000], Loss:  0.6108, Accuracy:  90.00%\nEpoch [12/20], Step [6000/6000], Loss:  0.1312, Accuracy:  100.00%\nEpoch [13/20], Step [6000/6000], Loss:  0.0705, Accuracy:  100.00%\nEpoch [14/20], Step [6000/6000], Loss:  1.6259, Accuracy:  70.00%\nEpoch [15/20], Step [6000/6000], Loss:  0.0538, Accuracy:  100.00%\nEpoch [16/20], Step [6000/6000], Loss:  0.2435, Accuracy:  80.00%\nEpoch [17/20], Step [6000/6000], Loss:  0.0061, Accuracy:  100.00%\nEpoch [18/20], Step [6000/6000], Loss:  0.1091, Accuracy:  100.00%\nEpoch [19/20], Step [6000/6000], Loss:  0.0157, Accuracy:  100.00%\nEpoch [20/20], Step [6000/6000], Loss:  0.1413, Accuracy:  90.00%"
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html#학습된-모델-테스트",
    "href": "posts/coach-ml-20240505/index.html#학습된-모델-테스트",
    "title": "[Pytorch] MNIST 실습",
    "section": "학습된 모델 테스트",
    "text": "학습된 모델 테스트\n\nlinear.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for i, (imgs, labels) in enumerate(test_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        imgs = imgs.view(-1, 28 * 28)\n\n        outputs = linear(imgs) # 구현\n\n        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n        total += imgs.size(0)\n        correct += (labels == argmax). sum().item()\n\n    print('Test accuracy for {} images: {: .2f}%'.format(total, correct / total * 100))\n\nTest accuracy for 10000 images:  91.99%"
  },
  {
    "objectID": "posts/coach-ml-20240505/index.html#개요",
    "href": "posts/coach-ml-20240505/index.html#개요",
    "title": "[Pytorch] MNIST 실습",
    "section": "",
    "text": "파이토치로 MNIST 머신러닝 실습해본 코드 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html",
    "href": "posts/coach-ml-kaggle-20230506/index.html",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "",
    "text": "Kaggle Korea - House price prediction 실습 기록용으로 남깁니다.\nKaggle 원문 링크\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#개요",
    "href": "posts/coach-ml-kaggle-20230506/index.html#개요",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "",
    "text": "Kaggle Korea - House price prediction 실습 기록용으로 남깁니다.\nKaggle 원문 링크"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "href": "posts/coach-ml-kaggle-20230506/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "Pytorch활용한 MNIST 데이터셋 로딩",
    "text": "Pytorch활용한 MNIST 데이터셋 로딩\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nroot = './data'\nmnist_train = dset.MNIST (root=root, train=True, transform=transforms.ToTensor(), download=True )\nmnist_test = dset.MNIST (root=root, train=False, transform=transforms.ToTensor(), download=True)\n\n# Train용 / Test용 데이터셋\ntrain_loader = DataLoader(mnist_train, batch_size=10, shuffle=True)\ntest_loader = DataLoader(mnist_test, batch_size=10, shuffle=True)\n\n\nFigure 1"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#학습준비가중치-초기화-등",
    "href": "posts/coach-ml-kaggle-20230506/index.html#학습준비가중치-초기화-등",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "학습준비(가중치 초기화 등)",
    "text": "학습준비(가중치 초기화 등)\n\nMNIST의 크기 : 28 * 28\nLoss : Cross Entropy\nOptimizer - SGD(Stochastic Gradient Descent)\nLearning rate = 0.1\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 모델 구현 (28*28 = 784 / 0~9라서 10개 / 가중치 사용하므로 bias)\nlinear = torch.nn.Linear(784, 10, bias=True).to(device) \n\n# weight init 가중치 초기화\ntorch.nn.init.normal_(linear.weight)\n\n# Loss fn - Cross Entropy Loss\ncriterion = torch.nn.CrossEntropyLoss().to(device)\n\n# optimizer - SGD\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#모델-학습",
    "href": "posts/coach-ml-kaggle-20230506/index.html#모델-학습",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "모델 학습",
    "text": "모델 학습\n\ntraining_epochs = 20 # training 반복 횟수\n\nfor epoch in range(training_epochs):\n  for i, (imgs, labels) in enumerate(train_loader):\n    labels = labels.to(device)\n    imgs = imgs.view(-1, 28 * 28).to(device)\n\n    outputs = linear(imgs) \n    loss = criterion(outputs, labels) \n\n    optimizer.zero_grad()# optimzier zero grad\n\n    loss.backward() # loss backward\n    optimizer.step() # optimzier step\n\n    _,argmax = torch.max(outputs, 1)\n    accuracy = (labels == argmax).float().mean()\n\n  if (i+1) % 100 == 0:\n    print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(\n    epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))\n\nEpoch [1/20], Step [6000/6000], Loss:  0.0273, Accuracy:  100.00%\nEpoch [2/20], Step [6000/6000], Loss:  0.0762, Accuracy:  100.00%\nEpoch [3/20], Step [6000/6000], Loss:  0.5928, Accuracy:  80.00%\nEpoch [4/20], Step [6000/6000], Loss:  0.2854, Accuracy:  90.00%\nEpoch [5/20], Step [6000/6000], Loss:  0.1373, Accuracy:  90.00%\nEpoch [6/20], Step [6000/6000], Loss:  0.0668, Accuracy:  100.00%\nEpoch [7/20], Step [6000/6000], Loss:  0.0253, Accuracy:  100.00%\nEpoch [8/20], Step [6000/6000], Loss:  0.0542, Accuracy:  100.00%\nEpoch [9/20], Step [6000/6000], Loss:  0.9203, Accuracy:  80.00%\nEpoch [10/20], Step [6000/6000], Loss:  0.1244, Accuracy:  90.00%\nEpoch [11/20], Step [6000/6000], Loss:  0.6108, Accuracy:  90.00%\nEpoch [12/20], Step [6000/6000], Loss:  0.1312, Accuracy:  100.00%\nEpoch [13/20], Step [6000/6000], Loss:  0.0705, Accuracy:  100.00%\nEpoch [14/20], Step [6000/6000], Loss:  1.6259, Accuracy:  70.00%\nEpoch [15/20], Step [6000/6000], Loss:  0.0538, Accuracy:  100.00%\nEpoch [16/20], Step [6000/6000], Loss:  0.2435, Accuracy:  80.00%\nEpoch [17/20], Step [6000/6000], Loss:  0.0061, Accuracy:  100.00%\nEpoch [18/20], Step [6000/6000], Loss:  0.1091, Accuracy:  100.00%\nEpoch [19/20], Step [6000/6000], Loss:  0.0157, Accuracy:  100.00%\nEpoch [20/20], Step [6000/6000], Loss:  0.1413, Accuracy:  90.00%"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#학습된-모델-테스트",
    "href": "posts/coach-ml-kaggle-20230506/index.html#학습된-모델-테스트",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "학습된 모델 테스트",
    "text": "학습된 모델 테스트\n\nlinear.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for i, (imgs, labels) in enumerate(test_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        imgs = imgs.view(-1, 28 * 28)\n\n        outputs = linear(imgs) # 구현\n\n        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n        total += imgs.size(0)\n        correct += (labels == argmax). sum().item()\n\n    print('Test accuracy for {} images: {: .2f}%'.format(total, correct / total * 100))\n\nTest accuracy for 10000 images:  91.99%"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#개념",
    "href": "posts/coach-ml-kaggle-20230506/index.html#개념",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "개념",
    "text": "개념\nRMSE(Root Mean Squeare Error)\nRoot    (4)\nMean    (3)\nSquare  (2)\nError   (1)\n(1) 실제값에서 예측값을 뺀 '오차'를\n(2) 합했을 때 음수의 영향을 제거하기 위해 '제곱'하고\n(3) '평균'오차로 만든 후\n(4) '루트'를 씌워 값의 크기를 작게 한다 (값을 작게하여 연산속도에 이점이 있다)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#파일-다운로드-및-알아보기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#파일-다운로드-및-알아보기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "파일 다운로드 및 알아보기",
    "text": "파일 다운로드 및 알아보기\nFile descriptions\ntrain.csv - 예측 모델을 만들기 위해 사용하는 학습 데이터입니다. \n    집의 정보와 예측할 변수인 가격(Price) 변수를 가지고 있습니다.\ntest.csv - 학습셋으로 만든 모델을 가지고 예측할 가격(Price) 변수를 제외한 집의 정보가\n    담긴 테스트 데이터 입니다.\nsample_submission.csv - 제출시 사용할 수 있는 예시 submission.csv 파일입니다.\nData fields\nID : 집을 구분하는 번호\ndate : 집을 구매한 날짜\nprice : 집의 가격(Target variable)\nbedrooms : 침실의 수\nbathrooms : 화장실의 수\nsqft_living : 주거 공간의 평방 피트(면적)\nsqft_lot : 부지의 평방 피트(면적)\nfloors : 집의 층 수\nwaterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\nview : 집이 얼마나 좋아 보이는지의 정도\ncondition : 집의 전반적인 상태\ngrade : King County grading 시스템 기준으로 매긴 집의 등급\nsqft_above : 지하실을 제외한 평방 피트(면적)\nsqft_basement : 지하실의 평방 피트(면적)\nyr_built : 지어진 년도\nyr_renovated : 집을 재건축한 년도\nzipcode : 우편번호\nlat : 위도\nlong : 경도\nsqft_living15 : 2015년 기준 주거 공간의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\nsqft_lot15 : 2015년 기준 부지의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#패키지-및-데이터-불러오기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#패키지-및-데이터-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "패키지 및 데이터 불러오기",
    "text": "패키지 및 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data_path = './data/train.csv'\ntest_data_path = './data/test.csv'\n\ndata = pd.read_csv(train_data_path)\ntest = pd.read_csv(test_data_path)\nprint('train data : {}'.format(data.shape))\nprint('test data : {}'.format(test.shape))\n\ntrain data : (15035, 21)\ntest data : (6555, 20)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#데이터-전처리",
    "href": "posts/coach-ml-kaggle-20230506/index.html#데이터-전처리",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\n정답컬럼 분리\n\ntest데이터와 달리 train data에는 컬럼이 1개 더 있음 (정답컬럼인 price)\n별도의 정답 데이터(y)로 분리\n\n\nprint('컬럼 분리 전')\nprint(data.columns)\nprint(test.columns)\n\n컬럼 분리 전\nIndex(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\n\n\n\n# y라는 변수에 price(정답)을 옮기고, 전체데이터를 백업(data_backup에 할당)하고 price컬럼 삭제\ny = data['price'] \ndata_backup = data.copy()\ndata.drop('price',axis=1, inplace=True)\n\n\nprint('컬럼 분리 후')\nprint(data.columns)\nprint(test.columns)\nprint(y.name)\n\n컬럼 분리 후\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nprice\n\n\n\n\n결측치 확인 및 제거\n\ntrain, test 데이터를 합쳐서 한번에 확인\n\n\n# 합치기\ndf_chk_missing = pd.concat((data, test), axis=0)\n\n# 향후 분할을 대비한 행 수 저장\ntrain_length = len(data)\ntest_length = len(test)\n\nprint(train_length, test_length)\n\n15035 6555\n\n\n\n결측치 확인방법1(pandas)\n\nisna()로 결측치를 확인\n\n\nprint(df_chk_missing.isna().sum())\n\nid               0\ndate             0\nbedrooms         0\nbathrooms        0\nsqft_living      0\nsqft_lot         0\nfloors           0\nwaterfront       0\nview             0\ncondition        0\ngrade            0\nsqft_above       0\nsqft_basement    0\nyr_built         0\nyr_renovated     0\nzipcode          0\nlat              0\nlong             0\nsqft_living15    0\nsqft_lot15       0\ndtype: int64\n\n\n\n\n결측치 확인방법2(missingno)\n\nmissingno 패키지로 컬럼별 결측치 시각화\n\n\nimport missingno\n\nmissingno.matrix(df_chk_missing)\n\n\n\n\n\n\n\n\n\n\n결측치 확인방법3(ydata_profiling)\n\nydata_profiling 패키지로 결측치 및 다양한 값 확인 가능\n렌더링 용량 문제로 실행결과는 이미지로 대체(RangeError: Maximum call stack size exceeded)\n\n\nfrom ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df_chk_missing, title=\"Profiling Report\")\nprofile\n\n\n\n\n실행결과 샘플\n\n\n\n결측치가 없으므로 다음 과정을 진행\n\n\n\n\n불필요한 변수 제거, 데이터 변환 등\n\n단순식별용 데이터 삭제\n\n가격과 관계없는 단순식별용 데이터인 id 삭제\n\n\nmain_id = df_chk_missing['id'][:train_length]\ntest_id = df_chk_missing['id'][train_length:]\ndel df_chk_missing['id']\n\n\n\n불필요한 데이터 삭제\n\n날짜 뒤에 T00000과 같이 시간데이터(로 추정됨)가 있는데, 모두 T00000으로만 되어있으므로 삭제\n\n\n# T000000으로 되어있는 값 세기\ndf_chk_missing['date'].str.contains('T000000').value_counts()\n\ndate\nTrue    21590\nName: count, dtype: int64\n\n\n\n# apply로 lambda함수를 사용하여, date컬럼의 앞자리만 저장\ndf_chk_missing['date'] = df_chk_missing['date'].apply(lambda x : str(x[:6]))\ndf_chk_missing.head()\n\n\n\n\n\n\n\n\n\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n0\n201410\n3\n1.00\n1180\n5650\n1.0\n0\n0\n3\n7\n1180\n0\n1955\n0\n98178\n47.5112\n-122.257\n1340\n5650\n\n\n1\n201502\n2\n1.00\n770\n10000\n1.0\n0\n0\n3\n6\n770\n0\n1933\n0\n98028\n47.7379\n-122.233\n2720\n8062\n\n\n2\n201502\n3\n2.00\n1680\n8080\n1.0\n0\n0\n3\n8\n1680\n0\n1987\n0\n98074\n47.6168\n-122.045\n1800\n7503\n\n\n3\n201406\n3\n2.25\n1715\n6819\n2.0\n0\n0\n3\n7\n1715\n0\n1995\n0\n98003\n47.3097\n-122.327\n2238\n6819\n\n\n4\n201501\n3\n1.50\n1060\n9711\n1.0\n0\n0\n3\n7\n1060\n0\n1963\n0\n98198\n47.4095\n-122.315\n1650\n9711\n\n\n\n\n\n\n\n\n\n\n로그변환\n\n치우친 분포를 정규분포에 가깝게 만들기\n\n\n분포가 치우쳐져 있는 항목 찾기(시각화)\n\nrow_plot = 5\ncol_plot = 4\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(24, 35)) \n\ncolumns = df_chk_missing.columns\ncolumns_idx = 1 # 첫 컬럼인 date(날짜)는 제외하기 위해 0이 아닌 1부터 시작\nfor row in range(row_plot):\n    for col in range(col_plot):\n        sns.kdeplot(data=df_chk_missing[columns[columns_idx]], ax=ax[row][col])\n        ax[row][col].set_title(columns[columns_idx])\n        columns_idx += 1\n        if columns_idx == len(columns) :\n            break\n\n\n\n\n\n\n\n\n\n아래의 항목들이 치우쳐져 있음\n\nsqft_living\nsqft_lot\nwaterfront (→유/무 지표로 0,1만 있는게 정상이므로 제외)\nsqft_above\nsqft_basement\nsqft_living15\nsqft_lot15\n\n\n\n# 변환대상 리스트에 저장\nskewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n\n# 그래프로 그리기 (변환 전/후 그래프를 함께 그릴 예정이므로 plot의 수는 두배)\nrow_plot = 6\ncol_plot = 2\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(15, 35)) \n\ncolumns = skewed_columns\ncolumns_idx = 0\n\n\nfor row in range(row_plot):\n    # 로그변환 대상만 식별 후 진행\n    if columns[row] in skewed_columns:\n        # 기존 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][0])\n        ax[row][0].set_title(columns[row])\n\n        # 로그변환\n        df_chk_missing[columns[row]] = np.log1p(df_chk_missing[columns[row]])\n\n        # 변환된 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][1])\n        ax[row][1].set_title(columns[row]+'_log')\n\n\n\n\n\n\n\n\n\n\n\ntrain, test 데이터로 정리\n\npreprocessed_train = df_chk_missing[:train_length].copy()\npreprocessed_test = df_chk_missing[train_length:].copy()\nprice_train = y.copy()\n\n# date(날짜)의 타입을 int로 변경 (변경하지 않는 경우 object타입으로 인한 오류 발생)\npreprocessed_train['date'] = preprocessed_train['date'].astype(int)\npreprocessed_test['date'] = preprocessed_test['date'].astype(int)\n\nprint(preprocessed_train.shape)\nprint(preprocessed_test.shape)\n\n(15035, 19)\n(6555, 19)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#scikit-learn-등-관련-패키지-불러오기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#scikit-learn-등-관련-패키지-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "Scikit-learn 등 관련 패키지 불러오기",
    "text": "Scikit-learn 등 관련 패키지 불러오기\n\n본래 사용하는 패키지는 모두 최상단에서 불러오는게 맞음!\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n모델 불러오고 Cross Validation으로 모델성능 측청\n\ngboost = GradientBoostingRegressor(random_state=1210)\nxgboost = xgb.XGBRegressor(random_state=1210)\nlightgbm = lgb.LGBMRegressor(random_state=1210)\n\nmodel_dict = {'GradientBoosting':gboost,\n              'XGBoost':xgboost,\n              'LigntGBM':lightgbm}\n\n# LightGBM의 메시지가 나오지 않도록 별도로 저장 후 출력\nmodel_cv_score = dict()\nfor model in model_dict.keys():\n    model_cv_score[model] = np.mean(cross_val_score(model_dict[model], X=preprocessed_train, y=price_train))\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001070 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2296\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 540497.991270\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2327\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542956.681826\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2331\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 543149.529265\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2332\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542032.619305\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2298\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 534776.444047\n\n\n\nfor model in model_dict.keys():\n    print(f'{model} : {model_cv_score[model]}')\n\nGradientBoosting : 0.8613647608814923\nXGBoost : 0.8762617283884332\nLigntGBM : 0.8818569800403846\n\n\n\n\n모델학습 및 예측\n\nScore가 가장 높았던 lightGBM으로 진행해보기\n\n\nmodel_dict['LigntGBM'].fit(preprocessed_train.values, y)\nprediction = model_dict['LigntGBM'].predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n\n\narray([1296687.09405506,  311847.90404507,  806735.28228208, ...,\n       1726006.82963994,  395020.94053356,  333594.29000994])\n\n\n\n\n제출용 DataFrame 및 csv파일 생성\n\ndf_submission = pd.DataFrame({'id' : test_id, \n                              'price' : prediction})\ndf_submission\n\n\n\n\n\n\n\n\n\nid\nprice\n\n\n\n\n0\n15208\n1.296687e+06\n\n\n1\n15209\n3.118479e+05\n\n\n2\n15210\n8.067353e+05\n\n\n3\n15211\n2.098083e+05\n\n\n4\n15212\n4.343237e+05\n\n\n...\n...\n...\n\n\n6550\n21758\n4.230647e+05\n\n\n6551\n21759\n5.111171e+05\n\n\n6552\n21760\n1.726007e+06\n\n\n6553\n21761\n3.950209e+05\n\n\n6554\n21762\n3.335943e+05\n\n\n\n\n6555 rows × 2 columns\n\n\n\n\n\ndf_submission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "",
    "text": "Local L/C 업무에 대해 아래의 문제점을 인식함\n\n정보와 기능이 각기 다른 ERP메뉴에 산재되어 비효율/불편함 발생\n\n주요기능 : L/C수신, L/C등록, 세금계산서 조회, 물품수령증 조회, 은행 네고(제출)\n\n전자화된 정보임에도 각 서류간에 동일한 항목이 일치되게 기입되어있는지 눈으로 확인중\n\n예를 들어, 4개의 서류에 24자리의 세금계산서번호가 모두 똑같게 들어있는지 확인 필요\n이러한 공통정보 중 하나라도 틀리면 물품의 대금의 전부를 지급받지 못하므로 중요함\n\n각 서류와 행위에 대한 법 조항이 있어 준수해야하나, 모두 인지하고있기 어려움\n\n예를 들어, A서류가 발행되면 몇일 이내에 B서류를 제출해야 함\n\n\n문제점을 해결하기 위해 아래의 방안을 도출함\n\nDB활용\n\n각 항목을 테이블로 Primary, 공통키를 지정하여 하나의 DB로 통합\nDB의 날짜조건과 서류존재여부를 활용해 준수사항에 대해 유저에게 지시\nDB의 정보를 대조하여 유저에게 결과를 공유\n대시보드용으로 많이 사용하는 streamlit으로 UI제공\n\n업무자동화\n\nDB내용을 기반으로 유저가 직접하던 등록이나 은행제출 자동화기능 수행\n\n\n\n\n[용어설명] L/C : 물품주문서이자 대금청구시 사용될 은행계좌와 비슷한 역할 (Letter of Credit, 신용장) 물품수령증 : 고객이 물건을 정상수령하였다는 서류, 대금청구에 사용할 수 있다 은행네고 : 정상수령했다는 서류 등을 제출하여, 은행에 준비되어있는 물품대금을 받을 수 있다\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#개요",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#개요",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "",
    "text": "추진배경\n\n\n모든 정보가 하나의 ERP에 있음에도, 기능 별로 메뉴가 구분되어 있어 비효율/불편함 ↑\n\nL/C수신, L/C등록, 세금계산서 조회, 물품수령증 조회, 은행네고\n\n모든 정보가 전자화되어있으나, 각 서류의 내용(주문번호 등)이 일치하는지 눈으로 확인중\n\n24자리 영문/숫자 혼합 등이 있으며, 틀리면 물품대금을 받지 못하므로 확인이 매우 중요함\n\n[용어설명]\n**L/C** : **물품주문서**이자 **대금청구시 사용될 은행계좌**와 비슷한 역할\n    (Letter of Credit, 신용장)\n**물품수령증** : 고객이 물건을 정상수령하였다는 서류, 대금청구에 사용할 수 있다\n**은행네고** : 정상수령했다는 서류 등을 제출하여, 은행에서 물품대금을 받을 수 있다\n\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting, win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드 : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#개념",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#개념",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "개념",
    "text": "개념\nRMSE(Root Mean Squeare Error)\nRoot    (4)\nMean    (3)\nSquare  (2)\nError   (1)\n(1) 실제값에서 예측값을 뺀 '오차'를\n(2) 합했을 때 음수의 영향을 제거하기 위해 '제곱'하고\n(3) '평균'오차로 만든 후\n(4) '루트'를 씌워 값의 크기를 작게 한다 (값을 작게하여 연산속도에 이점이 있다)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#파일-다운로드-및-알아보기",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#파일-다운로드-및-알아보기",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "파일 다운로드 및 알아보기",
    "text": "파일 다운로드 및 알아보기\nFile descriptions\ntrain.csv - 예측 모델을 만들기 위해 사용하는 학습 데이터입니다. 집의 정보와 예측할 변수인 가격(Price) 변수를 가지고 있습니다.\ntest.csv - 학습셋으로 만든 모델을 가지고 예측할 가격(Price) 변수를 제외한 집의 정보가 담긴 테스트 데이터 입니다.\nsample_submission.csv - 제출시 사용할 수 있는 예시 submission.csv 파일입니다.\nData fields\nID : 집을 구분하는 번호\ndate : 집을 구매한 날짜\nprice : 집의 가격(Target variable)\nbedrooms : 침실의 수\nbathrooms : 화장실의 수\nsqft_living : 주거 공간의 평방 피트(면적)\nsqft_lot : 부지의 평방 피트(면적)\nfloors : 집의 층 수\nwaterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\nview : 집이 얼마나 좋아 보이는지의 정도\ncondition : 집의 전반적인 상태\ngrade : King County grading 시스템 기준으로 매긴 집의 등급\nsqft_above : 지하실을 제외한 평방 피트(면적)\nsqft_basement : 지하실의 평방 피트(면적)\nyr_built : 지어진 년도\nyr_renovated : 집을 재건축한 년도\nzipcode : 우편번호\nlat : 위도\nlong : 경도\nsqft_living15 : 2015년 기준 주거 공간의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\nsqft_lot15 : 2015년 기준 부지의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#패키지-및-데이터-불러오기",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#패키지-및-데이터-불러오기",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "패키지 및 데이터 불러오기",
    "text": "패키지 및 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data_path = './data/train.csv'\ntest_data_path = './data/test.csv'\n\ndata = pd.read_csv(train_data_path)\ntest = pd.read_csv(test_data_path)\nprint('train data : {}'.format(data.shape))\nprint('test data : {}'.format(test.shape))\n\ntrain data : (15035, 21)\ntest data : (6555, 20)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#데이터-전처리",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#데이터-전처리",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\n정답컬럼 분리\n\ntest데이터와 달리 train data에는 컬럼이 1개 더 있음 (정답컬럼인 price)\n별도의 정답 데이터(y)로 분리\n\n\nprint('컬럼 분리 전')\nprint(data.columns)\nprint(test.columns)\n\n컬럼 분리 전\nIndex(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\n\n\n\n# y라는 변수에 price(정답)을 옮기고, 전체데이터를 백업(data_backup에 할당)하고 price컬럼 삭제\ny = data['price'] \ndata_backup = data.copy()\ndata.drop('price',axis=1, inplace=True)\n\n\nprint('컬럼 분리 후')\nprint(data.columns)\nprint(test.columns)\nprint(y.name)\n\n컬럼 분리 후\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nprice\n\n\n\n\n결측치 확인 및 제거\n\ntrain, test 데이터를 합쳐서 한번에 확인\n\n\n# 합치기\ndf_chk_missing = pd.concat((data, test), axis=0)\n\n# 향후 분할을 대비한 행 수 저장\ntrain_length = len(data)\ntest_length = len(test)\n\nprint(train_length, test_length)\n\n15035 6555\n\n\n\n결측치 확인방법1(pandas)\n\nisna()로 결측치를 확인\n\n\nprint(df_chk_missing.isna().sum())\n\nid               0\ndate             0\nbedrooms         0\nbathrooms        0\nsqft_living      0\nsqft_lot         0\nfloors           0\nwaterfront       0\nview             0\ncondition        0\ngrade            0\nsqft_above       0\nsqft_basement    0\nyr_built         0\nyr_renovated     0\nzipcode          0\nlat              0\nlong             0\nsqft_living15    0\nsqft_lot15       0\ndtype: int64\n\n\n\n\n결측치 확인방법2(missingno)\n\nmissingno 패키지로 컬럼별 결측치 시각화\n\n\nimport missingno\n\nmissingno.matrix(df_chk_missing)\n\n\n\n\n\n\n\n\n\n\n결측치 확인방법3(ydata_profiling)\n\nydata_profiling 패키지로 결측치 및 다양한 값 확인 가능\n렌더링 용량 문제로 실행결과는 이미지로 대체(RangeError: Maximum call stack size exceeded)\n\n\nfrom ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df_chk_missing, title=\"Profiling Report\")\nprofile\n\n\n\n\n실행결과 샘플\n\n\n\n결측치가 없으므로 다음 과정을 진행\n\n\n\n\n불필요한 변수 제거, 데이터 변환 등\n\n단순식별용 데이터 삭제\n\n가격과 관계없는 단순식별용 데이터인 id 삭제\n\n\nmain_id = df_chk_missing['id'][:train_length]\ntest_id = df_chk_missing['id'][train_length:]\ndel df_chk_missing['id']\n\n\n\n불필요한 데이터 삭제\n\n날짜 뒤에 T00000과 같이 시간데이터(로 추정됨)가 있는데, 모두 T00000으로만 되어있으므로 삭제\n\n\n# T000000으로 되어있는 값 세기\ndf_chk_missing['date'].str.contains('T000000').value_counts()\n\ndate\nTrue    21590\nName: count, dtype: int64\n\n\n\n# apply로 lambda함수를 사용하여, date컬럼의 앞자리만 저장\ndf_chk_missing['date'] = df_chk_missing['date'].apply(lambda x : str(x[:6]))\ndf_chk_missing.head()\n\n\n\n\n\n\n\n\n\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n0\n201410\n3\n1.00\n1180\n5650\n1.0\n0\n0\n3\n7\n1180\n0\n1955\n0\n98178\n47.5112\n-122.257\n1340\n5650\n\n\n1\n201502\n2\n1.00\n770\n10000\n1.0\n0\n0\n3\n6\n770\n0\n1933\n0\n98028\n47.7379\n-122.233\n2720\n8062\n\n\n2\n201502\n3\n2.00\n1680\n8080\n1.0\n0\n0\n3\n8\n1680\n0\n1987\n0\n98074\n47.6168\n-122.045\n1800\n7503\n\n\n3\n201406\n3\n2.25\n1715\n6819\n2.0\n0\n0\n3\n7\n1715\n0\n1995\n0\n98003\n47.3097\n-122.327\n2238\n6819\n\n\n4\n201501\n3\n1.50\n1060\n9711\n1.0\n0\n0\n3\n7\n1060\n0\n1963\n0\n98198\n47.4095\n-122.315\n1650\n9711\n\n\n\n\n\n\n\n\n\n\n로그변환\n\n치우친 분포를 정규분포에 가깝게 만들기\n\n\n분포가 치우쳐져 있는 항목 찾기(시각화)\n\nrow_plot = 5\ncol_plot = 4\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(24, 35)) \n\ncolumns = df_chk_missing.columns\ncolumns_idx = 1 # 첫 컬럼인 date(날짜)는 제외하기 위해 0이 아닌 1부터 시작\nfor row in range(row_plot):\n    for col in range(col_plot):\n        sns.kdeplot(data=df_chk_missing[columns[columns_idx]], ax=ax[row][col])\n        ax[row][col].set_title(columns[columns_idx])\n        columns_idx += 1\n        if columns_idx == len(columns) :\n            break\n\n\n\n\n\n\n\n\n\n아래의 항목들이 치우쳐져 있음\n\nsqft_living\nsqft_lot\nwaterfront (→유/무 지표로 0,1만 있는게 정상이므로 제외)\nsqft_above\nsqft_basement\nsqft_living15\nsqft_lot15\n\n\n\n# 변환대상 리스트에 저장\nskewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n\n# 그래프로 그리기 (변환 전/후 그래프를 함께 그릴 예정이므로 plot의 수는 두배)\nrow_plot = 6\ncol_plot = 2\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(15, 35)) \n\ncolumns = skewed_columns\ncolumns_idx = 0\n\n\nfor row in range(row_plot):\n    # 로그변환 대상만 식별 후 진행\n    if columns[row] in skewed_columns:\n        # 기존 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][0])\n        ax[row][0].set_title(columns[row])\n\n        # 로그변환\n        df_chk_missing[columns[row]] = np.log1p(df_chk_missing[columns[row]])\n\n        # 변환된 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][1])\n        ax[row][1].set_title(columns[row]+'_log')\n\n\n\n\n\n\n\n\n\n\n\ntrain, test 데이터로 정리\n\npreprocessed_train = df_chk_missing[:train_length].copy()\npreprocessed_test = df_chk_missing[train_length:].copy()\nprice_train = y.copy()\n\n# date(날짜)의 타입을 int로 변경 (변경하지 않는 경우 object타입으로 인한 오류 발생)\npreprocessed_train['date'] = preprocessed_train['date'].astype(int)\npreprocessed_test['date'] = preprocessed_test['date'].astype(int)\n\nprint(preprocessed_train.shape)\nprint(preprocessed_test.shape)\n\n(15035, 19)\n(6555, 19)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#scikit-learn-등-관련-패키지-불러오기",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#scikit-learn-등-관련-패키지-불러오기",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "Scikit-learn 등 관련 패키지 불러오기",
    "text": "Scikit-learn 등 관련 패키지 불러오기\n\n본래 사용하는 패키지는 모두 최상단에서 불러오는게 맞음!\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n모델 불러오고 Cross Validation으로 모델성능 측청\n\ngboost = GradientBoostingRegressor(random_state=1210)\nxgboost = xgb.XGBRegressor(random_state=1210)\nlightgbm = lgb.LGBMRegressor(random_state=1210)\n\nmodel_dict = {'GradientBoosting':gboost,\n              'XGBoost':xgboost,\n              'LigntGBM':lightgbm}\n\n# LightGBM의 메시지가 나오지 않도록 별도로 저장 후 출력\nmodel_cv_score = dict()\nfor model in model_dict.keys():\n    model_cv_score[model] = np.mean(cross_val_score(model_dict[model], X=preprocessed_train, y=price_train))\n\n\nfor model in model_dict.keys():\n    print(f'{model} : {model_cv_score[model]}')\n\nGradientBoosting : 0.8613647608814923\nXGBoost : 0.8762617283884332\nLigntGBM : 0.8818569800403846\n\n\n\n\n모델학습 및 예측\n\nScore가 가장 높았던 lightGBM으로 진행해보기\n\n\nmodel_dict['LigntGBM'].fit(preprocessed_train.values, y)\nprediction = model_dict['LigntGBM'].predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000583 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n\n\narray([1296687.09405506,  311847.90404507,  806735.28228208, ...,\n       1726006.82963994,  395020.94053356,  333594.29000994])\n\n\n\n\n제출용 DataFrame 및 csv파일 생성\n\ndf_submission = pd.DataFrame({'id' : test_id, \n                              'price' : prediction})\ndf_submission\n\n\n\n\n\n\n\n\n\nid\nprice\n\n\n\n\n0\n15208\n1.296687e+06\n\n\n1\n15209\n3.118479e+05\n\n\n2\n15210\n8.067353e+05\n\n\n3\n15211\n2.098083e+05\n\n\n4\n15212\n4.343237e+05\n\n\n...\n...\n...\n\n\n6550\n21758\n4.230647e+05\n\n\n6551\n21759\n5.111171e+05\n\n\n6552\n21760\n1.726007e+06\n\n\n6553\n21761\n3.950209e+05\n\n\n6554\n21762\n3.335943e+05\n\n\n\n\n6555 rows × 2 columns\n\n\n\n\n\ndf_submission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#github-repository",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "",
    "text": "각 주문서에 나뉘어있는 사용빈도가 높은 정보가 여러 탭에 나뉘어 있음\n\n탭 내에서도 많은 정보를 보여주기 위해 좁은 프레임(4행만 보임)에 많은 정보를 넣어 복사 등을 하기엔 불편함\n4행만 보이는 물품내역 프레임에 50여건의 물품이 있는 경우 많은 시간 소요\n\n크게 조회가능한 모드로 보는 경우에는 제품/모델명/HSCODE의 형식으로 문단형식으로 혼재되어있어 중복제거 등 가공 필수\n\n출발/도착지/품명 등을 전체 주문에 대해 확인하고자 하는 경우 건별로 메뉴진입 필요 (일괄로 조회하는 메뉴는 일부 정보 제외되어있음)\n\n아래의 방안으로 해결하고자 함\n\n조회속도가 빠르므로 필요한 정보를 필요할때마다 일괄 크롤링하도록 설계\n유저의 복사/가공/중복제거의 작업이 패턴화되어있어 미리 진행하여 결과물만 제공\n\n대상정보 : Sales Org, Plant(Code,Name), POL(출발지), POD(도착지), HSCODE, Description(물품명세)\n\n\n\n\n[용어설명] SR : 하나의 기본 선적 단위, Shipping request HSCODE : 해외로 물건을 보내기 위해 수출신고할 때, (의약품, 전자기기 등)물품 종류를 알 수 있는 제품 코드\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#개요",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#개요",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "",
    "text": "필요한 정보가 여러 탭에 산재되어있거나, 줄 글로 뭉쳐져있어 별도의 가공/복사과정없이 주요정보를 바로 사용하게 해주는 Tool\n확인가능한 정보 : Sales Org, Plant(Code,Name), POL, POD, HSCODE, Description"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#github-repository",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#사용법",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#사용법",
    "title": "[Python] Shipping request내 주요정보 크롤링 & 정리용 파일",
    "section": "사용법",
    "text": "사용법\n\n1건 확인 * 두번째 박스 sr_no 변수에 SR번호 1개를 입력하면 주요 정보 확인\n여러 건 확인 * 세번째 박스 sr_no 변수의 ’‘’와’’’사이에 모든 SR번호 입력 (, 즉 엔터를 기준으로 구분한다) (엑셀 등에서 표 형태로 붙여넣는 경우가 많기때문에 사용성을 고려한 옵션)\n엑셀로 저장 * 확인한 내용을 엑셀로 저장하고 싶은 경우, 네번째 박스 실행, 지정된 파일명+오늘날짜를 기준으로 다운로드 함 (몇번이나 쓸까 싶긴 하지만, 언제나 의외의 상황때문에 필요해질 수 있으니 미리 구현해두는 기능)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#사용법-및-설계의도",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#사용법-및-설계의도",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "사용법 및 설계의도",
    "text": "사용법 및 설계의도\n\n1건 확인 * 두번째 박스 sr_no 변수에 SR번호 1개를 입력하면 주요 정보 확인\n여러 건 확인 * 세번째 박스 sr_no 변수의 ’‘’와’’’사이에 모든 SR번호 입력 (, 즉 엔터를 기준으로 구분한다) (엑셀 등에서 표 형태로 붙여넣는 경우가 많기때문에 사용성을 고려한 옵션)\n엑셀로 저장 * 확인한 내용을 엑셀로 저장하고 싶은 경우, 네번째 박스 실행, 지정된 파일명+오늘날짜를 기준으로 다운로드 함 (몇번이나 쓸까 싶긴 하지만, 언제나 의외의 상황때문에 필요해질 수 있으니 미리 구현해두는 기능)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#추진배경",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "",
    "text": "Local L/C 업무에 대해 아래의 문제점을 인식함\n\n정보와 기능이 각기 다른 ERP메뉴에 산재되어 비효율/불편함 발생\n\n주요기능 : L/C수신, L/C등록, 세금계산서 조회, 물품수령증 조회, 은행 네고(제출)\n\n전자화된 정보임에도 각 서류간에 동일한 항목이 일치되게 기입되어있는지 눈으로 확인중\n\n예를 들어, 4개의 서류에 24자리의 세금계산서번호가 모두 똑같게 들어있는지 확인 필요\n이러한 공통정보 중 하나라도 틀리면 물품의 대금의 전부를 지급받지 못하므로 중요함\n\n각 서류와 행위에 대한 법 조항이 있어 준수해야하나, 모두 인지하고있기 어려움\n\n예를 들어, A서류가 발행되면 몇일 이내에 B서류를 제출해야 함\n\n\n문제점을 해결하기 위해 아래의 방안을 도출함\n\nDB활용\n\n각 항목을 테이블로 Primary, 공통키를 지정하여 하나의 DB로 통합\nDB의 날짜조건과 서류존재여부를 활용해 준수사항에 대해 유저에게 지시\nDB의 정보를 대조하여 유저에게 결과를 공유\n대시보드용으로 많이 사용하는 streamlit으로 UI제공\n\n업무자동화\n\nDB내용을 기반으로 유저가 직접하던 등록이나 은행제출 자동화기능 수행\n\n\n\n\n[용어설명] L/C : 물품주문서이자 대금청구시 사용될 은행계좌와 비슷한 역할 (Letter of Credit, 신용장) 물품수령증 : 고객이 물건을 정상수령하였다는 서류, 대금청구에 사용할 수 있다 은행네고 : 정상수령했다는 서류 등을 제출하여, 은행에 준비되어있는 물품대금을 받을 수 있다"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#구현-목적내용-및-사용한-언어패키지",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#구현-목적내용-및-사용한-언어패키지",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "구현 목적/내용 및 사용한 언어/패키지",
    "text": "구현 목적/내용 및 사용한 언어/패키지\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting, win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드 : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#구현-목적내용-사용한-언어패키지",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#구현-목적내용-사용한-언어패키지",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "구현 목적/내용 & 사용한 언어/패키지",
    "text": "구현 목적/내용 & 사용한 언어/패키지\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting, win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드 : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#구현-내용-사용한-언어패키지",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#구현-내용-사용한-언어패키지",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "구현 내용 & 사용한 언어/패키지",
    "text": "구현 내용 & 사용한 언어/패키지\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting활용을 위한 win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup, sqlite3 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드(참고용 기록) : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#효과",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#효과",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "효과",
    "text": "효과\n\nDB화 및 정보대조를 통한 Human error제거 및 물품대금 전부에 대한 정상 입수\n법적 준수사항에 대한 미준수로 인한 여러 Risk제거\n유저가 기능별 메뉴를 이동할 필요없이 하나의 통합UI에서 업무를 해결\n\n편의기능 추가 : 조치대상 내역을 클립보드로 일괄복사해주는 기능 등"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#구현내용-사용한-언어패키지-등-세부내용",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#구현내용-사용한-언어패키지-등-세부내용",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "구현내용 & 사용한 언어/패키지 등 세부내용",
    "text": "구현내용 & 사용한 언어/패키지 등 세부내용\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting활용을 위한 win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup, sqlite3 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드(참고용 기록) : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting활용을 위한 win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup, sqlite3 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드(참고용 기록) : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#추진배경",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "",
    "text": "각 주문서에 나뉘어있는 사용빈도가 높은 정보가 여러 탭에 나뉘어 있음\n\n탭 내에서도 많은 정보를 보여주기 위해 좁은 프레임(4행만 보임)에 많은 정보를 넣어 복사 등을 하기엔 불편함\n4행만 보이는 물품내역 프레임에 50여건의 물품이 있는 경우 많은 시간 소요\n\n크게 조회가능한 모드로 보는 경우에는 제품/모델명/HSCODE의 형식으로 문단형식으로 혼재되어있어 중복제거 등 가공 필수\n\n출발/도착지/품명 등을 전체 주문에 대해 확인하고자 하는 경우 건별로 메뉴진입 필요 (일괄로 조회하는 메뉴는 일부 정보 제외되어있음)\n\n아래의 방안으로 해결하고자 함\n\n조회속도가 빠르므로 필요한 정보를 필요할때마다 일괄 크롤링하도록 설계\n유저의 복사/가공/중복제거의 작업이 패턴화되어있어 미리 진행하여 결과물만 제공\n\n대상정보 : Sales Org, Plant(Code,Name), POL(출발지), POD(도착지), HSCODE, Description(물품명세)\n\n\n\n\n[용어설명] SR : 하나의 기본 선적 단위, Shipping request HSCODE : 해외로 물건을 보내기 위해 수출신고할 때, (의약품, 전자기기 등)물품 종류를 알 수 있는 제품 코드"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#효과",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#효과",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "효과",
    "text": "효과\n\n단건 또는 여러건의 주문(SR)에 대해 건당 1~2초 이내로 필요한 정보 수집\n클립보드 복사가 가능한 텍스트, 엑셀형태로 제공하여 요구사항에 대해 즉시대응 가능"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n단건 확인시, 코드셀에 붙여넣기 후 실행, 텍스트로 출력하며 pandas dataframe으로도 저장하여 필요시 엑셀도 제공\n여러건 확인시, 엑셀 등에서 복사한 표를 코드셀에 바로 붙여넣도록 설계(자동 분할, 편의성 고려함) 이후 작업은 단건 확인과 동일\n필요시 엑셀로 저장 (기존 업무유형상 출력텍스트가 더 많이 활용될 것으로 보여 별도 기능으로 추가함)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html",
    "title": "[Python] COO발급관리용 Tool",
    "section": "",
    "text": "부서원 모두의 공통 업무이며, 여러가지 불편사항이 존재하나 자동화방안에 대한 기존 고민X\n\n필요에 의해 ID가 3개로 나뉘어있으며, 공동인증서 활용으로 타이핑작업 및 ID/PW/공동인증서PW의 관리 및 입력 불편함\n여러 건의 COO를 발급하고 대응하지만, 각 업무는 COO 1건별로 메뉴에 진입하여 수행해야함\n\n심사완료여부, 발급거절시 사유확인, 출력 및 사본 저장 등\n\n\n발급실적 담당자는 월마다 부서 전체의 발급실적을 관리하기위해 별도의 작업을 수행\n\n3개의 ID에 접속하여 20여개 페이지의 표를 복사하고, 중복/심사거절건 제거 등 수작업 가공 진행\n\n위 2가지 문제를 해결할 방법에 대한 고민 및 해결방안 도출 : 데이터의 통합db화 및 편의기능 추가\n\n공용PC를 운영중인 부서로 주기적으로 활동하는 크롤러를 운영하여 db로 저장가능\nstreamlit을 활용해 db에 대한 검색과 확인 가능한 대시보드형 사이트 제작하여 일반유저도 손쉽게 사용\n희망시 심사번호를 streamlit사이트에 등록하여 대응상황 발생시 toast알림이 가도록 제작\n축적된 db에서 월 데이터를 추출할 수 있도록 버튼을 누르면 SQL쿼리 후 Excel저장기능 추가\n3개의 ID에 대한 접속버튼으로 자동로그인 기능 추가\n\nstreamlit사이트의 정보관리메뉴에서 접속ID/PW등을 수정하면 공용PC의 json파일을 수정하여 모든유저에게 반영\n\n\n\n\n[용어설명] COO : 원산지증명서, Country of Origin\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#추진배경",
    "title": "[Python] COO발급관리용 Tool",
    "section": "",
    "text": "부서원 모두의 공통 업무이며, 여러가지 불편사항이 존재하나 자동화방안에 대한 기존 고민X\n\n필요에 의해 ID가 3개로 나뉘어있으며, 공동인증서 활용으로 타이핑작업 및 ID/PW/공동인증서PW의 관리 및 입력 불편함\n여러 건의 COO를 발급하고 대응하지만, 각 업무는 COO 1건별로 메뉴에 진입하여 수행해야함\n\n심사완료여부, 발급거절시 사유확인, 출력 및 사본 저장 등\n\n\n발급실적 담당자는 월마다 부서 전체의 발급실적을 관리하기위해 별도의 작업을 수행\n\n3개의 ID에 접속하여 20여개 페이지의 표를 복사하고, 중복/심사거절건 제거 등 수작업 가공 진행\n\n위 2가지 문제를 해결할 방법에 대한 고민 및 해결방안 도출 : 데이터의 통합db화 및 편의기능 추가\n\n공용PC를 운영중인 부서로 주기적으로 활동하는 크롤러를 운영하여 db로 저장가능\nstreamlit을 활용해 db에 대한 검색과 확인 가능한 대시보드형 사이트 제작하여 일반유저도 손쉽게 사용\n희망시 심사번호를 streamlit사이트에 등록하여 대응상황 발생시 toast알림이 가도록 제작\n축적된 db에서 월 데이터를 추출할 수 있도록 버튼을 누르면 SQL쿼리 후 Excel저장기능 추가\n3개의 ID에 대한 접속버튼으로 자동로그인 기능 추가\n\nstreamlit사이트의 정보관리메뉴에서 접속ID/PW등을 수정하면 공용PC의 json파일을 수정하여 모든유저에게 반영\n\n\n\n\n[용어설명] COO : 원산지증명서, Country of Origin"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#효과",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#효과",
    "title": "[Python] COO발급관리용 Tool",
    "section": "효과",
    "text": "효과\n\n크롤러, db구축, 관리 및 편의기능을 포함한 대시보드형 사이트 구축으로 기존의 불편사항들을 해결\n\n자동로그인, 대응사항 toast알림으로 지속적인 새로고침 등 불필요한 작업제거\n월마다 진행되던 불필요한 데이터 가공작업 제거(SQL쿼리 월 조건등은 사용자가 strealit사이트에서 수정 가능)\n\n기존 발급거절 사유의 db화로 주요 케이스에 대한 분석 및 사전대응계획 수립가능\n\n주요 케이스는 사전에 신고한 제품의 단위와 신청시스템 단위의 차이로, 관련 담당자에 개선제언 예정"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#github-repository",
    "title": "[Python] COO발급관리용 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] COO발급관리용 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n저장할 db는 sqlite3으로 테이블 생성, 컬럼지정 등을 수행함 (컬럼별 조건은 하단 참조)\n\n\n접수번호 varchar PRIMARY KEY , → 대표Invoice와 고민했는데, 100% 유일값이라 Primary로 지정 증명서종류 varchar,  대표Invoice varchar(10), 접수일시 datetime,  처리상태 varchar,  Remark varchar\n\n\n각 기능은 아래의 파일로 나누어 개인/공용PC에서 실행\n\n\nMonitoringCOO(기본파일) : streamlit활용한 UI, json/pickle파일 읽기, 유저의 자동로그인, 월추출 데이터 저장 등\n\n마지막 스크레핑 시점을 표기하여 얼마나 최신화된 데이터인지 유저에게 공유\n\nMonitoringCOO_crawler : selenium으로 스크레핑, 스크레핑작업에 필요한 로그인 기능(pyautogui, pywin32로 이미지/키/윈도우 인식)\n\n유저가 기본파일에서 로그인기능을 사용하는 경우, 이 파일에서 import해서 사용하고 코드는 여기서 통합관리\n스크레핑작업은 기본적으로 Scheduler파일에서 실행되지만, 필요시 이 파일을 실행하여 수동 스크레핑 (코드는 여기서 통합관리)\n\nMonitoringCOO_push : 기본파일에서 유저가 등록해둔 대표Invoice번호를 db에서 조회하여, win11toast로 알림 (처음에는 파이썬과 호환성/속도가 좋은 pickle/list로 관리하고자 했으나, 사용자ID등 추가정보 관리가 필요하여 json/dict로 관리)\nScheduler : 스크레핑 주기/시간을 관리하는 파일. 주로 공용PC에서 작업 (9~17시 이후엔 데이터변경이 없으므로 이 시간대에만 작동하도록 설정, 서버설정 등을 고려하여 작업주기 반영 예정)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "",
    "text": "아래의 문제점/현황에 대해 인식\n\npdf서류(PI)에 시스템 등록에 필요한 주소 등 정보가 늘 빠져있어 별도 테이블을 참고하여 등록\n드래그 가능한 pdf를 제공받아 마우스로 일일히 드래그하여 복사/붙여넣기 반복\n제공자의 내부규정 문제로 1서류:1메일로 건별 메일로 수령, 많아지면 작업량 증가\n향후 분쟁대비를 위해, 주문번호를 파일명으로 하여 별도의 공용폴더에 저장\n여러 서류를 등록하다가 교차하여 잘못넣을시, 고객의 수입절차문제나 오배송 등 발생\n\n위의 문제점/현황으로 수입문제로 인해 고객이 물품을 받지 못할수 있는 상황을 방지하고자 해결방안 마련\n\npdf서류의 regex를 활용한 추출 및 별도 테이블의 정보를 매칭하여 정확도/속도 증가\n\n정확도 상승으로 인한 고객의 수입문제, 오배송 문제 등을 미연에 방지\n추출된 정보에 대해 검증조건을 부여하여 문제있을시 작업을 멈추고 유저에게 공유\n\n한국에서 홍콩으로 수출하지만, 책임 및 보험가입구간이 한국에서 미국으로 작성된 경우 등 논리오류\n\n\nwin32로 아웃룩에서 조건부 추출하여 여러 건에도 바로 서류를 추출하도록 설계\n추출한 주문번호로 파일명 지정 및 아카이브 자동 저장\n\n\n\n[용어설명] PI : 수출납품계약서로 발주자의 양식 등을 사용함, Proforma Invoice regex : 정규표현식, 특정한 규칙을 통해 문자를 검색/편집하는데 사용\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#추진배경",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "",
    "text": "아래의 문제점/현황에 대해 인식\n\npdf서류(PI)에 시스템 등록에 필요한 주소 등 정보가 늘 빠져있어 별도 테이블을 참고하여 등록\n드래그 가능한 pdf를 제공받아 마우스로 일일히 드래그하여 복사/붙여넣기 반복\n제공자의 내부규정 문제로 1서류:1메일로 건별 메일로 수령, 많아지면 작업량 증가\n향후 분쟁대비를 위해, 주문번호를 파일명으로 하여 별도의 공용폴더에 저장\n여러 서류를 등록하다가 교차하여 잘못넣을시, 고객의 수입절차문제나 오배송 등 발생\n\n위의 문제점/현황으로 수입문제로 인해 고객이 물품을 받지 못할수 있는 상황을 방지하고자 해결방안 마련\n\npdf서류의 regex를 활용한 추출 및 별도 테이블의 정보를 매칭하여 정확도/속도 증가\n\n정확도 상승으로 인한 고객의 수입문제, 오배송 문제 등을 미연에 방지\n추출된 정보에 대해 검증조건을 부여하여 문제있을시 작업을 멈추고 유저에게 공유\n\n한국에서 홍콩으로 수출하지만, 책임 및 보험가입구간이 한국에서 미국으로 작성된 경우 등 논리오류\n\n\nwin32로 아웃룩에서 조건부 추출하여 여러 건에도 바로 서류를 추출하도록 설계\n추출한 주문번호로 파일명 지정 및 아카이브 자동 저장\n\n\n\n[용어설명] PI : 수출납품계약서로 발주자의 양식 등을 사용함, Proforma Invoice regex : 정규표현식, 특정한 규칙을 통해 문자를 검색/편집하는데 사용"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#효과",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#효과",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "효과",
    "text": "효과\n\n정확도 향상으로 고객의 수입절차문제나 오배송을 미연에 방지하여 추가비용위험 제거 및 고객만족 제고\n건별 메일열람 - 논리오류 검증 - 시스템 등록(복사/붙여넣기, 별도테이블 참고) - 파일명 변경 및 저장 등 프로세스 제거 및 유저편의성 증대\n\n백그라운드에서 실행되며, 작업이 완료되면 윈도우 toast메시지로 알려 특이사항 발생시 인지 가능"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#github-repository",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\npywin32로 아웃룩을 제어하여 조건에 맞는 pdf첨부 열람 등 진행\n\nselenium은 chrome버전변경 등 영향이 커서 구현했다가 미사용\n\nxlwings로 Excel로 저장해둔 별도 참고용 테이블을 열람\n\nDRM암호화와 관계없이 파일을 읽을 수 있기 때문에 xlwings를 채택\n\nre로 pdf의 문자열을 검색하여 필요한 내용을 저장\nNERP_PI_LC(주요 ERP관련 기능에 대해 제작한 파이썬 패키지)으로 시스템 등록 등을 진행\nwin11toast로 모든 작업이 완료되면 알림\n[삭제기능] selenium으로 PI제공자에게 자동회신도 했었으나, chrome업데이트 등 안정성 문제로 제외\n\n아웃룩 등 smtp발송은 내부규정상 막혀있어 사용하지 않음"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231101/index.html",
    "href": "posts_miniprojects/sel-py-readPIAR-20231101/index.html",
    "title": "[Python] Peak타임 대응용 수출계약서(pdf) tabula리딩",
    "section": "",
    "text": "50~120건의 서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n시차, 주문접수, 생산계획 등이 맞물려 조정이 불가능한 상황\n\n동일 양식의 내용이 다른 서류 50~120건이며, 일부 내용은 별도의 수주시스템에 시스템화되어 올려져 있음\n시스템 제약으로 글자수 제한이 있어, 주문번호를 줄이는 등 별도의 작업 수행\n정확도가 높은 수주시스템의 내용(엑셀로 저장)을 기반으로, pdf로 보완하여 자동화 추진\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#추진배경",
    "title": "[Python] Peak타임 대응용 수출계약서(pdf) tabula리딩",
    "section": "",
    "text": "50~120건의 서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n시차, 주문접수, 생산계획 등이 맞물려 조정이 불가능한 상황\n\n동일 양식의 내용이 다른 서류 50~120건이며, 일부 내용은 별도의 수주시스템에 시스템화되어 올려져 있음\n시스템 제약으로 글자수 제한이 있어, 주문번호를 줄이는 등 별도의 작업 수행\n정확도가 높은 수주시스템의 내용(엑셀로 저장)을 기반으로, pdf로 보완하여 자동화 추진"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#효과",
    "href": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#효과",
    "title": "[Python] Peak타임 대응용 수출계약서(pdf) tabula리딩",
    "section": "효과",
    "text": "효과\n\nTool활용을 위한 기초작업(엑셀 다운로드, pdf저장)에 5~10분 정도 소요되어, 기존 작업시간(~4시간)대비 투입시간 감소\n피크타임 감소 및 생산계획 마감시간 단축"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#github-repository",
    "title": "[Python] Peak타임 대응용 수출계약서(pdf) tabula리딩",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-readPIAR-20231101/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] Peak타임 대응용 수출계약서(pdf) tabula리딩",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\njson으로 파일을 저장할 경로정보 및 변환할 코드정보를 관리\nxlwings로 Excel로 저장해둔 기본정보를 열람\n\nDRM암호화와 관계없이 파일을 읽을 수 있기 때문에 xlwings를 채택\n\ntabula로 pdf를 표 형태로 읽어, 지정된 자리의 정보를 읽고 json형태로 저장\njson형태로 저장된 정보를 pandas DataFrame으로 concat처리 후 저장\n시스템 등록을 위해 사용중인 별도의 VBA Tool에 저장된 Excel을 넘기면 업무 완료"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html",
    "href": "posts/prgms-sql-20240317/index.html",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#개요",
    "href": "posts/prgms-sql-20240317/index.html#개요",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#문제-평균-일일-대여-요금-구하기",
    "href": "posts/prgms-sql-20240317/index.html#문제-평균-일일-대여-요금-구하기",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "문제 : 평균 일일 대여 요금 구하기",
    "text": "문제 : 평균 일일 대여 요금 구하기\n\n\n\n문제 이미지"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#작성답안",
    "href": "posts/prgms-sql-20240317/index.html#작성답안",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT ROUND(AVG(DAILY_FEE)) AS AVERAGE_FEE\nFROM CAR_RENTAL_COMPANY_CAR\nWHERE CAR_TYPE = 'SUV'\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#정리",
    "href": "posts/prgms-sql-20240317/index.html#정리",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "정리",
    "text": "정리\n\nROUND : 반올림\nAVG : 평균\nAS ??? : 컬럼명을 ???으로 가져온다 (Alias 라고 함)\nWHERE : 작성한 조건을 기준으로 가져온다\n\nWHERE의 여러 형태예시\n\nWHERE CAR_TYPE = 'SUV'\nWHERE CAR_TYPE != 'SUV'\nWHERE DAILY_FEE &gt; 14000\nWHERE DAILY_FEE BETWEEN 14000 AND 16000\nWHERE DAILY_FEE IN (14000, 16000)\nWHERE CAR_TYPE IN ('SUV', '세단')"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html",
    "href": "posts/prgms-sql-20240318/index.html",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#개요",
    "href": "posts/prgms-sql-20240318/index.html#개요",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#문제-가장-비싼-상품-구하기",
    "href": "posts/prgms-sql-20240318/index.html#문제-가장-비싼-상품-구하기",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "문제 : 가장 비싼 상품 구하기",
    "text": "문제 : 가장 비싼 상품 구하기\n\n\n\n문제 이미지"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#작성답안",
    "href": "posts/prgms-sql-20240318/index.html#작성답안",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT MAX(PRICE) AS MAX_PRICE\nFROM PRODUCT\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#정리",
    "href": "posts/prgms-sql-20240318/index.html#정리",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "정리",
    "text": "정리\n\nMAX(컬럼명) : 최대값\nMIN(컬럼명) : 최소값"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "",
    "text": "50~120건의 pdf서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n서류제공은 해외의 시차, 고객의 주문시점으로 인해 아침에 제공\n생산을 하기위한 공장또한 다른 국가에 있고, 생산계획 마감시간 문제\n각자의 이유(고객이 다양해 시점을 조정하기 어려움, 생산투입자원 조정을 위한 마감시간의 존재)로 조정 어려움\n\n문제 개선을 위해 아래의 포인트를 확인하였음\n\n제공되는 pdf서류는 모두 1장의 동일한 양식이며, 드래그가 가능한 형태\n\n드래그가 가능하다면 컴퓨터가 인식하는데도 무리가 없을테니 자동화 도입이 가능할 것이라는 판단\n\n입력작업은 엑셀VBA를 활용한 자동화Tool이 개발되어있음\n시스템 글자수 제한으로 주문번호를 축약하는 별도작업 수행\n주문번호별로 지불조건 등이 내부시스템의 코드로 매칭되어있는 별도의 관리시스템이 있음\n\n확인한 사항을 바탕으로 아래의 개선을 수행\n\n관리시스템에 이미 있는 정확도가 높은 정보를 main으로 가져옴\n시스템에서 확인할 수 없는 pdf의 정보들을 크롤링하여 필요한 정보만 식별\n이미 개발되어있는 자동화입력Tool(엑셀VBA)에 연계가능한 형태로 데이터 가공\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#추진배경",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "",
    "text": "50~120건의 pdf서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n서류제공은 해외의 시차, 고객의 주문시점으로 인해 아침에 제공\n생산을 하기위한 공장또한 다른 국가에 있고, 생산계획 마감시간 문제\n각자의 이유(고객이 다양해 시점을 조정하기 어려움, 생산투입자원 조정을 위한 마감시간의 존재)로 조정 어려움\n\n문제 개선을 위해 아래의 포인트를 확인하였음\n\n제공되는 pdf서류는 모두 1장의 동일한 양식이며, 드래그가 가능한 형태\n\n드래그가 가능하다면 컴퓨터가 인식하는데도 무리가 없을테니 자동화 도입이 가능할 것이라는 판단\n\n입력작업은 엑셀VBA를 활용한 자동화Tool이 개발되어있음\n시스템 글자수 제한으로 주문번호를 축약하는 별도작업 수행\n주문번호별로 지불조건 등이 내부시스템의 코드로 매칭되어있는 별도의 관리시스템이 있음\n\n확인한 사항을 바탕으로 아래의 개선을 수행\n\n관리시스템에 이미 있는 정확도가 높은 정보를 main으로 가져옴\n시스템에서 확인할 수 없는 pdf의 정보들을 크롤링하여 필요한 정보만 식별\n이미 개발되어있는 자동화입력Tool(엑셀VBA)에 연계가능한 형태로 데이터 가공"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#효과",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#효과",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "효과",
    "text": "효과\n\n고객의 주문을 당기거나, 생산계획마감이 지연되거나, 담당자의 과도한 업무가중이 발생하지 않고 문제해결\n이미 개발된 자원(엑셀VBA)에 연동하여 큰 시간을 들이지 않고 개발했으며 업무 투입시간 또한 감소\n\n기존에는 4시간 이내의 투입시간이 있었지만, 이번 도입으로 5~10분 정도로 작업이 완료됨\n기존대비 빠른 완수로 생산담당자 만족, 생산계획이 미뤄지지 않아 납기 등 제품수령 고객도 만족"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#github-repository",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\njson으로 파일을 저장할 경로정보 및 변환할 코드정보를 관리\nxlwings로 Excel로 저장해둔 기본정보를 열람\n\nDRM암호화와 관계없이 파일을 읽을 수 있기 때문에 xlwings를 채택\n\ntabula로 pdf를 표 형태로 읽어, 지정된 자리의 정보를 읽고 json형태로 저장\njson형태로 저장된 정보를 pandas DataFrame으로 concat처리 후 저장\n시스템 등록을 위해 사용중인 별도의 VBA Tool에 저장된 Excel을 넘기면 업무 완료"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html",
    "href": "posts/prgms-sql-20240321/index.html",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#개요",
    "href": "posts/prgms-sql-20240321/index.html#개요",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#문제-3월에-태어난-여성-회원-목록-출력하기",
    "href": "posts/prgms-sql-20240321/index.html#문제-3월에-태어난-여성-회원-목록-출력하기",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "문제 : 3월에 태어난 여성 회원 목록 출력하기",
    "text": "문제 : 3월에 태어난 여성 회원 목록 출력하기\n\n\n다음은 식당 리뷰 사이트의 회원 정보를 담은 MEMBER_PROFILE 테이블입니다. MEMBER_PROFILE 테이블은 다음과 같으며 MEMBER_ID, MEMBER_NAME, TLNO, GENDER, DATE_OF_BIRTH는 회원 ID, 회원 이름, 회원 연락처, 성별, 생년월일을 의미합니다.\n\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nMEMBER_ID\n\n\nVARCHAR(100)\n\n\nFALSE\n\n\n\n\nMEMBER_NAME\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nTLNO\n\n\nVARCHAR(50)\n\n\nTRUE\n\n\n\n\nGENDER\n\n\nVARCHAR(1)\n\n\nTRUE\n\n\n\n\nDATE_OF_BIRTH\n\n\nDATE\n\n\nTRUE\n\n\n\n\n\n\n문제\n\n\nMEMBER_PROFILE 테이블에서 생일이 3월인 여성 회원의 ID, 이름, 성별, 생년월일을 조회하는 SQL문을 작성해주세요. 이때 전화번호가 NULL인 경우는 출력대상에서 제외시켜 주시고, 결과는 회원ID를 기준으로 오름차순 정렬해주세요.\n\n\n\n예시\n\n\nMEMBER_PROFILE 테이블이 다음과 같을 때\n\n\n\n\n\nMEMBER_ID\n\n\nMEMBER_NAME\n\n\nTLNO\n\n\nGENDER\n\n\nDATE_OF_BIRTH\n\n\n\n\n\n\njiho92@naver.com\n\n\n이지호\n\n\n01076432111\n\n\nW\n\n\n1992-02-12\n\n\n\n\njiyoon22@hotmail.com\n\n\n김지윤\n\n\n01032324117\n\n\nW\n\n\n1992-02-22\n\n\n\n\njihoon93@hanmail.net\n\n\n김지훈\n\n\n01023258688\n\n\nM\n\n\n1993-02-23\n\n\n\n\nseoyeons@naver.com\n\n\n박서연\n\n\n01076482209\n\n\nW\n\n\n1993-03-16\n\n\n\n\nyoonsy94@gmail.com\n\n\n윤서연\n\n\nNULL\n\n\nW\n\n\n1994-03-19\n\n\n\n\n\nSQL을 실행하면 다음과 같이 출력되어야 합니다.\n\n\n\n\n\nMEMBER_ID\n\n\nMEMBER_NAME\n\n\nGENDER\n\n\nDATE_OF_BIRTH\n\n\n\n\n\n\nseoyeons@naver.com\n\n\n박서연\n\n\nW\n\n\n1993-03-16\n\n\n\n\n\n\n주의사항\n\n\nDATE_OF_BIRTH의 데이트 포맷이 예시와 동일해야 정답처리 됩니다."
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#작성답안",
    "href": "posts/prgms-sql-20240321/index.html#작성답안",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT MEMBER_ID, MEMBER_NAME,  GENDER, TO_CHAR(DATE_OF_BIRTH, 'YYYY-MM-DD')\nFROM MEMBER_PROFILE\nWHERE GENDER = 'W'\n  AND TO_CHAR(DATE_OF_BIRTH, 'MON') = 'MAR'\n  AND TLNO IS NOT NULL\nORDER BY MEMBER_ID ASC\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#정리",
    "href": "posts/prgms-sql-20240321/index.html#정리",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "정리",
    "text": "정리\n\nWHERE 컬럼명 IS NOT NULL\nWHERE NOT 컬럼명 IS NULL"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html",
    "href": "posts/prgms-sql-20240319/index.html",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#개요",
    "href": "posts/prgms-sql-20240319/index.html#개요",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#문제-조건에-맞는-도서-리스트-출력하기",
    "href": "posts/prgms-sql-20240319/index.html#문제-조건에-맞는-도서-리스트-출력하기",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "문제 : 조건에 맞는 도서 리스트 출력하기",
    "text": "문제 : 조건에 맞는 도서 리스트 출력하기\n\n문제 설명\n다음은 어느 한 서점에서 판매중인 도서들의 도서 정보(BOOK) 테이블입니다.\nBOOK 테이블은 각 도서의 정보를 담은 테이블로 아래와 같은 구조로 되어있습니다.\nColumn name Type Nullable Description BOOK_ID INTEGER FALSE 도서 ID CATEGORY VARCHAR(N) FALSE 카테고리 (경제, 인문, 소설, 생활, 기술) AUTHOR_ID INTEGER FALSE 저자 ID PRICE INTEGER FALSE 판매가 (원) PUBLISHED_DATE DATE FALSE 출판일\n\n\n문제\nBOOK 테이블에서 2021년에 출판된 ‘인문’ 카테고리에 속하는 도서 리스트를 찾아서 도서 ID(BOOK_ID), 출판일 (PUBLISHED_DATE)을 출력하는 SQL문을 작성해주세요. 결과는 출판일을 기준으로 오름차순 정렬해주세요.\n예시 예를 들어 BOOK 테이블이 다음과 같다면\nBOOK_ID CATEGORY AUTHOR_ID PRICE PUBLISHED_DATE 1 인문 1 10000 2020-01-01 2 경제 2 9000 2021-02-05 3 인문 2 11000 2021-04-11 4 인문 3 10000 2021-03-15 5 생활 1 12000 2021-01-10 조건에 속하는 도서는 도서 ID 가 3, 4인 도서이므로 다음과 같습니다.\nBOOK_ID PUBLISHED_DATE 3 2021-04-11 4 2021-03-15 그리고 출판일 오름차순으로 정렬하여야 하므로 다음과 같은 결과가 나와야 합니다.\nBOOK_ID PUBLISHED_DATE 4 2021-03-15 3 2021-04-11\n\n\n주의사항\nPUBLISHED_DATE의 데이트 포맷이 예시와 동일해야 정답처리 됩니다."
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#작성답안",
    "href": "posts/prgms-sql-20240319/index.html#작성답안",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT BOOK_ID, TO_CHAR(PUBLISHED_DATE, 'YYYY-MM-DD')\nFROM BOOK\nWHERE CATEGORY = '인문' AND EXTRACT(YEAR FROM PUBLISHED_DATE) = 2021\nORDER BY PUBLISHED_DATE ASC\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#정리",
    "href": "posts/prgms-sql-20240319/index.html#정리",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "정리",
    "text": "정리\n\nTO_CHAR(표시형식 변경)\n\n\nSELECT TO_CHAR(컬럼명, ‘표시형식’) FROM 테이블명 표시형식(연도 4자리) : TO_CHAR(컬럼명, ‘YYYY’) 표시형식(월) : TO_CHAR(컬럼명, ‘MM’) 표시형식(일, 연기준 1~366) : TO_CHAR(컬럼명, ‘DDD’) 표시형식(일, 일기준 1~31) : TO_CHAR(컬럼명, ‘DD’) 표시형식(일, 요일) : TO_CHAR(컬럼명, ‘D’) * 지역설정에 따라 시작요일이 달라짐  표시형식(분기) : TO_CHAR(컬럼명, ‘Q’) 표시형식(주, 연기준 1~53) : TO_CHAR(컬럼명, ‘WW’) 표시형식(주, 월기준 1~53) : TO_CHAR(컬럼명, ‘W’)  표시형식(요일, MON/월) : TO_CHAR(컬럼명, ‘DY’) 표시형식(요일, MONDAY/월요일) : TO_CHAR(컬럼명, ‘DAY’) 표시형식(월, JAN/1월) : TO_CHAR(컬럼명, ‘MON’) 표시형식(월, JANUARY/1월) : TO_CHAR(컬럼명, ‘MONTH’)  표시형식(시간, 12시간표기) : TO_CHAR(컬럼명, ‘HH12’) 표시형식(시간, 24시간표기) : TO_CHAR(컬럼명, ‘HH24’) 표시형식(분) : TO_CHAR(컬럼명, ‘MI’) 표시형식(초) : TO_CHAR(컬럼명, ‘SS’)\n\n\nAND(모두 만족) / OR(하나라도 만족) / NOT(조건과 맞지 않는)\n\n\n연산자 우선순위(참고용) 1 괄호 2 NOT 3 비교 (&gt; &lt; = !) 4 AND 5 OR\n\n\nORDER BY 컬럼명 ASC (오름차순)\nORDER BY 컬럼명 DESC (내림차순)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html",
    "href": "posts/meta-dl-creditcard-20240609/index.html",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 3주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-logistic-regression-코드-작성한-것",
    "href": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-logistic-regression-코드-작성한-것",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "2주차 과제 Logistic regression 코드 작성한 것",
    "text": "2주차 과제 Logistic regression 코드 작성한 것\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# Train data\nX_train = np.array([[1., 1.],\n                   [1., 2.],\n                   [2., 1.],\n                   [3., 2.],\n                   [3., 3.],\n                   [2., 3.]],\n                   dtype=np.float32)\nY_train = np.array([[0.],\n                   [0.],\n                   [0.],\n                   [1.],\n                   [1.],\n                   [1.],],\n                   dtype=np.float32)\n\n# 회귀선 작성 전 분포확인\ncolors=['red' if L&gt;0.5 else 'blue' for L in Y_train]\nplt.scatter(X_train[:,0], X_train[:, 1], label='Logistics regression', color=colors)\n\n\n\nimage.png\n\n\n# 모델 학습\ntf.random.set_seed(2020)\nW = tf.Variable(tf.random.normal([2,1], mean=0.0))\nb = tf.Variable(tf.random.normal([1], mean=0.0))\n\ndef hypothesis(X):\n    z = tf.matmul(X, W) + b\n    sigmoid = 1 / (1 + tf.exp(-z))\n    return sigmoid\n\ndef cost_fn(H, Y):\n    cost = -tf.reduce_mean(Y*tf.math.log(H) + (1-Y)*tf.math.log(1-H))\n    return cost\n\nlearning_rate = 0.01\noptimizer = tf.optimizers.SGD(learning_rate)\n\nfor step in range(5001):\n    with tf.GradientTape() as g:\n        pred = hypothesis(X_train)\n        cost = cost_fn(pred, Y_train)\n\n        gradients = g.gradient(cost, [W,b])\n    \n    optimizer.apply_gradients(zip(gradients, [W, b]))\n\n    if step % 1000 == 0:\n        print(f'Step={step+1}, Cost = {cost}, W={W.numpy()}, b = {b.numpy()}')\n\nw_hat = W.numpy()\nb_hat = b.numpy()\n\nStep=1, Cost = 0.7932398319244385, W=[[-0.10415223] [0.68125504]], b = [0.3810195]  Step=1001, Cost = 0.5122759938240051, W=[[0.1809378] [0.55177015]], b = [-0.97815347]  Step=2001, Cost = 0.39883172512054443, W=[[0.5135696] [0.6884617]], b = [-1.9777462]  Step=3001, Cost = 0.32507583498954773, W=[[0.7515713] [0.8368167]], b = [-2.7877953]  Step=4001, Cost = 0.27400580048561096, W=[[0.9350327] [0.97824335]], b = [-3.4628296]  Step=5001, Cost = 0.2367737740278244, W=[[1.0848083] [1.1075894]], b = [-4.039375]\n\n# Slope(Coefficient) 확인\nslope = w_hat[0]/w_hat[1]\nxx = np.linspace(np.min(X_train[:,0]),np.max(X_train[:,0])) # min과 max 사이 구간의 숫자를 생성. x값\nyy = -slope*xx - b_hat/w_hat[1]                             # xx(x값)입력하여 y값 생성\n\n# train data분포\nplt.scatter(X_train[:, 0], X_train[:, 1], label='Logistics regression', color=colors)\n\n# 분류선(Decision Boundary)확인\nplt.plot(xx, yy, label='Decision Boundary')\nplt.legend()\n\n\n\nimage-2.png\n\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nX_test = np.array([[1., 0.],\n                   [0., 2.],\n                   [1., 1.],\n                   [3., 2.],\n                   [3., 3.],\n                   [2., 3.]],\n                   dtype=np.float32)\nY_test = np.array([[0.],\n                   [0.],\n                   [1.],\n                   [0.],\n                   [1.],\n                   [0.],],\n                   dtype=np.float32)\n\nY_actual = Y_test\nY_predicted = hypothesis(X_test)\nY_predicted_binary = np.where(Y_predicted &gt;= 0.5, 1, 0)\n\nCM_array = confusion_matrix(Y_actual, Y_predicted_binary, labels=[0, 1]) \nCM_array\n\narray([[2, 2], [1, 1]], dtype=int64)\n\n# Confusion matrix 시각화(seaborn)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass_labels = ['Negative', 'Positive']\nplt.figure(figsize=(8, 6))\nsns.heatmap(CM_array, annot=True, cmap='Blues',\n            xticklabels=class_labels, yticklabels=class_labels)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n\n\nimage-3.png\n\n\n# Confusion matrix 시각화(Scikit-learn)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=CM_array, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\nimage-4.png"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#multi-class-classfication-regression",
    "href": "posts/meta-dl-creditcard-20240609/index.html#multi-class-classfication-regression",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "Multi-class Classfication regression",
    "text": "Multi-class Classfication regression\n\n2가지 분류가 아닌 3가지 이상의 분류(A,B,C로 나누는 신용등급 등)\nBinary classification과 달리 하나의 Decision boundary로는 해결 불가\nOne vs All(Rest)\n\n하나의 대상과, 아닌 것’들’로 Binary Classification을 여러번 수행\n예를 들어 a, b, c 3가지를 분류하는 경우\n\n아래와 같은 같은 3개의 식으로 표현할 수 있고, \n아래와 같이 하나의 행렬로 한번에 표현할 수 있다  \n\nSoftmax : 각 결과값(\\(H_a, H_b, H_c\\))의 비율(확률)이 나오게 됨(총합이 1)\n\nCross entropy cost function\n\n정보량은 확률에 반비례한다고 정의 (정보량= $ 1 p$)\n\n특정 성씨의 사람을 뽑는다고 할 때, 한국의 주요 성씨인 김씨\\(1 \\over 10\\) vs 소수 성씨인 남궁씨\\(1 \\over 100\\)\n\n로그를 취하여 전개하면 Cross entropy 식이 된다\nEntropy : Measure for uncertainty (불확실성의 측정)  \nCase별 Cross entropy(cost function)\n\n출력값(결과값 softmax)과 실제값이 비슷한 경우 : 특정 값 산출\n출력값(결과값 softmax)과 실제값이 완전히 다른 경우 : 무한대\n출력값(결과값 softmax)과 실제값이 완전히 동일한 경우 : 0\n\nCross entropy를 개인이 직접구현한다면 놓칠 수 있는 부분(cross entropy의 무한대)의 문제\n\n파이토치 등 많은 사람이 참여한 패키지를 사용하면 방지할 수 있음\n컴퓨터에서의 Zero division error 등의 경우, 텐서플로우 등 패키지에선 분모에 0.00001등을 더하여 실제값엔 영향이 작게하며 오류 제거"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html",
    "href": "posts/prgms-sql-20240320/index.html",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#개요",
    "href": "posts/prgms-sql-20240320/index.html#개요",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#문제-조건에-부합하는-중고거래-댓글-조회하기",
    "href": "posts/prgms-sql-20240320/index.html#문제-조건에-부합하는-중고거래-댓글-조회하기",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "문제 : 조건에 부합하는 중고거래 댓글 조회하기",
    "text": "문제 : 조건에 부합하는 중고거래 댓글 조회하기\n\n    &lt;h6 class=\"guide-section-title\"&gt;문제 설명&lt;/h6&gt;\n    &lt;div class=\"markdown solarized-dark\"&gt;&lt;p&gt;다음은 중고거래 게시판 정보를 담은 &lt;code&gt;USED_GOODS_BOARD&lt;/code&gt; 테이블과 중고거래 게시판 첨부파일 정보를 담은 &lt;code&gt;USED_GOODS_REPLY&lt;/code&gt; 테이블입니다. &lt;code&gt;USED_GOODS_BOARD&lt;/code&gt; 테이블은 다음과 같으며 &lt;code&gt;BOARD_ID&lt;/code&gt;, &lt;code&gt;WRITER_ID&lt;/code&gt;, &lt;code&gt;TITLE&lt;/code&gt;, &lt;code&gt;CONTENTS&lt;/code&gt;, &lt;code&gt;PRICE&lt;/code&gt;, &lt;code&gt;CREATED_DATE&lt;/code&gt;, &lt;code&gt;STATUS&lt;/code&gt;, &lt;code&gt;VIEWS&lt;/code&gt;은 게시글 ID, 작성자 ID, 게시글 제목, 게시글 내용, 가격, 작성일, 거래상태, 조회수를 의미합니다.&lt;/p&gt;\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nBOARD_ID\n\n\nVARCHAR(5)\n\n\nFALSE\n\n\n\n\nWRITER_ID\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nTITLE\n\n\nVARCHAR(100)\n\n\nFALSE\n\n\n\n\nCONTENTS\n\n\nVARCHAR(1000)\n\n\nFALSE\n\n\n\n\nPRICE\n\n\nNUMBER\n\n\nFALSE\n\n\n\n\nCREATED_DATE\n\n\nDATE\n\n\nFALSE\n\n\n\n\nSTATUS\n\n\nVARCHAR(10)\n\n\nFALSE\n\n\n\n\nVIEWS\n\n\nNUMBER\n\n\nFALSE\n\n\n\n\n\nUSED_GOODS_REPLY 테이블은 다음과 같으며 REPLY_ID, BOARD_ID, WRITER_ID, CONTENTS, CREATED_DATE는 각각 댓글 ID, 게시글 ID, 작성자 ID, 댓글 내용, 작성일을 의미합니다.\n\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nREPLY_ID\n\n\nVARCHAR(10)\n\n\nFALSE\n\n\n\n\nBOARD_ID\n\n\nVARCHAR(5)\n\n\nFALSE\n\n\n\n\nWRITER_ID\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nCONTENTS\n\n\nVARCHAR(1000)\n\n\nTRUE\n\n\n\n\nCREATED_DATE\n\n\nDATE\n\n\nFALSE\n\n\n\n\n\n\n문제\n\n\nUSED_GOODS_BOARD와 USED_GOODS_REPLY 테이블에서 2022년 10월에 작성된 게시글 제목, 게시글 ID, 댓글 ID, 댓글 작성자 ID, 댓글 내용, 댓글 작성일을 조회하는 SQL문을 작성해주세요. 결과는 댓글 작성일을 기준으로 오름차순 정렬해주시고, 댓글 작성일이 같다면 게시글 제목을 기준으로 오름차순 정렬해주세요.\n\n\n\n예시\n\n\nUSED_GOODS_BOARD 테이블이 다음과 같고\n\n\n\n\n\nBOARD_ID\n\n\nWRITER_ID\n\n\nTITLE\n\n\nCONTENTS\n\n\nPRICE\n\n\nCREATED_DATE\n\n\nSTATUS\n\n\nVIEWS\n\n\n\n\n\n\nB0001\n\n\nkwag98\n\n\n반려견 배변패드 팝니다\n\n\n정말 저렴히 판매합니다. 전부 미개봉 새상품입니다.\n\n\n12000\n\n\n2022-10-01\n\n\nDONE\n\n\n250\n\n\n\n\nB0002\n\n\nlee871201\n\n\n국내산 볶음참깨\n\n\n직접 농사지은 참깨입니다.\n\n\n3000\n\n\n2022-10-02\n\n\nDONE\n\n\n121\n\n\n\n\nB0003\n\n\ngoung12\n\n\n배드민턴 라켓\n\n\n사놓고 방치만 해서 팝니다.\n\n\n9000\n\n\n2022-10-02\n\n\nSALE\n\n\n212\n\n\n\n\nB0004\n\n\nkeel1990\n\n\n디올 귀걸이\n\n\n신세계강남점에서 구입. 정품 아닐시 백퍼센트 환불\n\n\n130000\n\n\n2022-10-02\n\n\nSALE\n\n\n199\n\n\n\n\nB0005\n\n\nhaphli01\n\n\n스팸클래식 팔아요\n\n\n유통기한 2025년까지에요\n\n\n10000\n\n\n2022-10-02\n\n\nSALE\n\n\n121\n\n\n\n\n\nUSED_GOODS_REPLY 테이블이 다음과 같을 때\n\n\n\n\n\nREPLY_ID\n\n\nBOARD_ID\n\n\nWRITER_ID\n\n\nCONTENTS\n\n\nCREATED_DATE\n\n\n\n\n\n\nR000000001\n\n\nB0001\n\n\ns2s2123\n\n\n구매하겠습니다. 쪽지 드립니다.\n\n\n2022-10-02\n\n\n\n\nR000000002\n\n\nB0002\n\n\nhoho1112\n\n\n쪽지 주세요.\n\n\n2022-10-03\n\n\n\n\nR000000003\n\n\nB0006\n\n\nhwahwa2\n\n\n삽니다. 연락주세요.\n\n\n2022-10-03\n\n\n\n\nR000000004\n\n\nB0007\n\n\nhong02\n\n\n예약중\n\n\n2022-10-06\n\n\n\n\nR000000005\n\n\nB0009\n\n\nhanju23\n\n\n구매완료\n\n\n2022-10-07\n\n\n\n\n\nSQL을 실행하면 다음과 같이 출력되어야 합니다.\n\n\n\n\n\nTITLE\n\n\nBOARD_ID\n\n\nREPLY_ID\n\n\nWRITER_ID\n\n\nCONTENTS\n\n\nCREATED_DATE\n\n\n\n\n\n\n반려견 배변패드 팝니다\n\n\nB0001\n\n\nR000000001\n\n\ns2s2123\n\n\n구매하겠습니다. 쪽지 드립니다.\n\n\n2022-10-02\n\n\n\n\n국내산 볶음참깨\n\n\nB0002\n\n\nR000000002\n\n\nhoho1112\n\n\n쪽지 주세요.\n\n\n2022-10-03\n\n\n\n\n\n\n주의사항\n\n\nCREATED_DATE의 포맷이 예시의 포맷과 일치해야 정답처리 됩니다.\n\n\n  &lt;/div&gt;"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#작성답안",
    "href": "posts/prgms-sql-20240320/index.html#작성답안",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT board.TITLE, board.BOARD_ID,\n       reply.REPLY_ID, reply.WRITER_ID, reply.CONTENTS, TO_CHAR(reply.CREATED_DATE, 'YYYY-MM-DD')\nFROM USED_GOODS_BOARD board, USED_GOODS_REPLY reply\nWHERE TO_CHAR(board.CREATED_DATE, 'YYYYMM') = '202210'\n  AND board.BOARD_ID = reply.BOARD_ID\nORDER BY reply.CREATED_DATE ASC, board.TITLE ASC;\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#정리",
    "href": "posts/prgms-sql-20240320/index.html#정리",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "정리",
    "text": "정리\n\n각 테이블의 BOARD_ID 일치시키는 것을 실수하였음. 향후 동일케이스에 대해서는 고려하여 풀기\n다중정렬 &gt; ORDER BY reply.CREATED_DATE ASC, board.TITLE ASC; 앞의 컬럼일수록 정렬 우선순위를 가짐"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240609/index.html#개요",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 3주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#개념",
    "href": "posts/meta-dl-creditcard-20240609/index.html#개념",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "개념",
    "text": "개념\nRMSE(Root Mean Squeare Error)\nRoot    (4)\nMean    (3)\nSquare  (2)\nError   (1)\n(1) 실제값에서 예측값을 뺀 '오차'를\n(2) 합했을 때 음수의 영향을 제거하기 위해 '제곱'하고\n(3) '평균'오차로 만든 후\n(4) '루트'를 씌워 값의 크기를 작게 한다 (값을 작게하여 연산속도에 이점이 있다)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#파일-다운로드-및-알아보기",
    "href": "posts/meta-dl-creditcard-20240609/index.html#파일-다운로드-및-알아보기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "파일 다운로드 및 알아보기",
    "text": "파일 다운로드 및 알아보기\nFile descriptions\ntrain.csv - 예측 모델을 만들기 위해 사용하는 학습 데이터입니다. \n    집의 정보와 예측할 변수인 가격(Price) 변수를 가지고 있습니다.\ntest.csv - 학습셋으로 만든 모델을 가지고 예측할 가격(Price) 변수를 제외한 집의 정보가\n    담긴 테스트 데이터 입니다.\nsample_submission.csv - 제출시 사용할 수 있는 예시 submission.csv 파일입니다.\nData fields\nID : 집을 구분하는 번호\ndate : 집을 구매한 날짜\nprice : 집의 가격(Target variable)\nbedrooms : 침실의 수\nbathrooms : 화장실의 수\nsqft_living : 주거 공간의 평방 피트(면적)\nsqft_lot : 부지의 평방 피트(면적)\nfloors : 집의 층 수\nwaterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\nview : 집이 얼마나 좋아 보이는지의 정도\ncondition : 집의 전반적인 상태\ngrade : King County grading 시스템 기준으로 매긴 집의 등급\nsqft_above : 지하실을 제외한 평방 피트(면적)\nsqft_basement : 지하실의 평방 피트(면적)\nyr_built : 지어진 년도\nyr_renovated : 집을 재건축한 년도\nzipcode : 우편번호\nlat : 위도\nlong : 경도\nsqft_living15 : 2015년 기준 주거 공간의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\nsqft_lot15 : 2015년 기준 부지의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#패키지-및-데이터-불러오기",
    "href": "posts/meta-dl-creditcard-20240609/index.html#패키지-및-데이터-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "패키지 및 데이터 불러오기",
    "text": "패키지 및 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data_path = './data/train.csv'\ntest_data_path = './data/test.csv'\n\ndata = pd.read_csv(train_data_path)\ntest = pd.read_csv(test_data_path)\nprint('train data : {}'.format(data.shape))\nprint('test data : {}'.format(test.shape))\n\ntrain data : (15035, 21)\ntest data : (6555, 20)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#데이터-전처리",
    "href": "posts/meta-dl-creditcard-20240609/index.html#데이터-전처리",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\n정답컬럼 분리\n\ntest데이터와 달리 train data에는 컬럼이 1개 더 있음 (정답컬럼인 price)\n별도의 정답 데이터(y)로 분리\n\n\nprint('컬럼 분리 전')\nprint(data.columns)\nprint(test.columns)\n\n컬럼 분리 전\nIndex(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\n\n\n\n# y라는 변수에 price(정답)을 옮기고, 전체데이터를 백업(data_backup에 할당)하고 price컬럼 삭제\ny = data['price'] \ndata_backup = data.copy()\ndata.drop('price',axis=1, inplace=True)\n\n\nprint('컬럼 분리 후')\nprint(data.columns)\nprint(test.columns)\nprint(y.name)\n\n컬럼 분리 후\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nprice\n\n\n\n\n결측치 확인 및 제거\n\ntrain, test 데이터를 합쳐서 한번에 확인\n\n\n# 합치기\ndf_chk_missing = pd.concat((data, test), axis=0)\n\n# 향후 분할을 대비한 행 수 저장\ntrain_length = len(data)\ntest_length = len(test)\n\nprint(train_length, test_length)\n\n15035 6555\n\n\n\n결측치 확인방법1(pandas)\n\nisna()로 결측치를 확인\n\n\nprint(df_chk_missing.isna().sum())\n\nid               0\ndate             0\nbedrooms         0\nbathrooms        0\nsqft_living      0\nsqft_lot         0\nfloors           0\nwaterfront       0\nview             0\ncondition        0\ngrade            0\nsqft_above       0\nsqft_basement    0\nyr_built         0\nyr_renovated     0\nzipcode          0\nlat              0\nlong             0\nsqft_living15    0\nsqft_lot15       0\ndtype: int64\n\n\n\n\n결측치 확인방법2(missingno)\n\nmissingno 패키지로 컬럼별 결측치 시각화\n\n\nimport missingno\n\nmissingno.matrix(df_chk_missing)\n\n\n\n\n\n\n\n\n\n\n결측치 확인방법3(ydata_profiling)\n\nydata_profiling 패키지로 결측치 및 다양한 값 확인 가능\n렌더링 용량 문제로 실행결과는 이미지로 대체(RangeError: Maximum call stack size exceeded)\n\n\nfrom ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df_chk_missing, title=\"Profiling Report\")\nprofile\n\n\n\n\n실행결과 샘플\n\n\n\n결측치가 없으므로 다음 과정을 진행\n\n\n\n\n불필요한 변수 제거, 데이터 변환 등\n\n단순식별용 데이터 삭제\n\n가격과 관계없는 단순식별용 데이터인 id 삭제\n\n\nmain_id = df_chk_missing['id'][:train_length]\ntest_id = df_chk_missing['id'][train_length:]\ndel df_chk_missing['id']\n\n\n\n불필요한 데이터 삭제\n\n날짜 뒤에 T00000과 같이 시간데이터(로 추정됨)가 있는데, 모두 T00000으로만 되어있으므로 삭제\n\n\n# T000000으로 되어있는 값 세기\ndf_chk_missing['date'].str.contains('T000000').value_counts()\n\ndate\nTrue    21590\nName: count, dtype: int64\n\n\n\n# apply로 lambda함수를 사용하여, date컬럼의 앞자리만 저장\ndf_chk_missing['date'] = df_chk_missing['date'].apply(lambda x : str(x[:6]))\ndf_chk_missing.head()\n\n\n\n\n\n\n\n\n\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n0\n201410\n3\n1.00\n1180\n5650\n1.0\n0\n0\n3\n7\n1180\n0\n1955\n0\n98178\n47.5112\n-122.257\n1340\n5650\n\n\n1\n201502\n2\n1.00\n770\n10000\n1.0\n0\n0\n3\n6\n770\n0\n1933\n0\n98028\n47.7379\n-122.233\n2720\n8062\n\n\n2\n201502\n3\n2.00\n1680\n8080\n1.0\n0\n0\n3\n8\n1680\n0\n1987\n0\n98074\n47.6168\n-122.045\n1800\n7503\n\n\n3\n201406\n3\n2.25\n1715\n6819\n2.0\n0\n0\n3\n7\n1715\n0\n1995\n0\n98003\n47.3097\n-122.327\n2238\n6819\n\n\n4\n201501\n3\n1.50\n1060\n9711\n1.0\n0\n0\n3\n7\n1060\n0\n1963\n0\n98198\n47.4095\n-122.315\n1650\n9711\n\n\n\n\n\n\n\n\n\n\n로그변환\n\n치우친 분포를 정규분포에 가깝게 만들기\n\n\n분포가 치우쳐져 있는 항목 찾기(시각화)\n\nrow_plot = 5\ncol_plot = 4\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(24, 35)) \n\ncolumns = df_chk_missing.columns\ncolumns_idx = 1 # 첫 컬럼인 date(날짜)는 제외하기 위해 0이 아닌 1부터 시작\nfor row in range(row_plot):\n    for col in range(col_plot):\n        sns.kdeplot(data=df_chk_missing[columns[columns_idx]], ax=ax[row][col])\n        ax[row][col].set_title(columns[columns_idx])\n        columns_idx += 1\n        if columns_idx == len(columns) :\n            break\n\n\n\n\n\n\n\n\n\n아래의 항목들이 치우쳐져 있음\n\nsqft_living\nsqft_lot\nwaterfront (→유/무 지표로 0,1만 있는게 정상이므로 제외)\nsqft_above\nsqft_basement\nsqft_living15\nsqft_lot15\n\n\n\n# 변환대상 리스트에 저장\nskewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n\n# 그래프로 그리기 (변환 전/후 그래프를 함께 그릴 예정이므로 plot의 수는 두배)\nrow_plot = 6\ncol_plot = 2\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(15, 35)) \n\ncolumns = skewed_columns\ncolumns_idx = 0\n\n\nfor row in range(row_plot):\n    # 로그변환 대상만 식별 후 진행\n    if columns[row] in skewed_columns:\n        # 기존 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][0])\n        ax[row][0].set_title(columns[row])\n\n        # 로그변환\n        df_chk_missing[columns[row]] = np.log1p(df_chk_missing[columns[row]])\n\n        # 변환된 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][1])\n        ax[row][1].set_title(columns[row]+'_log')\n\n\n\n\n\n\n\n\n\n\n\ntrain, test 데이터로 정리\n\npreprocessed_train = df_chk_missing[:train_length].copy()\npreprocessed_test = df_chk_missing[train_length:].copy()\nprice_train = y.copy()\n\n# date(날짜)의 타입을 int로 변경 (변경하지 않는 경우 object타입으로 인한 오류 발생)\npreprocessed_train['date'] = preprocessed_train['date'].astype(int)\npreprocessed_test['date'] = preprocessed_test['date'].astype(int)\n\nprint(preprocessed_train.shape)\nprint(preprocessed_test.shape)\n\n(15035, 19)\n(6555, 19)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#scikit-learn-등-관련-패키지-불러오기",
    "href": "posts/meta-dl-creditcard-20240609/index.html#scikit-learn-등-관련-패키지-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "Scikit-learn 등 관련 패키지 불러오기",
    "text": "Scikit-learn 등 관련 패키지 불러오기\n\n본래 사용하는 패키지는 모두 최상단에서 불러오는게 맞음!\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n모델 불러오고 Cross Validation으로 모델성능 측청\n\ngboost = GradientBoostingRegressor(random_state=1210)\nxgboost = xgb.XGBRegressor(random_state=1210)\nlightgbm = lgb.LGBMRegressor(random_state=1210)\n\nmodel_dict = {'GradientBoosting':gboost,\n              'XGBoost':xgboost,\n              'LigntGBM':lightgbm}\n\n# LightGBM의 메시지가 나오지 않도록 별도로 저장 후 출력\nmodel_cv_score = dict()\nfor model in model_dict.keys():\n    model_cv_score[model] = np.mean(cross_val_score(model_dict[model], X=preprocessed_train, y=price_train))\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001070 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2296\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 540497.991270\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2327\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542956.681826\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2331\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 543149.529265\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2332\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542032.619305\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2298\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 534776.444047\n\n\n\nfor model in model_dict.keys():\n    print(f'{model} : {model_cv_score[model]}')\n\nGradientBoosting : 0.8613647608814923\nXGBoost : 0.8762617283884332\nLigntGBM : 0.8818569800403846\n\n\n\n\n모델학습 및 예측\n\nScore가 가장 높았던 lightGBM으로 진행해보기\n\n\nmodel_dict['LigntGBM'].fit(preprocessed_train.values, y)\nprediction = model_dict['LigntGBM'].predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n\n\narray([1296687.09405506,  311847.90404507,  806735.28228208, ...,\n       1726006.82963994,  395020.94053356,  333594.29000994])\n\n\n\n\n제출용 DataFrame 및 csv파일 생성\n\ndf_submission = pd.DataFrame({'id' : test_id, \n                              'price' : prediction})\ndf_submission\n\n\n\n\n\n\n\n\n\nid\nprice\n\n\n\n\n0\n15208\n1.296687e+06\n\n\n1\n15209\n3.118479e+05\n\n\n2\n15210\n8.067353e+05\n\n\n3\n15211\n2.098083e+05\n\n\n4\n15212\n4.343237e+05\n\n\n...\n...\n...\n\n\n6550\n21758\n4.230647e+05\n\n\n6551\n21759\n5.111171e+05\n\n\n6552\n21760\n1.726007e+06\n\n\n6553\n21761\n3.950209e+05\n\n\n6554\n21762\n3.335943e+05\n\n\n\n\n6555 rows × 2 columns\n\n\n\n\n\ndf_submission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#gridsearch",
    "href": "posts/meta-dl-creditcard-20240609/index.html#gridsearch",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "GridSearch",
    "text": "GridSearch\n\nLightGBM에 Grid Search 적용해보기\n\nfit 후 결과값 부연설명\n\n5 folds : cv = 5\n4 candidates : 2(max_depth) X 2(n_estimators)\n20 fits : 5 folds X 4 candidates\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_param = {\n    'n_estimators': [50, 100], #트리수\n    'max_depth': [1, 10], #트리깊이\n}\n\ngrid_model = GridSearchCV(lgb.LGBMRegressor(random_state=1210), \n                          param_grid=grid_param, \n                          scoring='neg_mean_squared_error',\n                           cv=5, verbose=1, n_jobs=5)\n\n\ngrid_model.fit(preprocessed_train.values, y)\n\n\ngrid_model.cv_results_\nparams = grid_model.cv_results_['params']\n\ndf_grid_result = pd.DataFrame(params)\ndf_grid_result['score'] = grid_model.cv_results_['mean_test_score']\n\n\ndf_grid_result\n\n\n\n\n\n\n\n\n\nmax_depth\nn_estimators\nscore\n\n\n\n\n0\n1\n50\n-4.787553e+10\n\n\n1\n1\n100\n-3.851269e+10\n\n\n2\n10\n50\n-1.723322e+10\n\n\n3\n10\n100\n-1.636420e+10\n\n\n\n\n\n\n\n\n\nGridSearch 기준 Score가 가장 좋은 파라메터로 진행해보기\n\n\nmodel = lgb.LGBMRegressor(max_depth=10, n_estimators=100, random_state=1210)\nmodel.fit(preprocessed_train.values, y)\nprediction = model.predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n\n\narray([1291842.41370212,  314132.92290945,  817260.44452776, ...,\n       1713971.79620206,  389405.58625426,  332109.32763046])"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#gridsearch",
    "href": "posts/coach-ml-kaggle-20230506/index.html#gridsearch",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "GridSearch",
    "text": "GridSearch\n\nLightGBM에 Grid Search 적용해보기\n\nfit 후 결과값 부연설명\n\n5 folds : cv = 5\n4 candidates : 2(max_depth) X 2(n_estimators)\n20 fits : 5 folds X 4 candidates\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_param = {\n    'n_estimators': [50, 100], #트리수\n    'max_depth': [1, 10], #트리깊이\n}\n\ngrid_model = GridSearchCV(lgb.LGBMRegressor(random_state=1210), \n                          param_grid=grid_param, \n                          scoring='neg_mean_squared_error',\n                           cv=5, verbose=1, n_jobs=5)\n\n\ngrid_model.fit(preprocessed_train.values, y)\n\n\ngrid_model.cv_results_\nparams = grid_model.cv_results_['params']\n\ndf_grid_result = pd.DataFrame(params)\ndf_grid_result['score'] = grid_model.cv_results_['mean_test_score']\n\n\ndf_grid_result\n\n\n\n\n\n\n\n\n\nmax_depth\nn_estimators\nscore\n\n\n\n\n0\n1\n50\n-4.787553e+10\n\n\n1\n1\n100\n-3.851269e+10\n\n\n2\n10\n50\n-1.723322e+10\n\n\n3\n10\n100\n-1.636420e+10\n\n\n\n\n\n\n\n\n\nGridSearch 기준 Score가 가장 좋은 파라메터로 진행해보기\n\n\nmodel = lgb.LGBMRegressor(max_depth=10, n_estimators=100, random_state=1210)\nmodel.fit(preprocessed_train.values, y)\nprediction = model.predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n\n\narray([1291842.41370212,  314132.92290945,  817260.44452776, ...,\n       1713971.79620206,  389405.58625426,  332109.32763046])"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-설명",
    "href": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-설명",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "2주차 과제 설명",
    "text": "2주차 과제 설명\n\ntf.random.set_seed(2024)\n\n서로 다른 모델을 비교하는 경우, 시작점이 다른 것 때문에 성능우위가 다르게 측정되는 경우 발생 (같은 성능임에도 다르게 나오거나, 좋은 모델이 더 나쁜 모델로 오인되는 경우)\n이러한 경우를 방지하고자, set_seed로 같은 지점에서 시작하게 할 수 있음\n딥러닝은 복잡한 다차원의 함수이므로, 좋은 시작점에 따라 달라질 수 있음 (좋은 Optimizer를 사용하고 좋은 데이터를 쓴다면 차이는 줄어들 수 있음)\n\nMatrix Multiplication\n\nA(1,2), B(2,1)와 같은 Matrix에서, A의 열(,2)과 B의 행(2,)의 숫자가 같아야 가능\n\nCost function에서 마이너스(-)를 붙이는 경우\n\n높을수록 안좋은 척도여야 할 때, 계산식이 높을수록 좋은 값인 경우 붙여서 변환\n\nLearning rate\n\nGD에서 안정적으로 최적점에 가게하기 위해 학습률을 조정\n사람이 지정하는 hyper parameter, 경험에 의해 넣는 경우가 많다(정답은 없음)\n\nConfusion Matrix\n\nRecall, Precision 으로 표현하는 것은 경영진 등에는 와닿지 않을 수 있으므로 시각화하여 보여주면 좋음"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html",
    "href": "posts/meta-dl-creditcard-20240602/index.html",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 2주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240602/index.html#개요",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 2주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#주차-과제-설명",
    "href": "posts/meta-dl-creditcard-20240602/index.html#주차-과제-설명",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "1주차 과제 설명",
    "text": "1주차 과제 설명\n\n큰 틀에서는 보통 아래의 순서로 진행\n\n데이터생성\nhypothesis\ncost function\noptimizer\ntrain\nPrediction (=Inference)\n\ntf.reduce_mean() : 열(row)끼리의 평균\nNon-Linear Modeling  &gt;\\(y = ax^2 + bx + c\\)\n\n구하고자 하는 값은 \\(a, b, c\\)\n모델 학습 전 임의의 값(\\(a,b,c\\))으로 추세선 긋기  \n내가 가정하는 식을 hypothesis에 넣고 학습\ndef hypothesis(x):\n  return a*(x)**2 + b*x + c\n\ndef cost_fn(pred_y, true_y):\n  return tf.reduce_mean(tf.square(pred_y - true_y))\n\noptimizer = tf.optimizers.Adam(learning_rate = 0.01)\n\ndef train():\n  with tf.GradientTape() as g:\n    pred = hypothesis(X)\n    cost = cost_fn(pred, Y)\n\n  gradients = g.gradient(cost, [a,b,c])                    # 기울기를 계산하는 부분\n  optimizer.apply_gradients(zip(gradients, [a,b,c]))       # 계산된 기울기를 업데이트 해주는 부분\n\nfor step in range(1,1001):\n  train()\n\n  if step % 100 == 0:\n    pred = hypothesis(X)\n    cost = cost_fn(pred, Y)\n    print(f\" step:{step} cost:{cost:.4f} a:{a.numpy()} b:{b.numpy()} c:{c.numpy()} \")\n    line_x = np.arange(min(X), max(X), 0.001)\n    line_y = a*(line_x)**2 + b*(line_x) + c\n모델 학습 후 변경된 추세선 확인하기"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#linear-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#linear-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear Regression의 Motivation\n\n수 많은 데이터를 묘사하는 하나의 선을 긋고, 그 선으로 향후 추이(예를 들어 광고비와 매출의 관계 등)를 판단\n랜덤하게 하나의 선을 긋고(임의의 W, b설정), 실제 데이터(점)과 선(예측값)의 거리[오차]를 줄인다\n\n선형성이 있다는 가정 하에 모델링 수행 &gt; \\(Y = B_0 + B_1X + \\epsilon\\)\n\n기존 수업의 \\(Y = WX+b\\) 식에서 \\(B_0\\)이 \\(b\\), \\(B_1X\\)이 \\(W\\)\n\n\\(B_0\\) : intercept (첫번째 기울기는 \\(B_1X\\)으로 표현)\n\\(B_1X\\) : slope, coefficients\n\\(B_0\\), \\(B_1X\\) 등을 합쳐서 parameter라고 부름 (가장 적절한 parameter를 찾는 것이 AI의 목적)\n\n\nhat (\\(\\hat{B}\\))은 최적화가 되었을 때 씌운다\n\n처음에 임의값을 두었다가 학습을 하며 최적화가 되면 씌움\n\nresidual(잔차)\n\n예측값 \\(\\hat{y}\\)(\\(\\hat{B}_0+\\hat{B}_1x_i\\))에 대해 정의된 오차 \\(e_i\\) (\\(y_i-\\hat{y_i}\\))\n\ni번째 잔차(\\(i\\)th residual)\n\n\nRSS(Residual Sum of Squares) : 잔차를 제곱해서 더한 것 &gt; \\(e_1^2 + e_2^2+...+e_n^2\\)\n\n제곱이 아닌 절대값으로 해도 개념적으로는 오차를 계산 가능\n최소자승법(least square)으로 최적의 \\(B\\)를 구하며 RSS가 줄어듦\n\n최소자승법 : 그래프에서 오차가 가장 낮은, 미분값이 0인 지점 찾기(오차가 0이 되는 지점을 미분으로 찾는 것이며, 이때의 오차는 Train set의 오차임)\n절대값으로 오차를 찾는 경우 미분을 활용할 수 없어, RSS를 사용\n현대에는 최소자승법을 잘 안쓰고 GD를 사용 (GD : 하나의 랜덤한 점을 찍고, 최소점을 향해 근사를 반복해나감[epoch반복])\n\n\n\\(B_0\\), \\(B_1X\\) 등 parameter를 구했을 때, 얼마나 신뢰할 수 있는 숫자인가\n\nStandard error(Variance에 루트를 씌우면 Standard error로, 본질적으로 같음)\n\nVariance는 모델의 안정성\nStandard error, \\(SE(\\hat{B}_1)^2\\) 계산을 통해 얼마나 안정적인지 판단\nSE의 계산식은 데이터(샘플)가 많아질수록 분모(Sum)가 커지므로, 모델의 Variance가 낮아짐\n\n→ 데이터가 많아지면 모델이 좋아진다 (최적의 \\(B\\)를 최소자승법으로 구해 모델을 만들고, 데이터 샘플의 수가 커지면 계산된 최적의 \\(B\\)가 가지는 SE가 줄어들음)\n\n\nConfidence intervals(신뢰구간)\n\n대학원 면접에서 많이 나오는 주제\nVariance(또는 SE)를 기반으로 신뢰구간을 구함 &gt; \\(\\hat{B}_1 +- 2*SE(\\hat{B}_1)\\)\n신뢰구간 95% / 평균 100 / 신뢰구간 80~120의 해석\n\n모집단에서 샘플링을 했을 때 평균이 100\n모집단 평균이 105, 110일수도 있지만 추정한 신뢰구간안에 실제 평균이 존재할 확률이 95%라는 뜻\n\nVariance가 낮아지면 신뢰구간이 좁아지며 좋아짐\n\n\nt-statistic\n\n$ t = _1-0 SE(_1) $\nSE가 낮을수록 좋은 값이라는 점에서, 위 수식(t값)이 클수록 좋다는 직관적 이해 가능\np-value : t값보다 클 확률 (즉 낮을수록 좋다)\n\n결과 표 보며 이해하기  \n\ncoefficient : 구한 parameter값\nStandard error, t-statistic은 단위에 따라 달라질 수 있는 값\np-value는 통일된 값으로, 0.0001보다 작으므로 Variance가 낮고 모델이 안정적이다 (p-value가 낮을수록 결과가 유의하다 라고 표현)\n\n\\(R^2\\)\n\n$ R^2$ = $ TSS - RSS TSS $ = $1 - RSS TSS $\nRSS(오차의 제곱을 모두 더한 잔차)가 클수록 작아지므로, 클수록 좋은 지표임을 이해\nTSS는 RSS보다 무조건 크므로, \\(R^2\\)는 0~1의 값을 갖는다\n\\(R^2\\)가 0.7이면 70%의 설명력을 가진다\n\n주의할 점\n\n상관관계에 대해 분석한 것으로, 인과관계가 아니다 (인과관계라면 상관관계는 있을 수 있다)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#multiple-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#multiple-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\n변수와 계수를 추가하면 Multiple Regression이 됨\n\n수식이 길어지고, * 편미분이 여러개가 나오게 되는 차이\n변수가 2개가 되어 기존의 2차원평면/regression line이 아닌 3차원평면/regression plain이 나옴\nmultiple matrix를 활용해 표현 (row, column)\nRSS도 행렬식으로 표현\n\n역행렬은 엄청난 연산이 필요한데, 이는 least square를 사용하지 않게된 이유임 (정형데이터 위주였던 과거와 달리 이미지 등 데이터와 이에 대한 행렬이 매우 커져, computing성능발전에도 역행렬 계산 어려워짐)\nleast square는 Bias를 최소화하는 방법인데, trade-off로 Variance커질 위험이 커서 사용하지 않게 됨\n\n현대는 모델의 Variance가 중요해짐. Overfitting과 관계된 Variance를 줄이기 위해 약간의 Bias상승은 감내\nLeast square(최소자승법)는 오차가 0이되는 지점을 미분으로 찾는 방법이며, 이때의 오차는 Train Dataset으로 오차가 너무 작아지면 Overfitting\n\n\n\nCorrelation 상관관계\n\nVariable간의 상관관계를 보는 이유 : 중복되는 변수의 과대평가, 타 변수의 과소평가를 방지 (예를 들어 연봉과 자산의 경우 상관관계가 있음)\n\nGD(Gradient Descent, 경사하강법)\n\n역행렬의 계산량문제로 최소자승법(least square)가 아닌 GD방식을 사용\n\\(a\\) (learning rate)로 적절히 작은 숫자를 곱해, 한번에 너무 많이 이동(하강)하지 않게 함\n\n보통 0.01사용\n\n$W = W - a * $ $ c W$\n코드로 이해하기 \n\nCost를 가중치W로 미분 (gradient 함수로 $ c W$구하기)\n\n  # gradient 계산\n  gradients = g.gradient(cost, [W,b])     \n\n가중치W 업데이트 (apply_gradients 함수로 \\(W\\) 업데이트)\n\n  # gradients에 따라 W와 b 업데이트\n  optimizer.apply_gradients(zip(gradients, [W,b])) \n\nSingle Regression에서는 없던 Multiple Regression의 고민\n\n변수의 갯수 (많이 쓴다고 좋은게 아니며, 최적의 갯수 찾기)\n\nforward(↔︎backward) selection\n\n변수의 갯수를 늘려가다가(↔︎줄여가다가), 성능이 낮아질때 직전 갯수로 사용\n\n변수의 갯수가 달라졌을 때 성능의 지표\n\nAIC(Akaike Information Criterion)\nBIC(Bayesian Information Criterion)\nAdjusted \\(R^2\\)\nCV(Cross-validation)\n\n\n요즘은 Linear Regression에서 발전된 알고리즘이 많이 나와서, 위의 것보다 먼저 알아야 하는게 많음\n\nQualitative Predictor(Categorical, 범주형 변수)\n\n숫자가 아닌 상태로 쓰이는 경우 (0남자 1여자 등, 여러개도 Okay) 활용하여 Regression에 반영할 수 있게 됨\n\n예를 들어, 구하고자하는 y가 키(신장)이라면, 성별변수 남자가 0이면 가중치는 음수, 반대면 양수가 될 수 있음\n\n\nInteraction impact(Synergy impact)\n\n변수가 구하고자하는 y가 아니라 다른 변수에도 영향을 미치는 경우\n\n서로 관계있는 두 변수를 곱하여 추가해줌(\\(X^1, X^2\\)가 관계있는 경우 \\(X^1*X^2\\)라는 변수로)\n결과 표로 이해하기  \n\nradio의 p-value가 유의하지 않아 변수제거를 했을 때, 파생변수인 radio*TV는 어떻게 할까?\n\n제거한다 (오리지널 변수를 제거한다면 파생변수도 제거, 오리지널이 있을때만 사용가능)\nHierarchy principle : 파생변수가 존재하려면 오리지널 변수도 있어야 함\n\nCoefficient 값 기준으로 radio가 TV보다 더 중요한 변수인가?\n\nCoefficient는 단위(unit)를 간과하므로, 높다고 해서 반드시 중요한 것은 아니다\n\n\n\n변수(feature)의 관계는 독립적인게 좋음\n\nNon-linearity의 Multiple regrssion 활용한 표현\n\n예를 들어 전반부는 정비례 / 후반부는 반비례 한다면\n\n\\(B_1 *나이 + B_2 * 나이^2\\) 와 같이 표현 가능\n위의 표현식은 변수의 독립성을 저해하는가?(사용해도 되나?)\n\n서로 다른관계를 묘사하는 것이라면 사용 가능\n\n연봉, 자산 모두 대출점수의 양의 상관관계라면 사용 불가\n\\(B_1 *나이\\)는 양의 상관관계, \\(B_2 * 나이^2\\)는 음의 상관관계라면 사용 가능\n\n\n\n다만 변수가 많아지면 한계가 있으므로, 비선형에 적합한 타 모델이 더 좋음\n\n상관관계를 빠르게 파악하는 법 : 모든 산점도(Scatter plot)를 그려보기\n\n미리 파악하여 상관관계가 있는 것을 빼고 모델링하면 더 좋은 결과가 나타남\n중복변수가 들어가거나 하더라도, Robust한 모델을 사용하는 것도 좋은 방법"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#logistic-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#logistic-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLinear regression으로 현재까지 예측문제를 풀었다면, Logistic regression으로는 분류문제를 풀 수 있다\n머신러닝으로 푸는 큰 3가지 문제 : 예측 vs 분류 vs 클러스터링\n\n분류 : 메일이 왔을 때 스팸인가 아닌가, 신용카드승인내역이 이상거래인가 정상거래인가\n\nHyphothesis의 반영\n\n기존의 Linear regression(\\(y=Wx+b\\))의 식으로는 놓치는 case가 많이 생김\nLogistic(Sigmoid) function을 활용하여 해결 가능  \n\nz가 아무리 커지거나 작아져도 0~1사이에서 벗어나지 않음\n0.5를 기준으로 판단가능 (Pass/Fail, 스팸/정상 등)\nz부분에 기존에 데이터를 학습했던 Linear regression식(\\(y=Wx+b\\))을 넣으면(plug-in), 분류문제를 푸는 함수로 바꿈\n값이 0.7이 나온다면, Pass확률이 70%인 것으로 해석가능\n\n\nDescision boundary 경계영역\n\n예를 들어 2가지 Case를 분류하는 Linear Line이 있다면, 그것이 Descision boundary (Non Linear한 경우라도 분류문제를 풀 수 있고, 그 Line은 Descision boundary)\n\nCost function에서는 문제가 생김\n\nSigmoid function으로 간편하게 분류문제를 푸는 함수로 바꿨지만, 기존처럼 미분을 하면 문제 발생\n\n값이 조금만 크거나 작아져도 기울기가 0이 됨 → 기존의 Mean Square방식 적용 불가  \nGD 사용시 최적 지점까지 가지 못하는 경우 발생\n\nCross entropy를 사용하게 됨\n  [Cost function으로 사용되기 위한 2가지 조건]\n  1. 클수록 나쁘고 작을수록 좋아야 함\n  2. 미분이 가능해야 함\n    (미분이 안되는 경우 : 평평한 부분이 있거나, 위아래로 변동이 큰 구간이 많은 경우)\n\n  * 두 조건을 충족하는 것은 쉽지 않으며, Accuracy는 1번만 충족\n  * Cross entropy는 위의 2개 조건을 모두 충족\n\n2가지 상황(y=0[pass], y=1[fail])에 대한 상황에 대해 다른 식 사용\n\n필요시 하나의 식으로도 나타낼 수 있음 (y값에 따라 한쪽 식이 0이 되는 형태)\n\n현대에서도 많이 쓰이는 함수인 Cross entropy (gpt4 등)\nGD경사하강법으로도 사용 가능"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#모형예측classification의-성과평가confusion-matrix",
    "href": "posts/meta-dl-creditcard-20240602/index.html#모형예측classification의-성과평가confusion-matrix",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "모형예측(Classification)의 성과평가(Confusion matrix)",
    "text": "모형예측(Classification)의 성과평가(Confusion matrix)\n\nAccuracy(정답률) : 실제값 중 맞춘 비율\n\n분류가 Imbalance한 경우 한계가 있음(신용카드 경우도 대다수가 정상거래) (예를 들어 암환자 비율이 90%이상일 때, 로직없이 그냥 암환자로만 판정해도 정답률 높음)\n\nPrecision(암으로 판정한 사람 중, 실제로 암) vs Recall(실제 암인 사람 중, 암으로 판정된 사람)\n\n암환자를 정상환자로 판단하는 것은 치명적\n암과 같은 케이스는 기본적으로 암으로 ’판단’하는 것이 많아져야하므로 Recall을 사용 (분자가 ’암 판정’인, 분자가 커질수록 점수가 높은 recall을 사용하는게 적절)\n프로젝트 특성에 따라 적합한 모델 뿐 아니라 적합한 지표를 사용하는 것이 중요\n\nPrecision이 더 중요한 케이스\n\n불량제품 하나를 검수하기 위해 1만개의 정상제품을 검사하면 비효율적\n1개의 불량제품을 감수하고 1만개를 살리는게 효율적\n\n분류가 balance한지 imbalance한지를 체크\nF1 score : Precision과 Recall의 조화평균\nConfusion matrix를 그래프로 visualize 해서 보여주면 더 설득에 용이"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html",
    "href": "posts/meta-dl-creditcard-20240526/index.html",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 1주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240526/index.html#개요",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 1주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-pytorch",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-pytorch",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensorflow, Pytorch",
    "text": "Tensorflow, Pytorch\n\n프레임워크 2가지 존재 : Tensorflow(텐서플로우, 구글), Pytorch(파이토치, 페이스북)\n\n알파고(딥마인드)시점까지는 ai의 90퍼센트 이상은 텐서플로우로 구현되었으나, 후발주자로 페이스북이 파이토치를 만들고 경쟁구도가 되었음\n\n파이토치 vs 텐서플로우\n\n파이토치 : high레벨에 가까운 pythonic함 (사람의 직관에 가까운, 추상화된) 대학원 등 교육 쪽에서 많이 사용(구현해보는 것에 중점) 사용자 증가로 긍정적 생태계 조성(텐서플로우에는 없는 함수가 개발될 수 있고, 디버깅 쉬워짐[참고Case많음], 참고강의 많음)\n텐서플로우 : low레벨에 가까운 효율성 (기계가 이해하기 쉬운 C언어와 같은) 산업 등 비즈니스 영역에서 많이 사용(효율성 중시)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensor",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensor",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensor",
    "text": "Tensor\n\nTensor : ai에서 사용하는 배열 (차원=rank), 고차원의 데이터 저장(숫자뿐 아니라 문자도 가능)\n\n0차원 scalar / 1차원 vector / 2차원 matric (2d tensor) /3차원 3 tensor (3d tendor) / N차원 N tensor\n참고영상 : https://youtu.be/m0qwxNA7IzI?si=FeyWcPYuun7T_QON\n\n\n\n\n\nimage.png\n\n\n\n고차원/비정형 데이터 필요성 예시\n\n이미지 데이터(흑백)는 3d tensor 필요\n이미지 데이터(컬러)는 4d tensor 필요\n영상 데이터는 5d tensor 필요\n(결론→) 비정형데이터의 처리에 있어 tensor가 필요\n\n기존에는 매출, 성장률 등 숫자(정형 데이터)만 썼다면, 이제는 이미지(비정형 데이터) 등도 데이터 분석에 사용하기 시작함"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-실습-constant-rank",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-실습-constant-rank",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensorflow 실습 (constant, rank)",
    "text": "Tensorflow 실습 (constant, rank)\n\n텐서플로우 2.0의 차이 &gt; 즉시 실행모드(Eager Mode)지원 (1.x버전에서는 그래프를 생성하고 초기화하는 등 별도 작업이 필요했었음) \nRank(축) : 차원의 수\nShape(형상) : 0, 1, 2차원 등 데이터의 차원\ndtype : string, float32, float16, int32, int8 등 데이터 타입\n\n\nimport numpy as np\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n2.16.1\n\n\n\na = tf.constant(2)\nb  = tf.constant([2,3])\nc = tf.constant([[2,3],[6,7]])\nd = tf.constant(['hello'])\n\nprint('[tf.rank 차원의 수 출력]')\nprint(tf.rank(a))\nprint(tf.rank(b))\nprint(tf.rank(c))\nprint(tf.rank(d))\nprint()\n\nprint('[변수 자체 출력]')\nprint(a)\nprint(b)\nprint(c)\nprint(d)\n\n[tf.rank 차원의 수 출력]\ntf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\n\n[변수 자체 출력]\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor([2 3], shape=(2,), dtype=int32)\ntf.Tensor(\n[[2 3]\n [6 7]], shape=(2, 2), dtype=int32)\ntf.Tensor([b'hello'], shape=(1,), dtype=string)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#계산-add-subtract-multiply",
    "href": "posts/meta-dl-creditcard-20240526/index.html#계산-add-subtract-multiply",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "계산 (add, subtract, multiply)",
    "text": "계산 (add, subtract, multiply)\n\nadd, subtract, multiply (+, -, * 기호로도 가능)\n\n\na = tf.constant(3)\nb = tf.constant(2)\n\n\nprint(tf.add(a,b))\nprint(a+b)\n\ntf.Tensor(5, shape=(), dtype=int32)\ntf.Tensor(5, shape=(), dtype=int32)\n\n\n\nprint(tf.subtract(a,b))\nprint(a-b)\n\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\n\n\n\nprint(tf.multiply(a,b))\nprint(a*b)\n\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#numpy와-tensor의-변환",
    "href": "posts/meta-dl-creditcard-20240526/index.html#numpy와-tensor의-변환",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "numpy와 tensor의 변환",
    "text": "numpy와 tensor의 변환\n\n둘 다 데이터를 담는 container\n학습이 잘되고 있는지, 중간결과 등을 확인할때 numpy형태로 cpu로 확인\n실제 계산은 tensor 형태로 gpu에서 수행\n\n\n# numpy()\nc = (a+b)\n\nprint(c)\nprint(type(c))\n\nprint()\nprint(c.numpy())\nprint(type(c.numpy()))\n\ntf.Tensor(5, shape=(), dtype=int32)\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;\n\n5\n&lt;class 'numpy.int32'&gt;\n\n\n\n# tf.convet_to_tensor()\nc_sqrt = np.sqrt(c,dtype=np.float32)\nc_tensor = tf.convert_to_tensor(c_sqrt)\n\nprint(c_sqrt)\nprint(type(c_sqrt))\n\nprint()\nprint(tf.convert_to_tensor(c_sqrt))\nprint(type(tf.convert_to_tensor(c_sqrt)))\n\n2.236068\n&lt;class 'numpy.float32'&gt;\n\ntf.Tensor(2.236068, shape=(), dtype=float32)\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#linear-regression",
    "href": "posts/meta-dl-creditcard-20240526/index.html#linear-regression",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n추세선 등을 그어서 쉽게 판단 가능 + X(투입)에 대한 Y(산출)을 판단해 데이터에 기반한 정량적 판단(객관적 증거)\n선형적관계 &gt; \\(Y = f(X) + \\epsilon\\) 을 기본적인 ML/DL의 식이라고 할 때,  \\(f\\)의 관계가 선형적 관계가 있다고 가정할 때 Linear regression  \\(income = f(education, seniority) + \\epsilon\\) 와 같은 예시를 들 수 있음\nSingle/Multi regression이 있다\n\n\n[용어정리]\nresponse, target : Y값 feature, input, predictor : X값 \\(\\epsilon\\)(엡실론) : 오차 \\(\\hat{x}\\) : 예측값 \\(x\\) (위의 기호는 hat)\n\n\n비선형적 관계를 다루는 모델을 사용하여 오차를 줄일 수 있음 단 오차를 0으로 만드는게 무조건 좋은 것은 아님 → 다뤄보지 못한 데이터가 나오면 성능이 떨어짐(과적합 overfitting)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#interpretability-vs-flexibility",
    "href": "posts/meta-dl-creditcard-20240526/index.html#interpretability-vs-flexibility",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "interpretability vs flexibility",
    "text": "interpretability vs flexibility\n &gt;[용어정리]\nflexibility : 성능, performance (100문제중 80문제를 찾추는가) interpretability : 해석 Least Squares : Linear regression\n\nDeep learning : 높은 성능 / 낮은 해석능력  효과적이고 빠름, 비선형적이면 좋은 모델을 뽑을 수 없음(선형이라는 가정 자체가 틀린 시작)\nLeast Squares : 성능은 DL보다 낮지만 높은 해석능력 (전통적 통계학의 기반을 둔 머신러닝, Statistical ML) 많은 시간과 데이터 필요, 비선형적 관계를 잘 모델링\n성능이 좋으니 DL만 사용? &gt; 관련 사례 DL로 판단하여 대출거절한 것은 차별에 해당한다는 판례 대출거절에 대해 근거를 제시해야하지만(자산부족 등), DL은 해석능력(interpretability)이 낮아 설명할 근거가 부족함 Least Squares(ㅣinear regression)로 근거를 제시했다면 차별이 되지 않았을 것"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#assessing-model-accuracy",
    "href": "posts/meta-dl-creditcard-20240526/index.html#assessing-model-accuracy",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "assessing model accuracy",
    "text": "assessing model accuracy\n\nerror의 정의 : 예측값과 실제값의 차이\n\n평균에 제곱한 에러 총합 MSE(Mean Squared Error), 줄일수록 좋음\n\n제곱을 활용하는 이유 : 나중에 미분(에러가 최소화되는 지점찾기)를 하는데, 이를 위해서 함 (다음 강의에서 설명예정)\n\\(MSE_{TR}\\)(Training set), \\(MSE_{TE}\\)(Test set)\n\n데이터 특성(복잡/단순)과 모델 적용(복잡/단순)에 따른 Training, Test MSE추이 \n\n검정(정답), 노랑(Linear), 초록(Smoothing splines)/ 빨강(\\(MSE_{TR}\\)), 회색(\\(MSE_{TE}\\))\n단순한 데이터에 복잡한 모델(초록)을 사용하니 과적합 발생 \n데이터가 복잡하지 않아 Linear를 사용하니 Error추이(우측그래프)도 좋음"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#bias-vs-variance-의-trade-off",
    "href": "posts/meta-dl-creditcard-20240526/index.html#bias-vs-variance-의-trade-off",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Bias vs Variance 의 Trade off",
    "text": "Bias vs Variance 의 Trade off\n\nerror는 Variance와 Bias로 이루어져 있다 &gt; Variance(V), Bias(B) 예시 (과녁) V 낮음 B 낮음 : 정중앙에 잘 모여있음 V 낮음 B 높음 : 잘 모여있지만 위치가 잘못됨 V 높음 B 낮음 : 정답 근처이지만 불안정하게 퍼져있음 V 높음 V 높음 : 정답 근처도 아니고, 불안정하게 퍼져있음\nBias낮음 : Training에서 적중률이 높다, Overfitting 정답 자체를 틀리는 것과 관계\nVariance높음 : 모델이 불안정하다 변동에 과민하게 반응하는 것과 관계 (결과의 극단적 변화, 무의미한 결과는 무의미한 것으로 간주해야 안정적인 모델)\n둘 다 낮추기는 힘듦 (B낮추려면 V높아짐, B낮추려면 V 높아짐)\n둘 다 낮추기 위한 단 하나의 방법 : 데이터를 추가한다\nBias와 Variance를 고려한 가장 error가 적은 부분 Sweet spot"
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html",
    "href": "posts/coach-ds-20230506/index.html",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "",
    "text": "국민건강보험공단 건강검진정보를 활용한 간단한 분석 공공데이터포털 국민건강보험공단_건강검진정보\n실습 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html#개요",
    "href": "posts/coach-ds-20230506/index.html#개요",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "",
    "text": "국민건강보험공단 건강검진정보를 활용한 간단한 분석 공공데이터포털 국민건강보험공단_건강검진정보\n실습 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html#데이터-불러오기",
    "href": "posts/coach-ds-20230506/index.html#데이터-불러오기",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('국민건강보험공단_건강검진정보_20221231.CSV', encoding=\"cp949\")\n\n\ndf.columns\n\nIndex(['기준년도', '가입자일련번호', '시도코드', '성별', '연령대코드(5세단위)', '신장(5cm단위)',\n       '체중(5kg단위)', '허리둘레', '시력(좌)', '시력(우)', '청력(좌)', '청력(우)', '수축기혈압',\n       '이완기혈압', '식전혈당(공복혈당)', '총콜레스테롤', '트리글리세라이드', 'HDL콜레스테롤', 'LDL콜레스테롤',\n       '혈색소', '요단백', '혈청크레아티닌', '혈청지오티(AST)', '혈청지피티(ALT)', '감마지티피', '흡연상태',\n       '음주여부', '구강검진수검여부', '치아우식증유무', '치석'],\n      dtype='object')\n\n\n\n음주여부, 흡연상태, 연령대코드, 성별코드간의 상관관계\n\n상관계수 구하기\n\n\nfiltered = df[['음주여부','흡연상태','연령대코드(5세단위)','성별']].copy()\nfiltered.corr()\n\n\n\n\n\n\n\n\n\n음주여부\n흡연상태\n연령대코드(5세단위)\n성별\n\n\n\n\n음주여부\n1.000000\n0.252058\n-0.357367\n-0.254963\n\n\n흡연상태\n0.252058\n1.000000\n-0.073601\n-0.536833\n\n\n연령대코드(5세단위)\n-0.357367\n-0.073601\n1.000000\n0.042162\n\n\n성별\n-0.254963\n-0.536833\n0.042162\n1.000000\n\n\n\n\n\n\n\n\n\n상관계수 시각화 (Seaborn heatmap)\n\n\n#mask옵션 사용하고, annot옵션으로 수치표기, fmt옵션으로 소수자리 지정하여 heatmap으로 표현\nmask = np.triu(np.ones_like(filtered.corr(), dtype=bool))\nsns.heatmap(filtered.corr(), annot=True, fmt='.2f', mask=mask)\n\n\n\n\n\n\n\n\n\n상관관계가 강한 순으로 나열\n\n성별과 흡연, 음주와 연령의 뚜렷한 상관관계\n흡연과 음주의 약한 상관관계 &gt; [참고] -1.0과 -0.7 사이이면, 강한 음적 선형관계 -0.7과 -0.3 사이이면, 뚜렷한 음적 선형관계 -0.3과 -0.1 사이이면, 약한 음적 선형관계 -0.1과 +0.1 사이이면, 거의 무시될 수 있는 선형관계 +0.1과 +0.3 사이이면, 약한 양적 선형관계 +0.3과 +0.7 사이이면, 뚜렷한 양적 선형관계 +0.7과 +1.0 사이이면, 강한 양적 선형관계\n\n\n\n#상관계수로 표를 만들어, 강한 상관관계가 있는 순으로 조사해보았습니다 (절대값으로 내림차순)\ncon_index = []\ncon_value = []\ncon_value_abs= []\nfor i in filtered.corr().columns:\n    for j in filtered.corr().index:\n        con_index.append(i +\"/\" + j)\n        con_value.append(filtered.corr()[i][j])\n        con_value_abs.append(abs(filtered.corr()[i][j]))\npd.DataFrame({'상관계수':con_value,\n             '절대값':con_value_abs,\n             '항목':con_index}).sort_values(by=['절대값'],ascending=False)\n\n\n\n\n\n\n\n\n\n상관계수\n절대값\n항목\n\n\n\n\n0\n1.000000\n1.000000\n음주여부/음주여부\n\n\n5\n1.000000\n1.000000\n흡연상태/흡연상태\n\n\n10\n1.000000\n1.000000\n연령대코드(5세단위)/연령대코드(5세단위)\n\n\n15\n1.000000\n1.000000\n성별/성별\n\n\n7\n-0.536833\n0.536833\n흡연상태/성별\n\n\n13\n-0.536833\n0.536833\n성별/흡연상태\n\n\n2\n-0.357367\n0.357367\n음주여부/연령대코드(5세단위)\n\n\n8\n-0.357367\n0.357367\n연령대코드(5세단위)/음주여부\n\n\n3\n-0.254963\n0.254963\n음주여부/성별\n\n\n12\n-0.254963\n0.254963\n성별/음주여부\n\n\n1\n0.252058\n0.252058\n음주여부/흡연상태\n\n\n4\n0.252058\n0.252058\n흡연상태/음주여부\n\n\n6\n-0.073601\n0.073601\n흡연상태/연령대코드(5세단위)\n\n\n9\n-0.073601\n0.073601\n연령대코드(5세단위)/흡연상태\n\n\n11\n0.042162\n0.042162\n연령대코드(5세단위)/성별\n\n\n14\n0.042162\n0.042162\n성별/연령대코드(5세단위)\n\n\n\n\n\n\n\n\n\n\n흡연과 음주에 대한 추이\n\n앞서 약한 상관관계였던 흡연과 음주에 대한 인원추이\n\n\n# 코드를 실제값으로 변환\nsmoke = {1 : \"흡연안함\", 2: \"끊음\", 3: \"흡연중\"}\ndrink = {0: \"안마심\", 1: \"마심\"}\n\ndf['흡연상태'] = df['흡연상태'].replace(smoke)\ndf['음주여부'] = df['음주여부'].replace(drink)\n\n\n# '흡연상태','음주여부'만 가져와 crosstab\nfiltered_drink_smoke = df[['흡연상태','음주여부']]\ndf_crosstab = pd.crosstab(index=filtered_drink_smoke['음주여부'],columns=filtered_drink_smoke['흡연상태'])\ndf_crosstab\n\n\n\n\n\n\n\n\n흡연상태\n끊음\n흡연안함\n흡연중\n\n\n음주여부\n\n\n\n\n\n\n\n마심\n132750\n360530\n161804\n\n\n안마심\n33815\n280441\n30504\n\n\n\n\n\n\n\n\n\n# countplot 시각화\nsns.set_style('whitegrid')\nplt.rc('font',family=\"Malgun Gothic\")\nsns.countplot(data=filtered_drink_smoke, x='흡연상태', hue='음주여부', palette='Set1').set_title('흡연상태별 음주여부') # palette = 그래프테마\n\nText(0.5, 1.0, '흡연상태별 음주여부')\n\n\n\n\n\n\n\n\n\n\n흡연중이거나 했다가 끊은 경우에는 음주하는 사람의 비중이 높음\n흡연을 안하는 경우는 음주여부의 비중 차이가 크지 않음\n\n\n\n음주여부에 따른 콜레스테롤과 감마지티피의 관계\n\n감마지티피 : 알콜에 의한 간장애의 지표를 나타내는 검사항목\n각 항목을 산점도로 시각화하여 파악\n\n\ndf_temp = df[['총콜레스테롤','감마지티피','음주여부','흡연상태']]\n\n\n# lmplot 산점도\nplt.figure(figsize=(14,14))\nsns.lmplot(data=df_temp, x='총콜레스테롤', y='감마지티피', hue='음주여부', col='흡연상태',markers=['x','o'] # markers로 음주여부에 따라 o,x로\n           ,scatter_kws={'s':20}) # 뭉쳐진부분이 잘보이도록 scatter_kws로 점크기조정\n\n&lt;Figure size 1400x1400 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n대체로 특정 구역에 몰려있음, 체중 120kg 이상에 대해서 추가분석\n\n\nweight_over120 = df.loc[(df['체중(5kg단위)'] &gt;= 120), ['총콜레스테롤','감마지티피','음주여부','흡연상태']]\nplt.figure(figsize=(14,14))\nsns.lmplot(data=weight_over120, x='총콜레스테롤', y='감마지티피', hue='음주여부', col='흡연상태',markers=['x','o']\n           ,scatter_kws={'s':20})\n\n&lt;Figure size 1400x1400 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n흡연중인 경우, 흡연을 안하거나 끊은 경우보다 감마지피티의 값이 높게 분포\n음주여부에 따라 콜레스테롤의 차이는 크게 보이지 않으나, 감마지티피의 경우 모두 상승하는 경향\n\n\n\n연령대별 시력 확인\n\n# 별도로 정의해둔 연령코드 딕셔너리\nage_code = {1: '0~4세',\n 2: '5~9세',\n 3: '10~14세',\n 4: '15~19세',\n 5: '20~24세',\n 6: '25~29세',\n 7: '30~34세',\n 8: '35~39세',\n 9: '40~44세',\n 10: '45~49세',\n 11: '50~54세',\n 12: '55~59세',\n 13: '60~64세',\n 14: '65~69세',\n 15: '70~74세',\n 16: '75~79세',\n 17: '80~84세',\n 18: '85세+'}\n\n# 실명(시력 9.9) 제거\nsight_right = df.drop(df.loc[df['시력(우)']==9.9,].index)\nsight_left = df.drop(df.loc[df['시력(좌)']==9.9,].index)\n\n#숫자료 표기되는 연령대코드를 연령구간으로 표기할 index 생성 후 replace로 변경 (barplot의 index로 사용)\nage_code_right = []\nfor i in np.sort(sight_right['연령대코드(5세단위)'].unique()).tolist():\n    age_code_right.append(age_code[i])\nage_code_left = []\nfor i in np.sort(sight_left['연령대코드(5세단위)'].unique()).tolist():\n    age_code_left.append(age_code[i])\n\nsight_right['연령대코드(5세단위)'] = sight_right['연령대코드(5세단위)'].replace(age_code)\nsight_left['연령대코드(5세단위)'] = sight_left['연령대코드(5세단위)'].replace(age_code)\n\n\n#시력(좌), (우) 그래프를 식별하기 용이하도록 (좌)그래프를 회전\nfig, axs = plt.subplots(ncols=2, figsize=(12,5),\n                       gridspec_kw={'wspace':0.5},)\nfig.suptitle('연령별 시력 평균(좌, 우)').set_size(20) #제목\n\nsns.set_context('talk')\n\nsns.barplot(data = sight_left, x='시력(좌)', y='연령대코드(5세단위)', hue='성별', orient='h', errorbar=None, ax=axs[0],\n            order=age_code_left).invert_xaxis()\nsns.barplot(data = sight_right, x='시력(우)', y='연령대코드(5세단위)', hue='성별', orient='h', errorbar=None, ax=axs[1],\n            order=age_code_right)\n\nfor ax in axs:\n    ax.set_ylabel('연령대') # 코드가 아니므로 y축 이름을 연령대로 변경\n\n\n\n\n\n\n\n\n\n모든 연령대에서 특정 성별의 시력이 높음\n연령이 높아질수록 시력도 낮아지며, 좌우시력의 큰 차이는 보이지 않음"
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html",
    "href": "posts/coach-ds-20221113/index.html",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "",
    "text": "KOSIS 지역별 / 상품군별 온라인쇼핑 해외직접판매액 데이터를 활용한 간단한 분석 KOSIS 상품군별 온라인쇼핑 해외직접판매액\n실습 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#개요",
    "href": "posts/coach-ds-20221113/index.html#개요",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "",
    "text": "KOSIS 지역별 / 상품군별 온라인쇼핑 해외직접판매액 데이터를 활용한 간단한 분석 KOSIS 상품군별 온라인쇼핑 해외직접판매액\n실습 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#분석",
    "href": "posts/coach-ds-20221113/index.html#분석",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "분석",
    "text": "분석\n\n데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('online_export.csv')#, encoding=\"cp949\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\n국가(대륙)별\n상품군별\n판매유형별\n시점\n데이터\n\n\n\n\n0\n0\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.1/4\n1054\n\n\n1\n1\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.2/4\n946\n\n\n2\n2\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.3/4\n791\n\n\n3\n3\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.4/4\n854\n\n\n4\n4\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2018.1/4\n2033\n\n\n\n\n\n\n\n\n\n데이터 확인 중 시점 컬럼에 p가 붙어있어서 의미를 확인해봄\n\nKOSIS주석 : e: 추정치, p: 잠정치, -: 자료없음, …: 미상자료, x: 비밀보호, ▽: 시계열 불연속\n\n\n\ndf['시점'].unique()\n\narray(['2017.1/4', '2017.2/4', '2017.3/4', '2017.4/4', '2018.1/4',\n       '2018.2/4', '2018.3/4', '2018.4/4', '2019.1/4', '2019.2/4',\n       '2019.3/4', '2019.4/4', '2020.1/4', '2020.2/4', '2020.3/4',\n       '2020.4/4', '2021.1/4', '2021.2/4', '2021.3/4', '2021.4/4',\n       '2022.1/4', '2022.2/4 p)'], dtype=object)\n\n\n\n\n시점 데이터의 분류 (연도, 분기의 구분), 단위표기\n\ndf['연도'] = df['시점'].map(lambda x : x.split('.')[0])\ndf['분기'] = df['시점'].map(lambda x : x.split('.')[1].split('/')[0])\ndf[['연도','분기']] = df[['연도','분기']].astype(int)\n\ndf = df.rename(columns={'데이터':'백만'})\ndf\n\n\n\n\n\n\n\n\n\nUnnamed: 0\n국가(대륙)별\n상품군별\n판매유형별\n시점\n백만\n연도\n분기\n\n\n\n\n0\n0\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.1/4\n1054\n2017\n1\n\n\n1\n1\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.2/4\n946\n2017\n2\n\n\n2\n2\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.3/4\n791\n2017\n3\n\n\n3\n3\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.4/4\n854\n2017\n4\n\n\n4\n4\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2018.1/4\n2033\n2018\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2767\n2767\n기타\n기 타\n면세점 이외\n2021.2/4\n1278\n2021\n2\n\n\n2768\n2768\n기타\n기 타\n면세점 이외\n2021.3/4\n1154\n2021\n3\n\n\n2769\n2769\n기타\n기 타\n면세점 이외\n2021.4/4\n1076\n2021\n4\n\n\n2770\n2770\n기타\n기 타\n면세점 이외\n2022.1/4\n2325\n2022\n1\n\n\n2771\n2771\n기타\n기 타\n면세점 이외\n2022.2/4 p)\n725\n2022\n2\n\n\n\n\n2772 rows × 8 columns\n\n\n\n\n\n\n데이터 자체에 대한 분석 (describe)\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\n백만\n연도\n분기\n\n\n\n\ncount\n2772.000000\n2772.000000\n2772.000000\n2772.000000\n\n\nmean\n1385.500000\n1613.116522\n2019.272727\n2.409091\n\n\nstd\n800.351798\n4273.426107\n1.600909\n1.114533\n\n\nmin\n0.000000\n-10003.000000\n2017.000000\n1.000000\n\n\n25%\n692.750000\n41.750000\n2018.000000\n1.000000\n\n\n50%\n1385.500000\n291.500000\n2019.000000\n2.000000\n\n\n75%\n2078.250000\n1181.000000\n2021.000000\n3.000000\n\n\nmax\n2771.000000\n47689.000000\n2022.000000\n4.000000\n\n\n\n\n\n\n\n\n\n특이하게도 최소값에 음수(-10003)이 있음\n\n해외역직구와 같은 경향인가 추측해보았지만, 전체 국가/연도에서 단 1개값만 그렇다고 보기는 어려움\n이상치로 보여 제거하고 분석을 진행하면 어떨까 싶음\n\n\n\n\n국가별 판매액에 대한 heatmap을 통한 파악\n\nsales_country = df.pivot_table(values='백만', index='국가(대륙)별', columns='연도', aggfunc='sum')\nsns.heatmap(data=sales_country, annot=True, fmt='.0f', cmap='Blues')\n\n\n\n\n\n\n\n\n\n미국, 일본, 중국의 판매액이 두드러짐\n\n\n\n판매액 상위 3개국에 대한 판매액 추세 시각화\n\nsns.lineplot(data=df[df['국가(대륙)별'].isin(['미국','중국','일본'])], \n             x='연도',y='백만',hue='국가(대륙)별', errorbar=None, estimator='sum'\n            )\nplt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0) #bbox_to_anchor(그래프와의 관격, 위/아래 위치)\n\n\n\n\n\n\n\n\n\n2020, 2021년도 급격한 하락이 보임\n\n당시의 큰 이벤트로는 코로나가 있으며, 해당 시기의 국가봉쇄/여객기 항편취소/화물기 감편 등이 원인일 것으로 보임\n\n\n\n\n2020년 국가별 주요 상품군 분석\n\n판매액 비중이 높은 주요 국가에 대해 분석 진행\n\n\ndf_2020 = df[df['연도']==2020].groupby(['국가(대륙)별','분기'])['백만'].sum().unstack().copy()\nsns.heatmap(data=df_2020, annot=True, fmt='.0f', cmap='Blues')\n\n\n\n\n\n\n\n\n\n주요 3개국(미국, 일본, 중국)에 대한 제품군별 판매액 분석\n\n\nfilter_rule = (df['연도']==2020) & (df['국가(대륙)별'].isin(['미국','중국','일본']))\ndf_2020_top3 = df[filter_rule][['국가(대륙)별','상품군별','백만']]\n\n# 잘 팔리는 상품군을 딕셔너리에 저장\nbest_category = {}\nbest2_category = {}\nfor i, country in enumerate(df_2020_top3['국가(대륙)별'].unique()):\n    filter_country = df_2020_top3['국가(대륙)별'] == country\n    globals()[country] = df_2020_top3[filter_country].groupby(['상품군별'])['백만'].sum().copy()\n    best_category[i] = globals()[country].sort_values(ascending=False).index[0]\n    best2_category[i]= globals()[country].sort_values(ascending=False).index[1]\n\n#그래프 기본 틀\nfig, axs = plt.subplots(ncols=3, figsize=(15,8), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.7},) #gridspec으로 그래프 사이 여백 설정\n\n#그래프 그리기 (반복문으로 그래프를 그리고, 범례설정과 국가/1위카테고리를 입력한 제목 생성)\nsub_title=[]\nfor i, country  in enumerate(df_2020_top3['국가(대륙)별'].unique()):\n    globals()[country].plot(kind='pie',startangle=145, autopct='%.1f%%', ax=axs[i], pctdistance=0.8, #rotatelabels=True,\n                           )\n    sub_title.append(\"[\"+country+\"]\")  \n    axs[i].set_title(sub_title[i]+ chr(10) +best_category[i])\n    axs[i].set_ylabel('')\n    axs[i].labels=None\n\n\n\n\n\n\n\n\n\n국가별 판매액 상위\n\n미국 : 의류 및 패션관련상품\n중국 : 음반 비디오 악기\n일본 : 의류 및 패션관련 상품\n\n4분기 판매가 두드러지는데, 특별히 비중이 많은 상품이 있는지 확인\n\n\nfilter_rule = (df['연도']==2020) & (df['국가(대륙)별'].isin(['미국','중국','일본'])) & (df['분기']==4)\ndf_2020_top3_quarter4 = df[filter_rule][['국가(대륙)별','상품군별','백만']]\n\n# 잘 팔리는 상품군을 딕셔너리에 저장\nbest_category = {}\nbest2_category = {}\nfor i, country in enumerate(df_2020_top3_quarter4['국가(대륙)별'].unique()):\n    filter_country = df_2020_top3_quarter4['국가(대륙)별'] == country\n    globals()[country] = df_2020_top3_quarter4[filter_country].groupby(['상품군별'])['백만'].sum().copy()\n    best_category[i] = globals()[country].sort_values(ascending=False).index[0]\n    best2_category[i]= globals()[country].sort_values(ascending=False).index[1]\n\n#그래프 기본 틀\nfig, axs = plt.subplots(ncols=3, figsize=(15,8), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.7},) #gridspec으로 그래프 사이 여백 설정\n\n#그래프 그리기 (반복문으로 그래프를 그리고, 범례설정과 국가/1위카테고리를 입력한 제목 생성)\nsub_title=[]\nfor i, country  in enumerate(df_2020_top3_quarter4['국가(대륙)별'].unique()):\n    globals()[country].plot(kind='pie',startangle=145, autopct='%.1f%%', ax=axs[i], pctdistance=0.8, #rotatelabels=True,\n                           )\n    sub_title.append(\"[\"+country+\"]\")  \n    axs[i].set_title(sub_title[i]+ chr(10) +best_category[i])\n    axs[i].set_ylabel('')\n    axs[i].labels=None\n\n\n\n\n\n\n\n\n\n4분기 국가별 판매액 상위 (변동없음)\n\n미국 : 의류 및 패션관련상품\n중국 : 음반 비디오 악기\n일본 : 의류 및 패션관련 상품\n\n비중이 아닌 상품군별 규모를 확인해봄\n\n음반 비디오 악기가 가장 규모가 컸고, 다음으로 의류 및 패션관련 상품의 규모가 컸음\n\n\n\n# 상위 3개국가에 대해 상품군별 시각화해서 분석함\nplt.figure(figsize=(20,5))\nsns.barplot(data=df_2020_top3, x='상품군별', y='백만', hue='국가(대륙)별', errorbar=None, dodge=False)\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=6, borderaxespad=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n미국, 일본은 ‘의류 및 패션상품’ 판매가 두드러지므로 해외직구활성화 방안 등을 강구한다면 해당 업종을 눈여겨 봐야할 듯 함\n두번째로는 ‘화장품’ 판매실적이 좋은데, 분류가 되어있지 않으니 기초/색조 여부 등을 보긴 해야겠지만\n\n기초화장품 쪽 주력인 업체가 있다면 진출을 고민하는 등의 방향을 고민해보면 좋을 듯 하고\n색조화장품 쪽 주력인 업체라면 해당 국가들의 미의 기준(선호색 등)을 파악하며 시작하면 좋을 듯 함\n\n중국은 ‘음반 비디오 악기’ 판매 실적이 좋은데, 2020년이라면 한류가 원인 중 하나가 아닐까 싶음\n\n간단히 구글링을 했을 때 ’한국구제문화교류진흥원’이란 곳의 보고서에 따르면 아래와 같은 상태 부분이 있음\n\n20년 1월~7월의 한국 3대 엔터테인먼트들의 주가는 상승했고, 원인 중 하나가 글로벌 팬덤의 확대와 빌보드 차트 진입과 중국 현지화 전략의 순항이라 함 (물론 위에서 말한 원인은 구체적 자료를 제시한 내용은 아님)\n중국판 미생의 방영 등 한류 자체는 긍정적인 상황으로 보임(물론 중국판 미생이 `18년 제작을 마쳤으니 위의 수치엔 영향 없을듯 함)\n\n\n중국의 한류에 대한 내용검색을 위한 보고서였지만, 제조업(화장품)관련 보고서 내용도 있었는데,\n\n화장품수출 강세는 색조화장품 뿐 아니라 기초화장품도 늘었다는 내용으로 보아 두 품목 모두 상승을 견인한 것으로 보임\n의류 강세관련하여, ’팬데믹으로 의류산업은 고전’중이지만 ’마스크, 방호복 등 관련 수요 증가’라는 내용이 있어 의류품목 관련 판단은 좀 더 해보아야겠음\n\n참고한 자료 : https://kofice.or.kr/b20industry/b20_industry_00_view.asp?seq=1134&tblID=gongji&clsID=0\n\n\n\n판매액 상위국가에 대한 주요 상품 시각화\n\nfilter_rule2 = (df['국가(대륙)별'].isin(['미국','일본','중국']))&(df['상품군별'].isin([\"의류 및 패션 관련상품\", \"화장품\", \"음반·비디오·악기\"]))\ndf_top3_categorical = df[filter_rule2].copy()\n\nfig, axs = plt.subplots(ncols=3, figsize=(15,4), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.3},) #gridspec으로 그래프 사이 여백 설정\n\nylabel_text = {0:'백만',1:'',2:''}\nfor i, country  in enumerate(df_top3_categorical['국가(대륙)별'].unique()):\n    sns.lineplot(data=df_top3_categorical[df_top3_categorical['국가(대륙)별'] == country], \n                 x='연도', y='백만', hue='상품군별', errorbar=None, marker='o', palette=[\"b\", \"r\", 'g'],  style='상품군별',\n                 ax = axs[i])\n    axs[i].set_title('국가(대륙별) - ' + country)\n    axs[i].spines[['top','right']].set_visible(False) # 그래프 테두리 왼쪽,위,오른쪽 안보이게(false)\n    axs[i].legend().set_visible(False)\n    axs[i].set_ylabel(ylabel_text[i])\naxs[2].legend(bbox_to_anchor=(2,1), loc=0, borderaxespad=4)\n\n\n\n\n\n\n\n\n\n의류 분야는 세 국가 모두 `21년을 기점으로 하락세\n화장품은 미국/중국 하락세이나 일본이 크게 성장하여, `22년의 화장품 판매액은 일본의 영향이 크지 않을까 추측\n음반은 `21~22년도에 중국에서 판매액 증가가 뚜렷\n실제로도 확인해보니, `22년도 판매액은 일본의 비중이 컸음(하단 그래프)\n\n\nplt.figure(figsize=(10,4))\nsns.barplot(data=df[(df['연도'] == 2022) & (df['상품군별'] == '화장품')],\n             x='국가(대륙)별', y='백만', hue='상품군별', dodge=False)\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=6, borderaxespad=0) #bbox_to_anchor(그래프와의 관격, 위/아래 위치)\n             #loc(좌우), borderaxespad(클수록 아래로)"
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#추가과제",
    "href": "posts/coach-ds-20221113/index.html#추가과제",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "추가과제",
    "text": "추가과제\n\n현재 분석과는 관계없지만, 함께 진행한 과제가 있어 기록만 해둠\n\n\n# Choropleth 시각화\n\n# 데이터 불러오기 & 가공(월 합계 등)\nraw_data = pd.read_csv(\n    '국가별_수출입현황_20221119232639.csv', \n    encoding=\"cp949\")\n\niso_table = pd.read_excel('iso_alpha.xlsx', engine='openpyxl')\niso_table = iso_table[['나라 이름','alpha-3']]\n\nraw_data_saved = raw_data.copy()\nraw_data_saved['2022년합계'] = raw_data_saved[raw_data_saved.columns[2:]].sum(axis=1)\n\n# Pivot table(국가별 합계) - 합계 0은 제외 (시각화되어있지않으면 0으로 간주, 메모리도 더 절약될 것이라 생각)\npivoted = raw_data_saved.groupby(['국가별(1)'])[['2022년합계']].sum()\npivoted = pivoted[pivoted['2022년합계']!=0]\n\n# replace로 국가명↔코드로 변환 (영문으로 변환되지 않은 국가['키리바티', '타지크' 등 생소한 국가]는 제외했습니다)\npivoted['iso_table']=pivoted.index\npivoted['국가명']=pivoted.index\npivoted['iso_table'] = pivoted['iso_table'].replace(iso_table['나라 이름'].tolist(),iso_table['alpha-3'].tolist())\n\npivoted['iso_table'] = pivoted['iso_table'][((pivoted['iso_table'].str.upper()) != (pivoted['iso_table'].str.lower()))]\npivoted\n\n\n\n\n\n\n\n\n\n2022년합계\niso_table\n국가명\n\n\n국가별(1)\n\n\n\n\n\n\n\n가나\n146268\nGHA\n가나\n\n\n가봉\n-709661\nGAB\n가봉\n\n\n가이아나\n20358\nGUY\n가이아나\n\n\n감비아\n-1354\nGMB\n감비아\n\n\n건지\n7\nNaN\n건지\n\n\n...\n...\n...\n...\n\n\n필리핀\n6247769\nPHL\n필리핀\n\n\n허드 앤 맥도날드 군도\n-27\nNaN\n허드 앤 맥도날드 군도\n\n\n헝가리\n3696148\nHUN\n헝가리\n\n\n호주\n-19871753\nAUS\n호주\n\n\n홍콩\n20619857\nHKG\n홍콩\n\n\n\n\n244 rows × 3 columns\n\n\n\n\n\n# plotly의 choropleth를 활용\n# 지도위에 마우스를 올리면 국가명/iso code / 무역수지(백만단위)를 표현\nimport plotly.express as px\n\nfig = px.choropleth(pivoted, locations=\"iso_table\",\n                    color=\"2022년합계\",\n                    hover_name=\"국가명\",\n                    color_continuous_scale=px.colors.sequential.Plasma,\n                   title='2022년 국가별 무역수지 현황')\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nQuarto rendring 문제로 출력 예시 이미지 첨부"
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html",
    "href": "posts/coach-ds-20231107/index.html",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "",
    "text": "공공데이터포털 자료로 인터렉티브맵(folium) 시각화 연습\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html#데이터-불러오기",
    "href": "posts/coach-ds-20231107/index.html#데이터-불러오기",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('medical_201909.csv', encoding=\"cp949\", low_memory=False)\n\n\nfolium 시각화 연습\n\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# 데이터 추출\ndf_elder = df[df['상권업종소분류명'] == '노인/치매병원'].copy()\n\n# folium 지도 생성 (fit_bounds에 위도/경도를 넣어 적정 zoom 상태로 시작)\nlatitude = df_elder['위도']\nlongitude = df_elder['경도']\nmain_folium_map = folium.Map(location=[latitude.mean(),longitude.mean()])\nmain_folium_map.fit_bounds([[latitude.min(), longitude.min()],[latitude.max(),longitude.max()]])\n\n# 시/도별 색상을 다르게 하기위해 색상표 추가 (별도 확인한 folium색상리스트와 데이터의 시도명 활용)\nindex_color= ['red', 'blue', 'green', 'purple', 'orange', 'darkred',\n         'lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue',\n         'darkpurple', 'white', 'pink', 'lightblue', 'lightgreen',\n         'gray', 'lightgray']\nindex_address = df_elder['시도명'].value_counts().index.tolist()\ncolor_table = {}\n\nfor i, area in enumerate(index_address):\n  color_table[area] = index_color[i]\n\n# 위도/경도 기준으로 지도에 마커 추가\n\n# 상호명은 아이콘 위에 마우스 올리면 보이게 함 (tooltip)\n# 주소는 클릭하면 조이게 하고, 세로로 뜨지않게 Popup(address, max_width=200) 을 활용\n# 시도별 색상부여를 위해 위에서 만든 색상표 사용, 시도명 null값은 딕셔터리.get(,'black)으로 처리\n# 요양병원에 맞는 아이콘 부여 (prefix='fa', icon='blind') prefix fa에 참고한 사이트 : https://glyphsearch.com/\n# MarkerCluster를 활용해 클러스터화(묶음) 수행\ncluster = MarkerCluster()\nfor idx in df_elder.index:\n  location = [df_elder.loc[idx, '위도'], df_elder.loc[idx, '경도']]\n  name = df_elder.loc[idx, '상호명']\n  address = df_elder.loc[idx, '도로명주소']\n  icon_color = color_table.get(df_elder.loc[idx, '시도명'],'black')\n\n  cluster.add_child(\n      folium.Marker(location,\n                    popup=folium.Popup(address, max_width=200),\n                    tooltip=name,\n                    icon=folium.Icon(color=icon_color, prefix='fa', icon='blind')\n                    )\n  ).add_to(main_folium_map)\n\nmain_folium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n썸네일용 이미지 업로드"
  },
  {
    "objectID": "posts/pycon-20230813/index.html",
    "href": "posts/pycon-20230813/index.html",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "",
    "text": "파이콘에서 들었던 ’짠내나는 데이터 다루기’세션 내용정리입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/pycon-20230813/index.html#개요",
    "href": "posts/pycon-20230813/index.html#개요",
    "title": "[Pycon2023] 짠내나는 데이터 다루기(박조은님 세션) 정리",
    "section": "",
    "text": "파이콘 세션 듣고 간단히 정리  \n파이콘 사이트 : https://2023.pycon.kr/"
  },
  {
    "objectID": "posts/pycon-20230813/index.html#세션내용-정리",
    "href": "posts/pycon-20230813/index.html#세션내용-정리",
    "title": "[Pycon2023] 짠내나는 데이터 다루기(박조은님 세션) 정리",
    "section": "세션내용 정리",
    "text": "세션내용 정리\n\n메모리 확보하기\n\n가상메모리 설정, 그래픽 설정 낮추기, 백그라운드 비활성화, 캐시 제거, 재부팅\n\n\n\n메모리 사용량 줄이기\n\n데이터 샘플링 (행/열 줄이기)\n\ndf.sample(n), df.sample(frac=0.1) * frac:비율\n도메인에 따라 샘플링 기준 달라짐(특정 상품군/기간/고객군 등)\n필요없는 컬럼 제거 #### 청크(Chunked Processing)\n메모리를 작은 청크로 나누어 처리(메모리에 청크 단위로 로드)\npd.read_csv(chunksize=100) * chunksize만큼의 row를 가져옴\n아래와 같은 형식으로 사용\n# Chunksize만큼 나누어서 리스트에 저장\nchunk_list = []\nfor chunk in pd.read_scv('sample.csv', chunksize=100):\n  # pd.to_numeric(, downcast='float)와 같은 옵션을 같이 사용하면 좋음\n  chunk_list.apppend(chunk)\n\n# 리스트를 concat을 활용하여 하나로 결합\npd.concat(chunk_list)\n추가검색해보니 Chunk size지정에 대한 용량별 가이드가 있어 참고해봄 (공식문서아님, 블로그글 참고함) 출처 : https://acepor.github.io/2017/08/03/using-chunksize/  #### Parquet형식 사용(데이터 압축)\n\nParquet : 효율적 데이터 저장/검색을 위한 오픈소스, 열 지향 형식, Java/Python/C++ 등 지원\n샘플코드(Codestral에게 유사하게 만들어달라고 함)\n\nimport pyarrow.parquet as pq\nimport glob\n\nparquet_files = glob.glob('yourdirectory/*.parquet')\n\nfor file in parquet_files:\n    metadata = pq.read_metadata(file)\n    print('Schema:', metadata.schema)\n    print('Other Metadata:', metadata.metadata)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 31.37GB → 4.95GB로 감소 #### 데이터 타입을 지정해서 불러오기\n\n아래와 같이 dtype을 지정해서 불러오는 경우 메모리 사용량이 절약될 수 있다\n\n  dtype_dict = {'기준년도':'uint16', '가입자일련번호':'uint32'}\n  pd.read_csv('sample.csv', dtype=dtype_dict)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 29MB → 5.6MB로 감소 #### 분산처리 프레임워크(Dask, Vaex, PySpark 등 사용)\nDask : 병렬처리를 위한 분산 컴퓨팅 프레임워크, 큰 데이터를 처리할 수 있음, pandas와 유사한 API\nVaex : 디스크 기반의 컬럼 지향방식을 활용하여 대용량 데이터 처리 \nPySpark : Apache spark의 Python API (대규모 데이터 처리를 위한 분산컴퓨팅프레임워크 Spark를 파이썬에서 사용)"
  },
  {
    "objectID": "posts/pycon-20230813/index.html#메모리-확보하기",
    "href": "posts/pycon-20230813/index.html#메모리-확보하기",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "메모리 확보하기",
    "text": "메모리 확보하기\n\n가상메모리 설정, 그래픽 설정 낮추기, 백그라운드 비활성화, 캐시 제거, 재부팅"
  },
  {
    "objectID": "posts/pycon-20230813/index.html#메모리-사용량-줄이기",
    "href": "posts/pycon-20230813/index.html#메모리-사용량-줄이기",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "메모리 사용량 줄이기",
    "text": "메모리 사용량 줄이기\n\n데이터 샘플링 (행/열 줄이기)\n\ndf.sample(n), df.sample(frac=0.1) * frac:비율\n도메인에 따라 샘플링 기준 달라짐(특정 상품군/기간/고객군 등)\n필요없는 컬럼 제거\n\n\n\n청크(Chunked Processing)\n\n메모리를 작은 청크로 나누어 처리(메모리에 청크 단위로 로드)\npd.read_csv(chunksize=100) * chunksize만큼의 row를 가져옴\n아래와 같은 형식으로 사용\n# Chunksize만큼 나누어서 리스트에 저장\nchunk_list = []\nfor chunk in pd.read_scv('sample.csv', chunksize=100):\n  # pd.to_numeric(, downcast='float)와 같은 옵션을 같이 사용하면 좋음\n  chunk_list.apppend(chunk)\n\n# 리스트를 concat을 활용하여 하나로 결합\npd.concat(chunk_list)\n추가검색해보니 Chunk size지정에 대한 용량별 가이드가 있어 참고해봄 (공식문서아님, 블로그글 참고함) 출처 : https://acepor.github.io/2017/08/03/using-chunksize/ \n\n\n\nParquet형식 사용(데이터 압축)\n\nParquet : 효율적 데이터 저장/검색을 위한 오픈소스, 열 지향 형식, Java/Python/C++ 등 지원\n샘플코드(Codestral에게 유사하게 만들어달라고 함)\n\nimport pyarrow.parquet as pq\nimport glob\n\nparquet_files = glob.glob('yourdirectory/*.parquet')\n\nfor file in parquet_files:\n    metadata = pq.read_metadata(file)\n    print('Schema:', metadata.schema)\n    print('Other Metadata:', metadata.metadata)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 31.37GB → 4.95GB로 감소\n\n\n\n데이터 타입을 지정해서 불러오기\n\n아래와 같이 dtype을 지정해서 불러오는 경우 메모리 사용량이 절약될 수 있다\n\n  dtype_dict = {'기준년도':'uint16', '가입자일련번호':'uint32'}\n  pd.read_csv('sample.csv', dtype=dtype_dict)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 29MB → 5.6MB로 감소\n\n\n\n분산처리 프레임워크(Dask, Vaex, PySpark 등 사용)\n\nDask : 병렬처리를 위한 분산 컴퓨팅 프레임워크, 큰 데이터를 처리할 수 있음, pandas와 유사한 API\nVaex : 디스크 기반의 컬럼 지향방식을 활용하여 대용량 데이터 처리 \nPySpark : Apache spark의 Python API (대규모 데이터 처리를 위한 분산컴퓨팅프레임워크 Spark를 파이썬에서 사용)"
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html",
    "href": "posts/hanbitn-copilot-20230914/index.html",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "",
    "text": "한빛앤 Copilot세미나 내용 정리(+Codeium, Codestral’)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html#개요",
    "href": "posts/hanbitn-copilot-20230914/index.html#개요",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "개요",
    "text": "개요\n\n한빛앤 코파일럿 세미나 다녀와서 정리"
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html#내용-정리",
    "href": "posts/hanbitn-copilot-20230914/index.html#내용-정리",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "내용 정리",
    "text": "내용 정리\n\nAuto complete : 코드를 먼저 쓰면 해주는 제안을 활용가능\n비사용자 대비 코딩시간 최대 55% 단축\n\n두 그룹에 자바스크립트 작성을 시킨 후 비교  https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/\n\n시간을 단축하고 개발자의 정신적에너지를 절약하여 고부가가치에 집중하게 하기\n보안이슈\n\n코파일럿은 제품 개선을 위해 데이터를 수집함(암호화 및 일부만 열람하게 한다고는 함)\n내 코드가 학습에 사용되는게 싫으면 세팅에서 수집하지 않도록 할 수 있음\n\n코파일럿 X\n\nChatGPT처럼 질문할 수 있고 답변받을 수 있음\n대화한 내용에 대해 물어볼만한 질문도 선택지로 제시\n각 프로젝트의 언어(Python 등)에 맞춰 답변\n\n대체제\n\nCodeium : 개인사용 무료, Autocomplete 가능, Chat 가능\nTabnine : 무료버전 있음, Local Machine mode(보안에 좋을 듯)\n\n사용 후기\n\nCopilot : 한달 무료 사용. Autocomplete기능 자체를 처음 써봐서 좋긴 했음 xml과 beautifulsoup관련 어려움이 있었는데 도움이 꽤 되었음\nCodeium : 개인플랜 무료 사용. Chat기능을 별도로 쓸 일이 없었음 설치형이라 회사에 도입은 어려울 것 같았음(설치는 늘 승인받으라해서)\n직업 개발자가 아니어서인지 고급기능을 쓸 일이 없어서, 내 경우는 Autocomplete정도로 충족이 되어 Codeium을 사용하기로 함\n(2024.06추가) 요즘은 코드특화된 모델이 많이 나와서 채팅 쪽은 오히려 코드스트랄 등이 체감상 더 나은 것 같음\n\nCodestral(Mistral Codemodel) : Mistral Lechat 접속 후 아래와 같이 모델을 변경하여 사용 가능"
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html",
    "href": "posts/dtcontest-ore-20240608/index.html",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(+공공데이터 API활용샘플)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html#개요",
    "href": "posts/dtcontest-ore-20240608/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n공공데이터를 csv로 저장하지 않고 API로 활용해보기로 함"
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240608/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "내용정리",
    "text": "내용정리\n\n과제 요약\n\n과제명 : 핵심 광물별 공급위기 요소 탐지 모델 개발\n과제 개요 : 핵심광물별 가격 변동성, 시세, 생산·수입국 현황 등으로 핵심 광물별로 공급 리스크를 줄일 수 있는 위기 요소 탐지 분석 모델 개발\n제출 서류 : 분석 결과보고서(양식활용), 분석 코드, 근거데이터 파일\n활용 계획 : 전기차 배터리, 반도체 등 첨단산업 민간기업에 다양한 핵심광물 수급현황 위기 요소 탐지에 대한 분석 방법, 결과 등을 제공하여 안정적인 수급리스크 정보지원 강화\n유의사항\n\n핵심광물의 수급현황 등을 기반으로 위기요소 탐지 분석\n\n광물가격 급등 등 변동성, 국제시세, 주요 생산·수입국 현황, 수입량·수입금액·수입의존도, 국내 비축확보율, 재자원화율 및 국내 주요 수요기업 정보 등\n\n인터넷에 공개되어 있는 모든 가용 데이터를 사용하여 자유롭게 작성\n\n단, 사용한 데이터는 출처를 표기하여 근거데이터로 제출(참고 사이트 : 관세청, 외교부, 기재부, 산업부, 무역협회, 수출입은행 등)\n\n\n\n\n\n분석대상 광물선정\n\n공모전 인지 및 시작이 늦어서, 최대한 효율적으로 진행하고자 함(1달이내 남음)\n분석대상 광물을 공모전에 언급된 모든 광물보다는, 중요한 광물 위주로 진행해보고자 함\n\n공모전에 언급된 광물 : 텅스텐, 코발트, 리튬, 망간, 니켈\n\n고려 요소\n\n업종별 소요량 등을 기준으로 가장 공통적인/주요한 광물을 찾아보고자 함\n상기 자료는 찾기가 어려워, 생산량, 소비량, 수출입물량을 기준으로 상대적 비교를 해보고자 함\n\n\n\n\n공공데이터포털 API사용을 위한 파이썬 함수 작성\n\nAPI로 데이터 호출해보니 아래의 내용을 확인할 수 있었음\n\n호출 url의 ’perPage’를 활용해 한번에 가져올 데이터 수(행)을 정할 수 있음\n호출된 json에서 currentCount와 totalCount로 가져온 데이터와 전체 데이터를 확인할 수 있음\n\n확인한 내용을 기반으로, 공공데이터포털에서 json을 지원하는 데이터는 별도 조작없이 가능하게 하자는 목적으로 함수작성함\n\n일단 1개만 호출하여 전체 데이터 수량을 확인하고, perPage값을 바꿔 전체 데이터를 불러옴\n\n\n\nimport requests\nimport json\nimport pandas as pd\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\n\n# API사용을 위한 개인키 입력\nserviceKey = '(개인키)'\n\n\n\n개별분석\n\n광종별 소요량\n\n공공데이터포털 - 한국광해광업공단_광종별 소비현황  https://www.data.go.kr/data/3070245/fileData.do#tab-layer-file\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070245/v1/uddi:d950e6bc-e6a0-407c-baad-dfa87d739ff1_202004091120'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\njson_1 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_1)\n\n{'currentCount': 410,\n 'matchCount': 410,\n 'page': 1,\n 'perPage': 410,\n 'totalCount': 410}\n\n\n\ndf_1 = pd.json_normalize(json_1['data'])\ndf_1\n\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n0\n293.867\n253.700\n201.000\n225.800\n220.751\n226.106\n247.578\n260.381\n246.674\n190.106\n198.989\n337.973\n272.189\n알루미늄\nAustria\n천톤\nEUROPE\nrefined\n\n\n1\n10.200\n10.200\n10.200\n29.598\n21.898\n30.197\n27.001\n27.052\n31.673\n29.178\n39.481\n3.276\n2.400\n알루미늄\nBelarus\n천톤\nEUROPE\nrefined\n\n\n2\n382.312\n344.280\n236.780\n202.967\n231.648\n184.982\n212.039\n233.065\n181.368\n138.809\n204.808\n257.193\n160.985\n알루미늄\nBelgium\n천톤\nEUROPE\nrefined\n\n\n3\n52.188\n53.527\n60.645\n87.027\n109.824\n110.526\n109.522\n111.927\n119.211\n95.958\n128.057\n121.134\n97.254\n알루미늄\nBulgaria\n천톤\nEUROPE\nrefined\n\n\n4\n77.771\n83.684\n76.755\n66.065\n24.672\n52.033\n96.463\n112.930\n136.757\n143.869\n159.163\n135.867\n129.308\n알루미늄\nCroatia\n천톤\nEUROPE\nrefined\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n405\n939.000\n892.000\n935.000\n962.000\n931.000\n788.980\n829.000\n1800.000\n1800.000\n950.000\n878.000\n973.000\n911.000\n아연\nU.S.A.\n천톤\nAMERICA\nslab\n\n\n406\n0.000\n0.520\n0.514\n0.304\n0.449\n0.316\n0.312\n0.235\n0.235\n0.299\n0.350\n0.360\n0.371\n아연\nUruguay\n천톤\nAMERICA\nslab\n\n\n407\n10.430\n6.656\n9.014\n4.872\n2.246\n1.994\n0.685\n0.596\n0.596\n0.442\n0.360\n0.300\n0.370\n아연\nVenezuela\n천톤\nAMERICA\nslab\n\n\n408\n207.319\n116.293\n180.000\n174.400\n176.000\n178.000\n145.000\n146.700\n146.700\n135.000\n122.000\n125.000\n96.000\n아연\nAustralia\n천톤\nOCEANIA\nslab\n\n\n409\n11.023\n9.275\n8.844\n7.238\n6.876\n8.684\n10.631\n11.550\n11.550\n10.653\n10.332\n9.407\n6.405\n아연\nNew Zealand\n천톤\nOCEANIA\nslab\n\n\n\n\n410 rows × 18 columns\n\n\n\n\n\n# 광종 및 소비량 확인\nprint(df_1['광종'].unique())\nprint(df_1.columns)\n\n['알루미늄' '카드뮴' '동' '연' '니켈' '주석' '아연']\nIndex(['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량', '광종', '국가', '단위', '대륙', '품목'],\n      dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_1[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']] = df_1[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']].astype(float)\ndf_1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 410 entries, 0 to 409\nData columns (total 18 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   2011 소비량  409 non-null    float64\n 1   2012 소비량  410 non-null    float64\n 2   2013 소비량  410 non-null    float64\n 3   2014 소비량  410 non-null    float64\n 4   2015 소비량  410 non-null    float64\n 5   2016 소비량  410 non-null    float64\n 6   2017 소비량  410 non-null    float64\n 7   2018 소비량  410 non-null    float64\n 8   2019 소비량  410 non-null    float64\n 9   2020 소비량  410 non-null    float64\n 10  2021 소비량  410 non-null    float64\n 11  2022 소비량  410 non-null    float64\n 12  2023 소비량  410 non-null    float64\n 13  광종        410 non-null    object \n 14  국가        410 non-null    object \n 15  단위        410 non-null    object \n 16  대륙        410 non-null    object \n 17  품목        410 non-null    object \ndtypes: float64(13), object(5)\nmemory usage: 57.8+ KB\n\n\n\n# 광종별 소비량\ndf1_consume = df_1.groupby(by=['광종'])[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']].sum()\ndf1_consume\n\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n\n\n광종\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n니켈\n1651.690\n1729.344\n1794.140\n1579.073\n1730.944\n1855.296\n2090.518\n2339.103\n2430.139\n2442.837\n2962.181\n3003.505\n3171.547\n\n\n동\n19488.142\n20281.667\n21085.937\n22704.666\n22716.929\n23112.305\n23236.699\n23825.029\n23937.201\n24764.119\n24777.553\n25897.648\n27632.445\n\n\n아연\n12510.399\n12055.051\n12885.691\n13796.903\n13784.174\n13863.425\n14058.430\n14021.779\n14021.779\n13603.269\n13257.980\n13839.233\n14185.938\n\n\n알루미늄\n44428.294\n47851.313\n50670.176\n54045.963\n56775.631\n59293.053\n59905.431\n62043.652\n62608.661\n63407.663\n68102.645\n67853.241\n68826.117\n\n\n연\n10459.383\n10506.481\n11288.386\n10863.942\n11293.988\n11533.887\n12240.931\n12354.902\n12686.026\n11897.099\n14463.857\n14831.478\n14532.497\n\n\n주석\n372.530\n353.471\n355.130\n389.926\n363.759\n380.857\n375.021\n373.503\n362.765\n380.887\n374.583\n359.100\n344.361\n\n\n카드뮴\n23933.352\n23396.250\n27150.332\n28807.514\n27119.446\n27233.503\n26733.110\n24011.194\n20911.834\n19882.427\n19813.410\n19521.471\n19994.543\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,4))\nsns.heatmap(df1_consume, annot=True, fmt=\".0f\")\n\n\n\n\n\n\n\n\n\n상대적으로 알루미늄의 소비량이 가장 많음\n\n공모전 조원에게 받은 한국지질자원연구서의 보고서에 따르면, 지각 내에 3번째로 많이 들어있음 (2022.02 비철금속 비축 효과성/타당성 평가분석 및 중장기 정부비축방향)\n항공, 건축, 전기, 내화 등 다양한 용도로 사용되고 있으며, 4차 핵심사업의 용도로는 부합하지 않는 것으로 판단됨\n\n\n\nfor ore in df_1['광종'].unique():\n    if ore == '알루미늄' or ore == '카드뮴':\n        continue\n    df1_consume.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n알루미늄을 제외하고 소비량이 상승하고 있는 광물 위주로 추림\n\n앞서 알루미늄에 대한 시각과 같이 4차 핵심사업의 용도로는 니켈만이 유력\n\n\n\ndf1_consume.loc['니켈'].plot()\n\n\n\n\n\n\n\n\n\n\n광종별 생산량\n\n공공데이터포털 - 한국광해광업공단_광종별 국가별 생산량  https://www.data.go.kr/data/3070256/fileData.do#/API%20%EB%AA%A9%EB%A1%9D/getuddi%3A6d31d5bc-5487-4fa1-a335-90f9d9623cc8_202004080953\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070256/v1/uddi:6d31d5bc-5487-4fa1-a335-90f9d9623cc8_202004080953'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\n\njson_2 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_2)\n\n{'currentCount': 695,\n 'matchCount': 695,\n 'page': 1,\n 'perPage': 695,\n 'totalCount': 695}\n\n\n\ndf_2 = pd.json_normalize(json_2['data'])\ndf_2\n\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n0\n561.354\n800.316\n657.100\n605.215\n787.404\n738.612\n740.380\n760.244\n1043.343\n619.748\n675.269\n669.926\n542.114\n알루미늄\nBosnia\n천톤\nEUROPE\nbauxite\n\n\n1\n0.000\n0.000\n0.000\n0.000\n11.900\n9.800\n12.200\n11.800\n14.300\n14.100\n14.500\n13.800\n13.800\n알루미늄\nCroatia\n천톤\nEUROPE\nbauxite\n\n\n2\n80.800\n90.129\n100.000\n71.100\n70.000\n110.000\n110.000\n110.000\n120.760\n123.496\n142.764\n120.000\n120.000\n알루미늄\nFrance\n천톤\nEUROPE\nbauxite\n\n\n3\n2324.000\n1815.328\n1844.000\n1876.000\n1831.270\n1880.000\n1927.145\n1559.360\n1379.123\n1428.639\n1227.000\n1173.000\n869.100\n알루미늄\nGreece\n천톤\nEUROPE\nbauxite\n\n\n4\n277.800\n255.100\n93.700\n14.400\n8.300\n16.700\n4.000\n5.000\n0.000\n0.000\n0.000\n0.000\n0.000\n알루미늄\nHungary\n천톤\nEUROPE\nbauxite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n690\n662.151\n648.619\n651.637\n649.218\n683.118\n691.400\n598.438\n620.202\n654.971\n685.400\n639.843\n569.613\n502.000\n아연\nCanada\n천톤\nAMERICA\nslab\n\n\n691\n322.116\n323.569\n322.781\n320.924\n326.834\n321.166\n327.000\n335.942\n393.418\n368.200\n357.000\n328.727\n344.151\n아연\nMexico\n천톤\nAMERICA\nslab\n\n\n692\n313.714\n319.280\n346.361\n336.454\n335.422\n341.518\n312.339\n333.677\n356.925\n318.619\n320.000\n349.500\n346.072\n아연\nPeru\n천톤\nAMERICA\nslab\n\n\n693\n248.000\n261.000\n233.000\n180.000\n172.000\n126.000\n132.000\n116.000\n115.000\n180.000\n216.000\n216.000\n220.000\n아연\nU.S.A.\n천톤\nAMERICA\nslab\n\n\n694\n517.000\n508.000\n504.000\n488.000\n489.000\n470.000\n471.000\n491.000\n436.000\n447.300\n432.512\n385.032\n467.044\n아연\nAustralia\n천톤\nOCEANIA\nslab\n\n\n\n\n695 rows × 18 columns\n\n\n\n\n\n# 광종 및 생산량 확인\nprint(df_2['광종'].unique())\nprint(df_2.columns)\n\n['알루미늄' '안티모니' '카드뮴' '동' '금' '연' '몰리브덴' '니켈' '은' '주석' '아연']\nIndex(['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량', '광종', '국가', '단위', '대륙', '품목'],\n      dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_2[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']] = df_2[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].astype(float)\ndf_2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 695 entries, 0 to 694\nData columns (total 18 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   2011 생산량  691 non-null    float64\n 1   2012 생산량  695 non-null    float64\n 2   2013 생산량  695 non-null    float64\n 3   2014 생산량  695 non-null    float64\n 4   2015 생산량  695 non-null    float64\n 5   2016 생산량  695 non-null    float64\n 6   2017 생산량  695 non-null    float64\n 7   2018 생산량  695 non-null    float64\n 8   2019 생산량  695 non-null    float64\n 9   2020 생산량  695 non-null    float64\n 10  2021 생산량  695 non-null    float64\n 11  2022 생산량  694 non-null    float64\n 12  2023 생산량  694 non-null    float64\n 13  광종        695 non-null    object \n 14  국가        695 non-null    object \n 15  단위        695 non-null    object \n 16  대륙        695 non-null    object \n 17  품목        695 non-null    object \ndtypes: float64(13), object(5)\nmemory usage: 97.9+ KB\n\n\n\n# 광종별 생산량\ndf2_produce = df_2.groupby(by=['광종'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum()\ndf2_produce\n\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n\n\n광종\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n금\n2570.328\n2644.576\n2886.315\n3017.911\n3083.170\n3180.769\n3.238410e+03\n3.263709e+03\n3.211746e+03\n3.066197e+03\n3.041123e+03\n3.147309e+03\n3.138277e+03\n\n\n니켈\n3467.292\n4140.150\n4538.125\n3925.512\n3970.324\n3839.161\n4.229470e+03\n4.623132e+03\n4.964282e+03\n4.972969e+03\n5.467607e+03\n6.303324e+03\n7.216948e+03\n\n\n동\n35816.170\n37138.574\n39170.057\n41019.128\n42338.447\n43631.801\n4.364408e+04\n4.403226e+04\n4.399504e+04\n4.471063e+04\n4.588754e+04\n4.758409e+04\n5.004668e+04\n\n\n몰리브덴\n268.602\n266.883\n278.853\n303.501\n296.261\n283.849\n2.915410e+02\n2.731790e+02\n2.764260e+02\n2.893510e+02\n2.688010e+02\n2.858460e+02\n2.804480e+02\n\n\n아연\n25676.388\n25887.690\n26613.454\n27099.928\n27286.373\n26155.233\n2.563553e+04\n2.541092e+04\n2.568426e+04\n2.614851e+04\n2.747017e+04\n2.712567e+04\n2.750890e+04\n\n\n안티모니\n156163.000\n174973.000\n192551.000\n158041.000\n155999.000\n165096.000\n1.590890e+05\n1.601380e+05\n1.227800e+05\n1.142970e+05\n8.946900e+04\n8.533900e+04\n8.443900e+04\n\n\n알루미늄\n295511.409\n308026.690\n351054.187\n311065.456\n328534.809\n341423.066\n3.746015e+05\n4.011228e+05\n4.261662e+05\n4.469730e+05\n4.548925e+05\n4.549827e+05\n4.715010e+05\n\n\n연\n15435.226\n15814.869\n16593.862\n16246.173\n15871.816\n16294.015\n1.629023e+04\n1.651490e+04\n1.688362e+04\n1.626823e+04\n1.897150e+04\n2.043378e+04\n1.952917e+04\n\n\n은\n23282.594\n24903.406\n26174.528\n27279.117\n27848.245\n28038.988\n2.643421e+04\n2.651451e+04\n2.625192e+04\n2.425894e+04\n2.563196e+04\n2.477051e+04\n2.527272e+04\n\n\n주석\n670.472\n662.043\n648.298\n694.752\n669.020\n639.671\n7.081480e+02\n7.180450e+02\n7.278870e+02\n7.277420e+02\n7.273120e+02\n6.603090e+02\n6.635190e+02\n\n\n카드뮴\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n2.532580e+07\n2.817260e+07\n2.505329e+07\n2.473016e+07\n2.486415e+07\n2.447090e+07\n2.477489e+07\n\n\n\n\n\n\n\n\n\n앞서 소비량 분석에서 4차 핵심산업에 부합하지 않을 것으로 배제했던 광물을 제외하고 확인\n\n\nfor ore in df_2['광종'].unique():\n    if ore not in ['동', '연', '주석', '아연', '알루미늄', '카드뮴']:\n        df2_produce.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n생산량이 감소하고 있는 안티모니는 어떤 금속인지에 대해 조사\n\n방염, 페인트 합금, 고무 등에 사용되는 것으로, 안티모니도 대상에서 배제\n\n\n\nfor ore in df_2['광종'].unique():\n    if ore not in ['동', '연', '주석', '아연', '알루미늄', '카드뮴','안티모니']:\n        df2_produce.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n# 니켈에 대한 국가별 생산량\ndf2_produce_country = df_2[df_2['광종']=='니켈'].groupby(by=['국가'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum()\ndf2_produce_country\n\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n\n\n국가\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlbania\n3.528\n0.728\n2.086\n4.889\n6.309\n3.952\n5.301\n4.204\n2.830\n3.764\n3.615\n1.423\n0.548\n\n\nAustralia\n325.227\n407.700\n433.872\n403.943\n378.205\n323.656\n293.966\n274.539\n265.220\n285.144\n249.845\n251.507\n241.140\n\n\nAustria\n0.500\n0.500\n0.500\n0.600\n0.700\n0.700\n0.700\n0.700\n0.700\n0.700\n0.700\n1.000\n1.000\n\n\nBotswana\n15.675\n17.948\n22.848\n14.952\n16.788\n16.878\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nBrazil\n117.801\n148.344\n130.611\n160.924\n159.906\n155.788\n137.303\n130.454\n109.944\n136.633\n136.788\n142.700\n131.400\n\n\nCanada\n361.470\n358.551\n380.471\n378.353\n384.236\n393.333\n369.061\n323.373\n317.894\n282.033\n235.219\n191.807\n214.341\n\n\nChina\n559.536\n684.172\n803.857\n638.237\n554.597\n537.173\n723.282\n841.119\n957.049\n857.237\n912.392\n974.155\n1044.821\n\n\nColombia\n75.636\n103.188\n98.772\n82.400\n73.400\n74.170\n81.200\n86.200\n81.200\n72.200\n76.600\n83.600\n77.400\n\n\nCuba\n102.532\n94.926\n72.236\n64.839\n64.731\n62.759\n65.041\n63.620\n58.430\n63.144\n64.929\n66.101\n62.213\n\n\nDominican Republic\n26.996\n30.372\n18.800\n0.000\n0.000\n19.826\n31.264\n38.428\n56.900\n42.632\n54.640\n56.800\n38.848\n\n\nFinland\n67.605\n65.473\n63.831\n62.433\n54.122\n75.687\n95.917\n104.337\n100.952\n104.781\n90.887\n129.668\n139.599\n\n\nFrance\n13.916\n13.546\n12.916\n8.812\n6.761\n4.287\n2.417\n3.700\n6.900\n7.300\n8.900\n6.842\n7.125\n\n\nGreece\n39.630\n40.150\n36.177\n39.891\n36.863\n36.463\n35.861\n33.609\n25.689\n13.071\n8.834\n3.124\n0.000\n\n\nGuatemala\n0.000\n2.422\n10.184\n49.899\n67.324\n53.434\n68.147\n53.708\n52.456\n72.537\n84.864\n69.181\n20.177\n\n\nIndia\n0.000\n0.000\n1.010\n2.028\n1.861\n0.329\n0.092\n0.077\n0.055\n0.000\n0.000\n0.000\n0.000\n\n\nIndonesia\n246.597\n640.591\n834.330\n167.151\n176.035\n308.017\n555.580\n907.673\n1250.383\n1411.019\n1915.958\n2745.018\n3589.948\n\n\nIvory Coast\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.576\n6.852\n16.421\n18.988\n27.849\n33.534\n36.279\n\n\nJapan\n156.883\n169.556\n177.810\n177.782\n192.789\n195.565\n187.046\n186.736\n182.652\n171.044\n180.721\n163.401\n153.248\n\n\nKazakhstan\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nKosovo\n15.132\n8.872\n15.214\n13.448\n13.300\n6.825\n15.621\n9.179\n8.548\n11.769\n10.646\n0.339\n0.360\n\n\nMacedonia\n20.892\n20.951\n20.001\n18.054\n17.699\n10.607\n7.175\n10.100\n15.202\n17.747\n17.714\n8.349\n3.812\n\n\nMadagascar\n0.000\n11.390\n50.298\n74.108\n94.542\n84.210\n70.948\n66.366\n67.466\n19.800\n58.704\n71.258\n75.828\n\n\nMorocco\n0.217\n0.288\n0.160\n0.000\n0.203\n0.188\n0.196\n0.126\n0.131\n0.142\n0.144\n0.144\n0.144\n\n\nMyanmar\n0.800\n9.800\n3.340\n39.428\n45.804\n45.200\n43.570\n35.604\n28.890\n49.078\n42.254\n16.868\n11.802\n\n\nNew Caledonia\n168.626\n177.076\n212.777\n237.223\n270.729\n300.193\n319.476\n324.139\n296.106\n272.152\n242.887\n266.171\n302.940\n\n\nNorway\n92.753\n91.940\n91.380\n90.846\n91.465\n92.914\n86.672\n91.054\n92.290\n91.374\n91.485\n82.163\n95.253\n\n\nPapua New Guinea\n0.000\n5.283\n11.370\n20.987\n25.581\n22.269\n34.666\n35.355\n32.722\n33.659\n31.594\n34.304\n33.604\n\n\nPhilippines\n319.353\n317.621\n313.050\n443.909\n466.754\n345.506\n379.377\n389.966\n341.325\n328.911\n393.687\n340.127\n382.568\n\n\nPoland\n1.700\n1.600\n1.200\n1.560\n1.680\n1.540\n1.440\n1.400\n1.440\n1.420\n1.440\n1.600\n1.680\n\n\nRussia\n536.000\n522.700\n506.000\n503.436\n492.916\n413.482\n372.562\n368.667\n379.762\n386.934\n311.033\n367.972\n354.259\n\n\nSouth Africa\n79.320\n78.446\n83.508\n89.656\n98.646\n91.104\n91.451\n82.730\n82.603\n64.030\n69.804\n66.623\n65.699\n\n\nSouth Korea\n16.863\n23.673\n28.130\n24.964\n39.007\n45.616\n47.402\n45.593\n46.269\n45.604\n47.270\n31.959\n40.022\n\n\nSpain\n0.000\n2.398\n7.574\n8.631\n7.213\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nTurkey\n0.848\n3.814\n1.178\n3.035\n8.637\n9.337\n14.767\n13.559\n4.742\n20.201\n11.647\n14.479\n14.400\n\n\nU.S.A.\n0.000\n0.000\n0.000\n4.300\n27.167\n24.114\n22.081\n17.573\n13.494\n16.718\n18.353\n17.475\n16.429\n\n\nUkraine\n22.475\n20.833\n26.751\n25.129\n20.842\n18.920\n15.605\n16.331\n15.133\n15.910\n15.657\n10.093\n0.733\n\n\nUnited Kingdom\n37.400\n39.400\n42.400\n39.100\n39.094\n43.104\n37.090\n38.211\n34.976\n35.177\n30.654\n33.384\n35.851\n\n\nVenezuela\n26.800\n16.200\n6.522\n5.000\n9.702\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nVietnam\n0.000\n0.000\n1.200\n6.932\n8.607\n4.272\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nZambia\n2.888\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.230\n3.780\n3.680\n3.942\n6.550\n\n\nZimbabwe\n11.693\n9.698\n15.761\n18.633\n16.109\n17.743\n16.617\n17.850\n16.278\n16.336\n16.213\n16.213\n16.927\n\n\n\n\n\n\n\n\n\n# 니켈에 대한 국가별 생산량의 총 합계\ndf2_produce_country_total = df2_produce_country[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum(axis=1)\ndf2_produce_country_total.sort_values(ascending=False)\n\n국가\nIndonesia             14748.300\nChina                 10087.627\nRussia                 5515.723\nPhilippines            4762.154\nCanada                 4190.142\nAustralia              4133.964\nNew Caledonia          3390.495\nJapan                  2295.233\nBrazil                 1798.596\nNorway                 1181.589\nFinland                1155.292\nColombia               1065.966\nSouth Africa           1043.620\nCuba                    905.501\nMadagascar              744.918\nGuatemala               604.333\nUnited Kingdom          485.841\nSouth Korea             482.372\nDominican Republic      415.506\nMyanmar                 372.438\nGreece                  349.362\nPapua New Guinea        321.394\nUkraine                 224.412\nZimbabwe                206.071\nMacedonia               188.303\nU.S.A.                  177.704\nIvory Coast             140.499\nKosovo                  129.253\nTurkey                  120.644\nBotswana                105.089\nFrance                  103.422\nVenezuela                64.224\nAlbania                  43.177\nSpain                    25.816\nZambia                   22.070\nVietnam                  21.011\nPoland                   19.700\nAustria                   9.000\nIndia                     5.452\nMorocco                   2.083\nKazakhstan                0.000\ndtype: float64\n\n\n\n니켈 총생산량 기준 상위 10개국에 대한 시각화진행\n\n인도네시아, 중국, 러시아, 필리핀 순으로 상위 생산국\n\n\n\n# 니켈 총생산량 내림차순 기준 상위 10개국 Pie chart\ntarget_country = df2_produce_country_total.sort_values(ascending=False)[0:10].index.tolist()\ndf2_produce_country_total.loc[target_country].plot(kind='pie',startangle=145, autopct='%.1f%%', pctdistance=0.8)\n\n\n\n\n\n\n\n\n\n니켈에 대한 모델링 등을 할때, 주요 생산국의 관련 지표를 넣으면 좋을 듯 함\n\n\n\n광종별 수출입 현황\n\n공공데이터포털 - 한국광해광업공단_광종별 국내 수출입 현황  https://www.data.go.kr/data/3070177/fileData.do\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070177/v1/uddi:09b79733-b804-4ec6-9968-dd0f58638b55_202004090938'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\n\njson_3 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_3)\n\n{'currentCount': 116,\n 'matchCount': 116,\n 'page': 1,\n 'perPage': 116,\n 'totalCount': 116}\n\n\n\ndf_3 = pd.json_normalize(json_3['data'])\ndf_3\n\n\n\n\n\n\n\n\n\n광종\n분류\n수입금액(천불)\n수입중량(톤)\n수출금액(천불)\n수출중량(톤)\n연도\n\n\n\n\n0\n금광\n금속광\n67000\n5705.000\n116\n1005.000\n2021\n\n\n1\n은광\n금속광\n302335\n37635.000\n79\n63.000\n2021\n\n\n2\n동광\n금속광\n6017233\n2097948.000\n1185720\n560664.000\n2021\n\n\n3\n연광\n금속광\n2366631\n642414.000\n670\n1102.000\n2021\n\n\n4\n아연광\n금속광\n2044885\n1819759.000\n34679\n23804.000\n2021\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111\n하석\n비금속광\n144\n321.000\n1\n0.000\n2022\n\n\n112\n무연탄\n석탄광\n1661049\n5384000.000\n251\n375.000\n2022\n\n\n113\n유연탄\n석탄광\n26230599\n117752000.000\n0\n0.000\n2022\n\n\n114\n갈탄\n석탄광\n65\n107.000\n0\n0.000\n2022\n\n\n115\n토탄\n석탄광\n25324\n110185.000\n41\n21.000\n2022\n\n\n\n\n116 rows × 7 columns\n\n\n\n\n\n# 광종 및 소비량 확인\nprint(df_3['광종'].unique())\nprint(df_3.columns)\n\n['금광' '은광' '동광' '연광' '아연광' '철광' '텅스텐광' '몰리브덴광' '망간광' '주석광' '창연' '안티모니광'\n '비소광' '황철석' '니켈' '코발트' '크롬광' '리튬광' '티타늄광' '질코늄광' '알루미늄광' '백금광' '탄탈륨광'\n '바나듐광' '니오븀광' '게르마늄광' '기타 금속' '인상흑연' '토상흑연' '활석' '납석' '장석' '고령토류' '석회석류'\n '규석' '규사' '황' '규조토' '형석' '인광석' '규회석' '운모' '주사' '홍주석' '규선석' '남정석' '중정석'\n '마그네사이트' '석고' '불석' '수정' '붕소광' '금강석' '하석' '기타 비금속' '무연탄' '유연탄' '갈탄' '토탄'\n '기타 석탄']\nIndex(['광종', '분류', '수입금액(천불)', '수입중량(톤)', '수출금액(천불)', '수출중량(톤)', '연도'], dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_3[['수입중량(톤)', '수출중량(톤)',]] = df_3[['수입중량(톤)', '수출중량(톤)']].astype(float)\ndf_3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 116 entries, 0 to 115\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   광종        116 non-null    object \n 1   분류        116 non-null    object \n 2   수입금액(천불)  116 non-null    int64  \n 3   수입중량(톤)   116 non-null    float64\n 4   수출금액(천불)  116 non-null    int64  \n 5   수출중량(톤)   116 non-null    float64\n 6   연도        116 non-null    int64  \ndtypes: float64(2), int64(3), object(2)\nmemory usage: 6.5+ KB\n\n\n\n공모전에 언급된 주요광물에 대해서 추림\n\n\ndf3_sorted = df_3[df_3['광종'].isin(['텅스텐광','망간광','니켈','코발트','리튬광','알루미늄광'])]\ndf3_sorted\n\n\n\n\n\n\n\n\n\n광종\n분류\n수입금액(천불)\n수입중량(톤)\n수출금액(천불)\n수출중량(톤)\n연도\n\n\n\n\n6\n텅스텐광\n금속광\n60\n1.0\n84\n8.0\n2021\n\n\n8\n망간광\n금속광\n290356\n1325095.0\n0\n0.0\n2021\n\n\n14\n니켈\n금속광\n340685\n3181534.0\n42\n86.0\n2021\n\n\n15\n코발트\n금속광\n1\n0.0\n0\n0.0\n2021\n\n\n17\n리튬광\n금속광\n1047778\n94839.0\n53933\n4980.0\n2021\n\n\n20\n알루미늄광\n금속광\n33126\n424540.0\n45\n156.0\n2021\n\n\n66\n텅스텐광\n금속광\n607\n40.0\n3559\n1241.0\n2022\n\n\n68\n망간광\n금속광\n283632\n1085930.0\n5\n113.0\n2022\n\n\n74\n니켈\n금속광\n382427\n2653287.0\n206\n108.0\n2022\n\n\n75\n코발트\n금속광\n95\n12.0\n0\n0.0\n2022\n\n\n79\n알루미늄광\n금속광\n33150\n417762.0\n46\n478.0\n2022\n\n\n\n\n\n\n\n\n\nfor ore in ['텅스텐광','망간광','니켈','코발트','리튬광','알루미늄광']:\n    print(df3_sorted[df3_sorted['광종']==ore])\n    print()\n\n      광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n6   텅스텐광  금속광        60      1.0        84      8.0  2021\n66  텅스텐광  금속광       607     40.0      3559   1241.0  2022\n\n     광종   분류  수입금액(천불)    수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n8   망간광  금속광    290356  1325095.0         0      0.0  2021\n68  망간광  금속광    283632  1085930.0         5    113.0  2022\n\n    광종   분류  수입금액(천불)    수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n14  니켈  금속광    340685  3181534.0        42     86.0  2021\n74  니켈  금속광    382427  2653287.0       206    108.0  2022\n\n     광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n15  코발트  금속광         1      0.0         0      0.0  2021\n75  코발트  금속광        95     12.0         0      0.0  2022\n\n     광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n17  리튬광  금속광   1047778  94839.0     53933   4980.0  2021\n\n       광종   분류  수입금액(천불)   수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n20  알루미늄광  금속광     33126  424540.0        45    156.0  2021\n79  알루미늄광  금속광     33150  417762.0        46    478.0  2022\n\n\n\n[광종별 간단 요약]\n\n텅스텐 : 수입 금액/중량 증가(가격비 비슷), 수출 금액/중량 증가\n망간 : 수입 금액/중량 감소\n니켈 : 수입 금액 증가, 수입 중량 감소 (가격 증가)\n코발트 : 수입 급격한 증가, 수출은 없음\n리튬 : 비교가능 데이터 없음\n\n[수출입 데이터 기준 고려사항]\n\n니켈 : 수입량과 가격 모두 증가\n니켈(수입량 증가와 가격 증가 추세로, 추가분석 필요하여 하단에서 계속 진행)\n코발트(수입 급격히 증가했으나 수출실적은 전혀없는 상황으로 국내 매장량 등 분석하고자 함)\n\n시간 상 관련 수치자료를 찾지 못하여 간단히 몇가지 검색해보니, 소요량에 대한 기사 발견 \n\n[기사]한국기업의 귀속생산량이 전세계 총합 대비 1% 이하 수준 https://www.businesspost.co.kr/BP?command=article_view&num=353613\n[한국경제인협회] 니켈, 리튬, 코발트에 대한 주요 생산국 및 수입량 추이 https://www.fki.or.kr/main/news/statement_detail.do?bbs_id=00035573&category=ST\n\n\n\n\n\n\n결론\n\n추가적인 분석에 따라 달라지겠지만 현재로서는 아래와 같음\n니켈\n\n4차 핵심사업에 소요되는 광물로 2011년 대비 2배이상 소요량 증가\n생산량이 증가하고 있으나 주요 3개국 생산량이 58.3%로 편중으로 인한 리스크 큼(생산국:인도네시아, 중국, 러시아)\n자료 시기가 2021, 2022년으로 많지는 않지만, 수입량의 감소에도 수입금액이 늘었다는 것은 가격 상승이 예상되는 상황\n소요량, 생산량, 수입데이터 기준 가격상승 예상 등 리스크가 있어보여 위험요소 탐지가 필요한 광물로 보임\n\n코발트\n\n수입량은 급격히 증가했으나, 수출량은 전무한 상황으로 생산자체가 불가하거나 국내 사용만으로도 부족한 상황이 아닐지 추측\n간단히 구글링하였을 때, 기사에 따르면 코발트 생산은 콩고 생산량이 68.8%일 정도로 편중이 심함\n코발트 또한 이차 전지에 활용되는 핵심광물로 위험 요소 탐지가 필요해보임\n\n니켈에 대해 분석한 내용을 모델링할 때 감안하거나, 최종보고서의 핵심광물 위험요소에 대한 당위성 부여 등에 활용하고자 함"
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html#개요",
    "href": "posts/coach-ds-20231107/index.html#개요",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "개요",
    "text": "개요"
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html",
    "href": "posts/dtcontest-ore-20240610/index.html",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(+github활용한 온라인db구축)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html#개요",
    "href": "posts/dtcontest-ore-20240610/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n단순 API로딩이 아니라, 각자 바로 데이터를 로딩할 수 있는 매체에 대한 고민 후 구현\n\n코딩이 익숙하지 않은 팀원이 쉽게 이용할 수 있도록하고, readme에 바로 사용할 수 있게 샘플코드 제공\ngithub를 활용해 pandas에서 바로 로딩할 수 있도록 구현"
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240610/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "내용정리",
    "text": "내용정리\n\n도입목적\n\n본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n개인서버(NAS)에서 매일 특정시간 구동하여 별도의 수작업없이 자동으로 최신화\n\n\n\n구동방식\n\n공공데이터 리스트와 API키가 저장된 json파일 로딩\n지정된 공공데이터를 다운로드하고 csv파일로 저장\n바로 로딩하기위한 파일 주소생성, 업데이트 날짜 저장\nREADME 파일에 파일주소와 업데이트 날짜 등 업데이트\ngit_push함수로 github repository에 자동업로드\n\n\n\ngithub reposiroty주소\nhttps://github.com/KR9268/db_datagokr\n\n\n샘플코드(패키지 및 함수)\n\nimport requests\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport subprocess\nimport os\nimport time\n\ndef json_load(json_path, encoding='utf-8'):\n    with open(json_path, 'r', encoding=encoding) as file:\n        json_data = json.load(file)\n    return json_data\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\ndef update_readme(new_content_list):\n    # Open the README.md file in read mode\n    with open('README.md', 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    # Find the index of the line that starts with '* 데이터 현황'\n    index = next((i for i, line in enumerate(lines) if line.startswith('* 데이터 현황')), None)\n\n    # If the line is found, remove the following lines and insert new content\n    if index is not None:\n        lines = lines[:index+1] # Remove the following lines\n        #lines.extend(new_content) # Insert new content\n        lines.extend(new_content_list) # Insert new content\n\n    # Open the README.md file in write mode and write the updated content\n    with open('README.md', 'w', encoding='utf-8') as file:\n        file.writelines(lines)\n\ndef git_push():\n    # Get a list of all .csv files in the current directory\n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    # Stage all .csv files\n    for file in csv_files:\n        subprocess.call(['git', 'add', file])\n\n    subprocess.call(['git', 'add', 'README.md'])\n    # Commit the changes with a message\n    subprocess.call(['git', 'commit', '-m', 'Automatic commit'])\n\n    # Push the changes to the remote repository\n    subprocess.call(['git', 'push'])\n\n\n\n샘플코드(메인코드)\n\n# json load\nserviceKey = json_load('option.json')['serviceKey']\ndb_list = json_load('db_list.json', 'cp949')\n\n\n# main\n\n# 작업하기\ntxt_for_readme = ['\\n']\nfor each in db_list:\n    # 다운로드\n    url = f\"{each['base_url']}{each['address_get']}?page=1&perPage=1&serviceKey={serviceKey}\"\n    json_data = download_from_data_go_kr_with_json(url)\n    result_data = chk_json_status_of_data_go_kr(json_data)\n    \n    # 저장\n    if result_data['currentCount'] == result_data['totalCount']:\n        pd.json_normalize(json_data['data']).to_csv(f\"{each['file_name_to']}.csv\",encoding='cp949', index=False)\n\n    # 파일주소 및 이름, 업데이트시간 저장\n    owner = 'KR9268'\n    repo = 'db_datagokr'\n    branch = 'main'\n    file_path = f\"{each['file_name_to']}.csv\"\n\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}\"\n\n    txt_for_readme.append(f\"  *  {datetime.strftime(datetime.now(),'%Y-%m-%d')}업데이트 : {each['name']}\\n{url}\\n\")\n    time.sleep(1)\n\n# 업데이트 내역과 파일 git push\nupdate_readme(txt_for_readme)\ngit_push()"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "",
    "text": "본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\nrepository에서 바로 복사해서 사용가능한 샘플을 제공하여 쉽게 데이터로딩 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n추가하고자 하는 데이터가 공공데이터포털의 데이터라면 쉽게 작업내역에 추가가능 (리스트 추가권한을 주거나, discord봇 접수 등 생각해보았으나 구현시간대비 효용이 작아 직접 추가진행)\n\n개인서버(NAS)에서 매일 특정시간 구동하여, github서버로 자동 push(업로드)하여 별도의 수작업없이 db 최신화\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#추진배경",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#추진배경",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "",
    "text": "본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\nrepository에서 바로 복사해서 사용가능한 샘플을 제공하여 쉽게 데이터로딩 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n추가하고자 하는 데이터가 공공데이터포털의 데이터라면 쉽게 작업내역에 추가가능 (리스트 추가권한을 주거나, discord봇 접수 등 생각해보았으나 구현시간대비 효용이 작아 직접 추가진행)\n\n개인서버(NAS)에서 매일 특정시간 구동하여, github서버로 자동 push(업로드)하여 별도의 수작업없이 db 최신화"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#효과",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#효과",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "효과",
    "text": "효과\n\n특정 공공데이터 포털의 데이터를 사용하기 위해 했던 아래의 작업이 제거됨\n\n공공데이터포털에 접속하여 csv파일 저장\n저장해둔 csv파일 찾기\n해당 파일을 분석했던 jupyter파일로 찾아가 API호출주소나 키를 찾기\ncsv파일을 저장한 뒤 추가 업데이트가 되었는지 확인하고 재다운로드\n한국어 데이터 로딩을 위한 encoding규격 등의 숙지가 필요없음 (저장시 규격을 지정하고, 로딩을 위한 샘플코드에 규격을 지정해둠)\n\n위의 사항을 달성하기 위한 부가작업이 제거됨\n\ncsv파일 저장 및 업데이트 시기 기록, 서버에 업로드 및 파일주소 저장"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#github-repository",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#github-repository",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "github repository",
    "text": "github repository\n관련 github레포\n\nAPI개인키 등 정보가 있어 레포에는 csv파일과 현황만 저장함 (구현을 위해 필요한 db_list나 개인키 등의 json파일은 모두 ignore리스트로 관리)\ngit clone해둔 개인서버 디렉토리에서 자동 실행"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\njson_load(json) : 개인키와 db_list 로딩을 위한 함수. 어려운 기능은 아니지만 여러번 쓰여 함수화 \ndownload_from_data_go_kr_with_json(requests, json) : 공공데이터포털 API호출용 (실제 자료 + 전체/다운 현황 등 수신)\nchk_json_status_of_data_go_kr : 전체/실제 건수 등 별도 처리용(대조하여 누락없이 전체자료 저장하기 위함)\nupdate_readme : README파일에 파일 현황(데이터명/주소/링크 등) 업데이트용\ngit_push(subprocess) : 새롭게 추가된 모든사항(ignore대상 제외)을 자동으로 업로드(auto commit)하기 위한 용도\n위의 함수들을 기반으로 아래의 내용을 구현\n\n\njson으로 민감정보 등을 코드에 표기하지 않고 별도관리 (+ gitignore로 이중보안)\n1건만 먼저 호출하여 전체건수를 확인 후, 한번에 전체 건을 호출 (10000건 지정 등 과도한 세팅값의 하드코딩 지양)\npandas로 encoding cp949 지정하여 csv로 저장\ngithub raw링크형식을 활용하여 바로 파일로딩할 수 있도록 주소 생성(db에서 아래 형식으로 지정해둔 파일명 사용)\n\n[\n  {\n  \"name\": \"공공데이터포털의 데이터명\",\n  \"base_url\": \"API Base Url\",\n  \"address_get\": \"해당 데이터의 API주소(API목록 란에서 확인)\",\n  \"file_name_to\": \"CSV로 저장할 파일명(.csv제외)\"\n  },\n  {\n  \"name\": \"공공데이터포털의 데이터명2\",\n  \"base_url\": \"API Base Url\",\n  \"address_get\": \"해당 데이터의 API주소(API목록 란에서 확인)\",\n  \"file_name_to\": \"CSV로 저장할 파일명(.csv제외)\"\n  }\n]\n\ntime.sleep()으로 너무 자주 호출하여 서버에 무리가지 않도록 세팅"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "샘플코드(패키지 및 함수)",
    "text": "샘플코드(패키지 및 함수)\n\nimport requests\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport subprocess\nimport os\nimport time\n\ndef json_load(json_path, encoding='utf-8'):\n    with open(json_path, 'r', encoding=encoding) as file:\n        json_data = json.load(file)\n    return json_data\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\ndef update_readme(new_content_list):\n    # Open the README.md file in read mode\n    with open('README.md', 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    # Find the index of the line that starts with '* 데이터 현황'\n    index = next((i for i, line in enumerate(lines) if line.startswith('* 데이터 현황')), None)\n\n    # If the line is found, remove the following lines and insert new content\n    if index is not None:\n        lines = lines[:index+1] # Remove the following lines\n        #lines.extend(new_content) # Insert new content\n        lines.extend(new_content_list) # Insert new content\n\n    # Open the README.md file in write mode and write the updated content\n    with open('README.md', 'w', encoding='utf-8') as file:\n        file.writelines(lines)\n\ndef git_push():\n    # Get a list of all .csv files in the current directory\n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    # Stage all .csv files\n    for file in csv_files:\n        subprocess.call(['git', 'add', file])\n\n    subprocess.call(['git', 'add', 'README.md'])\n    # Commit the changes with a message\n    subprocess.call(['git', 'commit', '-m', 'Automatic commit'])\n\n    # Push the changes to the remote repository\n    subprocess.call(['git', 'push'])"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수-1",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수-1",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "샘플코드(패키지 및 함수)",
    "text": "샘플코드(패키지 및 함수)\n\n# json load\nserviceKey = json_load('option.json')['serviceKey']\ndb_list = json_load('db_list.json', 'cp949')\n\n\n# main\n\n# 작업하기\ntxt_for_readme = ['\\n']\nfor each in db_list:\n    # 다운로드\n    url = f\"{each['base_url']}{each['address_get']}?page=1&perPage=1&serviceKey={serviceKey}\"\n    json_data = download_from_data_go_kr_with_json(url)\n    result_data = chk_json_status_of_data_go_kr(json_data)\n    \n    # 저장\n    if result_data['currentCount'] == result_data['totalCount']:\n        pd.json_normalize(json_data['data']).to_csv(f\"{each['file_name_to']}.csv\",encoding='cp949', index=False)\n\n    # 파일주소 및 이름, 업데이트시간 저장\n    owner = 'KR9268'\n    repo = 'db_datagokr'\n    branch = 'main'\n    file_path = f\"{each['file_name_to']}.csv\"\n\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}\"\n\n    txt_for_readme.append(f\"  *  {datetime.strftime(datetime.now(),'%Y-%m-%d')}업데이트 : {each['name']}\\n{url}\\n\")\n    time.sleep(1)\n\n# 업데이트 내역과 파일 git push\nupdate_readme(txt_for_readme)\ngit_push()"
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html",
    "href": "posts/dtcontest-ore-20240612/index.html",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(지표 및 사용할 데이터에 대한 고민)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html#개요",
    "href": "posts/dtcontest-ore-20240612/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n사용하기로 한 지표에 대한 재조사 및 통합사용하기로 의견제안 예정\n모델의 Feature로 원자재의 벌크운송에 대한 운임지수(BDI)사용 의견제안 예정"
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240612/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "내용정리",
    "text": "내용정리\n\n지난 회의정리\n\n모델링에 사용할 지표 2가지 선정\n\n수급안정화지수\n시장위험지수\n\n광물별 가격 영향 미치는 요소 생각해보기\n모델링 관련 아이디어\n\n\n\n회의내용에 대한 Self고찰 및 아이디어 Develope\n\n모델링에 사용할 지표에 대한 분석\n\n데이터를 살펴보다보니 둘의 움직임이 거의 같게 나타남\n가격리스크, 중장기적 시계 등 공통적인 요소가 서로 많은 지표임을 발견\n포함항목을 좀 더 구체적으로 명시한 수급안정화지수로 통합사용 하는 것으로 의견 제안 예정\n\n광종별 중장기 가격리스크, 세계 수급비율(공급/소비), 세계 공급(매장)편중도, 국내 수입증가율, 국내 수입국 편중도 등\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\ndf_source = pd.read_csv('비교_한국광해광업공단_수급안정화지수_코발트_20240520 (1).csv', encoding='cp949')\ndf_source[['기간','시장위험지수','수급안정화지수']].plot(x='기간')\n\n\n\n\n\n\n\n\n\n광물별 고려요소 및 모델링에 쓸 데이터 관련 아이디어\n\n고려대상 광물들의 세계 생산량 비중 등을 고려할 때 국제수송(수입)을 고려해야할 것으로 보임\n\n조사를 통해 상하이 컨테이너 운임지수(SCFI)를 확인, 국가별 운임지수에 대한 가중치를 반영한 지표로 주요 수입국에 대해 반영 고려\n\n그러나 광물운송의 특성상 컨테이너로 운송하지 않기 때문에 다른 지수를 모색\n\n벌크선에 대한 지수인 발틱운임지수(BDI) 도입 검토\n\n공급 대비 운송량에 대한 수요를 알 수 있음\n벌크선은 광물, 곡물 등을 운송하므로 원자재에 대한 글로벌 수요와 공급을 간접적으로 측정 원자재 소요에 대한 미래의 경제선행지표로도 간주되기도 함\n구체적인 운송수요가 있을 때만 예약되는 벌크선 특성상 특정 목적에 의해 조정되는 경우가 적음\n위키백과에서 위의 내용들 발췌 : https://en.wikipedia.org/wiki/Baltic_Dry_Index\n\n발틱운임지수(BDI)를 데이터로 사용하는 것으로 의견 제안 예정\n\n주요 생산국에 대한 여러 요소의 고려\n\n전쟁, 전염병, 인건비 등을 대상으로 고려할 수는 없을지에 대한 고민\n\n수치화하기 어렵거나, 자료를 구하기 어려울 것으로 보여 일단 Drop"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "",
    "text": "스터디 진행하며 진행한 과제 기록(MNIST, Softmax)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html#개요",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "개요",
    "text": "개요\n참여중인 딥러닝 스터디 3주차 기록입니다.\n\nSoftmax로 MNIST다루기\n강사님이 주신 샘플코드 참고해서, 나에게 맞추거나 추가공부 진행"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html#과제-작성-softmax-linear",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html#과제-작성-softmax-linear",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "과제 작성 (Softmax / Linear)",
    "text": "과제 작성 (Softmax / Linear)\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n\n\nMnist Dataset로딩 및 전처리\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nfor i in (x_train, y_train, x_test, y_test):\n    print(i.shape)\n\n(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n\n\n\nfloat변환\n\nx_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n\n\n\nFlatten\n\n# Flatten (num_features=784)\nnum_features = 784 # 28*28 (Data의 Shape)\n\nprint('Flatten전 : ', x_train.shape, x_test.shape)\nx_train, x_test = x_train.reshape(-1, num_features), x_test.reshape(-1, num_features)\nprint('Flatten후 : ', x_train.shape, x_test.shape)\n\nFlatten전 :  (60000, 28, 28) (10000, 28, 28)\nFlatten후 :  (60000, 784) (10000, 784)\n\n\n\n\nNormalize\n\n# Normalize (0~255사이의 값을 0~1 사이의 값으로)\nx_train, x_test = x_train / 255., x_test / 255.\n\n\n\n\n함수 및 파라메터 설정\n\n# Parameters\nlearning_rate = 0.01\ntraining_steps = 1000\nbatch_size = 256\n\nnum_classes = 10 # MNIST의 0~9 숫자 10개\nnum_features = 784 # 28*28 (Data의 Shape)\n\n# Variables\nW = tf.Variable(tf.ones([num_features, num_classes]), name='weight')\nb = tf.Variable(tf.zeros([num_classes]), name='bias')\n\n# Functions\ndef softmax(x):\n    z = tf.matmul(x, W) + b\n    sm = tf.nn.softmax(z)\n    return sm\n\ndef cross_entropy(y_pred, y_true):\n    y_true = tf.one_hot(y_true, depth=num_classes)\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.) # clip_by_value에서 1e-9 최소값지정사유 : \n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1))\n\n\ntf.nn.softmax : 0~1 사이로 출력\ntf.one_hot(y_true, depth=num_classes) : y_true인 대상을 depth에 맞춰 원핫인코딩\n  # 샘플\n  tf.one_hot([2], depth=5).numpy()\n  &gt;&gt;&gt; array([[0., 0., 1., 0., 0.]], dtype=float32)\ntf.clip_by_value(y_pred, 1e-9, 1.) : y_pred인 대상을 제시한 min, max에 맞춰 변환\n  # 샘플\n  # 변환 전\n  t = tf.constant([[-1, 0, 1], [2, 3, 4]], dtype=tf.float32)\n  t.numpy()\n  &gt;&gt;&gt; array([[-1.,  0.,  1.],\n     [ 2.,  3.,  4.]], dtype=float32)\n  # 변환 후\n  tf.clip_by_value(t, clip_value_min=0, clip_value_max=2).numpy()\n  &gt;&gt;&gt; array([[0., 0., 1.],\n     [2., 2., 2.]], dtype=float32)\n\n\n# Optimization\ndef accuracy(y_pred, y_true):\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = 0)\n\ndef batch_maker(text, data_row, batch_size):\n    random_idx = np.random.randint(data_row, size = batch_size)\n    return x_train[random_idx], y_train[random_idx]\n\ndef run_optimization(x, y):\n    with tf.GradientTape() as g:\n        pred = softmax(x)\n        cost = cross_entropy(pred, y)\n\n    gradients = g.gradient(cost, [W, b])\n\n    optimizer.apply_gradients(zip(gradients, [W, b]))\n\n# Optimizer (Stochastic Gradient Descent)\noptimizer = tf.optimizers.SGD(learning_rate)\n\nfor step in range(training_steps): # training_steps = 1000로 위에서 지정해둠\n    batch_x, batch_y = batch_maker('training', 60000, batch_size) # batch_size = 256로 위에서 지정해둠\n\n    run_optimization(batch_x, batch_y)\n\n    if step % 100 == 0:\n        pred = softmax(batch_x)\n        cost = cross_entropy(pred, batch_y)\n        acc = accuracy(pred, batch_y)\n        print(f\"Step : {step} | loss : {cost} / accuracy : {acc}\")\n\nStep : 0 | loss : 2.287696123123169 / accuracy : 0.6875\nStep : 100 | loss : 1.5578452348709106 / accuracy : 0.7578125\nStep : 200 | loss : 1.188745379447937 / accuracy : 0.8359375\nStep : 300 | loss : 1.010088324546814 / accuracy : 0.796875\nStep : 400 | loss : 0.8610934019088745 / accuracy : 0.859375\nStep : 500 | loss : 0.807215690612793 / accuracy : 0.8359375\nStep : 600 | loss : 0.7144550085067749 / accuracy : 0.8671875\nStep : 700 | loss : 0.7559677362442017 / accuracy : 0.81640625\nStep : 800 | loss : 0.6656553745269775 / accuracy : 0.8515625\nStep : 900 | loss : 0.59657222032547 / accuracy : 0.8828125\n\n\n\n\n학습 후 Validation(Test)\n\n# W와 b가 학습된 model로 Validation(Test Dataset사용)\nprediction = softmax(x_test)\nprint(f\"Test accuracy : {accuracy(prediction, y_test)}\")\n\nTest accuracy : 0.8716999888420105\n\n\n\n\n추가적으로 검증기능 구현해보기\n\n추가적으로, 아래의 기능을 구현해보았음\n\nnumber_to_look을 입력하여 원하는 횟수만큼 모델검증\nrandint를 활용하여 랜덤추출, tested_list로 추출내역 관리하여 중복회피\npyplot으로 형태/예측/정답을 시각화\n\n\n\nimport koreanize_matplotlib\n\n# 시각화로 Validation 확인\nnumber_to_look = 4\ntested_list = []\n\nfor i in range(number_to_look):\n    plt.figure(figsize=(2,2))\n    # 테스트할 데이터 랜덤추출\n    idx = -1\n    while idx not in tested_list:\n        idx = np.random.randint(0, y_test.shape[0])\n        tested_list.append(idx)\n\n    # 결과 확인\n    plt.xlabel(f\"예상:{np.argmax(prediction[idx])} | 정답:{y_test[idx]}\")\n    plt.imshow(np.reshape(x_test[idx], [28, 28]), cmap=plt.cm.binary)\n    plt.show()"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "",
    "text": "스터디 진행하며 진행한 과제 기록(MNIST, Neural Network)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html#개요",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "개요",
    "text": "개요\n참여중인 딥러닝 스터디 3주차 기록입니다.\n\nNeural Network로 MNIST다루기\n강사님이 주신 샘플코드 참고해서, 나에게 맞추거나 추가공부 진행"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html#과제-작성-neuralnetwork-nonlinear",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html#과제-작성-neuralnetwork-nonlinear",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "과제 작성 (NeuralNetwork / NonLinear)",
    "text": "과제 작성 (NeuralNetwork / NonLinear)\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom keras.utils import to_categorical\n\nfrom tensorflow.keras.datasets import mnist\n\n\nMnist Dataset로딩 및 전처리\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nfor i in (x_train, y_train, x_test, y_test):\n    print(i.shape)\n\n(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n\n\n\n# Shape 오류 발생하여 원핫인코딩 수행\n# X는 (28, 28)인데 Y는 그냥 정답(5면 5)여서 그런듯 함\n# ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 10)\n\nx_train = x_train.reshape((60000, 28, 28, 1))\ny_train_one_hot = to_categorical(y_train, num_classes=10)\n\nx_test = x_test.reshape((10000, 28, 28, 1))\ny_test_one_hot = to_categorical(y_test, num_classes=10)\n\n\n\n모델구성\n\nCodestral에게 MNIST데이터셋에 적합한 파라미터와 레이어로 조정해달라고 하여 맞춤\n수업 때 거의 ReLU에요라고 들었는데 마지막 빼고는 거의 ReLU가 사용되었음\nSoftmax는 주로 마지막 층에 쓰인다고 들은 적이 있는데 여기서도 동일하게 되었음 (추가로 알아볼때도 주로 마지막레이어에 쓰인다는 내용 다수 확인)\n\n\n# 모델 구성\n# Mistral에게 Mnist데이터셋이 적합한 레이어와 파라메터로 구성해달라고 해서 조정\nmodel = Sequential([\n    layers.Input((28,28,1)),\n    layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n    layers.Dropout(0.25),\n    layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\nModel: \"sequential_14\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_40 (Conv2D)              │ (None, 28, 28, 32)     │           320 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_52          │ (None, 28, 28, 32)     │           128 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_41 (Conv2D)              │ (None, 28, 28, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_53          │ (None, 28, 28, 64)     │           256 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_27 (MaxPooling2D) │ (None, 14, 14, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_39 (Dropout)            │ (None, 14, 14, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_42 (Conv2D)              │ (None, 14, 14, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_54          │ (None, 14, 14, 128)    │           512 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_28 (MaxPooling2D) │ (None, 7, 7, 128)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_40 (Dropout)            │ (None, 7, 7, 128)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_14 (Flatten)            │ (None, 6272)           │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (Dense)                │ (None, 256)            │     1,605,888 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_55          │ (None, 256)            │         1,024 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_41 (Dropout)            │ (None, 256)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (Dense)                │ (None, 10)             │         2,570 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 1,703,050 (6.50 MB)\n\n\n\n Trainable params: 1,702,090 (6.49 MB)\n\n\n\n Non-trainable params: 960 (3.75 KB)\n\n\n\n\n\n모델학습 및 학습과정 시각화\n\nhistory = model.fit(x_train, y_train_one_hot, epochs=10, batch_size=64, verbose=0)\nhistory.history\n\n{'accuracy': [0.9526000022888184,\n  0.9849333167076111,\n  0.9872333407402039,\n  0.9904000163078308,\n  0.9906499981880188,\n  0.9917166829109192,\n  0.9925500154495239,\n  0.9933333396911621,\n  0.9935333132743835,\n  0.9948333501815796],\n 'loss': [0.09701579809188843,\n  0.014779138378798962,\n  0.01159658282995224,\n  0.009130637161433697,\n  0.008214634843170643,\n  0.007283014710992575,\n  0.0065074339509010315,\n  0.005834747105836868,\n  0.005641818046569824,\n  0.004907044116407633]}\n\n\n\nplt.plot(history.history['loss'], label='Train loss')\nplt.xlabel('The number of learning')\nplt.ylabel('Cost')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n모델평가 및 모델 활용한 예측\n\n모델평가\n\n\nmodel.evaluate(x_test, y_test_one_hot, verbose=0)\n\n[0.003361478913575411, 0.994700014591217]\n\n\n\n예측\n\n\ny_predict_one_hot = model.predict(x_test)\ny_predict = np.argmax(y_predict_one_hot, axis=1)\ny_predict\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step\n\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=int64)\n\n\n\ny_test\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n\n\n\n\n[추가] 예측치가 틀린값 추출 후 시각화해보기\n\n예측 틀린 값 확인\n\n\nimport pandas as pd\n\ndf_answersheet = pd.DataFrame(y_test, columns=['Y_test(정답)'])\ndf_answersheet['Y_pred(예측)'] = np.argmax(y_predict_one_hot, axis=1)\ndf_answersheet['비교'] = (df_answersheet['Y_test(정답)'] == df_answersheet['Y_pred(예측)'])\ndf_answersheet\n\n\n\n\n\n\n\n\n\nY_test(정답)\nY_pred(예측)\n비교\n\n\n\n\n0\n7\n7\nTrue\n\n\n1\n2\n2\nTrue\n\n\n2\n1\n1\nTrue\n\n\n3\n0\n0\nTrue\n\n\n4\n4\n4\nTrue\n\n\n...\n...\n...\n...\n\n\n9995\n2\n2\nTrue\n\n\n9996\n3\n3\nTrue\n\n\n9997\n4\n4\nTrue\n\n\n9998\n5\n5\nTrue\n\n\n9999\n6\n6\nTrue\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\n\ndf_answersheet['비교'].value_counts()\n\n비교\nTrue     9947\nFalse      53\nName: count, dtype: int64\n\n\n\n예측 틀린 값들의 시각화 및 정답/예측치 비교\n\n\nidx_false = df_answersheet[df_answersheet['비교'] == False].index\nidx_false\n\nIndex([ 445,  449,  947, 1014, 1232, 1242, 1247, 1709, 1878, 1901, 2035, 2070,\n       2118, 2130, 2135, 2414, 2454, 2597, 2654, 2896, 2939, 2953, 3422, 3520,\n       3808, 3985, 4027, 4176, 4284, 4571, 4639, 4699, 4740, 4761, 5749, 5955,\n       6571, 6576, 6597, 6625, 8408, 9009, 9015, 9019, 9587, 9620, 9638, 9642,\n       9664, 9679, 9692, 9698, 9729],\n      dtype='int64')\n\n\n\nimport koreanize_matplotlib\n\n# 시각화로 Validation 확인\nfor i in idx_false.tolist():\n    plt.figure(figsize=(2,2))\n\n    # 결과 확인\n    plt.xlabel(f\"정답:{y_test[i]} | 예상:{y_predict[i]}\")\n    plt.imshow(np.reshape(x_test[i], [28, 28]), cmap=plt.cm.binary)\n    plt.show()"
  }
]