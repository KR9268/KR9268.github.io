[
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html",
    "title": "[Python] COO발급관리용 Tool",
    "section": "",
    "text": "부서원 모두의 공통 업무이며, 여러가지 불편사항이 존재하나 자동화방안에 대한 기존 고민X\n\n필요에 의해 ID가 3개로 나뉘어있으며, 공동인증서 활용으로 타이핑작업 및 ID/PW/공동인증서PW의 관리 및 입력 불편함\n여러 건의 COO를 발급하고 대응하지만, 각 업무는 COO 1건별로 메뉴에 진입하여 수행해야함\n\n심사완료여부, 발급거절시 사유확인, 출력 및 사본 저장 등\n\n\n발급실적 담당자는 월마다 부서 전체의 발급실적을 관리하기위해 별도의 작업을 수행\n\n3개의 ID에 접속하여 20여개 페이지의 표를 복사하고, 중복/심사거절건 제거 등 수작업 가공 진행\n\n위 2가지 문제를 해결할 방법에 대한 고민 및 해결방안 도출 : 데이터의 통합db화 및 편의기능 추가\n\n공용PC를 운영중인 부서로 주기적으로 활동하는 크롤러를 운영하여 db로 저장가능\nstreamlit을 활용해 db에 대한 검색과 확인 가능한 대시보드형 사이트 제작하여 일반유저도 손쉽게 사용\n희망시 심사번호를 streamlit사이트에 등록하여 대응상황 발생시 toast알림이 가도록 제작\n축적된 db에서 월 데이터를 추출할 수 있도록 버튼을 누르면 SQL쿼리 후 Excel저장기능 추가\n3개의 ID에 대한 접속버튼으로 자동로그인 기능 추가\n\nstreamlit사이트의 정보관리메뉴에서 접속ID/PW등을 수정하면 공용PC의 json파일을 수정하여 모든유저에게 반영\n\n\n\n\n[용어설명] COO : 원산지증명서, Country of Origin\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#추진배경",
    "title": "[Python] COO발급관리용 Tool",
    "section": "",
    "text": "부서원 모두의 공통 업무이며, 여러가지 불편사항이 존재하나 자동화방안에 대한 기존 고민X\n\n필요에 의해 ID가 3개로 나뉘어있으며, 공동인증서 활용으로 타이핑작업 및 ID/PW/공동인증서PW의 관리 및 입력 불편함\n여러 건의 COO를 발급하고 대응하지만, 각 업무는 COO 1건별로 메뉴에 진입하여 수행해야함\n\n심사완료여부, 발급거절시 사유확인, 출력 및 사본 저장 등\n\n\n발급실적 담당자는 월마다 부서 전체의 발급실적을 관리하기위해 별도의 작업을 수행\n\n3개의 ID에 접속하여 20여개 페이지의 표를 복사하고, 중복/심사거절건 제거 등 수작업 가공 진행\n\n위 2가지 문제를 해결할 방법에 대한 고민 및 해결방안 도출 : 데이터의 통합db화 및 편의기능 추가\n\n공용PC를 운영중인 부서로 주기적으로 활동하는 크롤러를 운영하여 db로 저장가능\nstreamlit을 활용해 db에 대한 검색과 확인 가능한 대시보드형 사이트 제작하여 일반유저도 손쉽게 사용\n희망시 심사번호를 streamlit사이트에 등록하여 대응상황 발생시 toast알림이 가도록 제작\n축적된 db에서 월 데이터를 추출할 수 있도록 버튼을 누르면 SQL쿼리 후 Excel저장기능 추가\n3개의 ID에 대한 접속버튼으로 자동로그인 기능 추가\n\nstreamlit사이트의 정보관리메뉴에서 접속ID/PW등을 수정하면 공용PC의 json파일을 수정하여 모든유저에게 반영\n\n\n\n\n[용어설명] COO : 원산지증명서, Country of Origin"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#효과",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#효과",
    "title": "[Python] COO발급관리용 Tool",
    "section": "효과",
    "text": "효과\n\n크롤러, db구축, 관리 및 편의기능을 포함한 대시보드형 사이트 구축으로 기존의 불편사항들을 해결\n\n자동로그인, 대응사항 toast알림으로 지속적인 새로고침 등 불필요한 작업제거\n월마다 진행되던 불필요한 데이터 가공작업 제거(SQL쿼리 월 조건등은 사용자가 strealit사이트에서 수정 가능)\n\n기존 발급거절 사유의 db화로 주요 케이스에 대한 분석 및 사전대응계획 수립가능\n\n주요 케이스는 사전에 신고한 제품의 단위와 신청시스템 단위의 차이로, 관련 담당자에 개선제언 예정"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#github-repository",
    "title": "[Python] COO발급관리용 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-monitoringCOO-20240220/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] COO발급관리용 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n저장할 db는 sqlite3으로 테이블 생성, 컬럼지정 등을 수행함 (컬럼별 조건은 하단 참조)\n\n\n접수번호 varchar PRIMARY KEY , → 대표Invoice와 고민했는데, 100% 유일값이라 Primary로 지정 증명서종류 varchar,  대표Invoice varchar(10), 접수일시 datetime,  처리상태 varchar,  Remark varchar\n\n\n각 기능은 아래의 파일로 나누어 개인/공용PC에서 실행\n\n\nMonitoringCOO(기본파일) : streamlit활용한 UI, json/pickle파일 읽기, 유저의 자동로그인, 월추출 데이터 저장 등\n\n마지막 스크레핑 시점을 표기하여 얼마나 최신화된 데이터인지 유저에게 공유\n\nMonitoringCOO_crawler : selenium으로 스크레핑, 스크레핑작업에 필요한 로그인 기능(pyautogui, pywin32로 이미지/키/윈도우 인식)\n\n유저가 기본파일에서 로그인기능을 사용하는 경우, 이 파일에서 import해서 사용하고 코드는 여기서 통합관리\n스크레핑작업은 기본적으로 Scheduler파일에서 실행되지만, 필요시 이 파일을 실행하여 수동 스크레핑 (코드는 여기서 통합관리)\n\nMonitoringCOO_push : 기본파일에서 유저가 등록해둔 대표Invoice번호를 db에서 조회하여, win11toast로 알림 (처음에는 파이썬과 호환성/속도가 좋은 pickle/list로 관리하고자 했으나, 사용자ID등 추가정보 관리가 필요하여 json/dict로 관리)\nScheduler : 스크레핑 주기/시간을 관리하는 파일. 주로 공용PC에서 작업 (9~17시 이후엔 데이터변경이 없으므로 이 시간대에만 작동하도록 설정, 서버설정 등을 고려하여 작업주기 반영 예정)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "",
    "text": "아래의 문제점/현황에 대해 인식\n\npdf서류(PI)에 시스템 등록에 필요한 주소 등 정보가 늘 빠져있어 별도 테이블을 참고하여 등록\n드래그 가능한 pdf를 제공받아 마우스로 일일히 드래그하여 복사/붙여넣기 반복\n제공자의 내부규정 문제로 1서류:1메일로 건별 메일로 수령, 많아지면 작업량 증가\n향후 분쟁대비를 위해, 주문번호를 파일명으로 하여 별도의 공용폴더에 저장\n여러 서류를 등록하다가 교차하여 잘못넣을시, 고객의 수입절차문제나 오배송 등 발생\n\n위의 문제점/현황으로 수입문제로 인해 고객이 물품을 받지 못할수 있는 상황을 방지하고자 해결방안 마련\n\npdf서류의 regex를 활용한 추출 및 별도 테이블의 정보를 매칭하여 정확도/속도 증가\n\n정확도 상승으로 인한 고객의 수입문제, 오배송 문제 등을 미연에 방지\n추출된 정보에 대해 검증조건을 부여하여 문제있을시 작업을 멈추고 유저에게 공유\n\n한국에서 홍콩으로 수출하지만, 책임 및 보험가입구간이 한국에서 미국으로 작성된 경우 등 논리오류\n\n\nwin32로 아웃룩에서 조건부 추출하여 여러 건에도 바로 서류를 추출하도록 설계\n추출한 주문번호로 파일명 지정 및 아카이브 자동 저장\n\n\n\n[용어설명] PI : 수출납품계약서로 발주자의 양식 등을 사용함, Proforma Invoice regex : 정규표현식, 특정한 규칙을 통해 문자를 검색/편집하는데 사용\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#추진배경",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "",
    "text": "아래의 문제점/현황에 대해 인식\n\npdf서류(PI)에 시스템 등록에 필요한 주소 등 정보가 늘 빠져있어 별도 테이블을 참고하여 등록\n드래그 가능한 pdf를 제공받아 마우스로 일일히 드래그하여 복사/붙여넣기 반복\n제공자의 내부규정 문제로 1서류:1메일로 건별 메일로 수령, 많아지면 작업량 증가\n향후 분쟁대비를 위해, 주문번호를 파일명으로 하여 별도의 공용폴더에 저장\n여러 서류를 등록하다가 교차하여 잘못넣을시, 고객의 수입절차문제나 오배송 등 발생\n\n위의 문제점/현황으로 수입문제로 인해 고객이 물품을 받지 못할수 있는 상황을 방지하고자 해결방안 마련\n\npdf서류의 regex를 활용한 추출 및 별도 테이블의 정보를 매칭하여 정확도/속도 증가\n\n정확도 상승으로 인한 고객의 수입문제, 오배송 문제 등을 미연에 방지\n추출된 정보에 대해 검증조건을 부여하여 문제있을시 작업을 멈추고 유저에게 공유\n\n한국에서 홍콩으로 수출하지만, 책임 및 보험가입구간이 한국에서 미국으로 작성된 경우 등 논리오류\n\n\nwin32로 아웃룩에서 조건부 추출하여 여러 건에도 바로 서류를 추출하도록 설계\n추출한 주문번호로 파일명 지정 및 아카이브 자동 저장\n\n\n\n[용어설명] PI : 수출납품계약서로 발주자의 양식 등을 사용함, Proforma Invoice regex : 정규표현식, 특정한 규칙을 통해 문자를 검색/편집하는데 사용"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#효과",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#효과",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "효과",
    "text": "효과\n\n정확도 향상으로 고객의 수입절차문제나 오배송을 미연에 방지하여 추가비용위험 제거 및 고객만족 제고\n건별 메일열람 - 논리오류 검증 - 시스템 등록(복사/붙여넣기, 별도테이블 참고) - 파일명 변경 및 저장 등 프로세스 제거 및 유저편의성 증대\n\n백그라운드에서 실행되며, 작업이 완료되면 윈도우 toast메시지로 알려 특이사항 발생시 인지 가능"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#github-repository",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-autoPIforl001-20231215/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\npywin32로 아웃룩을 제어하여 조건에 맞는 pdf첨부 열람 등 진행\n\nselenium은 chrome버전변경 등 영향이 커서 구현했다가 미사용\n\nxlwings로 Excel로 저장해둔 별도 참고용 테이블을 열람\n\nDRM암호화와 관계없이 파일을 읽을 수 있기 때문에 xlwings를 채택\n\nre로 pdf의 문자열을 검색하여 필요한 내용을 저장\nNERP_PI_LC(주요 ERP관련 기능에 대해 제작한 파이썬 패키지)으로 시스템 등록 등을 진행\nwin11toast로 모든 작업이 완료되면 알림\n[삭제기능] selenium으로 PI제공자에게 자동회신도 했었으나, chrome업데이트 등 안정성 문제로 제외\n\n아웃룩 등 smtp발송은 내부규정상 막혀있어 사용하지 않음"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "",
    "text": "Local L/C 업무에 대해 아래의 문제점을 인식함\n\n정보와 기능이 각기 다른 ERP메뉴에 산재되어 비효율/불편함 발생\n\n주요기능 : L/C수신, L/C등록, 세금계산서 조회, 물품수령증 조회, 은행 네고(제출)\n\n전자화된 정보임에도 각 서류간에 동일한 항목이 일치되게 기입되어있는지 눈으로 확인중\n\n예를 들어, 4개의 서류에 24자리의 세금계산서번호가 모두 똑같게 들어있는지 확인 필요\n이러한 공통정보 중 하나라도 틀리면 물품의 대금의 전부를 지급받지 못하므로 중요함\n\n각 서류와 행위에 대한 법 조항이 있어 준수해야하나, 모두 인지하고있기 어려움\n\n예를 들어, A서류가 발행되면 몇일 이내에 B서류를 제출해야 함\n\n\n문제점을 해결하기 위해 아래의 방안을 도출함\n\nDB활용\n\n각 항목을 테이블로 Primary, 공통키를 지정하여 하나의 DB로 통합\nDB의 날짜조건과 서류존재여부를 활용해 준수사항에 대해 유저에게 지시\nDB의 정보를 대조하여 유저에게 결과를 공유\n대시보드용으로 많이 사용하는 streamlit으로 UI제공\n\n업무자동화\n\nDB내용을 기반으로 유저가 직접하던 등록이나 은행제출 자동화기능 수행\n\n\n\n\n[용어설명] L/C : 물품주문서이자 대금청구시 사용될 은행계좌와 비슷한 역할 (Letter of Credit, 신용장) 물품수령증 : 고객이 물건을 정상수령하였다는 서류, 대금청구에 사용할 수 있다 은행네고 : 정상수령했다는 서류 등을 제출하여, 은행에 준비되어있는 물품대금을 받을 수 있다\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#추진배경",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "",
    "text": "Local L/C 업무에 대해 아래의 문제점을 인식함\n\n정보와 기능이 각기 다른 ERP메뉴에 산재되어 비효율/불편함 발생\n\n주요기능 : L/C수신, L/C등록, 세금계산서 조회, 물품수령증 조회, 은행 네고(제출)\n\n전자화된 정보임에도 각 서류간에 동일한 항목이 일치되게 기입되어있는지 눈으로 확인중\n\n예를 들어, 4개의 서류에 24자리의 세금계산서번호가 모두 똑같게 들어있는지 확인 필요\n이러한 공통정보 중 하나라도 틀리면 물품의 대금의 전부를 지급받지 못하므로 중요함\n\n각 서류와 행위에 대한 법 조항이 있어 준수해야하나, 모두 인지하고있기 어려움\n\n예를 들어, A서류가 발행되면 몇일 이내에 B서류를 제출해야 함\n\n\n문제점을 해결하기 위해 아래의 방안을 도출함\n\nDB활용\n\n각 항목을 테이블로 Primary, 공통키를 지정하여 하나의 DB로 통합\nDB의 날짜조건과 서류존재여부를 활용해 준수사항에 대해 유저에게 지시\nDB의 정보를 대조하여 유저에게 결과를 공유\n대시보드용으로 많이 사용하는 streamlit으로 UI제공\n\n업무자동화\n\nDB내용을 기반으로 유저가 직접하던 등록이나 은행제출 자동화기능 수행\n\n\n\n\n[용어설명] L/C : 물품주문서이자 대금청구시 사용될 은행계좌와 비슷한 역할 (Letter of Credit, 신용장) 물품수령증 : 고객이 물건을 정상수령하였다는 서류, 대금청구에 사용할 수 있다 은행네고 : 정상수령했다는 서류 등을 제출하여, 은행에 준비되어있는 물품대금을 받을 수 있다"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#효과",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#효과",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "효과",
    "text": "효과\n\nDB화 및 정보대조를 통한 Human error제거 및 물품대금 전부에 대한 정상 입수\n법적 준수사항에 대한 미준수로 인한 여러 Risk제거\n유저가 기능별 메뉴를 이동할 필요없이 하나의 통합UI에서 업무를 해결\n\n편의기능 추가 : 조치대상 내역을 클립보드로 일괄복사해주는 기능 등"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#github-repository",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-local-20240122/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-local-20240122/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] LocalL/C 관리용 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n정보가 파편화되어있어 하나의 Tool로써 확인하고 관리하기 위해서 Streamlit 기반으로 만듦\n\n\n정보 저장 및 조회\n\n내부정보는 ERP에서 가져와서 db에 적재(SAP Scripting활용을 위한 win32com 사용)\n외부정보는 xml을 읽어서 Tag로 필요한 정보를 찾아 db에 적재(Beautifulsoup, sqlite3 사용) (외부정보라고는 하나, ERP에 저장되어있는 xml을 불러들여서 사용함)\n데이터 저장 및 최초 쿼리는 SQL문으로 가져오나, join등 필요한 사후처리는 pandas를 활용\n\n\n\nERP에 직접 입력하는 등의 수작업을 자동으로 수행\n\n\n자동화 기능\n\nERP 수주내역 등록(고정정보는 Master화, 변동정보는 Streamlit 텍스트박스 활용)\n준수사항(법령 등)의 자동체크\n\n특정 날짜 내에 완료해야한다던가, 일치해야하는 내용 등을 자동으로 검수\nStreamlit의 table내 체크박스표기(True,False)를 활용하여 이상여부를 직관적으로 확인 가능\n사용자가 어떤 행동을 해야하는지 참고사항란을 통해 지시(연장요청, 수령증발행요청 등)\n\n보유내역 및 관리대상(작업이 완료되지 않은 건)의 Filter 기능 제공(드롭박스로 선택)\nERP의 ID, PW를 입력해두어 작업 자동화 수행\n\n개인PC에서만 사용하는 Tool이며, 표기는 ***과 같이 암호화 표기되어 관리\n\n\n\n\n설계시 고려사항, 특이사항, 참고사항\n\n\n추가/삭제/변경 등 변동될 수 있는 정보는 Hardcoding이 아닌 db형태로 저장\n\n오류 등 상황에 대비하여 실행시 기존 db를 복사해두는 로직 구현해두었으나, 자주 실행시 과생성되어 향후 수정 예정\n\nERP관련 기능은 SAP메뉴(T-code)기준으로 함수화하여 관리\n정보조회 관련 기능은 기능별 dataframe 생성/변환하는 방향으로 함수화하여 관리\nERP제어(SAP Scripting) 주요기능을 구현한 ’NERP_PI_LC’는 자체제작한 것으로 정리하여 업로드 예정(pip 미등록)\n수익자기준 주요 EDI코드(참고용 기록) : 내국신용장(LOCADV), 물품수령증(LOCRCT)\n\nBeautifulSoup를 위해 정리해둔 딕셔너리(locrct_id, locadv_id)는 표준규격일 것으로 예상하여 재사용가능할 것으로 예상\n\n\n\nLocal L/C에 대한 세부정보 참고가능한 사이트\n\nKTNET - 이용안내 - 상세업무절차"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "",
    "text": "본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\nrepository에서 바로 복사해서 사용가능한 샘플을 제공하여 쉽게 데이터로딩 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n추가하고자 하는 데이터가 공공데이터포털의 데이터라면 쉽게 작업내역에 추가가능 (리스트 추가권한을 주거나, discord봇 접수 등 생각해보았으나 구현시간대비 효용이 작아 직접 추가진행)\n\n개인서버(NAS)에서 매일 특정시간 구동하여, github서버로 자동 push(업로드)하여 별도의 수작업없이 db 최신화\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#추진배경",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#추진배경",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "",
    "text": "본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\nrepository에서 바로 복사해서 사용가능한 샘플을 제공하여 쉽게 데이터로딩 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n추가하고자 하는 데이터가 공공데이터포털의 데이터라면 쉽게 작업내역에 추가가능 (리스트 추가권한을 주거나, discord봇 접수 등 생각해보았으나 구현시간대비 효용이 작아 직접 추가진행)\n\n개인서버(NAS)에서 매일 특정시간 구동하여, github서버로 자동 push(업로드)하여 별도의 수작업없이 db 최신화"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#효과",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#효과",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "효과",
    "text": "효과\n\n특정 공공데이터 포털의 데이터를 사용하기 위해 했던 아래의 작업이 제거됨\n\n공공데이터포털에 접속하여 csv파일 저장\n저장해둔 csv파일 찾기\n해당 파일을 분석했던 jupyter파일로 찾아가 API호출주소나 키를 찾기\ncsv파일을 저장한 뒤 추가 업데이트가 되었는지 확인하고 재다운로드\n한국어 데이터 로딩을 위한 encoding규격 등의 숙지가 필요없음 (저장시 규격을 지정하고, 로딩을 위한 샘플코드에 규격을 지정해둠)\n\n위의 사항을 달성하기 위한 부가작업이 제거됨\n\ncsv파일 저장 및 업데이트 시기 기록, 서버에 업로드 및 파일주소 저장"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#github-repository",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#github-repository",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "github repository",
    "text": "github repository\n관련 github레포\n\nAPI개인키 등 정보가 있어 레포에는 csv파일과 현황만 저장함 (구현을 위해 필요한 db_list나 개인키 등의 json파일은 모두 ignore리스트로 관리)\ngit clone해둔 개인서버 디렉토리에서 자동 실행"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\njson_load(json) : 개인키와 db_list 로딩을 위한 함수. 어려운 기능은 아니지만 여러번 쓰여 함수화 \ndownload_from_data_go_kr_with_json(requests, json) : 공공데이터포털 API호출용 (실제 자료 + 전체/다운 현황 등 수신)\nchk_json_status_of_data_go_kr : 전체/실제 건수 등 별도 처리용(대조하여 누락없이 전체자료 저장하기 위함)\nupdate_readme : README파일에 파일 현황(데이터명/주소/링크 등) 업데이트용\ngit_push(subprocess) : 새롭게 추가된 모든사항(ignore대상 제외)을 자동으로 업로드(auto commit)하기 위한 용도\n위의 함수들을 기반으로 아래의 내용을 구현\n\n\njson으로 민감정보 등을 코드에 표기하지 않고 별도관리 (+ gitignore로 이중보안)\n1건만 먼저 호출하여 전체건수를 확인 후, 한번에 전체 건을 호출 (10000건 지정 등 과도한 세팅값의 하드코딩 지양)\npandas로 encoding cp949 지정하여 csv로 저장\ngithub raw링크형식을 활용하여 바로 파일로딩할 수 있도록 주소 생성(db에서 아래 형식으로 지정해둔 파일명 사용)\n\n[\n  {\n  \"name\": \"공공데이터포털의 데이터명\",\n  \"base_url\": \"API Base Url\",\n  \"address_get\": \"해당 데이터의 API주소(API목록 란에서 확인)\",\n  \"file_name_to\": \"CSV로 저장할 파일명(.csv제외)\"\n  },\n  {\n  \"name\": \"공공데이터포털의 데이터명2\",\n  \"base_url\": \"API Base Url\",\n  \"address_get\": \"해당 데이터의 API주소(API목록 란에서 확인)\",\n  \"file_name_to\": \"CSV로 저장할 파일명(.csv제외)\"\n  }\n]\n\ntime.sleep()으로 너무 자주 호출하여 서버에 무리가지 않도록 세팅"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "샘플코드(패키지 및 함수)",
    "text": "샘플코드(패키지 및 함수)\n\nimport requests\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport subprocess\nimport os\nimport time\n\ndef json_load(json_path, encoding='utf-8'):\n    with open(json_path, 'r', encoding=encoding) as file:\n        json_data = json.load(file)\n    return json_data\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\ndef update_readme(new_content_list):\n    # Open the README.md file in read mode\n    with open('README.md', 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    # Find the index of the line that starts with '* 데이터 현황'\n    index = next((i for i, line in enumerate(lines) if line.startswith('* 데이터 현황')), None)\n\n    # If the line is found, remove the following lines and insert new content\n    if index is not None:\n        lines = lines[:index+1] # Remove the following lines\n        #lines.extend(new_content) # Insert new content\n        lines.extend(new_content_list) # Insert new content\n\n    # Open the README.md file in write mode and write the updated content\n    with open('README.md', 'w', encoding='utf-8') as file:\n        file.writelines(lines)\n\ndef git_push():\n    # Get a list of all .csv files in the current directory\n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    # Stage all .csv files\n    for file in csv_files:\n        subprocess.call(['git', 'add', file])\n\n    subprocess.call(['git', 'add', 'README.md'])\n    # Commit the changes with a message\n    subprocess.call(['git', 'commit', '-m', 'Automatic commit'])\n\n    # Push the changes to the remote repository\n    subprocess.call(['git', 'push'])"
  },
  {
    "objectID": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수-1",
    "href": "posts_miniprojects/dtcontest-ore-py-datagokr/index.html#샘플코드패키지-및-함수-1",
    "title": "[Python] Github, API활용한 공공데이터 저장소 만들기",
    "section": "샘플코드(패키지 및 함수)",
    "text": "샘플코드(패키지 및 함수)\n\n# json load\nserviceKey = json_load('option.json')['serviceKey']\ndb_list = json_load('db_list.json', 'cp949')\n\n\n# main\n\n# 작업하기\ntxt_for_readme = ['\\n']\nfor each in db_list:\n    # 다운로드\n    url = f\"{each['base_url']}{each['address_get']}?page=1&perPage=1&serviceKey={serviceKey}\"\n    json_data = download_from_data_go_kr_with_json(url)\n    result_data = chk_json_status_of_data_go_kr(json_data)\n    \n    # 저장\n    if result_data['currentCount'] == result_data['totalCount']:\n        pd.json_normalize(json_data['data']).to_csv(f\"{each['file_name_to']}.csv\",encoding='cp949', index=False)\n\n    # 파일주소 및 이름, 업데이트시간 저장\n    owner = 'KR9268'\n    repo = 'db_datagokr'\n    branch = 'main'\n    file_path = f\"{each['file_name_to']}.csv\"\n\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}\"\n\n    txt_for_readme.append(f\"  *  {datetime.strftime(datetime.now(),'%Y-%m-%d')}업데이트 : {each['name']}\\n{url}\\n\")\n    time.sleep(1)\n\n# 업데이트 내역과 파일 git push\nupdate_readme(txt_for_readme)\ngit_push()"
  },
  {
    "objectID": "index_miniprojects.html",
    "href": "index_miniprojects.html",
    "title": "Mini Projects",
    "section": "",
    "text": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)\n\n\n\nPython\n\n\nscikit-learn\n\n\nKNeighborsClassifier\n\n\ncategory_encoders\n\n\noptuna\n\n\nOptunaSearchCV\n\n\njoblib\n\n\nsqlite3\n\n\npandas\n\n\n\nPython, Scikit-learn(KNeighborsClassifier 등)을 활용한 운송수단 예측 머신러닝 모델 (진행중, category_encoders변환 후 OptunaSearchCV 파라미터 최적화 진행)\n\n\n\nKibok Park\n\n\n2024-08-01\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] Github, API활용한 공공데이터 저장소 만들기\n\n\n\nPython\n\n\nrequests\n\n\njson\n\n\nsubprocess\n\n\npandas\n\n\n\n[Python] requests(공공데이터API호출용), json(json규격 자료저장), subprocess(github push자동화), pandas(csv저장)\n\n\n\nKibok Park\n\n\n2024-06-10\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool\n\n\n\nPython\n\n\nSAP Scripting\n\n\n\nPython, win32를 활용한 SAP Scripting\n\n\n\nKibok Park\n\n\n2024-03-06\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] COO발급관리용 Tool\n\n\n\nPython\n\n\nwin11toast\n\n\nsqlite3\n\n\nstreamlit\n\n\npandas\n\n\nselenium\n\n\n\n[Python] selenium(웹스크레핑), sqlite3(db), win11toast(알림), streamlit(UI)\n\n\n\nKibok Park\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] LocalL/C 관리용 Tool\n\n\n\nPython\n\n\nSAP Scripting\n\n\nStreamlit\n\n\nsqlite3\n\n\nBeautifulSoup\n\n\n\nPython, Streamlit을 활용한 업무자동화\n\n\n\nKibok Park\n\n\n2024-01-22\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] 아웃룩 메일열람 & pdf regex리딩 & 시스템 자동등록\n\n\n\nPython\n\n\nre\n\n\nxlwings\n\n\npandas\n\n\npdfminer\n\n\n\n[Python] re(regex), xlwings(암호화 Excel리딩), pdfminer(pdf리딩), pywin32(outlook)\n\n\n\nKibok Park\n\n\n2023-12-15\n\n\n\n\n\n\n\n\n\n\n\n\n[Python] Peak타임 대응용 수출계약서pdf tabula리딩\n\n\n\nPython\n\n\ntabula\n\n\nxlwings\n\n\npathlib\n\n\npandas\n\n\n\n[Python] tabula(pdf리딩[표 형태], xlwings(암호화 Excel리딩)\n\n\n\nKibok Park\n\n\n2023-11-02\n\n\n\n\n\n\n\n\nNo matching items\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 4주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#과제-설명",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#과제-설명",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "과제 설명",
    "text": "과제 설명\n\n과제 : 월간 데이콘 신용카드 사용자 연체 예측 AI 경진대회\n\nhttps://dacon.io/competitions/official/235713/overview/description\n\n아래 내용 진행해보기\n\n불균형데이터에 대해 다양한 불균형처리기법 사용해보기\n최종 모델 결정해보기"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#전처리-해둔-데이터-읽고-데이터셋-나누기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#전처리-해둔-데이터-읽고-데이터셋-나누기",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "전처리 해둔 데이터 읽고 데이터셋 나누기",
    "text": "전처리 해둔 데이터 읽고 데이터셋 나누기\n\nfrom pkb_sqlite3 import DB_sqlite3\n\ndb_controller = DB_sqlite3('Dacon_creditcard_overdue.db')\ndf_train = db_controller.search_db_show_df('SELECT * FROM train')\ndf_train_pre = db_controller.search_db_show_df('SELECT * FROM train_pre')\ndf_test = db_controller.search_db_show_df('SELECT * FROM test_pre')\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ntrain = pd.concat([df_train_pre, df_train['credit']], axis=1)\nx_test = df_test.copy()\n\nx_train, x_validate = train_test_split(train, test_size=0.3, random_state=42, stratify=train['credit'])"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#불균형데이터-처리실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#불균형데이터-처리실습",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "불균형데이터 처리실습",
    "text": "불균형데이터 처리실습\n\nUndersampling - RUS\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# RandomUnderSampler 적용\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_rus.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (6765, 35)\n\n\n\n\nUndersampling - ENN\n\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\n# ENN 적용\nenn = EditedNearestNeighbours()\nX_enn, y_enn = enn.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_enn.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (5712, 35)\n\n\n\n\nUndersampling - Tomek Links\n\nfrom imblearn.under_sampling import TomekLinks\n\n# Tomek Links 적용\ntomek = TomekLinks()\nX_tomek, y_tomek = tomek.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_tomek.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (14117, 35)\n\n\n\n\nOversampling - ROS\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Random Oversampling 적용\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_ros.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (35631, 35)\n\n\n\n\nOversampling - SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\n# SMOTE Oversampling\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_smote.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (35631, 35)\n\n\n\n\nOversampling - ADASYN\n\nfrom imblearn.over_sampling import ADASYN\n\n# ADASYN Oversampling\nadasyn = ADASYN(random_state=42)\nX_adasyn, y_adasyn = adasyn.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_adasyn.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (35665, 35)\n\n\n\n\nHybrid Method - SMOTEENN\n\nfrom imblearn.combine import SMOTEENN\n\n# SMOTEENN Oversampling\nsmoteenn = SMOTEENN(random_state=42)\nX_smoteenn, y_smoteenn = smoteenn.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_smoteenn.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (10892, 35)\n\n\n\n\nHybrid Method - SMOTETomek\n\nfrom imblearn.combine import SMOTETomek\n\n# SMOTETomek Oversampling\nsmotetomek = SMOTETomek(random_state=42)\nX_smotetomek, y_smotetomek = smotetomek.fit_resample(x_train.drop(columns=['credit']), x_train['credit'])\n\n\nprint(f\"\"\"* 적용 전 {x_train.drop(columns=['credit']).shape}\n* 적용 후 {X_smotetomek.shape}\"\"\")\n\n* 적용 전 (18519, 35)\n* 적용 후 (31997, 35)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#lightgbm활용한-불균형처리별-성능비교",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#lightgbm활용한-불균형처리별-성능비교",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "LightGBM활용한 불균형처리별 성능비교",
    "text": "LightGBM활용한 불균형처리별 성능비교\n\n수업 중 가장 빠른 모델이었던 LightGBM을 활용해서 비교\n\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n\ndef compute_for_samplers(train, test, sampler, return_metric_only= False):\n  \n  X_sampled, y_sampled = sampler.fit_resample(train.drop(['credit'], axis = 1), train[\"credit\"])\n  model = LGBMClassifier(random_state = 42)\n  model.fit(X_sampled, y_sampled)\n  y_pred = model.predict(test.drop(['credit'], axis = 1))\n  y_proba = model.predict_proba(test.drop(['credit'], axis = 1))\n  y_test = test['credit']\n\n  accuracy = accuracy_score(y_test, y_pred)\n  logloss = log_loss(y_test, y_proba)\n  cf = confusion_matrix(y_test, y_pred)\n  if return_metric_only:\n    return accuracy, cf, auc\n  else:\n    return {'acc':accuracy,\n            'logloss':logloss,\n            'cf':cf,\n            'X_sampled' : X_sampled,\n            'y_sampled' : y_sampled,\n            'model' : model,\n            'y_pred' : y_pred,\n            'y_proba' : y_proba}\n\n\nfrom imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nsampler = {'RUS':RandomUnderSampler(random_state=42),\n            'ENN':EditedNearestNeighbours(),\n            'TOMEKLINKS':TomekLinks(),\n            'ROS':RandomOverSampler(random_state=42),\n            'SMOTE':SMOTE(random_state=42),\n            'ADASYN':ADASYN(random_state=42),\n            'SMOTEENN':SMOTEENN(random_state=42),\n            'SMOTETomek':SMOTETomek(random_state=42)}\nsampler_result = dict()\n\n\nfor each_sampler in sampler:\n    print(f'\\n{each_sampler}')\n    sampler_result[each_sampler] = compute_for_samplers(x_train, \n                                                        x_validate, \n                                                        sampler[each_sampler]\n                                                        )\n\n\nRUS\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1262\n[LightGBM] [Info] Number of data points in the train set: 6765, number of used features: 33\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n\nENN\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1252\n[LightGBM] [Info] Number of data points in the train set: 5712, number of used features: 33\n[LightGBM] [Info] Start training from score -0.929419\n[LightGBM] [Info] Start training from score -4.607273\n[LightGBM] [Info] Start training from score -0.518794\n\nTOMEKLINKS\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1303\n[LightGBM] [Info] Number of data points in the train set: 14117, number of used features: 33\n[LightGBM] [Info] Start training from score -1.834230\n[LightGBM] [Info] Start training from score -1.759900\n[LightGBM] [Info] Start training from score -0.403166\n\nROS\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1340\n[LightGBM] [Info] Number of data points in the train set: 35631, number of used features: 33\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n\nSMOTE\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003803 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7329\n[LightGBM] [Info] Number of data points in the train set: 35631, number of used features: 33\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n\nADASYN\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 7336\n[LightGBM] [Info] Number of data points in the train set: 35665, number of used features: 33\n[LightGBM] [Info] Start training from score -1.073800\n[LightGBM] [Info] Start training from score -1.123079\n[LightGBM] [Info] Start training from score -1.099566\n\nSMOTEENN\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6992\n[LightGBM] [Info] Number of data points in the train set: 10892, number of used features: 33\n[LightGBM] [Info] Start training from score -0.680013\n[LightGBM] [Info] Start training from score -0.892431\n[LightGBM] [Info] Start training from score -2.480144\n\nSMOTETomek\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 7273\n[LightGBM] [Info] Number of data points in the train set: 31997, number of used features: 33\n[LightGBM] [Info] Start training from score -1.086003\n[LightGBM] [Info] Start training from score -1.080280\n[LightGBM] [Info] Start training from score -1.130299\n\n\n\ndf = pd.DataFrame(sampler_result)\ndf.loc[['acc','logloss','cf']].transpose().sort_values('logloss')\n\n\n\n\n\n\n\n\nacc\nlogloss\ncf\n\n\n\n\nADASYN\n0.692618\n0.789301\n[[12, 139, 816], [7, 456, 1417], [7, 54, 5030]]\n\n\nSMOTE\n0.69287\n0.793357\n[[5, 137, 825], [3, 455, 1422], [4, 47, 5040]]\n\n\nSMOTETomek\n0.691358\n0.797204\n[[10, 141, 816], [13, 449, 1418], [4, 58, 5029]]\n\n\nTOMEKLINKS\n0.692996\n0.808826\n[[36, 119, 812], [24, 405, 1451], [14, 17, 5060]]\n\n\nROS\n0.612875\n0.930659\n[[281, 202, 484], [190, 780, 910], [737, 550, ...\n\n\nRUS\n0.504661\n1.011385\n[[378, 255, 334], [412, 834, 634], [1293, 1004...\n\n\nSMOTEENN\n0.325397\n1.237064\n[[387, 426, 154], [558, 1002, 320], [1889, 200...\n\n\nENN\n0.589317\n2.293334\n[[357, 0, 610], [633, 2, 1245], [772, 0, 4319]]\n\n\n\n\n\n\n\n\nUnder/Over/Hybrid Sampling별로 1가지씩 해보는 것으로 결정\n\nTOMEKLINKS\nADASYN\nSMOTETomek"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#모델-성능비교-및-선택",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#모델-성능비교-및-선택",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "모델 성능비교 및 선택",
    "text": "모델 성능비교 및 선택\n\n전처리한 기본 데이터를 기준으로 모델 비교 해보기(Weight 미적용)\n\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\nimport time\n\n# 모델 리스트\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42),\n    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n    \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n}\n\nrsts = {}\n\nsampler = {'TOMEKLINKS':TomekLinks(),\n           'ADASYN':ADASYN(random_state=42),\n           'SMOTETomek':SMOTETomek(random_state=42)}\n\nfor each_sample_type in sampler:\n    X_train, y_train = sampler[each_sample_type].fit_resample(x_train.drop(['credit'], axis = 1), x_train[\"credit\"])\n    X_test, y_test = sampler[each_sample_type].fit_resample(x_validate.drop(['credit'], axis = 1), x_validate[\"credit\"])\n\n    # 학습 및 평가\n    for name, model in models.items():\n        print(f\"\\n{name} + {each_sample_type}\")\n        start = time.time()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_proba = model.predict_proba(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        logloss = log_loss(y_test, y_proba)\n        cf = confusion_matrix(y_test, y_pred)\n        rsts[f\"{name} + {each_sample_type}\"] = {'acc':accuracy,\n                    'logloss':logloss,\n                    'cf':cf,\n                    'model' : model,\n                    'y_pred' : y_pred,\n                    'y_proba' : y_proba,\n                    'time' : time.time() - start,\n                    'classification_report':classification_report(y_test, y_pred)}\n        \n        print(f\"\"\"* logloss : {logloss}\"\"\")\n\n\nRandom Forest + TOMEKLINKS\n* logloss : 0.7620037176688708\n\nLightGBM + TOMEKLINKS\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000823 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1303\n[LightGBM] [Info] Number of data points in the train set: 14117, number of used features: 33\n[LightGBM] [Info] Start training from score -1.834230\n[LightGBM] [Info] Start training from score -1.759900\n[LightGBM] [Info] Start training from score -0.403166\n* logloss : 0.7692630280717329\n\nXGBoost + TOMEKLINKS\n* logloss : 0.7849133307714561\n\nCatBoost + TOMEKLINKS\n* logloss : 0.7740753039975535\n\nExtra Trees + TOMEKLINKS\n* logloss : 0.8551848758758333\n\nRandom Forest + ADASYN\n* logloss : 0.7478335144023597\n\nLightGBM + ADASYN\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7336\n[LightGBM] [Info] Number of data points in the train set: 35665, number of used features: 33\n[LightGBM] [Info] Start training from score -1.073800\n[LightGBM] [Info] Start training from score -1.123079\n[LightGBM] [Info] Start training from score -1.099566\n* logloss : 0.747220175202705\n\nXGBoost + ADASYN\n* logloss : 0.7657405243628196\n\nCatBoost + ADASYN\n* logloss : 0.7674443572640199\n\nExtra Trees + ADASYN\n* logloss : 0.8141975026995196\n\nRandom Forest + SMOTETomek\n* logloss : 0.7699790781735832\n\nLightGBM + SMOTETomek\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003627 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7273\n[LightGBM] [Info] Number of data points in the train set: 31997, number of used features: 33\n[LightGBM] [Info] Start training from score -1.086003\n[LightGBM] [Info] Start training from score -1.080280\n[LightGBM] [Info] Start training from score -1.130299\n* logloss : 0.7650168946174349\n\nXGBoost + SMOTETomek\n* logloss : 0.7927170149729137\n\nCatBoost + SMOTETomek\n* logloss : 0.7819625428548069\n\nExtra Trees + SMOTETomek\n* logloss : 0.8334713382827283\n\n\n\ndf_results = pd.DataFrame(rsts).loc[['acc','logloss','cf','time'],:]\ndf_results.transpose().sort_values('logloss')\n\n\n\n\n\n\n\n\nacc\nlogloss\ncf\ntime\n\n\n\n\n\\nLightGBM + ADASYN\n0.637518\n0.74722\n[[3159, 1178, 872], [1976, 1551, 1451], [7, 54...\n0.650462\n\n\n\\nRandom Forest + ADASYN\n0.64354\n0.747834\n[[3166, 1339, 704], [1861, 1966, 1151], [126, ...\n9.090805\n\n\n\\nRandom Forest + TOMEKLINKS\n0.717713\n0.762004\n[[138, 120, 709], [29, 278, 662], [79, 60, 3802]]\n2.631492\n\n\n\\nLightGBM + SMOTETomek\n0.623706\n0.765017\n[[2341, 1148, 754], [1543, 1491, 1199], [4, 41...\n0.58901\n\n\n\\nXGBoost + ADASYN\n0.630384\n0.765741\n[[2817, 1532, 860], [1738, 1868, 1372], [21, 1...\n0.861137\n\n\n\\nCatBoost + ADASYN\n0.635293\n0.767444\n[[2834, 1522, 853], [1671, 1919, 1388], [18, 1...\n17.005267\n\n\n\\nLightGBM + TOMEKLINKS\n0.711247\n0.769263\n[[36, 119, 812], [18, 228, 723], [11, 14, 3916]]\n0.349786\n\n\n\\nRandom Forest + SMOTETomek\n0.629805\n0.769979\n[[2352, 1274, 617], [1454, 1832, 947], [98, 22...\n7.562343\n\n\n\\nCatBoost + TOMEKLINKS\n0.709035\n0.774075\n[[57, 116, 794], [34, 232, 703], [23, 40, 3878]]\n6.656712\n\n\n\\nCatBoost + SMOTETomek\n0.622261\n0.781963\n[[2110, 1413, 720], [1313, 1784, 1136], [20, 1...\n15.503353\n\n\n\\nXGBoost + TOMEKLINKS\n0.708014\n0.784913\n[[84, 118, 765], [43, 232, 694], [46, 50, 3845]]\n0.490033\n\n\n\\nXGBoost + SMOTETomek\n0.611508\n0.792717\n[[2037, 1496, 710], [1360, 1726, 1147], [31, 9...\n1.109563\n\n\n\\nExtra Trees + ADASYN\n0.623511\n0.814198\n[[3054, 1437, 718], [1802, 2039, 1137], [197, ...\n5.692895\n\n\n\\nExtra Trees + SMOTETomek\n0.618891\n0.833471\n[[2352, 1304, 587], [1388, 1887, 958], [152, 3...\n5.461775\n\n\n\\nExtra Trees + TOMEKLINKS\n0.691339\n0.855185\n[[225, 105, 637], [65, 244, 660], [167, 180, 3...\n2.29099\n\n\n\n\n\n\n\n\nlogloss와 time을 기준으로 아래 2가지 조합 선정\n\nLightGBM + ADASYN\nRandom Forest + TOMEKLINKS"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#불균형-처리-모델선택-case별-비교",
    "href": "posts/meta-cm-sql_and_ml_xai-20250114/index.html#불균형-처리-모델선택-case별-비교",
    "title": "[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)",
    "section": "불균형 처리 + 모델선택 Case별 비교",
    "text": "불균형 처리 + 모델선택 Case별 비교\n\n앞서 테스트를 통해 구한 조합에 대해 테스트\n\n아래 조합으로 테스트\n\nLightGBM (Weighted)\nLightGBM + ADASYN\nRandom Forest (Weighted)\nRandom Forest + TOMEKLINKS\n\n\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import classification_report\nimport time\n\n# 모델 리스트\nmodels = {\n    \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", random_state=42),\n}\n\nrsts_weight = {}\n\nsampler = {'TOMEKLINKS':TomekLinks(),\n           'ADASYN':ADASYN(random_state=42)\n           }\n\nfor each_sample_type in sampler:\n    X_train, y_train = sampler[each_sample_type].fit_resample(x_train.drop(['credit'], axis = 1), x_train[\"credit\"])\n    X_test, y_test = sampler[each_sample_type].fit_resample(x_validate.drop(['credit'], axis = 1), x_validate[\"credit\"])\n\n    # 학습 및 평가\n    for name, model in models.items():\n        print(f\"\\n{name} + {each_sample_type}\")\n        start = time.time()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        y_proba = model.predict_proba(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        logloss = log_loss(y_test, y_proba)\n        cf = confusion_matrix(y_test, y_pred)\n        rsts_weight[f\"{name} + {each_sample_type}\"] = {'acc':accuracy,\n                    'logloss':logloss,\n                    'cf':cf,\n                    'model' : model,\n                    'y_pred' : y_pred,\n                    'y_proba' : y_proba,\n                    'time' : time.time() - start,\n                    'classification_report':classification_report(y_test, y_pred)}\n\n\nRandom Forest + TOMEKLINKS\n\nLightGBM + TOMEKLINKS\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001311 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1303\n[LightGBM] [Info] Number of data points in the train set: 14117, number of used features: 33\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n\nRandom Forest + ADASYN\n\nLightGBM + ADASYN\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004629 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7336\n[LightGBM] [Info] Number of data points in the train set: 35665, number of used features: 33\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n\n\n\ndf_results_weight = pd.DataFrame(rsts_weight).loc[['acc','logloss','cf','time'],:]\ndf_results_weight.transpose().sort_values('logloss')\n\n\n\n\n\n\n\n\nacc\nlogloss\ncf\ntime\n\n\n\n\nLightGBM + ADASYN\n0.633198\n0.750138\n[[3019, 1317, 873], [1902, 1624, 1452], [6, 54...\n1.112855\n\n\nRandom Forest + TOMEKLINKS\n0.717543\n0.751406\n[[134, 110, 723], [31, 260, 678], [68, 50, 3823]]\n2.637113\n\n\nRandom Forest + ADASYN\n0.6421\n0.757042\n[[3172, 1311, 726], [1891, 1940, 1147], [127, ...\n9.652027\n\n\nLightGBM + TOMEKLINKS\n0.602008\n0.92511\n[[305, 189, 473], [122, 377, 470], [658, 427, ...\n0.504209\n\n\n\n\n\n\n\n\n\n추가 테스트\n\n이진분류가 아닌 경우에 대해 한번 해보고 싶어서 적용\n\nCatboost (Weighted)\nXGBoost (Weighted)\n\n\n\n# 클래스 비율 계산\nclass_counts = x_train['credit'].value_counts()\nclass_weights = {cls: max(class_counts) / count for cls, count in class_counts.items()}\nclass_weights\n\n{2.0: 1.0, 1.0: 2.707317073170732, 0.0: 5.266962305986696}\n\n\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\nimport time\n\n# 모델 리스트\nmodels = {\n    # XGBoost : eval_metric=\"mlogloss\n    \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42),\n    # CatBoost : {0.0: 1.0, 1.0: 2.707317073170732, 2.0: 5.266962305986696}로 설정\n    \"CatBoost\": CatBoostClassifier(class_weights=[1.0, 2.707317073170732, 5.266962305986696]\n                                   , verbose=0, random_state=42),\n}\n\nX_train = x_train.drop(['credit'], axis = 1)\ny_train = x_train['credit']\nX_test = x_validate.drop(['credit'], axis = 1)\ny_test = x_validate['credit']\nrsts_additional = {}\n\n# 학습 및 평가\nfor name, model in models.items():\n    print(f\"\\n{name}\")\n    start = time.time()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    logloss = log_loss(y_test, y_proba)\n    cf = confusion_matrix(y_test, y_pred)\n    rsts_additional[name] = {'acc':accuracy,\n                  'logloss':logloss,\n                  'cf':cf,\n                  'model' : model,\n                  'y_pred' : y_pred,\n                  'y_proba' : y_proba,\n                  'time' : time.time() - start,\n                  'classification_report':classification_report(y_test, y_pred, zero_division=0)\n                  }\n    \n    print(f\"\"\"* logloss : {logloss}\"\"\")\n\n\nXGBoost\n* logloss : 0.7854300507522355\n\nCatBoost\n* logloss : 0.8862784774127822\n\n\n\ndf_results_additional = pd.DataFrame(rsts_additional).loc[['acc','logloss','cf','time'],:]\ndf_results_additional.transpose().sort_values('logloss')\n\n\n\n\n\n\n\n\nacc\nlogloss\ncf\ntime\n\n\n\n\nXGBoost\n0.696019\n0.78543\n[[56, 142, 769], [20, 524, 1336], [26, 120, 49...\n1.066831\n\n\nCatBoost\n0.693122\n0.886278\n[[0, 134, 833], [0, 444, 1436], [0, 33, 5058]]\n10.805283"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 최종과제\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#과제-목표-및-데이터-설명",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#과제-목표-및-데이터-설명",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "과제 목표 및 데이터 설명",
    "text": "과제 목표 및 데이터 설명\n\n과제 목표\n\n차량소유 여부 등 데이터를 바탕으로, 고객의 신용도를 예측(대금연체를 기준으로 한 신용도)\nLogloss의 최소화\n\n데이터 설명\n\n데이터 출처 : 월간 데이콘 신용카드 사용자 연체 예측 AI 경진대회 데이터\n\n세부 출처 : https://mp.weixin.qq.com/s/upjzuPg5AMIDsGxlpqnoCg\n\n변수 설명 (데이콘설명 원본링크)\n\ngender: 성별\ncar: 차량 소유 여부\nreality: 부동산 소유 여부\nchild_num: 자녀 수\nincome_total: 연간 소득\nincome_type: 소득 분류 [‘Commercial associate’, ‘Working’, ‘State servant’, ‘Pensioner’, ‘Student’]\nedu_type: 교육 수준 [‘Higher education’ ,‘Secondary / secondary special’, ‘Incomplete higher’, ‘Lower secondary’, ‘Academic degree’]\nfamily_type: 결혼 여부 [‘Married’, ‘Civil marriage’, ‘Separated’, ‘Single / not married’, ‘Widow’]\nhouse_type: 생활 방식 [‘Municipal apartment’, ‘House / apartment’, ‘With parents’, ‘Co-op apartment’, ‘Rented apartment’, ‘Office apartment’]\nDAYS_BIRTH: 출생일\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미\n\nDAYS_EMPLOYED: 업무 시작일\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미\n양수 값은 고용되지 않은 상태를 의미함\n\nFLAG_MOBIL: 핸드폰 소유 여부\nwork_phone: 업무용 전화 소유 여부\nphone: 전화 소유 여부\nemail: 이메일 소유 여부\noccyp_type: 직업 유형\n\nfamily_size: 가족 규모\nbegin_month: 신용카드 발급 월\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미\n\ncredit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도\n\n낮을 수록 높은 신용의 신용카드 사용자를 의미함"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#eda-기초통계-등",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#eda-기초통계-등",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "EDA : 기초통계 등",
    "text": "EDA : 기초통계 등\n\n데이터 로딩 및 train x,y로 분할\n\n\nfrom pkb_sqlite3 import DB_sqlite3\nimport pandas as pd\n\n# db로 저장해둔 데이터 읽기\ndb_controller = DB_sqlite3('Dacon_creditcard_overdue.db')\n\ndf_train = db_controller.search_db_show_df('SELECT * FROM train')\ndf_test = db_controller.search_db_show_df('SELECT * FROM test')\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\n# 데이터 나누기\nY_train = df_train['credit'].copy()\nX_train = df_train.drop('credit',axis=1).copy()\nX_test = df_test.copy()\n\n# 나눠진 데이터 확인\nprint(f\"\"\"* 전체 : {df_train.shape}\n* x_train : {X_train.shape}\n* y_train : {Y_train.shape}\"\"\")\n\n* 전체 : (26457, 20)\n* x_train : (26457, 19)\n* y_train : (26457,)\n\n\n\ndescribe()를 활용한 데이터 확인\n\nindex컬럼은 삭제\nunique와 top을 기준으로, gender, income_type 등 컬럼은 범주형 변수로 추측\ncount를 기준으로, occyp_type컬럼은 결측치가 많을 것으로 추측\ncount대비 freq값을 참고하여, house_type 등 컬럼이 편향되어 있음을 추측\n\n\n\nX_train.describe(include='all').transpose().reset_index()\n\n\n\n\n\n\n\n\nindex\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n0\nindex\n26457.0\nNaN\nNaN\nNaN\n13228.0\n7637.622372\n0.0\n6614.0\n13228.0\n19842.0\n26456.0\n\n\n1\ngender\n26457\n2\nF\n17697\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\ncar\n26457\n2\nN\n16410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nreality\n26457\n2\nY\n17830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nchild_num\n26457.0\nNaN\nNaN\nNaN\n0.428658\n0.747326\n0.0\n0.0\n0.0\n1.0\n19.0\n\n\n5\nincome_total\n26457.0\nNaN\nNaN\nNaN\n187306.524493\n101878.367995\n27000.0\n121500.0\n157500.0\n225000.0\n1575000.0\n\n\n6\nincome_type\n26457\n5\nWorking\n13645\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\nedu_type\n26457\n5\nSecondary / secondary special\n17995\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\nfamily_type\n26457\n5\nMarried\n18196\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\nhouse_type\n26457\n6\nHouse / apartment\n23653\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10\nDAYS_BIRTH\n26457.0\nNaN\nNaN\nNaN\n-15958.053899\n4201.589022\n-25152.0\n-19431.0\n-15547.0\n-12446.0\n-7705.0\n\n\n11\nDAYS_EMPLOYED\n26457.0\nNaN\nNaN\nNaN\n59068.750728\n137475.427503\n-15713.0\n-3153.0\n-1539.0\n-407.0\n365243.0\n\n\n12\nFLAG_MOBIL\n26457.0\nNaN\nNaN\nNaN\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n13\nwork_phone\n26457.0\nNaN\nNaN\nNaN\n0.224742\n0.41742\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n14\nphone\n26457.0\nNaN\nNaN\nNaN\n0.294251\n0.455714\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n15\nemail\n26457.0\nNaN\nNaN\nNaN\n0.09128\n0.288013\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n16\noccyp_type\n18286\n18\nLaborers\n4512\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n17\nfamily_size\n26457.0\nNaN\nNaN\nNaN\n2.196848\n0.916717\n1.0\n2.0\n2.0\n3.0\n20.0\n\n\n18\nbegin_month\n26457.0\nNaN\nNaN\nNaN\n-26.123294\n16.55955\n-60.0\n-39.0\n-24.0\n-12.0\n0.0"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#결측치-처리-직업유형occyp_type의-결측치는-무직을-의미할까",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#결측치-처리-직업유형occyp_type의-결측치는-무직을-의미할까",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "결측치 처리 : 직업유형(occyp_type)의 결측치는 무직을 의미할까?",
    "text": "결측치 처리 : 직업유형(occyp_type)의 결측치는 무직을 의미할까?\n\n결측치가 69%에 해당하여 삭제가 아닌 대체로 진행 필요함\n결측치에 맥락이나 의미가 있는지에 대한 고민\n\n직업유형에 ’무직’을 뜻하는 단어가 없음 → None은 무직을 의미하는가?\narray([None, 'Laborers', 'Managers', 'Sales staff',\n    'High skill tech staff', 'Core staff', 'Drivers', 'Medicine staff',\n    'Accountants', 'Realty agents', 'Security staff', 'Cleaning staff',\n    'Private service staff', 'Cooking staff', 'Secretaries',\n    'HR staff', 'IT staff', 'Low-skill Laborers',\n    'Waiters/barmen staff'], dtype=object)\n실제로 직업이 없음을 의미하는 데이터로 검증\n\n방법 : DAYS_EMPLOYED(고용일)이 양수인 경우 무직. 이를 대조함\n\n직업유형이 None인 값 중, 고용기간이 있는 값과 없는 값이 공존\n\n# DAYS_EMPLOYED 고용기간 있음\nlen(X_train[(X_train['occyp_type'].isna())&(X_train['DAYS_EMPLOYED']&lt;0)])\n&gt;&gt;&gt; 3733\n# DAYS_EMPLOYED 고용기간 없음\nlen(X_train[(X_train['occyp_type'].isna())&(X_train['DAYS_EMPLOYED']&lt;0)])\n&gt;&gt;&gt; 4438\n\n고용기간도 없는 경우는 ’NoJob’으로 대체\n\n\n\n\n\n# 결측치 처리 함수\ndef fill_occyp_type(row):\n    if pd.isna(row['occyp_type']):\n        if row['DAYS_EMPLOYED'] &gt; 0:\n            return 'NoJob'\n    return row['occyp_type']\n\nX_train['occyp_type'] = X_train.apply(fill_occyp_type, axis=1)\nX_train['occyp_type'].unique()\n\narray([None, 'Laborers', 'Managers', 'Sales staff',\n       'High skill tech staff', 'Core staff', 'Drivers', 'Medicine staff',\n       'Accountants', 'NoJob', 'Realty agents', 'Security staff',\n       'Cleaning staff', 'Private service staff', 'Cooking staff',\n       'Secretaries', 'HR staff', 'IT staff', 'Low-skill Laborers',\n       'Waiters/barmen staff'], dtype=object)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#trainvalidatetest-데이터셋으로-정리",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#trainvalidatetest-데이터셋으로-정리",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "train/validate/test 데이터셋으로 정리",
    "text": "train/validate/test 데이터셋으로 정리\n\nfrom sklearn.model_selection import train_test_split\n\n# index컬럼 삭제\nX_train = X_train.drop(columns=['index'])\nX_test = X_test.drop(columns=['index'])\n\n# 데이터 분할\nx_train, x_validate, y_train, y_validate= train_test_split(X_train, Y_train, test_size=0.3, random_state=42, stratify=Y_train)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#빠른-처리를-위한-gpu설정",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#빠른-처리를-위한-gpu설정",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "빠른 처리를 위한 GPU설정",
    "text": "빠른 처리를 위한 GPU설정\n\nCUDA 설치 : https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local\nCUDA버전 확인 : 12.1\nCUDA버전에 맞는, GPU지원되는 torch버전으로 설치\n\n\n!nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2025 NVIDIA Corporation\nBuilt on Wed_Jan_15_19:38:46_Pacific_Standard_Time_2025\nCuda compilation tools, release 12.8, V12.8.61\nBuild cuda_12.8.r12.8/compiler.35404655_0\n\n\n\n# 설치된 torch버전을 확인해, torch.version.cuda가 None인 경우 CUDA지원버전으로 torch 재설치\nimport torch\n\n# torch가 GPU지원버전인지 확인(+cu121)\nprint(torch.__version__)\nprint(torch.version.cuda)  # CUDA 버전 확인 (None이면 GPU 미지원 버전)\nprint(torch.cuda.is_available())  # False면 GPU 미지원\n\n# 사용가능한 GPU확인\nprint(torch.cuda.device_count())  # 사용 가능한 GPU 개수\nprint(torch.cuda.get_device_name(0))  # 첫 번째 GPU 이름\n\n2.5.1+cu121\n12.1\nTrue\n1\nNVIDIA GeForce RTX 2060 SUPER"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#평가-metric-설정",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#평가-metric-설정",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "평가 Metric 설정",
    "text": "평가 Metric 설정\n\n대회 규정에 맞춰 Logloss를 primary metric으로 설정\n데이터를 좀 더 용이하게 파악하고, 소통 등의 이점을 위해 보조 지표 설정\n\nbalance accuracy, weighted f1-score, roc_auc"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#autogluon-활용한-베이스-모델",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#autogluon-활용한-베이스-모델",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "Autogluon 활용한 베이스 모델",
    "text": "Autogluon 활용한 베이스 모델\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\n# 저장할 경로 지정\npath = 'autogluon_1basemodel' # 저장할 경로\n\n# Autogluon이 지원하지 않는 metric함수 설정(logloss로 평가예정으로 여기서는 미사용)\ndef weighted_f1_autogluon(y_true, y_pred_proba):\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    return f1_score(y_true, y_pred, average='weighted')\n\n# AutoGluon을 위해 데이터프레임으로 병합\ntrain_data = x_train\ntrain_data['target'] = y_train\n\nval_data = x_validate\nval_data['target'] = y_validate\n\n# AutoGluon 학습\npredictor = TabularPredictor(\n    label='target', \n    problem_type='multiclass', \n    eval_metric='log_loss', \n    verbosity=2,\n    path=path\n)\npredictor.fit(\n    train_data,\n    tuning_data=val_data,\n    time_limit=3600,  # 시간 제한 설정 (1시간)\n    num_gpus=1\n)\n\nWarning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_1basemodel\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.12.8\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.26100\nCPU Count:          8\nMemory Avail:       5.73 GB / 15.94 GB (35.9%)\nDisk Space Avail:   504.61 GB / 931.51 GB (54.2%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n    Recommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n    presets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n    presets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n    presets='high'         : Strong accuracy with fast inference speed.\n    presets='good'         : Good accuracy with very fast inference speed.\n    presets='medium'       : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ... Time limit = 3600s\nAutoGluon will save models to \"e:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\autogluon_1basemodel\"\nTrain Data Rows:    18519\nTrain Data Columns: 18\nTuning Data Rows:    7938\nTuning Data Columns: 18\nLabel Column:       target\nProblem Type:       multiclass\nPreprocessing data ...\nTrain Data Class Count: 3\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    5876.50 MB\n    Train Data (Original)  Memory Usage: 13.65 MB (0.2% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 6 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Useless Original Features (Count: 1): ['FLAG_MOBIL']\n        These features carry no predictive signal and should be manually investigated.\n        This is typically a feature which has the same value for all rows.\n        These features do not need to be present at inference time.\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])  : 3 | ['income_total', 'family_size', 'begin_month']\n        ('int', [])    : 6 | ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'work_phone', 'phone', ...]\n        ('object', []) : 8 | ['gender', 'car', 'reality', 'income_type', 'edu_type', ...]\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])  : 5 | ['income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\n        ('float', [])     : 3 | ['income_total', 'family_size', 'begin_month']\n        ('int', [])       : 3 | ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n        ('int', ['bool']) : 6 | ['gender', 'car', 'reality', 'work_phone', 'phone', ...]\n    0.4s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 1.49 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.44s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': [{}],\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n    'CAT': [{}],\n    'XGB': [{}],\n    'FASTAI': [{}],\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 3599.56s of the 3599.55s of remaining time.\n    -2.1126  = Validation score   (-log_loss)\n    0.04s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 3599.42s of the 3599.42s of remaining time.\n    -2.5663  = Validation score   (-log_loss)\n    0.05s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 3599.30s of the 3599.29s of remaining time.\n    -0.8157  = Validation score   (-log_loss)\n    29.12s   = Training   runtime\n    0.11s    = Validation runtime\nFitting model: LightGBMXT ... Training model for up to 3570.04s of the 3570.03s of remaining time.\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n\n\n[1000]  valid_set's multi_logloss: 0.760515\n\n\n    -0.7594  = Validation score   (-log_loss)\n    25.16s   = Training   runtime\n    0.22s    = Validation runtime\nFitting model: LightGBM ... Training model for up to 3544.53s of the 3544.52s of remaining time.\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n    -0.7514  = Validation score   (-log_loss)\n    11.64s   = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestGini ... Training model for up to 3532.74s of the 3532.74s of remaining time.\n    -0.7675  = Validation score   (-log_loss)\n    1.74s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ... Training model for up to 3530.57s of the 3530.56s of remaining time.\n    -0.7735  = Validation score   (-log_loss)\n    1.94s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost ... Training model for up to 3528.25s of the 3528.24s of remaining time.\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -0.7615  = Validation score   (-log_loss)\n    26.21s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini ... Training model for up to 3501.99s of the 3501.99s of remaining time.\n    -0.7841  = Validation score   (-log_loss)\n    1.15s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: ExtraTreesEntr ... Training model for up to 3500.37s of the 3500.37s of remaining time.\n    -0.7812  = Validation score   (-log_loss)\n    1.14s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: XGBoost ... Training model for up to 3482.78s of the 3482.78s of remaining time.\ne:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [02:32:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\ne:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [02:32:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    -0.7579  = Validation score   (-log_loss)\n    5.54s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 3477.17s of the 3477.17s of remaining time.\n    -0.8183  = Validation score   (-log_loss)\n    36.99s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 3440.13s of the 3440.13s of remaining time.\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n    -0.7414  = Validation score   (-log_loss)\n    39.25s   = Training   runtime\n    0.12s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3400.63s of remaining time.\n    Ensemble Weights: {'RandomForestGini': 0.32, 'LightGBM': 0.2, 'KNeighborsDist': 0.12, 'CatBoost': 0.12, 'LightGBMLarge': 0.12, 'NeuralNetTorch': 0.08, 'XGBoost': 0.04}\n    -0.7193  = Validation score   (-log_loss)\n    0.67s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 200.09s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 16983.3 rows/s (7938 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"e:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\autogluon_1basemodel\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x2543c01fec0&gt;\n\n\n\n# 단순 참고용\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n\n# 예측 및 평가\ntest_data = x_validate\ntest_data['target'] = y_validate\n\ny_pred_proba_ag = predictor.predict_proba(test_data)\ny_pred_ag = predictor.predict(test_data)\n\ny_pred = predictor.predict(test_data)\ny_proba = predictor.predict_proba(test_data)\naccuracy = accuracy_score(test_data['target'], y_pred)\nlogloss = log_loss(test_data['target'], y_proba)\ncf = confusion_matrix(test_data['target'], y_pred)\n\n\npredictor.leaderboard(x_validate, extra_metrics=['balanced_accuracy', 'f1_weighted', 'roc_auc_ovr_weighted'])\n\n\n\n\n\n\n\n\nmodel\nscore_test\nbalanced_accuracy\nf1_weighted\nroc_auc_ovr_weighted\nscore_val\neval_metric\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-0.719272\n0.506493\n0.679734\n0.759321\n-0.719287\nlog_loss\n1.010035\n0.467401\n122.088188\n0.006076\n0.001995\n0.671924\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n-0.741424\n0.473018\n0.658814\n0.734866\n-0.741424\nlog_loss\n0.145274\n0.116242\n39.248754\n0.145274\n0.116242\n39.248754\n1\nTrue\n13\n\n\n2\nLightGBM\n-0.751353\n0.453062\n0.644461\n0.723493\n-0.751353\nlog_loss\n0.158483\n0.090607\n11.635407\n0.158483\n0.090607\n11.635407\n1\nTrue\n5\n\n\n3\nXGBoost\n-0.757927\n0.465824\n0.651667\n0.720750\n-0.757927\nlog_loss\n0.178037\n0.036000\n5.536725\n0.178037\n0.036000\n5.536725\n1\nTrue\n11\n\n\n4\nLightGBMXT\n-0.759434\n0.459947\n0.645968\n0.718343\n-0.759434\nlog_loss\n0.324177\n0.224359\n25.162158\n0.324177\n0.224359\n25.162158\n1\nTrue\n4\n\n\n5\nCatBoost\n-0.761541\n0.439993\n0.633287\n0.713087\n-0.761541\nlog_loss\n0.074158\n0.026571\n26.207678\n0.074158\n0.026571\n26.207678\n1\nTrue\n8\n\n\n6\nRandomForestGini\n-0.767504\n0.534628\n0.685285\n0.752369\n-0.767504\nlog_loss\n0.331259\n0.102593\n1.737886\n0.331259\n0.102593\n1.737886\n1\nTrue\n6\n\n\n7\nRandomForestEntr\n-0.773531\n0.532764\n0.684485\n0.752317\n-0.773531\nlog_loss\n0.299343\n0.100369\n1.936155\n0.299343\n0.100369\n1.936155\n1\nTrue\n7\n\n\n8\nExtraTreesEntr\n-0.781154\n0.514000\n0.672208\n0.737642\n-0.781154\nlog_loss\n0.451465\n0.101042\n1.143195\n0.451465\n0.101042\n1.143195\n1\nTrue\n10\n\n\n9\nExtraTreesGini\n-0.784103\n0.515487\n0.674453\n0.736337\n-0.784103\nlog_loss\n0.368600\n0.121832\n1.149729\n0.368600\n0.121832\n1.149729\n1\nTrue\n9\n\n\n10\nNeuralNetFastAI\n-0.815732\n0.439552\n0.628491\n0.674919\n-0.815732\nlog_loss\n0.096067\n0.113365\n29.115618\n0.096067\n0.113365\n29.115618\n1\nTrue\n3\n\n\n11\nNeuralNetTorch\n-0.818329\n0.411790\n0.606342\n0.648959\n-0.818329\nlog_loss\n0.063465\n0.040002\n36.994816\n0.063465\n0.040002\n36.994816\n1\nTrue\n12\n\n\n12\nKNeighborsUnif\n-2.112598\n0.473339\n0.607854\n0.666518\n-2.112598\nlog_loss\n0.036525\n0.056145\n0.042302\n0.036525\n0.056145\n0.042302\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n-2.566280\n0.530617\n0.647806\n0.697774\n-2.566280\nlog_loss\n0.053282\n0.053392\n0.054998\n0.053282\n0.053392\n0.054998\n1\nTrue\n2"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#xaishap를-통한-데이터-확인-및-불필요한-변수-제거",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#xaishap를-통한-데이터-확인-및-불필요한-변수-제거",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "XAI(SHAP)를 통한 데이터 확인 및 불필요한 변수 제거",
    "text": "XAI(SHAP)를 통한 데이터 확인 및 불필요한 변수 제거\n\nWeightedEnsemble_L2모델은 KernelExplainer 실행시 76~79시간 소요 예상\n데이터의 확인과 변수를 걸러낼 용도이므로, 빠르게 가능하면서 2번째로 성능이 좋았던 모델로 SHAP 확인\n\nX_train을 넣엇으나, 데이터구성이 다르다는 오류 발생 (autogluon이 제거했던 feature가 있었음)\n\n확인해보니 휴대폰 소유여부로, 요즘 시대에 중요한 변수는 아닐 것 같은 생각이 들어 제외하기로 함\n\nset(X_train) - set(model.feature_name())\n&gt;&gt;&gt; {'FLAG_MOBIL'}\n\n\n\nimport numpy as np\nimport shap\n\n# FLAG_MOBIL 드랍\nX_train = X_train.drop(columns='FLAG_MOBIL')\n\n# Autogluon의 모델 가져오기\nmodel = predictor._trainer.load_model(\"LightGBMLarge\").model\n\n# SHAP계산을 위해 Autogluon의 피쳐형태와 동일하게 만들기\nX_train_transformed = predictor.transform_features(X_train)\n\n# Explainer를 활용한 SHAP 계산\nexplainer = shap.TreeExplainer(model)\nshap_values_train = explainer.shap_values(X_train_transformed)\n\n# 전체 SHAP값의 절대값의 평균으로 Feature Importance 구하기\nshap_values_mean = np.mean(np.abs(shap_values_train), axis=(0, 2))  # (35,) 크기\n\nprint(f\"\"\"SHAP value의 Shape              : {shap_values_train.shape}\nSHAP Feature Importance의 Shape : {shap_values_mean.shape}\"\"\")\n\nSHAP value의 Shape              : (26457, 17, 3)\nSHAP Feature Importance의 Shape : (17,)\n\n\n\nFeature importance 확인\n\n소득이나 교육수준 등 일반적으로 영향을 미칠것으로 보이는 feature들이 상위에 보임\nimportance가 0인 경우는 없어, 현재 상황에서 feature삭제는 별도로 하지 않음\n\n지난 실습때는 전체fit & SHAP importance산출 후 0이었던 FLAG_MOBIL을 직접 삭제\n이번에는 autogluon을 통해 영향없는 feature로 자동 제외됨\n\n\n\n\nimport pandas as pd\n\nfeature_importance = pd.DataFrame({\n    'Feature': [f'{X_train.columns[i]}' for i in range(shap_values_mean.shape[0])],\n    'Importance': shap_values_mean\n}).sort_values(by='Importance', ascending=False)\n\nfeature_importance\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n11\nwork_phone\n0.309656\n\n\n5\nincome_type\n0.126218\n\n\n6\nedu_type\n0.093286\n\n\n4\nincome_total\n0.087204\n\n\n16\nbegin_month\n0.083925\n\n\n12\nphone\n0.029627\n\n\n13\nemail\n0.021359\n\n\n14\noccyp_type\n0.020650\n\n\n3\nchild_num\n0.019754\n\n\n10\nDAYS_EMPLOYED\n0.018939\n\n\n8\nhouse_type\n0.017252\n\n\n7\nfamily_type\n0.014357\n\n\n2\nreality\n0.014261\n\n\n9\nDAYS_BIRTH\n0.012142\n\n\n1\ncar\n0.011479\n\n\n0\ngender\n0.010771\n\n\n15\nfamily_size\n0.008313\n\n\n\n\n\n\n\n\n신용도 높음(0)과 낮음(2)에 대해서 SHAP Summary plot 시각화\n\n회색은 영향도가 낮은 값이라고 하며, 해당하는 값인 소득수준 등이 제외할 정도로 무의미한 값으로는 판단되지 않아 삭제는 하지 않음\n전반적으로 섞여있거나 유의미하게 특징적인 부분은 발견하지 못함\n\n\n\nimport matplotlib.pyplot as plt\nimport shap\n\n# 글꼴 크기 조정\nplt.rcParams.update({'font.size': 8})\n\n# 그림 생성 및 크기 설정 (2개의 서브플롯)\nfig, axes = plt.subplots(1, 2, figsize=(30, 8))\n\n# Class 0에 대한 SHAP 값 시각화\nplt.sca(axes[0])\nshap.summary_plot(\n    shap_values_train[:, :, 0],\n    X_train_transformed,\n    feature_names=X_train_transformed.columns,\n    plot_type=\"dot\",\n    show=False,\n    title=\"Class 0\"\n)\naxes[0].set_xlabel(\"SHAP value\")\n\n# Class 2에 대한 SHAP 값 시각화\nplt.sca(axes[1])\nshap.summary_plot(\n    shap_values_train[:, :, 2],\n    X_train_transformed,\n    feature_names=X_train_transformed.columns,\n    plot_type=\"dot\",\n    show=False,\n    title=\"Class 2\"\n)\naxes[1].set_xlabel(\"SHAP value\")\n\n# 전체 그림 제목 설정\nplt.suptitle(\"SHAP Summary Plots for Classes 0 and 2\", fontsize=16)\n\n# 그림 간격 조정\nplt.tight_layout()\n\n# 그림 표시\nplt.show()"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#데이터-전처리를-위한-추가-eda",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#데이터-전처리를-위한-추가-eda",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "데이터 전처리를 위한 추가 EDA",
    "text": "데이터 전처리를 위한 추가 EDA\n\n수치형 변수의 EDA 및 전처리 방향\n\n유무(0,1)만을 나타내는 값은 별도로 처리하지 않음 : work_phone, phone, email\n음수변수는 최소값 조정 : DAYS_BIRTH, DAYS_EMPLOYED, begin_month\n왜도/첨도/이상치 확인에 따라 추가적인 조정 : DAYS_BIRTH, DAYS_EMPLOYED, begin_month, child_num, income_total, family_size\n\n왜도가 크고, IQR기준 이상치를 보유한 3개 변수에 대해 로그변환 진행 : child_num, income_total, DAYS_EMPLOYED\n타 변수 대비 값의 규모(천~만 단위)가 달라 발생할 수 있는 문제를 방지하고자 정규화(Min-Max) : DAYS_BIRTH\n\n\n\n# 수치형 변수 추출\nx_numerical = X_train.select_dtypes(include=['float64', 'int64'])\n\n\n전반적인 시각화 및 왜도/첨도 확인\n\n왜도가 큰 3개 변수 : child_num, income_total, DAYS_EMPLOYED\n\n\n\n# 왜도/첨도 분석\ndf_to_analyze = x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month', 'child_num', 'income_total', 'family_size']]\n\nnumeric_skew = df_to_analyze.skew().reset_index().rename(columns={0: '왜도', 'index':'feature_s'}).sort_values(by='왜도')\nnumeric_kurt = df_to_analyze.kurtosis().reset_index().rename(columns={0: '첨도', 'index':'feature_k'}).sort_values(by='첨도')\n\npd.concat([numeric_skew.reset_index(), numeric_kurt.reset_index()], axis=1)[['feature_s','왜도','feature_k','첨도']]\n\n\n\n\n\n\n\n\nfeature_s\n왜도\nfeature_k\n첨도\n\n\n\n\n0\nbegin_month\n-0.290050\nDAYS_BIRTH\n-1.046188\n\n\n1\nDAYS_BIRTH\n-0.185986\nbegin_month\n-1.041906\n\n\n2\nfamily_size\n1.431759\nDAYS_EMPLOYED\n1.161734\n\n\n3\nDAYS_EMPLOYED\n1.777596\nfamily_size\n10.578051\n\n\n4\nincome_total\n2.659271\nincome_total\n16.359621\n\n\n5\nchild_num\n2.852376\nchild_num\n29.172394\n\n\n\n\n\n\n\n\n# 히스토그램 시각화를 통한 확인\nimport matplotlib.pyplot as plt\nimport math\n\n# 시각화 대상\ncolumns_to_visualize = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month', 'child_num', 'income_total', 'family_size']\ndf_to_visualize = x_numerical[columns_to_visualize]\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(columns_to_visualize)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(columns_to_visualize):\n    ax = axes[var_idx]\n    ax.hist(df_to_visualize.iloc[:, var_idx], bins='auto', color='skyblue', edgecolor='black')\n    ax.set_title(f'{var_nm}')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n이상치 확인\n\nDAYS_EMPLOYED의 경우 365243값에 3천여개가 모여있으며, 양수인 경우 무직임을 표현하므로 1로 변경을 고려했으나, 모델에 확실히 다른 값임을 알리기 위해 변환하지 않음\n나머지 변수는 이상치의 적정 수준과 판단이 어려워 별도로 처리하지 않음\n\nIQR기준으로 판단된 이상치가 실제 이상치로 봐야할지에 대한 부분이 모호\nchild_num : 자녀가 19명인 경우는 확실히 이상치라는 느낌이지만, 몇명부터 이상치로 봐야할지 판단 어려움\nfamily_size : 가족구성원도 시대상에 따라 평균치가 감소할 순 있겠으나, 이상치의 기준선이 모호함\n\n\n\n\n# IQR기준 이상치\ndef summarize_unique_outliers(x_numerical, x_outliers):\n    # 각 컬럼별로 이상치의 고유값을 추출하여 딕셔너리로 저장\n    unique_outliers = x_numerical[x_outliers].apply(lambda col: col.unique().tolist())\n\n    # 딕셔너리 형태로 반환\n    unique_outliers_dict = unique_outliers.to_dict()\n\n    return unique_outliers_dict\n\nunique_outliers = summarize_unique_outliers(x_numerical, x_outliers)\n\nprint(f\"\"\"* 이상치로 판명된 Unique값 현황\nchild_num : {unique_outliers['child_num']}\nincome_total : {unique_outliers['income_total']}\nDAYS_EMPLOYED : {unique_outliers['DAYS_EMPLOYED']}\nfamily_size : {unique_outliers['family_size']}\"\"\")\n\n* 이상치로 판명된 Unique값 현황\nchild_num : [nan, 3.0, 4.0, 5.0, 14.0, 19.0, 7.0]\nincome_total : [nan, 450000.0, 405000.0, 585000.0, 495000.0, 540000.0, 459000.0, 720000.0, 382500.0, 427500.0, 562500.0, 630000.0, 432000.0, 391500.0, 675000.0, 612000.0, 1575000.0, 652500.0, 387000.0, 945000.0, 418500.0, 634500.0, 423000.0, 517500.0, 900000.0, 560250.0, 445500.0, 616500.0, 765000.0, 396000.0, 472500.0, 594000.0, 810000.0, 468000.0, 661500.0, 716323.5, 787500.0, 990000.0, 494100.0, 441000.0, 531000.0, 607500.0, 742500.0, 697500.0, 1125000.0, 414000.0, 1350000.0]\nDAYS_EMPLOYED : [nan, 365243.0, -9391.0, -9404.0, -12332.0, -9988.0, -11940.0, -8091.0, -9957.0, -7310.0, -11062.0, -7514.0, -8671.0, -10121.0, -12278.0, -7415.0, -8553.0, -7593.0, -7401.0, -9683.0, -7346.0, -9925.0, -7379.0, -8375.0, -7536.0, -10600.0, -7851.0, -9575.0, -9255.0, -8298.0, -8163.0, -12179.0, -13245.0, -9258.0, -8100.0, -10384.0, -10821.0, -8538.0, -8772.0, -9359.0, -15072.0, -7733.0, -7979.0, -7471.0, -11951.0, -8412.0, -7824.0, -11589.0, -12169.0, -7840.0, -8022.0, -8316.0, -9485.0, -7624.0, -10149.0, -8953.0, -11451.0, -10454.0, -9479.0, -11183.0, -9870.0, -10079.0, -8109.0, -8684.0, -7465.0, -11157.0, -11083.0, -9269.0, -15038.0, -8643.0, -7371.0, -10094.0, -8479.0, -10290.0, -7734.0, -8469.0, -9866.0, -8760.0, -10689.0, -7752.0, -8737.0, -8855.0, -10236.0, -7627.0, -8794.0, -9136.0, -9751.0, -12423.0, -10437.0, -13102.0, -7369.0, -10050.0, -7288.0, -7718.0, -12148.0, -7866.0, -7827.0, -9947.0, -7341.0, -7830.0, -7400.0, -14810.0, -7804.0, -12949.0, -14473.0, -7614.0, -9046.0, -8131.0, -9447.0, -7404.0, -12827.0, -8649.0, -8369.0, -9178.0, -8405.0, -12621.0, -8063.0, -9225.0, -11884.0, -8284.0, -10936.0, -9745.0, -7698.0, -8377.0, -8497.0, -7364.0, -8624.0, -8083.0, -10364.0, -10994.0, -8348.0, -8206.0, -7888.0, -10490.0, -13800.0, -7778.0, -8448.0, -8862.0, -8015.0, -7551.0, -8714.0, -9422.0, -10758.0, -8509.0, -7347.0, -8803.0, -7811.0, -7413.0, -8157.0, -10110.0, -10993.0, -8143.0, -7747.0, -11693.0, -8040.0, -8691.0, -8033.0, -8875.0, -12647.0, -10762.0, -7557.0, -10773.0, -8601.0, -11552.0, -7522.0, -7738.0, -11542.0, -8756.0, -9152.0, -8987.0, -9482.0, -9029.0, -9076.0, -11906.0, -8140.0, -7949.0, -11907.0, -7622.0, -9239.0, -9748.0, -9349.0, -7278.0, -7500.0, -9724.0, -8036.0, -9975.0, -8767.0, -8443.0, -7871.0, -13415.0, -7513.0, -8966.0, -14018.0, -9044.0, -9240.0, -7276.0, -9564.0, -8044.0, -8175.0, -9385.0, -7679.0, -8376.0, -8647.0, -7786.0, -8254.0, -8995.0, -10155.0, -8710.0, -8290.0, -9419.0, -10909.0, -8071.0, -7343.0, -9581.0, -10475.0, -10217.0, -9363.0, -12490.0, -7373.0, -9236.0, -9131.0, -10361.0, -9508.0, -8535.0, -12870.0, -7640.0, -11398.0, -9756.0, -8152.0, -11202.0, -7900.0, -9052.0, -14536.0, -8808.0, -14413.0, -9194.0, -11954.0, -8325.0, -8801.0, -9794.0, -10843.0, -11555.0, -8230.0, -8618.0, -9389.0, -10629.0, -7916.0, -7504.0, -11272.0, -7566.0, -10353.0, -8072.0, -9325.0, -9311.0, -8757.0, -7676.0, -8411.0, -9698.0, -10647.0, -11061.0, -14338.0, -9830.0, -14887.0, -9320.0, -7314.0, -11332.0, -7838.0, -10147.0, -9527.0, -12455.0, -8011.0, -10305.0, -7280.0, -13010.0, -10746.0, -7953.0, -8491.0, -7706.0, -10052.0, -9316.0, -11603.0, -10688.0, -7835.0, -8410.0, -11692.0, -7518.0, -9574.0, -12917.0, -9559.0, -12513.0, -8870.0, -7494.0, -7591.0, -8256.0, -7756.0, -8342.0, -9270.0, -9181.0, -11683.0, -8510.0, -8189.0, -10232.0, -8863.0, -12462.0, -10627.0, -12163.0, -9989.0, -10452.0, -7736.0, -11448.0, -8205.0, -12253.0, -8066.0, -9019.0, -10777.0]\nfamily_size : [nan, 5.0, 6.0, 7.0, 15.0, 20.0, 9.0]\n\n\n\n# IQR기준 이상치 시각화\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\n\ndef detect_outliers_fast(df):\n    # Calculate IQR without NaN values\n    Q1 = df.quantile(0.25, interpolation='midpoint')\n    Q3 = df.quantile(0.75, interpolation='midpoint')\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Vectorized outlier detection with NaN handling\n    outlier_flags = df.apply(\n        lambda col: ~col.between(lower_bound[col.name], upper_bound[col.name]) & col.notna()\n    )\n    return outlier_flags\n\n\n# 시각화 대상\ncolumns_to_visualize = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month', 'child_num', 'income_total', 'family_size']\ndf_to_visualize = x_numerical[columns_to_visualize]\n\n# 구한 이상치가 0인 경우를 제외하고 표기\nx_outliers = detect_outliers_fast(df_to_visualize)\noutliers_to_see = x_outliers.sum()[x_outliers.sum()&gt;0]\n\n# 설정: 가로 그래프 개수\ncols = 2\nnum_vars = len(outliers_to_see.index)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 4))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(outliers_to_see.index):\n    ax = axes[var_idx]\n    sns.boxplot(x=df_to_visualize[var_nm], ax=ax, color='lightblue')\n    ax.set_title(f'{var_nm}')\n    ax.set_xlabel(var_nm)\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n수치형 변수의 전처리 진행 : 최소값 조정 및 로그변환\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 최소값 조정\nx_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] = x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] + x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()*-1\n\n# 로그변환\nx_numerical[['child_num', 'income_total', 'DAYS_EMPLOYED']] = x_numerical[['child_num', 'income_total', 'DAYS_EMPLOYED']].apply(lambda x : np.log1p(x))\n\n# Min-Max Scaler 적용\nmin_max_scaler = MinMaxScaler()\nx_numerical[['DAYS_BIRTH']]  = min_max_scaler.fit_transform(x_numerical[['DAYS_BIRTH']])\n\n\nx_numerical.head(3)\n\n\n\n\n\n\n\n\nchild_num\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\n\n\n\n\n0\n0.000000\n12.218500\n0.644982\n9.306105\n1\n0\n0\n0\n2.0\n54.0\n\n\n1\n0.693147\n12.419170\n0.789362\n9.559165\n1\n0\n0\n1\n3.0\n55.0\n\n\n2\n0.000000\n13.017005\n0.347624\n9.330787\n1\n0\n1\n0\n2.0\n38.0\n\n\n\n\n\n\n\n\n\n범주형 변수의 EDA 및 전처리 방향\n\n전체 변수 : gender, car, reality, income_type, family_type, house_type, occyp_type, edu_type\n모델의 혼동(상위관계 여부)을 방지하고자 OneHotEncoder : gender, car, reality, income_type, family_type, house_type\n\n대다수의 변수가 2~6개의 unique값을 가져 크게 어려움은 없을 것으로 보임\n\n순서가 중요한 Ordinal변수에 대해서 OrdinalEncoder로 우선순위를 지정 : edu_type\nunique값이 18개로 많은 편이며, 모델의 상하관계 혼동도 방지하고자 LeaveOneOutEncoder(타겟 인코딩) : occyp_type\n\n\n# 범주형 변수 추출\nx_categorical = X_train.select_dtypes(include=['object'])\n\n\n# 전반적인 시각화\n\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# credit 데이터 합치기\ncombined_data = x_categorical.copy()\ncombined_data['credit'] = y_train.copy()  # 동일한 인덱스를 기준으로 credit 추가\n\n# 범주형 변수 설정\ncategorical_columns = ['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\ncols = 3\nnum_vars = len(categorical_columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 5))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor idx, var_nm in enumerate(categorical_columns):\n    ax = axes[idx]\n\n    # 데이터 그룹화 및 스택 데이터 계산\n    stacked_data = combined_data.groupby([var_nm, 'credit']).size().unstack(fill_value=0)\n\n    # 각 카테고리의 막대 그래프 그리기\n    bottom_values = np.zeros(len(stacked_data))\n    for credit_value in stacked_data.columns:\n        ax.bar(stacked_data.index, stacked_data[credit_value], bottom=bottom_values, label=f'Credit {credit_value}')\n        bottom_values += stacked_data[credit_value]\n\n    # 그래프 설정\n    ax.set_title(f'Stacked Bar Plot of {var_nm}')\n    ax.set_xlabel(var_nm)\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(title='Credit', loc='upper right')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n범주형 변수의 전처리 진행 : OrdinalEncoder (edu_type)\n\n# Label인코딩 후 (Order값은 낮은 것부터 입력)\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\norder = [\n    'Lower secondary',\n    'Secondary / secondary special',\n    'Incomplete higher',\n    'Higher education',\n    'Academic degree'\n]\n\no_encoder = OrdinalEncoder(categories=[order])\noe_df = o_encoder.fit_transform(x_categorical[['edu_type']])\nx_categorical['edu_type'] = oe_df.flatten()\n\n\n\n범주형 변수의 전처리 진행 : OneHotEncoder (gender, car, reality, income_type, family_type, house_type)\n\n# One-hot 인코딩 데이터프레임 생성\n\nfrom sklearn.preprocessing import OneHotEncoder\n\none_hot_columns = ['gender', 'car', 'reality', 'income_type', 'family_type', 'house_type']\n\noh_encoder = OneHotEncoder(sparse_output=False)\nonehot_encoded = oh_encoder.fit_transform(x_categorical[one_hot_columns])\nencoded_col_names = oh_encoder.get_feature_names_out(input_features=one_hot_columns)\n\ndf_onehot_encoded = pd.DataFrame(onehot_encoded, columns=encoded_col_names, index=x_categorical.index)\n\n# Ont-hot인코딩된 데이터프레임으로 교체\nx_categorical.drop(one_hot_columns, axis=1, inplace=True)\nx_categorical = pd.concat([x_categorical, df_onehot_encoded], axis=1)\n\n\n\n범주형 변수의 전처리 진행 : LeaveOneOutEncoder (occyp_type)\n\nimport category_encoders as ce\nimport joblib\n\nx_column = 'occyp_type'\n\nleave_one_out_encoder = ce.LeaveOneOutEncoder(cols=x_column, sigma=0.1, return_df=True)\ndf_target_encoded = leave_one_out_encoder.fit_transform(x_categorical[x_column], Y_train)\n\njoblib.dump(leave_one_out_encoder, 'encoder_leave_one_out.pkl')\n\nx_categorical['occyp_type'] = df_target_encoded['occyp_type']"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#전처리-기능-함수화-및-저장",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#전처리-기능-함수화-및-저장",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "전처리 기능 함수화 및 저장",
    "text": "전처리 기능 함수화 및 저장\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder\nimport category_encoders as ce\nimport joblib\nfrom pkb_sqlite3 import DB_sqlite3\n\ndef preprocess_x_data(train_or_test:str, dataset:list[DataFrame], output_name:dict)-&gt;None:\n\n    # 파라메터 검증 및 데이터셋 복사\n    if train_or_test == 'train':\n        if len(dataset) == 2:\n            X_train = dataset[0].copy()\n            Y_train = dataset[1].copy()\n        else:\n            return 'train인 경우 x,y 데이터가 필요합니다'\n    elif train_or_test == 'test':\n        if len(dataset) == 1:\n            X_train = dataset[0].copy()\n        else:\n            return 'test인 경우 x 데이터만 필요합니다'\n    else:\n        return 'train 또는 test만 입력 가능합니다'\n\n    \n    # (공통)결측치 대체 : occyp_type\n    def fill_occyp_type(row):\n        if pd.isna(row['occyp_type']):\n            if row['DAYS_EMPLOYED'] &gt; 0:\n                return 'NoJob'\n        return row['occyp_type']\n\n    X_train['occyp_type'] = X_train.apply(fill_occyp_type, axis=1)\n\n    # 수치형 변수 작업\n    x_numerical = X_train.select_dtypes(include=['float64', 'int64'])\n\n    ## 최소값 조정\n    x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] = x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] + x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()*-1\n\n    ## 로그변환\n    x_numerical[['child_num', 'income_total', 'DAYS_EMPLOYED']] = x_numerical[['child_num', 'income_total', 'DAYS_EMPLOYED']].apply(lambda x : np.log1p(x))\n\n    ## Min-Max Scaler 적용\n    min_max_scaler = MinMaxScaler()\n    x_numerical[['DAYS_BIRTH']]  = min_max_scaler.fit_transform(x_numerical[['DAYS_BIRTH']])\n\n\n    # 범주형 변수 작업\n    x_categorical = X_train.select_dtypes(include=['object'])\n\n    ## Label인코딩\n    order = [\n        'Lower secondary',\n        'Secondary / secondary special',\n        'Incomplete higher',\n        'Higher education',\n        'Academic degree'\n    ]\n\n    o_encoder = OrdinalEncoder(categories=[order])\n    oe_df = o_encoder.fit_transform(x_categorical[['edu_type']])\n    x_categorical['edu_type'] = oe_df.flatten()\n\n    ## One-hot 인코딩\n    one_hot_columns = ['gender', 'car', 'reality', 'income_type', 'family_type', 'house_type']\n\n    oh_encoder = OneHotEncoder(sparse_output=False)\n    onehot_encoded = oh_encoder.fit_transform(x_categorical[one_hot_columns])\n    encoded_col_names = oh_encoder.get_feature_names_out(input_features=one_hot_columns)\n\n    df_onehot_encoded = pd.DataFrame(onehot_encoded, columns=encoded_col_names, index=x_categorical.index)\n\n    x_categorical.drop(one_hot_columns, axis=1, inplace=True)\n    x_categorical = pd.concat([x_categorical, df_onehot_encoded], axis=1)\n\n\n    ## LeaveOneOut인코딩\n    x_column = 'occyp_type'\n\n    ### 파일로 저장해둔 인코더가 있으면 로딩, 아니면 생성\n    if os.path.isfile('encoder_leave_one_out.pkl'):\n        leave_one_out_encoder = joblib.load('encoder_leave_one_out.pkl')\n    else:\n        leave_one_out_encoder = ce.LeaveOneOutEncoder(cols=x_column, sigma=0.1, return_df=True)\n\n    ### 인코딩된 데이터 생성 및 인코더 업데이트\n    if train_or_test == 'train':\n        df_target_encoded = leave_one_out_encoder.fit_transform(x_categorical[x_column], Y_train)\n    elif train_or_test == 'test':\n        df_target_encoded = leave_one_out_encoder.transform(x_categorical[x_column])\n    joblib.dump(leave_one_out_encoder, 'encoder_leave_one_out.pkl')\n\n    x_categorical['occyp_type'] = df_target_encoded['occyp_type']\n\n\n    # 나누어 작업한 수치형/범주형 변수 합치고 저장\n    x_train_preprocessed = pd.concat([x_categorical, x_numerical], axis=1)\n    if train_or_test == 'train':\n        x_train_preprocessed = pd.concat([x_train_preprocessed, Y_train], axis=1)\n    elif train_or_test == 'test':\n        pass\n\n    ## CSV저장 \n    x_train_preprocessed.to_csv(output_name['csv_file_name']) # 'train_preprocessed.csv'\n\n    ## DB저장 (아래 함수는 이미 테이블이 있는 경우 덮어씀)\n    db_controller.df_to_table(output_name['db_table_name'], x_train_preprocessed) # 'train_pre'\n\n    return '작업이 완료되었습니다'\n\n\n# train 데이터 전처리\npreprocess_x_data(train_or_test='train', \n                  dataset=[X_train, Y_train], \n                  output_name={'csv_file_name':'train_preprocessed.csv', \n                               'db_table_name':'train_pre'}\n                  )\n\n'작업이 완료되었습니다'\n\n\n\n# 전처리 후 db저장된 결과 확인\ndb_controller.search_db_show_df(\"SELECT * FROM train_pre\").head(3)\n\n\n\n\n\n\n\n\nedu_type\noccyp_type\ngender_F\ngender_M\ncar_N\ncar_Y\nreality_N\nreality_Y\nincome_type_Commercial associate\nincome_type_Pensioner\n...\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\ncredit\n\n\n\n\n0\n3.0\n1.461445\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n12.218500\n0.644982\n9.306105\n1\n0\n0\n0\n2.0\n54.0\n1.0\n\n\n1\n1.0\n1.488029\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n12.419170\n0.789362\n9.559165\n1\n0\n0\n1\n3.0\n55.0\n1.0\n\n\n2\n3.0\n1.652069\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n13.017005\n0.347624\n9.330787\n1\n0\n1\n0\n2.0\n38.0\n2.0\n\n\n\n\n3 rows × 35 columns\n\n\n\n\n# test 데이터 전처리\npreprocess_x_data(train_or_test='test', \n                  dataset=[X_test], \n                  output_name={'csv_file_name':'test_preprocessed.csv', \n                               'db_table_name':'test_pre'}\n                  )\n\n'작업이 완료되었습니다'"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#전처리한-데이터-및-autogluon활용한-모델-고도화",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#전처리한-데이터-및-autogluon활용한-모델-고도화",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "전처리한 데이터 및 autogluon활용한 모델 고도화",
    "text": "전처리한 데이터 및 autogluon활용한 모델 고도화\n\nfrom sklearn.model_selection import train_test_split\n\npre_df_train = db_controller.search_db_show_df(\"SELECT * FROM train_pre\")\n\n# 데이터 나누기\nY_train = pre_df_train['credit'].copy()\npre_X_train = pre_df_train.drop('credit',axis=1).copy()\nX_test = df_test.copy()\n\n\n# 데이터 분할\npre_x_train, pre_x_validate, y_train, y_validate= train_test_split(pre_X_train, Y_train, test_size=0.3, random_state=42, stratify=Y_train)\n\n\nfrom autogluon.tabular import TabularPredictor\nimport numpy as np\n\n# 저장할 경로 지정\npath = 'autogluon_2secondmodel' # 저장할 경로\n\n# AutoGluon을 위해 데이터프레임으로 병합\ntrain_data = pre_x_train\ntrain_data['target'] = y_train\n\nval_data = pre_x_validate\nval_data['target'] = y_validate\n\n# AutoGluon 학습\npredictor = TabularPredictor(\n    label='target', \n    problem_type='multiclass', \n    eval_metric='log_loss', \n    verbosity=2,\n    path=path,\n)\npredictor.fit(\n    train_data,\n    tuning_data=val_data,\n    time_limit=3600,  # 시간 제한 설정 (1시간)\n    num_gpus=1,\n    presets='best_quality',\n    use_bag_holdout=True\n)\n\nWarning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_2secondmodel\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.12.8\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.26100\nCPU Count:          8\nMemory Avail:       5.04 GB / 15.94 GB (31.6%)\nDisk Space Avail:   504.61 GB / 931.51 GB (54.2%)\n===================================================\nPresets specified: ['best_quality']\nSetting dynamic_stacking from 'auto' to False. Reason: Skip dynamic_stacking when use_bag_holdout is enabled. (use_bag_holdout=True)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ... Time limit = 3600s\nAutoGluon will save models to \"e:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\autogluon_2secondmodel\"\nTrain Data Rows:    18519\nTrain Data Columns: 34\nTuning Data Rows:    7938\nTuning Data Columns: 34\nLabel Column:       target\nProblem Type:       multiclass\nPreprocessing data ...\nTrain Data Class Count: 3\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    5148.66 MB\n    Train Data (Original)  Memory Usage: 6.86 MB (0.1% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 25 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Useless Original Features (Count: 1): ['FLAG_MOBIL']\n        These features carry no predictive signal and should be manually investigated.\n        This is typically a feature which has the same value for all rows.\n        These features do not need to be present at inference time.\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 30 | ['edu_type', 'occyp_type', 'gender_F', 'gender_M', 'car_N', ...]\n        ('int', [])   :  3 | ['work_phone', 'phone', 'email']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', [])     :  8 | ['edu_type', 'occyp_type', 'child_num', 'income_total', 'DAYS_BIRTH', ...]\n        ('int', ['bool']) : 25 | ['gender_F', 'gender_M', 'car_N', 'car_Y', 'reality_N', ...]\n    0.1s = Fit runtime\n    33 features in original data used to generate 33 features in processed data.\n    Train Data (Processed) Memory Usage: 2.25 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n    To change this, specify the eval_metric parameter of Predictor()\nuse_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\nLarge model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n    'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n    'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n    'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 110 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2399.27s of the 3599.79s of remaining time.\n    -2.4262  = Validation score   (-log_loss)\n    0.12s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2398.82s of the 3599.34s of remaining time.\n    -2.4431  = Validation score   (-log_loss)\n    0.04s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2398.47s of the 3598.99s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.37%)\n    -0.8011  = Validation score   (-log_loss)\n    86.42s   = Training   runtime\n    1.02s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 2303.53s of the 3504.05s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.35%)\n    -0.7725  = Validation score   (-log_loss)\n    10.48s   = Training   runtime\n    2.17s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 2282.75s of the 3483.27s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.36%)\n    -0.7655  = Validation score   (-log_loss)\n    7.32s    = Training   runtime\n    1.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 2267.33s of the 3467.85s of remaining time.\n    -0.7434  = Validation score   (-log_loss)\n    2.07s    = Training   runtime\n    0.39s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2263.26s of the 3463.78s of remaining time.\n    -0.7396  = Validation score   (-log_loss)\n    2.31s    = Training   runtime\n    3.34s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 2256.21s of the 3456.73s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.52%)\n    -0.7751  = Validation score   (-log_loss)\n    97.28s   = Training   runtime\n    0.2s     = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2155.91s of the 3356.43s of remaining time.\n    -0.7802  = Validation score   (-log_loss)\n    1.54s    = Training   runtime\n    0.56s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2152.48s of the 3353.00s of remaining time.\n    -0.7828  = Validation score   (-log_loss)\n    1.42s    = Training   runtime\n    0.51s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 2144.57s of the 3345.09s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.59%)\n    -0.7668  = Validation score   (-log_loss)\n    21.91s   = Training   runtime\n    1.11s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2118.51s of the 3319.03s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=0.22%)\n    -0.8113  = Validation score   (-log_loss)\n    251.72s  = Training   runtime\n    0.31s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1863.68s of the 3064.20s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.73%)\n    -0.7542  = Validation score   (-log_loss)\n    9.23s    = Training   runtime\n    1.39s    = Validation runtime\nFitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1847.37s of the 3047.89s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.45%)\n    -0.7746  = Validation score   (-log_loss)\n    63.28s   = Training   runtime\n    0.15s    = Validation runtime\nFitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1781.19s of the 2981.71s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=0.19%)\n    -0.7953  = Validation score   (-log_loss)\n    293.93s  = Training   runtime\n    0.32s    = Validation runtime\nFitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1484.24s of the 2684.76s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.44%)\n    -0.7565  = Validation score   (-log_loss)\n    23.01s   = Training   runtime\n    6.31s    = Validation runtime\nFitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1446.16s of the 2646.68s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.35%)\n    -0.798   = Validation score   (-log_loss)\n    153.76s  = Training   runtime\n    1.06s    = Validation runtime\nFitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1288.58s of the 2489.10s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.22%)\n    -0.7616  = Validation score   (-log_loss)\n    164.84s  = Training   runtime\n    0.51s    = Validation runtime\nFitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1120.44s of the 2320.96s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.29%)\n    -0.7789  = Validation score   (-log_loss)\n    59.55s   = Training   runtime\n    25.56s   = Validation runtime\nFitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1022.29s of the 2222.81s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=0.20%)\n    -0.8058  = Validation score   (-log_loss)\n    349.2s   = Training   runtime\n    0.3s     = Validation runtime\nFitting model: XGBoost_r33_BAG_L1 ... Training model for up to 670.00s of the 1870.52s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=2.37%)\n    -0.7541  = Validation score   (-log_loss)\n    53.77s   = Training   runtime\n    5.68s    = Validation runtime\nFitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 607.29s of the 1807.81s of remaining time.\n    -0.7793  = Validation score   (-log_loss)\n    2.61s    = Training   runtime\n    0.46s    = Validation runtime\nFitting model: CatBoost_r137_BAG_L1 ... Training model for up to 603.09s of the 1803.61s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.31%)\n    -0.7803  = Validation score   (-log_loss)\n    238.16s  = Training   runtime\n    0.38s    = Validation runtime\nFitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 361.91s of the 1562.43s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.40%)\n    -0.7958  = Validation score   (-log_loss)\n    33.49s   = Training   runtime\n    0.29s    = Validation runtime\nFitting model: CatBoost_r13_BAG_L1 ... Training model for up to 325.24s of the 1525.76s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.51%)\n    -0.7724  = Validation score   (-log_loss)\n    253.21s  = Training   runtime\n    0.32s    = Validation runtime\nFitting model: RandomForest_r195_BAG_L1 ... Training model for up to 68.68s of the 1269.20s of remaining time.\n    -0.7592  = Validation score   (-log_loss)\n    5.03s    = Training   runtime\n    0.25s    = Validation runtime\nFitting model: LightGBM_r188_BAG_L1 ... Training model for up to 62.45s of the 1262.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.72%)\n    -0.7681  = Validation score   (-log_loss)\n    9.7s     = Training   runtime\n    1.63s    = Validation runtime\nFitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 46.45s of the 1246.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=0.37%)\n    -0.8079  = Validation score   (-log_loss)\n    53.69s   = Training   runtime\n    1.32s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1189.04s of remaining time.\n    Ensemble Weights: {'RandomForestEntr_BAG_L1': 0.545, 'XGBoost_r33_BAG_L1': 0.182, 'LightGBM_r131_BAG_L1': 0.136, 'RandomForestGini_BAG_L1': 0.091, 'NeuralNetTorch_r22_BAG_L1': 0.045}\n    -0.7294  = Validation score   (-log_loss)\n    1.07s    = Training   runtime\n    0.0s     = Validation runtime\nFitting 108 L2 models, fit_strategy=\"sequential\" ...\nFitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1187.95s of the 1187.62s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.64%)\n    -0.725   = Validation score   (-log_loss)\n    64.84s   = Training   runtime\n    0.94s    = Validation runtime\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 1119.59s of the 1119.25s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.45%)\n    -0.7236  = Validation score   (-log_loss)\n    15.0s    = Training   runtime\n    0.63s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 1099.98s of the 1099.65s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.46%)\n    -0.7241  = Validation score   (-log_loss)\n    19.31s   = Training   runtime\n    0.36s    = Validation runtime\nFitting model: RandomForestGini_BAG_L2 ... Training model for up to 1076.25s of the 1075.92s of remaining time.\n    -0.7461  = Validation score   (-log_loss)\n    9.01s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1065.35s of the 1065.01s of remaining time.\n    -0.7314  = Validation score   (-log_loss)\n    11.5s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 1052.02s of the 1051.68s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.95%)\n    -0.7243  = Validation score   (-log_loss)\n    103.33s  = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 945.95s of the 945.62s of remaining time.\n    -0.7325  = Validation score   (-log_loss)\n    1.95s    = Training   runtime\n    0.32s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 941.86s of the 941.53s of remaining time.\n    -0.7323  = Validation score   (-log_loss)\n    2.0s     = Training   runtime\n    0.28s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 937.88s of the 937.54s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=2.38%)\n    -0.7239  = Validation score   (-log_loss)\n    39.8s    = Training   runtime\n    0.65s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 894.90s of the 894.56s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=0.99%)\n    -0.7266  = Validation score   (-log_loss)\n    123.35s  = Training   runtime\n    1.24s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 767.75s of the 767.41s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.81%)\n    -0.7285  = Validation score   (-log_loss)\n    62.95s   = Training   runtime\n    0.41s    = Validation runtime\nFitting model: CatBoost_r177_BAG_L2 ... Training model for up to 700.47s of the 700.14s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.99%)\n    -0.7241  = Validation score   (-log_loss)\n    84.97s   = Training   runtime\n    0.09s    = Validation runtime\nFitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 612.85s of the 612.52s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=1.00%)\n    -0.7241  = Validation score   (-log_loss)\n    159.99s  = Training   runtime\n    1.38s    = Validation runtime\nFitting model: LightGBM_r131_BAG_L2 ... Training model for up to 448.89s of the 448.55s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.81%)\n    -0.7243  = Validation score   (-log_loss)\n    42.78s   = Training   runtime\n    1.19s    = Validation runtime\nFitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 400.51s of the 400.17s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=1.65%)\n    -0.7288  = Validation score   (-log_loss)\n    106.63s  = Training   runtime\n    1.11s    = Validation runtime\nFitting model: CatBoost_r9_BAG_L2 ... Training model for up to 290.24s of the 289.91s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=4.81%)\n    -0.7243  = Validation score   (-log_loss)\n    239.08s  = Training   runtime\n    0.23s    = Validation runtime\nFitting model: LightGBM_r96_BAG_L2 ... Training model for up to 48.37s of the 48.04s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.37%)\n    -0.7234  = Validation score   (-log_loss)\n    17.5s    = Training   runtime\n    1.25s    = Validation runtime\nFitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 25.29s of the 24.96s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=0.94%)\n    Time limit exceeded... Skipping NeuralNetTorch_r22_BAG_L2.\nFitting model: XGBoost_r33_BAG_L2 ... Training model for up to 19.01s of the 18.68s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=4, gpus=0, memory=9.66%)\n    -0.9506  = Validation score   (-log_loss)\n    23.08s   = Training   runtime\n    0.41s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -6.81s of remaining time.\n    Ensemble Weights: {'XGBoost_BAG_L2': 0.348, 'NeuralNetFastAI_BAG_L2': 0.261, 'NeuralNetTorch_r79_BAG_L2': 0.261, 'RandomForestEntr_BAG_L2': 0.087, 'LightGBM_r96_BAG_L2': 0.043}\n    -0.7216  = Validation score   (-log_loss)\n    1.13s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 3607.99s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 223.0 rows/s (7938 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"e:\\0_Backup\\14.Python\\metacode_202412_study_creditmodeling\\autogluon_2secondmodel\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x2548a58e870&gt;\n\n\n\n리더보드\n\n\npredictor.leaderboard(pre_x_validate, extra_metrics=['balanced_accuracy', 'f1_weighted', 'roc_auc_ovr_weighted'])\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\neval_metric\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L3\n-0.721621\n-0.721621\nlog_loss\n66.038581\n35.591943\n2484.289781\n0.013003\n0.001998\n1.126650\n3\nTrue\n48\n\n\n1\nLightGBM_r96_BAG_L2\n-0.723399\n-0.723399\nlog_loss\n60.437314\n32.402319\n2207.033812\n1.693396\n1.249881\n17.496655\n2\nTrue\n46\n\n\n2\nLightGBMXT_BAG_L2\n-0.723595\n-0.723595\nlog_loss\n59.718009\n31.786725\n2204.540157\n0.974091\n0.634287\n15.003000\n2\nTrue\n31\n\n\n3\nXGBoost_BAG_L2\n-0.723949\n-0.723949\nlog_loss\n59.537626\n31.807227\n2229.341661\n0.793707\n0.654789\n39.804505\n2\nTrue\n38\n\n\n4\nCatBoost_r177_BAG_L2\n-0.724051\n-0.724051\nlog_loss\n59.057112\n31.241331\n2274.504236\n0.313194\n0.088893\n84.967080\n2\nTrue\n41\n\n\n5\nLightGBM_BAG_L2\n-0.724066\n-0.724066\nlog_loss\n59.263655\n31.515064\n2208.849041\n0.519737\n0.362626\n19.311885\n2\nTrue\n32\n\n\n6\nNeuralNetTorch_r79_BAG_L2\n-0.724087\n-0.724087\nlog_loss\n60.398604\n32.529583\n2349.523849\n1.654686\n1.377145\n159.986692\n2\nTrue\n42\n\n\n7\nCatBoost_BAG_L2\n-0.724257\n-0.724257\nlog_loss\n59.122199\n31.250390\n2292.868759\n0.378280\n0.097952\n103.331603\n2\nTrue\n35\n\n\n8\nLightGBM_r131_BAG_L2\n-0.724293\n-0.724293\nlog_loss\n60.727484\n32.341873\n2232.318439\n1.983566\n1.189435\n42.781282\n2\nTrue\n43\n\n\n9\nCatBoost_r9_BAG_L2\n-0.724313\n-0.724313\nlog_loss\n59.698261\n31.379199\n2428.618669\n0.954343\n0.226760\n239.081513\n2\nTrue\n45\n\n\n10\nNeuralNetFastAI_BAG_L2\n-0.724968\n-0.724968\nlog_loss\n60.431796\n32.088797\n2254.376706\n1.687878\n0.936359\n64.839550\n2\nTrue\n30\n\n\n11\nNeuralNetTorch_BAG_L2\n-0.726645\n-0.726645\nlog_loss\n60.429449\n32.390139\n2312.890353\n1.685530\n1.237701\n123.353197\n2\nTrue\n39\n\n\n12\nLightGBMLarge_BAG_L2\n-0.728510\n-0.728510\nlog_loss\n59.943033\n31.565219\n2252.492023\n1.199115\n0.412781\n62.954867\n2\nTrue\n40\n\n\n13\nNeuralNetFastAI_r191_BAG_L2\n-0.728848\n-0.728848\nlog_loss\n60.845650\n32.260897\n2296.170694\n2.101732\n1.108459\n106.633538\n2\nTrue\n44\n\n\n14\nWeightedEnsemble_L2\n-0.729363\n-0.729363\nlog_loss\n21.676457\n16.020015\n431.424106\n0.025999\n0.001998\n1.068229\n2\nTrue\n29\n\n\n15\nRandomForestEntr_BAG_L2\n-0.731364\n-0.731364\nlog_loss\n60.195911\n31.371770\n2201.035728\n1.451993\n0.219332\n11.498572\n2\nTrue\n34\n\n\n16\nExtraTreesEntr_BAG_L2\n-0.732284\n-0.732284\nlog_loss\n61.272550\n31.437372\n2191.534572\n2.528632\n0.284934\n1.997415\n2\nTrue\n37\n\n\n17\nExtraTreesGini_BAG_L2\n-0.732482\n-0.732482\nlog_loss\n61.319811\n31.474750\n2191.488311\n2.575893\n0.322311\n1.951155\n2\nTrue\n36\n\n\n18\nRandomForestEntr_BAG_L1\n-0.739567\n-0.739567\nlog_loss\n3.211556\n3.338512\n2.307917\n3.211556\n3.338512\n2.307917\n1\nTrue\n7\n\n\n19\nRandomForestGini_BAG_L1\n-0.743406\n-0.743406\nlog_loss\n3.129520\n0.386959\n2.070676\n3.129520\n0.386959\n2.070676\n1\nTrue\n6\n\n\n20\nRandomForestGini_BAG_L2\n-0.746055\n-0.746055\nlog_loss\n60.456664\n31.374071\n2198.547097\n1.712746\n0.221633\n9.009941\n2\nTrue\n33\n\n\n21\nXGBoost_r33_BAG_L1\n-0.754110\n-0.754110\nlog_loss\n7.492479\n5.680814\n53.769325\n7.492479\n5.680814\n53.769325\n1\nTrue\n21\n\n\n22\nLightGBMLarge_BAG_L1\n-0.754213\n-0.754213\nlog_loss\n2.008240\n1.389452\n9.230089\n2.008240\n1.389452\n9.230089\n1\nTrue\n13\n\n\n23\nLightGBM_r131_BAG_L1\n-0.756506\n-0.756506\nlog_loss\n7.308746\n6.312452\n23.005960\n7.308746\n6.312452\n23.005960\n1\nTrue\n16\n\n\n24\nRandomForest_r195_BAG_L1\n-0.759182\n-0.759182\nlog_loss\n2.094807\n0.254650\n5.030918\n2.094807\n0.254650\n5.030918\n1\nTrue\n26\n\n\n25\nCatBoost_r9_BAG_L1\n-0.761596\n-0.761596\nlog_loss\n1.885603\n0.509276\n164.842572\n1.885603\n0.509276\n164.842572\n1\nTrue\n18\n\n\n26\nLightGBM_BAG_L1\n-0.765499\n-0.765499\nlog_loss\n1.329988\n1.015724\n7.321401\n1.329988\n1.015724\n7.321401\n1\nTrue\n5\n\n\n27\nXGBoost_BAG_L1\n-0.766790\n-0.766790\nlog_loss\n1.154552\n1.108733\n21.912512\n1.154552\n1.108733\n21.912512\n1\nTrue\n11\n\n\n28\nLightGBM_r188_BAG_L1\n-0.768142\n-0.768142\nlog_loss\n2.751938\n1.632201\n9.696105\n2.751938\n1.632201\n9.696105\n1\nTrue\n27\n\n\n29\nCatBoost_r13_BAG_L1\n-0.772391\n-0.772391\nlog_loss\n2.387702\n0.321269\n253.209605\n2.387702\n0.321269\n253.209605\n1\nTrue\n25\n\n\n30\nLightGBMXT_BAG_L1\n-0.772460\n-0.772460\nlog_loss\n2.659072\n2.172474\n10.480810\n2.659072\n2.172474\n10.480810\n1\nTrue\n4\n\n\n31\nCatBoost_r177_BAG_L1\n-0.774637\n-0.774637\nlog_loss\n0.575816\n0.149715\n63.282011\n0.575816\n0.149715\n63.282011\n1\nTrue\n14\n\n\n32\nCatBoost_BAG_L1\n-0.775142\n-0.775142\nlog_loss\n0.737181\n0.202798\n97.279686\n0.737181\n0.202798\n97.279686\n1\nTrue\n8\n\n\n33\nLightGBM_r96_BAG_L1\n-0.778905\n-0.778905\nlog_loss\n27.299296\n25.557928\n59.550207\n27.299296\n25.557928\n59.550207\n1\nTrue\n19\n\n\n34\nExtraTrees_r42_BAG_L1\n-0.779346\n-0.779346\nlog_loss\n2.905398\n0.455039\n2.608203\n2.905398\n0.455039\n2.608203\n1\nTrue\n22\n\n\n35\nExtraTreesGini_BAG_L1\n-0.780239\n-0.780239\nlog_loss\n4.402335\n0.555433\n1.539585\n4.402335\n0.555433\n1.539585\n1\nTrue\n9\n\n\n36\nCatBoost_r137_BAG_L1\n-0.780273\n-0.780273\nlog_loss\n1.036823\n0.376350\n238.156574\n1.036823\n0.376350\n238.156574\n1\nTrue\n23\n\n\n37\nExtraTreesEntr_BAG_L1\n-0.782794\n-0.782794\nlog_loss\n4.142951\n0.514300\n1.421637\n4.142951\n0.514300\n1.421637\n1\nTrue\n10\n\n\n38\nNeuralNetTorch_r79_BAG_L1\n-0.795339\n-0.795339\nlog_loss\n0.421243\n0.315833\n293.931864\n0.421243\n0.315833\n293.931864\n1\nTrue\n15\n\n\n39\nNeuralNetFastAI_r102_BAG_L1\n-0.795822\n-0.795822\nlog_loss\n0.691814\n0.294400\n33.487108\n0.691814\n0.294400\n33.487108\n1\nTrue\n24\n\n\n40\nNeuralNetFastAI_r191_BAG_L1\n-0.797977\n-0.797977\nlog_loss\n2.015678\n1.061679\n153.761470\n2.015678\n1.061679\n153.761470\n1\nTrue\n17\n\n\n41\nNeuralNetFastAI_BAG_L1\n-0.801051\n-0.801051\nlog_loss\n1.336139\n1.015028\n86.418419\n1.336139\n1.015028\n86.418419\n1\nTrue\n3\n\n\n42\nNeuralNetTorch_r22_BAG_L1\n-0.805793\n-0.805793\nlog_loss\n0.508158\n0.299280\n349.202000\n0.508158\n0.299280\n349.202000\n1\nTrue\n20\n\n\n43\nNeuralNetFastAI_r145_BAG_L1\n-0.807897\n-0.807897\nlog_loss\n1.738020\n1.318004\n53.690834\n1.738020\n1.318004\n53.690834\n1\nTrue\n28\n\n\n44\nNeuralNetTorch_BAG_L1\n-0.811305\n-0.811305\nlog_loss\n0.536539\n0.309353\n251.715736\n0.536539\n0.309353\n251.715736\n1\nTrue\n12\n\n\n45\nXGBoost_r33_BAG_L2\n-0.950595\n-0.950595\nlog_loss\n59.534849\n31.566392\n2212.616866\n0.790931\n0.413954\n23.079710\n2\nTrue\n47\n\n\n46\nKNeighborsUnif_BAG_L1\n-2.426187\n-2.426187\nlog_loss\n0.120591\n0.082648\n0.120874\n0.120591\n0.082648\n0.120874\n1\nTrue\n1\n\n\n47\nKNeighborsDist_BAG_L1\n-2.443102\n-2.443102\nlog_loss\n0.161030\n0.080063\n0.043268\n0.161030\n0.080063\n0.043268\n1\nTrue\n2"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#제출용-csv파일-생성-및-제출하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250202/index.html#제출용-csv파일-생성-및-제출하기",
    "title": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험",
    "section": "제출용 CSV파일 생성 및 제출하기",
    "text": "제출용 CSV파일 생성 및 제출하기\n\n제출결과\n\nPublic점수 : 0.7384256792\nPrivate점수 : 0.7185042016\n\n\n\n# 필요한 데이터 불러오기\npre_df_test = db_controller.search_db_show_df(\"SELECT * FROM test_pre\")\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\n# 예측 및 제출 데이터 생성\ny_pred = predictor.predict_proba(pre_df_test)\ndf_sample_submission.iloc[:,1:]=y_pred\n\n\n# 저장 및 값 확인\ndf_sample_submission.to_csv('submission_pkb.csv', index=False)\ndf_sample_submission.head(5)\n\n\n\n\n\n\n\n\nindex\n0\n1\n2\n\n\n\n\n0\n26457\n0.047518\n0.103970\n0.848513\n\n\n1\n26458\n0.126270\n0.316703\n0.557027\n\n\n2\n26459\n0.084927\n0.117985\n0.797088\n\n\n3\n26460\n0.091055\n0.101548\n0.807397\n\n\n4\n26461\n0.083222\n0.160538\n0.756240"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html",
    "href": "posts/meta-dl-creditcard-20240526/index.html",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 1주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240526/index.html#개요",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 1주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-pytorch",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-pytorch",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensorflow, Pytorch",
    "text": "Tensorflow, Pytorch\n\n프레임워크 2가지 존재 : Tensorflow(텐서플로우, 구글), Pytorch(파이토치, 페이스북)\n\n알파고(딥마인드)시점까지는 ai의 90퍼센트 이상은 텐서플로우로 구현되었으나, 후발주자로 페이스북이 파이토치를 만들고 경쟁구도가 되었음\n\n파이토치 vs 텐서플로우\n\n파이토치 : high레벨에 가까운 pythonic함 (사람의 직관에 가까운, 추상화된) 대학원 등 교육 쪽에서 많이 사용(구현해보는 것에 중점) 사용자 증가로 긍정적 생태계 조성(텐서플로우에는 없는 함수가 개발될 수 있고, 디버깅 쉬워짐[참고Case많음], 참고강의 많음)\n텐서플로우 : low레벨에 가까운 효율성 (기계가 이해하기 쉬운 C언어와 같은) 산업 등 비즈니스 영역에서 많이 사용(효율성 중시)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensor",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensor",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensor",
    "text": "Tensor\n\nTensor : ai에서 사용하는 배열 (차원=rank), 고차원의 데이터 저장(숫자뿐 아니라 문자도 가능)\n\n0차원 scalar / 1차원 vector / 2차원 matric (2d tensor) /3차원 3 tensor (3d tendor) / N차원 N tensor\n참고영상 : https://youtu.be/m0qwxNA7IzI?si=FeyWcPYuun7T_QON\n\n\n\n\n\nimage.png\n\n\n\n고차원/비정형 데이터 필요성 예시\n\n이미지 데이터(흑백)는 3d tensor 필요\n이미지 데이터(컬러)는 4d tensor 필요\n영상 데이터는 5d tensor 필요\n(결론→) 비정형데이터의 처리에 있어 tensor가 필요\n\n기존에는 매출, 성장률 등 숫자(정형 데이터)만 썼다면, 이제는 이미지(비정형 데이터) 등도 데이터 분석에 사용하기 시작함"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-실습-constant-rank",
    "href": "posts/meta-dl-creditcard-20240526/index.html#tensorflow-실습-constant-rank",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Tensorflow 실습 (constant, rank)",
    "text": "Tensorflow 실습 (constant, rank)\n\n텐서플로우 2.0의 차이 &gt; 즉시 실행모드(Eager Mode)지원 (1.x버전에서는 그래프를 생성하고 초기화하는 등 별도 작업이 필요했었음) \nRank(축) : 차원의 수\nShape(형상) : 0, 1, 2차원 등 데이터의 차원\ndtype : string, float32, float16, int32, int8 등 데이터 타입\n\n\nimport numpy as np\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n2.16.1\n\n\n\na = tf.constant(2)\nb  = tf.constant([2,3])\nc = tf.constant([[2,3],[6,7]])\nd = tf.constant(['hello'])\n\nprint('[tf.rank 차원의 수 출력]')\nprint(tf.rank(a))\nprint(tf.rank(b))\nprint(tf.rank(c))\nprint(tf.rank(d))\nprint()\n\nprint('[변수 자체 출력]')\nprint(a)\nprint(b)\nprint(c)\nprint(d)\n\n[tf.rank 차원의 수 출력]\ntf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\n\n[변수 자체 출력]\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor([2 3], shape=(2,), dtype=int32)\ntf.Tensor(\n[[2 3]\n [6 7]], shape=(2, 2), dtype=int32)\ntf.Tensor([b'hello'], shape=(1,), dtype=string)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#계산-add-subtract-multiply",
    "href": "posts/meta-dl-creditcard-20240526/index.html#계산-add-subtract-multiply",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "계산 (add, subtract, multiply)",
    "text": "계산 (add, subtract, multiply)\n\nadd, subtract, multiply (+, -, * 기호로도 가능)\n\n\na = tf.constant(3)\nb = tf.constant(2)\n\n\nprint(tf.add(a,b))\nprint(a+b)\n\ntf.Tensor(5, shape=(), dtype=int32)\ntf.Tensor(5, shape=(), dtype=int32)\n\n\n\nprint(tf.subtract(a,b))\nprint(a-b)\n\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\n\n\n\nprint(tf.multiply(a,b))\nprint(a*b)\n\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#numpy와-tensor의-변환",
    "href": "posts/meta-dl-creditcard-20240526/index.html#numpy와-tensor의-변환",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "numpy와 tensor의 변환",
    "text": "numpy와 tensor의 변환\n\n둘 다 데이터를 담는 container\n학습이 잘되고 있는지, 중간결과 등을 확인할때 numpy형태로 cpu로 확인\n실제 계산은 tensor 형태로 gpu에서 수행\n\n\n# numpy()\nc = (a+b)\n\nprint(c)\nprint(type(c))\n\nprint()\nprint(c.numpy())\nprint(type(c.numpy()))\n\ntf.Tensor(5, shape=(), dtype=int32)\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;\n\n5\n&lt;class 'numpy.int32'&gt;\n\n\n\n# tf.convet_to_tensor()\nc_sqrt = np.sqrt(c,dtype=np.float32)\nc_tensor = tf.convert_to_tensor(c_sqrt)\n\nprint(c_sqrt)\nprint(type(c_sqrt))\n\nprint()\nprint(tf.convert_to_tensor(c_sqrt))\nprint(type(tf.convert_to_tensor(c_sqrt)))\n\n2.236068\n&lt;class 'numpy.float32'&gt;\n\ntf.Tensor(2.236068, shape=(), dtype=float32)\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#linear-regression",
    "href": "posts/meta-dl-creditcard-20240526/index.html#linear-regression",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\n추세선 등을 그어서 쉽게 판단 가능 + X(투입)에 대한 Y(산출)을 판단해 데이터에 기반한 정량적 판단(객관적 증거)\n선형적관계 &gt; \\(Y = f(X) + \\epsilon\\) 을 기본적인 ML/DL의 식이라고 할 때,  \\(f\\)의 관계가 선형적 관계가 있다고 가정할 때 Linear regression  \\(income = f(education, seniority) + \\epsilon\\) 와 같은 예시를 들 수 있음\nSingle/Multi regression이 있다\n\n\n[용어정리]\nresponse, target : Y값 feature, input, predictor : X값 \\(\\epsilon\\)(엡실론) : 오차 \\(\\hat{x}\\) : 예측값 \\(x\\) (위의 기호는 hat)\n\n\n비선형적 관계를 다루는 모델을 사용하여 오차를 줄일 수 있음 단 오차를 0으로 만드는게 무조건 좋은 것은 아님 → 다뤄보지 못한 데이터가 나오면 성능이 떨어짐(과적합 overfitting)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#interpretability-vs-flexibility",
    "href": "posts/meta-dl-creditcard-20240526/index.html#interpretability-vs-flexibility",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "interpretability vs flexibility",
    "text": "interpretability vs flexibility\n &gt;[용어정리]\nflexibility : 성능, performance (100문제중 80문제를 찾추는가) interpretability : 해석 Least Squares : Linear regression\n\nDeep learning : 높은 성능 / 낮은 해석능력  효과적이고 빠름, 비선형적이면 좋은 모델을 뽑을 수 없음(선형이라는 가정 자체가 틀린 시작)\nLeast Squares : 성능은 DL보다 낮지만 높은 해석능력 (전통적 통계학의 기반을 둔 머신러닝, Statistical ML) 많은 시간과 데이터 필요, 비선형적 관계를 잘 모델링\n성능이 좋으니 DL만 사용? &gt; 관련 사례 DL로 판단하여 대출거절한 것은 차별에 해당한다는 판례 대출거절에 대해 근거를 제시해야하지만(자산부족 등), DL은 해석능력(interpretability)이 낮아 설명할 근거가 부족함 Least Squares(ㅣinear regression)로 근거를 제시했다면 차별이 되지 않았을 것"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#assessing-model-accuracy",
    "href": "posts/meta-dl-creditcard-20240526/index.html#assessing-model-accuracy",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "assessing model accuracy",
    "text": "assessing model accuracy\n\nerror의 정의 : 예측값과 실제값의 차이\n\n평균에 제곱한 에러 총합 MSE(Mean Squared Error), 줄일수록 좋음\n\n제곱을 활용하는 이유 : 나중에 미분(에러가 최소화되는 지점찾기)를 하는데, 이를 위해서 함 (다음 강의에서 설명예정)\n\\(MSE_{TR}\\)(Training set), \\(MSE_{TE}\\)(Test set)\n\n데이터 특성(복잡/단순)과 모델 적용(복잡/단순)에 따른 Training, Test MSE추이 \n\n검정(정답), 노랑(Linear), 초록(Smoothing splines)/ 빨강(\\(MSE_{TR}\\)), 회색(\\(MSE_{TE}\\))\n단순한 데이터에 복잡한 모델(초록)을 사용하니 과적합 발생 \n데이터가 복잡하지 않아 Linear를 사용하니 Error추이(우측그래프)도 좋음"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240526/index.html#bias-vs-variance-의-trade-off",
    "href": "posts/meta-dl-creditcard-20240526/index.html#bias-vs-variance-의-trade-off",
    "title": "[M_Study_1주차] Tensorflow / Linear Regression",
    "section": "Bias vs Variance 의 Trade off",
    "text": "Bias vs Variance 의 Trade off\n\nerror는 Variance와 Bias로 이루어져 있다 &gt; Variance(V), Bias(B) 예시 (과녁) V 낮음 B 낮음 : 정중앙에 잘 모여있음 V 낮음 B 높음 : 잘 모여있지만 위치가 잘못됨 V 높음 B 낮음 : 정답 근처이지만 불안정하게 퍼져있음 V 높음 V 높음 : 정답 근처도 아니고, 불안정하게 퍼져있음\nBias낮음 : Training에서 적중률이 높다, Overfitting 정답 자체를 틀리는 것과 관계\nVariance높음 : 모델이 불안정하다 변동에 과민하게 반응하는 것과 관계 (결과의 극단적 변화, 무의미한 결과는 무의미한 것으로 간주해야 안정적인 모델)\n둘 다 낮추기는 힘듦 (B낮추려면 V높아짐, B낮추려면 V 높아짐)\n둘 다 낮추기 위한 단 하나의 방법 : 데이터를 추가한다\nBias와 Variance를 고려한 가장 error가 적은 부분 Sweet spot"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - PySpark\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html#선택한-데이터셋-정제할-방법-고민gh-archive",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html#선택한-데이터셋-정제할-방법-고민gh-archive",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "선택한 데이터셋 정제할 방법 고민(gh archive)",
    "text": "선택한 데이터셋 정제할 방법 고민(gh archive)\n\njson 읽고 스키마 확인하기\ngithub = spark.read.json(\"../data/*.json.gz\")\ngithub.printSchema()\n&gt;&gt;&gt; root\n|-- actor: struct (nullable = true)\n|    |-- avatar_url: string (nullable = true)\n|    |-- display_login: string (nullable = true)\n|    |-- gravatar_id: string (nullable = true)\n|    |-- id: long (nullable = true)\n|    |-- login: string (nullable = true)\n|    |-- url: string (nullable = true)\n|-- created_at: string (nullable = true)\n\n\n스키마 지정하기\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType\nimport pyspark.sql.functions as F\n\nactor_schema = StructType([\n    StructField('login', StringType(), True),\n    StructField('url', StringType(), True)\n])\n\npayload_schema = StructType([\n    StructField('repository_id', LongType(), True),\n    StructField('size', LongType(), True),\n    StructField('distinct_size', LongType(), True),\n    StructField('message', StringType(), True)\n])\n\nrepo_schema = StructType([\n    StructField('name', StringType(), True),\n    StructField('url', StringType(), True)\n])\n\n데이터를 보기에 nested json인데, 타입이 struct가 아니라 string이 나온 경우\n\nSpark가 inference를 하지 못했을 상황으로, 스키마 정의해주어야 함\n\nnew_df = github.withColumn('actor_json', F.from_json('actor', actor_schema)) \\\n            .select('created_at', 'id', 'payload', 'type', 'actor_json.*', 'repo')\n\nF.from_json('actor', actor_schema)으로 필요한 데이터만 가져오게 됨\n.select문의 actor_json.*으로, actor_json.login과 actor_json.url을 불러옴\n\n데이터가 잘 읽혔다면 withColumn을 사용할 필요없이 바로 .select로 쿼리가능\n\nactor_json.login와 같은 경우 F.col(actor_json.login).alias('actor_url')과 같이 컬럼명 지정 가능\n\nambiguous 관련 오류를 방지할 수 있다\n\nnew_df = github.select('created_at', 'id', 'payload', F.col(actor_json.login).alias('actor_url'))\n\n\n\n\n데이터 읽고 정제하기\n\nfilter 예시\n\nnew_df = new_df.filter(col(\"login\") != \"github-actions[bot]\")\n\nregexp_replace(정규식 사용) 예시\n\nnew_df = new_df.withColumn('created_at', F.trim(F.regexp_replace(new_df.created_at, \"[TZ]\", \" \")))\n\nto_timestamp 예시\n\nnew_df = new_df.withColumn('created_dt', F.to_timestamp(new_df.created_at, 'yyyy-MM-dd HH:mm:ss'))\n\nUDF(UserDefineFunction) 예시\n\nF.udf(function, type)형식으로 생성, F.Col을 사용해서 적용 ```python def check_repo_name(name): sp = name.split(“/”) if not sp: return name else: return sp[-1]\n\n\nudf_check_repo_name = F.udf(check_repo_name, StringType())\nnew_df = new_df.withColumn(‘repo_name’, udf_check_repo_name(F.col(‘name’)))\n\n### 기타 참고\n* Pyspark의 EOF에러는 메모리 문제일 수 있음\n\n* Window function\n  * 예시 : 전체가 아닌 repo_name을 기준으로 임시window(파티션)을 생성하여 size기준으로 내림차순\n  ```python\n  from pyspark.sql.window import Window\n  from pyspark.sql.functions import col, row_number\n\n  w = Window.partitionBy(\"repo_name\").orderBy(F.desc(col(\"size\")))\n  new_df.withColumn(\"row\", row_number().over(w)) \\\n        .filter(col(\"row\") == 1).drop(\"row\") \\\n        .count()\n* row_number().over(w) : w에 할당된 윈도우 끼리만 row_number를 매김\n* .filter(col(\"row\") == 1).drop(\"row\") : (내림차순 후) 첫째행만 filter한다 + 이후 필요없어진 row컬럼을 drop\n\nElastic search 사용을 대비해, id를 남기는 것이 좋음\n\n남기지 않더라도 ES에서 지정해 줌"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html#shuffle-partition",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html#shuffle-partition",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Shuffle & Partition",
    "text": "Shuffle & Partition\n\nPartitioning(파티셔닝)\n\n(Spark는 분산처리를 위한 플랫폼이며) 파티셔닝은 데이터를 여러 클러스터 노드로 분할하는 알고리즘\nPartition(파티션) : 데이터의 조각, 데이터 처리의 단위 (데이터 자체가 아니고 meta정보)\n파티션 개수 : RDD변환 연산을 실행할 태스크 개수와 동일\n주의할 점\n\n태스크 수가 적으면 클러스터를 충분히 활용하지 못함\n\n노드가 100개인데 파티션이 2개라면, 태스크가 2개이므로 98개의 노드가 놀게 됨\n\n태스크가 처리할 데이터 분량이 (너무크면) 메모리 리소스를 초과할 수 있다\n\n데이터가 큰데, 파티션이 하나라면 메모리 리소스를 초과할 수 있다. (Spark는 기본적으로 데이터를 메모리에 올려서 처리하므로, 적절히 나누는게 중요)\n\n파티션의 수를 정하는데 정답은 없음. 처음 추천세팅값은 아래와 같음\n\n클러스터 코어 개수보다 3~4배 더 많은 양으로 파티션 개수 설정\n실험하며 태스크 관리작업에 병목이 없도록 결정(변경)\n\n느리다면 파티션을 나눠 많은 클러스터가 사용되도록 (병렬처리 갯수를 늘리도록)\n메모리 부족 등으로 태스크가 죽는다면, 파티션의 크기를 줄인다 \n\n\n\n\nPartitioner(파티셔너)\n\n파티셔닝을 담당하는 객체, 여러 구현체가 있음\n파티션의 수를 부여하지 않아도, 기본 파티션 개수가 있음\n\n초기 SpartContext생성옵션에서 지정가능 (spark.default.parallelism)\n\nHashPartitioner (디폴트 파티셔너, 해싱을 해서 나눔)\n\n파티션의 수(numberOfPartitions)를 부여하면, 해싱을 하여 partitionIndex를 만들어줌\n\nRangePartitioner\n\n정렬된 RDD를 범위 간격으로 분할 (직접 사용하는 경우가 많지 않음. 사용하기 전에 정렬을 해야하기 때문)\n\nUser Defined Partitioner\n\n직접 구현, pair RDD에서만 사용가능\n구현해야하는 구현체는, patitioner라는 구현체를 상속받아서 / 2가지 function을 overide해야 함\n\nnumPartitions : 파티션의 수를 리턴\ngetPartition(key) : key를 받아서, index를 리턴 \n\n\n\nPartition(파티션)\n\n처리하고자 하는 데이터가 분할할 수 없는 파일인 경우 무용지물\n처리대상 데이터의 저장 방식(포맷, 압축 등)은 (같은 파티셔너를 쓰더라도)작업 성능에 영향을 준다\n\n가지고 있는 데이터가 어떤지도 성능에 영향을 미칠 수 있다 \n\n\nPartition관련 파라미터\n\nspark.default.parallelism(많이 사용)\nspark.files.maxPartitionBytes\nspark.files.openCostInBytes\nspark.sql.files.minPartitionNum\nspark.sql.leafNodeDefaultParallelism\nspark.sql.shuffle.partitions(많이 사용)\nspark.sql.adaptive.advisoryPartitionSizeInBytes (spark.sql.adaptive.enabled) \n\nShuffling(셔플링)\n\n파티션 간의 물리적 데이터 이동 (Join, reduce와 같이 데이터의 이동이 일어나는 경우)\n새로운 RDD파티션을 만들기 위해 여러 파티션의 데이터를 합칠 때 발생\n셔플링은 2가지 과정을 거쳐 일어남\n\n디스크에 중간 병합 결과를 저장하고, 이를 읽어들여 병합\n셔플링할 데이터를 네트워크로 전송하는 비용(Disk I/O) 발생 \n\n\nShuffling 발생조건\n\n파티셔너를 명시적으로 변경하는 경우\n\n(디폴트 파티셔닝이 되어있었으므로)파티션을 새롭게 만드므로 반드시 발생\nHashPartitioner의 개수를 변경하는 경우도 (다시 해싱하며 파티션인덱스가 바뀌므로) 발생\n\naggregate와 같은 셔플링이 발생하는 API\n\n예시 : rdd.aggregateByKey(zero, seqFunc, comboFunc, 100) [파티션의 기준이 key가 되는 것]\n\n파티셔너를 제거하는 경우\n\nmap, flatMap은 RDD의 partitioner를 제거한다\n\nmap만 했을 때는 파티션을 새로 만들지는 않지만, 모든 데이터를 순회했기때문에 제거한 것에 해당\nmap을 한 뒤에, 다른 변환을 연이어 사용하게되면 셔플링이 발생함 (이미 제거된 것에서 파티션을 만드는 것이 됨)\nrdd.map(lambda x: (x, x*x)).map(swap).count()\nrdd.map(lambda x: (x, x*x)).reduceByKey(lambda v1, v2: v1+v2).count()\nmap, flatMap 변환 연산자 뒤에 사용하면 셔플링이 발생하는 연산자\naggregateByKey, foldByKey, reduceByKey, groupByKey, join, \nleftOuterJoin, rightOuterJoin, fullOuterJoin, subtractByKey\n셔플링이 발생하는 function\n\ncoalesce는 노드간 데이터 이동없이 파티션 수만 줄어드는 알고리즘이 되어있지만, ’shuffle=True’적용시는 발생\n\nsubtract, intersection, groupWith\nsortByKey\npartitionBy, coalesce (shuffle=True)\n\npairRDD의 경우, map대신 mapValues를 사용하면 셔플링으로부터 자유로움 \n\n\nShuffling Optimization\n\n셔플링이 발생한다고 아예 안쓸 수는 없으며, Optimization방법을 고민\nspark.shuffle.service.enabled = true\nspark.shuffle.manager : sort-based shuffling(파일을 적게쓰고 메모리를 효율적으로 사용)\n\nhash-based shuffling보다 효율적\n\nspark.shuffle.spill.compress: 디스크로 내보낼 데이터의 압축 여부 (아주 조금 도움)\nspark.shuffle.memoryFraction: 셔플링에 쓸 메모리 임계치\n\n낮추면, 큰 데이터인 경우 디스크에도 썼다 올리면서 느려지지만 죽지 않음\n\nspark.shuffle.spill: 셔플링에 쓸 메모리를 제한할지 여부\nspark.shuffle.compress: 중간 파일의 압축 여부\nspark.shuffle.spill.batchSize: 데이터 디스크로 보낼 때 직렬화, 역직렬화할 객체 개수\n\n직렬화할 Batch단위 설정\n\nspark.shuffle.service.port: 외부 셔플링 서비스 활성화할 경우 서비스가 사용할 포트\n\n셔플링이 무거운 작업이므로, 효율적으로 할 수 있게 도와주는 3rd파티 서비스(어플리케이션)이 있음 \n\n\n파티션 변경 이유\n\n작업 부하를 효과적으로 분산\n메모리 문제 방지\n(많은 케이스)특정 데이터들이 같은 파티션에 속해야 하는 경우\n\n예를 들어, 같은 group-name을 가진 데이터는 동시 처리되지 않아야 하는 경우\nAPI호출을 특정 시간내 일정 횟수만 호출해야하는 경우\n같은 파티션이라면 병렬이 아닌 순차처리\nrepartition(partitionExprs=group-name)을 사용해 컬럼 값이 같은 데이터는 같은 파티션으로\n\nskew발생할 수 있음 (특정 노드가 놀아서 비효율적일 순 있으나, 필요한 경우이므로 감안)\n\n\n파티션 변경 명령어\n\npartitionBy\ncoalesce\nrepartition\nrepartitionAndSortWithinPartition \n\n\n파티션 단위 연산\n\nmapPartition : (map과 달리) 전체 데이터가 아닌 파티션 별로 iteration을 할 수 있다\n\niterator를 리턴하므로, 한번 더 순회해주어야 함(함수 시그니처에 주의)\n위의 예시였던 같은 group-name을 한 파티션에 넣은 경우에도, 파티션 내의 group-name을 확인 후 작업하도록 할 수 있음\n특정 파티션 작업을 완료한 후, system.gc(가비지콜렉팅)을 해주며 메모리 관리도 가능\n\nmapPartitionsWithIndex : iterator와 함께 index도 받을 수 있다\n\nn번째 파티션마다 다른 처리 가능 \n\n\nShuffling 파라미터\n\nmaxReqsInFlight: Request가 동시에 얼마나 갈지 설정\nmemory.fraction: memory 중 일부만 사용하고 나머지는 hfile 로 disk 에 내림\nmaxReqs : (동시에X, 전체) Request 건수 설정\nexecutor.memoryOverhead"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html#sparksql",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html#sparksql",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "SparkSQL",
    "text": "SparkSQL\n\nSparkSQL\n\nSQL명령을 작성하면 Dataframe연산으로 변환한다 \n\nUDF (사용자 정의 함수, pyspark.sql.functions.udf)\n\nUDF 코드예시\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\nslen = udf(lambda s: len(s), IntegerType()) \n@udf\ndef to_upper(s):\n    if s is not None: \n        return s.upper()\n\n@udf(returnType=IntegerType()) \ndef add_one(x):\n    if x is not None: \n        return x + 1\n\ndf = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\ndf.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n\n\nGrouping\n\nGroupby\nrollup : 다양한 컬럼을 그룹화 키로 설정 가능(심화된 버전의 groupby)\n\n그룹별 결과 뿐 아니라 총 집계도 따로 해 줌\n\ncube : 모든 차원에 대해 작업 수행(rollup의 고차원적인 사용)\n\n테이블에 있는 모든 정보를 빠르고 쉽게 조회할 수 있는 요약정보 테이블 생성 \n\n\nWindow(pyspark.sql.Window)\n\nwindow코드예시\nimport pyspark.sql.Window\n\nwindow = Window.partitionBy('platform').orderBy(desc(col(\"Quantity\"))) \n\nf.rank().over(window) # partitionBy에서 지정한 platform끼리 function.rank를 매김\n\nWindow.currentRow \nWindow.rangeBetween\n\nHive\n\n실습시 만들었던 temporary테이블은 SparkContext가 내려가면 사라지며, 저장하고 싶다면 Hive를 사용\nHive는 (HDFS와 HDFS와 호환되는)파일시스템에 보관된 데이터에 대한 분석 툴\nSQL과 유사한 쿼리 언어인 HiveQL제공\n쿼리를 Spark, MapReduce, Tez작업으로 변환\nSpark에 SparkSQL커맨드라인이 있듯이, beeline이라는 H(ive)QL 실행가능한 커맨드라인 인터페이스 내장\nACID 트랜잭션을 지원\n작동\n\n실행 계획을 생성하고 SQL 쿼리를 처리하기 위한 YARN 작업을 자동 생성\n분산형 어플리케이션으로 작동\n작업 결과를 어플리케이션이나 HDFS 로 반환 \n\n\nTable Catalog\n\nSpark가 저장하는 등록된 Dataframe테이블(임시로 생성하는 등록된 Temporary Dataframe)\n단순한 in-memory Map으로 구현되어, SparkContext를 내리면 함께 내려감\nHive가 enable된 경우는 영구적으로 하이브(메타스토어)에 저장\n\nHive enable과 함께, Hive 어플리케이션을 실행해 나오는 Hive마스터 주소를 Spark옵션에 작성해주어야 함\nSparksession을 만들 때, Hive.enable=True, Hive마스터 주소를 함께 넣어야 함\n\n임시 vs 영구 여부에 따른 차이\n\n테이블 임시등록(임시 Table Catalog) : spark.createOrReplaceTempView\n테이블 영구등록(영구 Hive) : df.write.saveAsTable\n\nTable catalog도 Hive도 각자의 메타스토어에 저장됨 \n\nSparkSQL실행방법\n\n테이블 등록 후 쿼리 실행 : spark.sql(“select * FROM table”)\nSpark SQL Shell(커맨드라인) : bin/spark-sql 실행 후 쿼리\n\n메타스토어에 lock을 걸기 때문에 기존의 Spark shell은 중지 필요 (2개 동시 실행은 안됨)\n\nJDBC서버에서 실행 : Thrift server enable을 해둔 후, (중간다리역할을 하는) Thrift server로 쿼리를 JDBC서버로 보낼 수 있음"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html#optimization",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html#optimization",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Optimization",
    "text": "Optimization\n\nSpark job을 Optimize 하기(정답이 있는 부분은 아님)\n\n아래의 항목을 단계별로 확인(앞쪽 항목일수록 중요, 항목들을 반복/순회하며 최적화)\n\nData확인\n\ncompression방식, 데이터 포맷(에 따라 파티셔너가 동작하지 않을 수 있음)\n분할 불가능한 데이터\n특정 raw에 이상한 값이 있는 경우 (처리 불가한 상태가 될 수 있음)\n\nSpark DAG확인\n\n(실행순서 조정 등을 통해 최적화해주는)Spark의 카탈리스트 엔진(옵티마이저)\n카탈리스트 옵티마이저가 작업을 끝내면 DAG(Directed acyclic graph)형태로 나옴\n트리모양의 DAG가, 내가 원하는 방향으로 작성되었는지 확인(내가 코드를 원하는 SparkOperation으로 전환했는지)\n리소스 확인\n디스크, 메모리 부족 여부\n\n갑자기 죽는 경우 메모리가 부족하거나, 메타스토어 등에서 lock을 잡아 강제종료된 경우\n\n\nUser application 확인\n\n내가 작성한 Spark코드의 이상 여부\n\n\n중간마다 cache와 checkpoint를 활용해 불필요한 반복을 줄임 \n\nOptimization Options\n\n나의 상황에 따라 drvier와 executor의 메모리 등을 조정\n\n느린 이유가 print작업이 많아서라면 driver메모리\n느린 이유가 join작업에서 dataframe이 크다면 executor메모리\n\nYarn cluster기준의 옵션 샘플1\n\n단순참고)하단의 driver/executor메모리(8G)와 코어(4)옵션은 페타바이트 수준의 데이터를 다룰 정도의 옵션\n\n&lt;path-to-spark-submit&gt;/spark-submit \\\n    --class &lt;project.class.path&gt; \\\n    --name HelloWorld \\    # job name지정\n    --master yarn \\        # master(현재 실습기준으로는 local)\n    --deploy-mode client \\\n    --driver-cores 4 \\     # optimization옵션에 해당\n    --driver-memory 8g \\   # optimization옵션에 해당\n    --num-executors 100 \\  # optimization옵션에 해당\n    --executor-cores 4 \\   # optimization옵션에 해당 (병렬처리를 얼마나 많이 할지)\n    --executor-memory 8g \\ # optimization옵션에 해당\n    --conf spark.logConf=true \\ \n    &lt;jar file path&gt; \\\n    args=value\nYarn cluster기준의 옵션 샘플2\n\n사용하는 클러스터에 맞는 옵션 사용\n\n실습중인 gh archive데이터도 규모에 따라 메모리가 터질 수 있으니 바꿔보기\n\nspark-submit\n--driver-cores 1      # 실습시 바꿔볼 수 있는 부분\n--driver-memory 1g    # 실습시 바꿔볼 수 있는 부분\n--num-executors 1     # 실습시 바꿔볼 수 있는 부분\n--executor-cores 1    # 실습시 바꿔볼 수 있는 부분\n--executor-memory 1g  # 실습시 바꿔볼 수 있는 부분\n--conf spark.yarn.am.cores=1\n--conf spark.yarn.am.memory=1g\n--conf spark.yarn.am.memoryOverhead=1g\n--conf spark.driver.memoryOverhead=1g     # 실습시 바꿔볼 수 있는 부분\n--conf spark.executor.memoryOverhead=1g   # 실습시 바꿔볼 수 있는 부분\n--conf spark.sql.shuffle.partitions=1 \n\n\n\nCatalyst Engine\n\nDataframe DSL 과 SQL 표현식을 하위 레벨의 RDD 연산으로 변환한다\nDataframe function이 pyspark.sql과 같이 sql패키지에 있던 이유\n\n카탈리스트 엔진이 SQL표현식을 하위 레벨의 RDD연산으로 변환하기 때문\n\n실행순서\n\nExpression(SQL 등 작성) → Parsed Logical Plan  → Analyzed Logical Plan → Optimized Logical Plan  → Physical Plan\n\n기본 구조 : 노드로 구성된 트리 (DAG구조)  \n\nTungsten Project\n\nspark 1.5 와 함께 공개, 메모리 관리 방식 혁신\n\n객체를 이진수로 인코딩하고 메모리에서 직접 참조\non-heap: 이진 인코딩 객체를 long 배열에 저장\noff-heap: spark 가 메모리 주소 직접 할당하고 해제\n텅스텐 셔플링 매니저: 이진 인코딩 기반 정렬 셔플링 \n\n\nSpark DAG 확인방법 3가지\n\nExplain method : dataframe.explain(True)와 같이 작성하여 실행\n\nParsed Logical Plan부터 Physical Plan까지 프린트 됨\n\nSpark UI의 SQL탭에서 확인 가능\n실행중인 Job목록에서, +버튼(datails)을 누르면 Plan 확인 가능 \n\nDAG(Logical/Physical Plan) 샘플화면  \nPartitions Statistics\n\n파티션 별로 각 컬럼의 통계(lower bound, upper bound, NULL count 등) 활용\n\n예를 들어, filtering으로 30보다 작은 것을 찾을 때, lower bound가 40이면 파티션을 건너뛰어 작업성능 최적화\n\n이러한 컬럼 통계는 dataframe을 메모리에 캐시하면 자동으로 계산\n\ncache를 불필요한 계산을 막아 성능이 오르는 부분 외에, 컬럼 통계 반영으로 성능이 오르는 부분도 있음 \n\n\nPartition Pruning\n\n(얻고자 하는 데이터를 포함하지 않은)파티션을 제외하고 읽는 것\nStatic Partition Pruning(정적 프루닝)\n\njoin이 발생하는 상황에서는 정적으로 pruning하기 어려움(큰 DB를 Join하여 optimization효과 기대 어려움)\n\nDynamic Partition Pruning(동적 프루닝)\n\nSQL 논리계획, 물리적 실행계획 변경 (필터를 삽입해준다)"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240827/index.html#advanced-topics",
    "href": "posts/meta-de-spark_and_airflow-20240827/index.html#advanced-topics",
    "title": "[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\n3rd-party 어플리케이션\n\nZepplin : notebook (Jupyter와 같은 역할)\n\nvisualization이나 Spark 백엔드 사용 등이 편리함\nSearch interpreters에서 Spark를 검색하여, hostname이나 포트(실습환경기준 sparkmaster:7077)입력하여 노트북 사용 설정\n\nJupyter는 이러한 설정을 해도, 별도의 옵션을 넣어야해서 상대적으로 번거로움\n\n링크 : https://zeppelin.apache.org/docs/0.11.1/quickstart/install.html\n\nMLLib\n\n머신러닝 알고리즘들을 분산 방식으로 구현한 라이브러리\n알고리즘의 종류가 많지 않음(꾸준히 늘어나고 있음)\n기본적인 알고리즘은 있음(kmeans, decision tree, random forest, logistics regresson 등)\n지속적으로 늘어나는 데이터셋에 머신러닝을 적용할 수 있도록 지원\n\nSpark Streaming\n\n실시간성 데이터에 사용\nSpark는 기본적으로 배치 처리를 지향\nSpark Streaming은 mini-batch를 사용\n\n특정 시간 간격 내에 유입된 데이터 블록을 RDD로 구성\n즉 완벽한 스트리밍은 아님. 시간간격은 사용자가 설정\n\nSpark Streaming Job으로 유입할 수 있는 외부 데이터 시스템\n\n단순 파일 시스템\nTCP/IP 접속\nkafka, flume 등 분산 시스템 (kafka + Spark stream 조합은 실무에서 많이 쓰임)\n\n각 데이터 소스별로 receiver를 제공하므로, kafka나 IP 등에 맞춰 맞는 receiver사용\n사용방법\n\nSpark Context와 Duration(미니배치를 생성할 시간간격)을 Streaming Context에 전달\nsc.textFile() → sc.textFileStream()\n\n새로운 배치가 인입되면서 state가 변경된다\nStream용으로 mapWithState, updateStateByKey 등의 API사용\n\n\n\nGraphX\n\nSpark의 그래프처리 API\n\n그래프로 데이터를 구성하여 그래프 알고리즘을 적용하고자 하는 경우\nVertexRDD, EdgeRDD를 구성해, Graph오브젝트[Graph(vertices, edges)]에 넣어 Graph데이터 구조를 만듦\n\nRelationship도 따로 정의 가능\n\norg.apache.spark.graphx._\n가능한 알고리즘 : subgraph(그래프 안 서브그래프 찾기), mask(마스킹), pagerank, strongly connected components 등"
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html",
    "href": "posts/coach-ml-mnist-20230301/index.html",
    "title": "[Pytorch] MNIST 실습",
    "section": "",
    "text": "파이토치로 MNIST 머신러닝 실습해본 코드 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html#개요",
    "href": "posts/coach-ml-mnist-20230301/index.html#개요",
    "title": "[Pytorch] MNIST 실습",
    "section": "",
    "text": "파이토치로 MNIST 머신러닝 실습해본 코드 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "href": "posts/coach-ml-mnist-20230301/index.html#pytorch활용한-mnist-데이터셋-로딩",
    "title": "[Pytorch] MNIST 실습",
    "section": "Pytorch활용한 MNIST 데이터셋 로딩",
    "text": "Pytorch활용한 MNIST 데이터셋 로딩\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nroot = './data'\nmnist_train = dset.MNIST (root=root, train=True, transform=transforms.ToTensor(), download=True )\nmnist_test = dset.MNIST (root=root, train=False, transform=transforms.ToTensor(), download=True)\n\n# Train용 / Test용 데이터셋\ntrain_loader = DataLoader(mnist_train, batch_size=10, shuffle=True)\ntest_loader = DataLoader(mnist_test, batch_size=10, shuffle=True)\n\n\nFigure 1"
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html#학습준비가중치-초기화-등",
    "href": "posts/coach-ml-mnist-20230301/index.html#학습준비가중치-초기화-등",
    "title": "[Pytorch] MNIST 실습",
    "section": "학습준비(가중치 초기화 등)",
    "text": "학습준비(가중치 초기화 등)\n\nMNIST의 크기 : 28 * 28\nLoss : Cross Entropy\nOptimizer - SGD(Stochastic Gradient Descent)\nLearning rate = 0.1\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 모델 구현 (28*28 = 784 / 0~9라서 10개 / 가중치 사용하므로 bias)\nlinear = torch.nn.Linear(784, 10, bias=True).to(device) \n\n# weight init 가중치 초기화\ntorch.nn.init.normal_(linear.weight)\n\n# Loss fn - Cross Entropy Loss\ncriterion = torch.nn.CrossEntropyLoss().to(device)\n\n# optimizer - SGD\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html#모델-학습",
    "href": "posts/coach-ml-mnist-20230301/index.html#모델-학습",
    "title": "[Pytorch] MNIST 실습",
    "section": "모델 학습",
    "text": "모델 학습\n\ntraining_epochs = 20 # training 반복 횟수\n\nfor epoch in range(training_epochs):\n  for i, (imgs, labels) in enumerate(train_loader):\n    labels = labels.to(device)\n    imgs = imgs.view(-1, 28 * 28).to(device)\n\n    outputs = linear(imgs) \n    loss = criterion(outputs, labels) \n\n    optimizer.zero_grad()# optimzier zero grad\n\n    loss.backward() # loss backward\n    optimizer.step() # optimzier step\n\n    _,argmax = torch.max(outputs, 1)\n    accuracy = (labels == argmax).float().mean()\n\n  if (i+1) % 100 == 0:\n    print('Epoch [{}/{}], Step [{}/{}], Loss: {: .4f}, Accuracy: {: .2f}%'.format(\n    epoch+1, training_epochs, i+1, len(train_loader), loss.item(), accuracy.item()* 100))\n\nEpoch [1/20], Step [6000/6000], Loss:  0.0273, Accuracy:  100.00%\nEpoch [2/20], Step [6000/6000], Loss:  0.0762, Accuracy:  100.00%\nEpoch [3/20], Step [6000/6000], Loss:  0.5928, Accuracy:  80.00%\nEpoch [4/20], Step [6000/6000], Loss:  0.2854, Accuracy:  90.00%\nEpoch [5/20], Step [6000/6000], Loss:  0.1373, Accuracy:  90.00%\nEpoch [6/20], Step [6000/6000], Loss:  0.0668, Accuracy:  100.00%\nEpoch [7/20], Step [6000/6000], Loss:  0.0253, Accuracy:  100.00%\nEpoch [8/20], Step [6000/6000], Loss:  0.0542, Accuracy:  100.00%\nEpoch [9/20], Step [6000/6000], Loss:  0.9203, Accuracy:  80.00%\nEpoch [10/20], Step [6000/6000], Loss:  0.1244, Accuracy:  90.00%\nEpoch [11/20], Step [6000/6000], Loss:  0.6108, Accuracy:  90.00%\nEpoch [12/20], Step [6000/6000], Loss:  0.1312, Accuracy:  100.00%\nEpoch [13/20], Step [6000/6000], Loss:  0.0705, Accuracy:  100.00%\nEpoch [14/20], Step [6000/6000], Loss:  1.6259, Accuracy:  70.00%\nEpoch [15/20], Step [6000/6000], Loss:  0.0538, Accuracy:  100.00%\nEpoch [16/20], Step [6000/6000], Loss:  0.2435, Accuracy:  80.00%\nEpoch [17/20], Step [6000/6000], Loss:  0.0061, Accuracy:  100.00%\nEpoch [18/20], Step [6000/6000], Loss:  0.1091, Accuracy:  100.00%\nEpoch [19/20], Step [6000/6000], Loss:  0.0157, Accuracy:  100.00%\nEpoch [20/20], Step [6000/6000], Loss:  0.1413, Accuracy:  90.00%"
  },
  {
    "objectID": "posts/coach-ml-mnist-20230301/index.html#학습된-모델-테스트",
    "href": "posts/coach-ml-mnist-20230301/index.html#학습된-모델-테스트",
    "title": "[Pytorch] MNIST 실습",
    "section": "학습된 모델 테스트",
    "text": "학습된 모델 테스트\n\nlinear.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for i, (imgs, labels) in enumerate(test_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        imgs = imgs.view(-1, 28 * 28)\n\n        outputs = linear(imgs) # 구현\n\n        _, argmax = torch.max(outputs, 1) # max()를 통해 최종 출력이 가장 높은 class 선택\n        total += imgs.size(0)\n        correct += (labels == argmax). sum().item()\n\n    print('Test accuracy for {} images: {: .2f}%'.format(total, correct / total * 100))\n\nTest accuracy for 10000 images:  91.99%"
  },
  {
    "objectID": "posts/infcon2024-20240802/index.html",
    "href": "posts/infcon2024-20240802/index.html",
    "title": "[인프콘2024] ‘혹시 당신은 데이터를 모르는 백엔드 개발자인가요’ 세션 정리",
    "section": "",
    "text": "인프콘에서 들었던 ‘혹시 당신은 데이터를 모르는 백엔드 개발자인가요’ 세션 내용정리\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/infcon2024-20240802/index.html#직무팀에-따른-데이터에-대한-관점-차이",
    "href": "posts/infcon2024-20240802/index.html#직무팀에-따른-데이터에-대한-관점-차이",
    "title": "[인프콘2024] ‘혹시 당신은 데이터를 모르는 백엔드 개발자인가요’ 세션 정리",
    "section": "직무/팀에 따른 데이터에 대한 관점 차이",
    "text": "직무/팀에 따른 데이터에 대한 관점 차이\n\n데이터에 대한 접근\n\n백엔드 엔지니어 : CRUD(CREATE READ UPDATE DELETE)\n데이터 팀 : AIRFLOW, DATA LAKE, OLAP, HADOOP 등\n\n\n\n행 vs 열\n\n백엔드 엔지니어 : 행을 잘 읽고 쓰는게 중요\n\n고객1이 어떤 물건을 얼마에 샀는지를 보는게 중요\n\n데이터팀 : 열을 잘 보는게 중요\n\n비즈니스 인사이트를 위한 부분. 어떤 품목이 몇개나 팔렸나 등\n\n열 기준의 데이터 관점을 잘 알게되었을 때의 장점\n\n데이터 분석비용 감소, 확장성/생산성 증가\n\n\n\n\n데이터에 대한 관점차이로 발생하는 문제\n\n예시) 분석비용의 증가\n\n엔지니어 : 행 데이터를 nested json, array 등으로 처리하는 것이 간편\n데이터 분석가 : 비즈니스 분석시 열 단위 분석한 아래와 같은 경우.\n\n아래와 같은 경우, row의 각 json 등을 모두 스캔해야하여 분석 비용이 많이 들 수 있음 \n\n정규화를 통한 해결, 앞으로 이 데이터가 분석에 쓰일 것인가를 고민\n\n안쓰일 것이라면 json으로도 문제가 되지 않겠지만, 그렇지 않다면 문제\n다만 대부분은 쓰일 것으로 예상됨\n\n\n예시) 생산성의 감소\n\n엔지니어 : 결제방식 컬럼의 DB Comment(설명)이 없어 알 수 없는 상황 + 기존 결제방식(비즈니스)에 영향을 줄 수 있음\n\n이후 새로운 결제방식은 boolean인 컬럼으로 추가 \n\n데이터 분석가 : 분석을 하려고 하니, 결제방식컬럼과 함께, 다른 타입의 결제방식 컬럼들이 다수 존재하여 어려움\n데이터 카탈로그를 통해, 데이터에 대한 설명 등을 문서화하며 검색가능하도록 통합관리\n\n’결제’와 같은 키워드 검색시, 테이블 이름/설명/컬럼명/문서 등을 출력\n데이터 카탈로그 등을 구축하지 못한다면, 적어도 Comment라도 남기기\n\n데이터가 어디에서 왔는지 등을 함께 명시\n\n\n\n\n\n\n대용량 트래픽 vs 대용량 데이터\n\n엔지니어 : 대용량 트래픽을 고객에게 에러 발생없이 제공하는 것이 중요\n데이터 분석가 : 테이블 당 데이터의 수(row)가 많고 일반적인 RDBMS는 쿼리 등이 어려울 수 있음\n\n분산처리 등이 중요해짐\n\n단순히 DB에 쓰고 지우는 것이 데이터관리의 전부는 아님\n\nLive Production DB에 무거운 분석쿼리를 날린다면, 고객의 쿼리 등이 중단/지연될 수 있음\n\n분석환경은 분리해서 운용"
  },
  {
    "objectID": "posts/infcon2024-20240802/index.html#데이터의-무결성",
    "href": "posts/infcon2024-20240802/index.html#데이터의-무결성",
    "title": "[인프콘2024] ‘혹시 당신은 데이터를 모르는 백엔드 개발자인가요’ 세션 정리",
    "section": "데이터의 무결성",
    "text": "데이터의 무결성\n\nGIGO : 잘못된 데이터를 넣고 좋은 결과가 나올 수는 없음\n데이터의 무결성을 지킨 데이터 필요\n\n유효성 : 정의된 범위에서 데이터 발생\n\n문제예시 - 유료서비스의 활성화 기간을 분석하고자 ’만료일’을 확인하고자 함\n\n비즈니스 로직 변경으로, Application에서만 저장되던 데이터를 엔지니어가 직접 일괄로 변경함\n프로모션 적용/컴플레인 대응/환불요청 등을 위해 고객센터 등에서 처리한 데이터 (로그가 남지 않음)\n\n\n정확성 : 실제 값을 정확하게 나타냄\n\n문제예시 - 유저 데이터의 물리삭제(회원탈퇴)\n\n탈퇴한 회원의 DELETE쿼리 실행\n회원의 탈퇴가 Human error인 경우\n유저의 활동을 통해 서비스 중인 현황을 분석하고자 할 때, 데이터가 없다면 분석 자체가 불가\n\n데이터의 저장비용(삭제)과 데이터의 가치(보존)에 대한 고민이 들 수 있음\n\n향후 어떻게 쓰일 지 알 수 없으므로 저장하는 것이 좋을 수 있음\n\n\n일관성 : 1개 컬럼은 1개의 값을 가져야 함\n\n문제예시 - 완료시점(Completed_at)을 하나의 컬럼으로 함께 사용하여, 후에 어떤 컬럼의 완료인지 알기 어려움\n\nCompleted_at을 함께사용하는 주문/배송 컬럼"
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html",
    "href": "posts/dtcontest-ore-20240612/index.html",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(지표 및 사용할 데이터에 대한 고민)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html#개요",
    "href": "posts/dtcontest-ore-20240612/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n사용하기로 한 지표에 대한 재조사 및 통합사용하기로 의견제안 예정\n모델의 Feature로 원자재의 벌크운송에 대한 운임지수(BDI)사용 의견제안 예정"
  },
  {
    "objectID": "posts/dtcontest-ore-20240612/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240612/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)",
    "section": "내용정리",
    "text": "내용정리\n\n지난 회의정리\n\n모델링에 사용할 지표 2가지 선정\n\n수급안정화지수\n시장위험지수\n\n광물별 가격 영향 미치는 요소 생각해보기\n모델링 관련 아이디어\n\n\n\n회의내용에 대한 Self고찰 및 아이디어 Develope\n\n모델링에 사용할 지표에 대한 분석\n\n데이터를 살펴보다보니 둘의 움직임이 거의 같게 나타남\n가격리스크, 중장기적 시계 등 공통적인 요소가 서로 많은 지표임을 발견\n포함항목을 좀 더 구체적으로 명시한 수급안정화지수로 통합사용 하는 것으로 의견 제안 예정\n\n광종별 중장기 가격리스크, 세계 수급비율(공급/소비), 세계 공급(매장)편중도, 국내 수입증가율, 국내 수입국 편중도 등\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\ndf_source = pd.read_csv('비교_한국광해광업공단_수급안정화지수_코발트_20240520 (1).csv', encoding='cp949')\ndf_source[['기간','시장위험지수','수급안정화지수']].plot(x='기간')\n\n\n\n\n\n\n\n\n\n광물별 고려요소 및 모델링에 쓸 데이터 관련 아이디어\n\n고려대상 광물들의 세계 생산량 비중 등을 고려할 때 국제수송(수입)을 고려해야할 것으로 보임\n\n조사를 통해 상하이 컨테이너 운임지수(SCFI)를 확인, 국가별 운임지수에 대한 가중치를 반영한 지표로 주요 수입국에 대해 반영 고려\n\n그러나 광물운송의 특성상 컨테이너로 운송하지 않기 때문에 다른 지수를 모색\n\n벌크선에 대한 지수인 발틱운임지수(BDI) 도입 검토\n\n공급 대비 운송량에 대한 수요를 알 수 있음\n벌크선은 광물, 곡물 등을 운송하므로 원자재에 대한 글로벌 수요와 공급을 간접적으로 측정 원자재 소요에 대한 미래의 경제선행지표로도 간주되기도 함\n구체적인 운송수요가 있을 때만 예약되는 벌크선 특성상 특정 목적에 의해 조정되는 경우가 적음\n위키백과에서 위의 내용들 발췌 : https://en.wikipedia.org/wiki/Baltic_Dry_Index\n\n발틱운임지수(BDI)를 데이터로 사용하는 것으로 의견 제안 예정\n\n주요 생산국에 대한 여러 요소의 고려\n\n전쟁, 전염병, 인건비 등을 대상으로 고려할 수는 없을지에 대한 고민\n\n수치화하기 어렵거나, 자료를 구하기 어려울 것으로 보여 일단 Drop"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240630/index.html",
    "href": "posts/meta-dl-creditcard-20240630/index.html",
    "title": "[M_Study_6주차] 자연어처리 및 RNN관련 기초내용",
    "section": "",
    "text": "참여중인 딥러닝 스터디 6주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240630/index.html#자연어처리-관련-기초적인-내용",
    "href": "posts/meta-dl-creditcard-20240630/index.html#자연어처리-관련-기초적인-내용",
    "title": "[M_Study_6주차] 자연어처리 및 RNN관련 기초내용",
    "section": "자연어처리 관련 기초적인 내용",
    "text": "자연어처리 관련 기초적인 내용\n\n시계열데이터(Sequential Data)\n\n시계열데이터(Sequential Data) : 문장(순서를 가진 단어들), 영상, 주가, 태풍의 이동경로 등  (자연어처리도 시계열데이터의 하위분야 중 하나)\n기본적으로 Input - Output - 그 사이의 function 을 파악하면 좋으며, 아래는 예시임\n\n태풍으로 인한 날짜별 피해액을 알고자 할 때 : Input도 Output도 시계열\n태풍의 소멸시기 예측 : Input은 시계열이지만 Output은 single output\n단어가 문법적으로 맞는지 : 시계열 문제\n문장의 주제가 어떤 것인지(과학? 문학?) : single output\n그림을 묘사하는 문장 생성 : 시계열이 아닌 Input과 시계열인 Output\n\n\n\n\nProblem types 예시\n\nOne-to-one : (Image classification) 숫자 이미지를 input으로 받아 정답 숫자를 output\nMany-to-one : (Frame → Class) 태풍의 이동경로, 속도를 input으로 하여 언제사라질지 output\nMany-to-many : (Frames → Classes) 태풍의 이동경로, 속도를 input으로 하여 날짜별 피해액 output\nOne-to-many : (Image → Words. Image captioning) 고양이 사진을 input으로, 이에 대한 묘사 문장을 output\nMany-to-many : (Video → words. Video captioning) 5개 단어의 영어문장을 input으로, 3개 단어의 한글문장으로 output\n\n\n\nWord Embedding\n\n단어를 모델에 인식시키려면 숫자로 매핑하는 과정이 필요 (알파벳은 유니코드 등으로 표현이 되지만, 단어는 그렇지 못함)\n숫자로 매핑하며 단어의 다양한 관계를 나타낼 수 있도록 벡터로 표현. 이를 Word Embedding이라고 함\n\n임의의 정수로 매핑할 때, 비슷한 단어일수록 숫자의 차이가 적게 만듦 (baby 12, boy 13, child 14와 같이)\n다만 girl이라는 단어가 나온다면 문제가 생길 수 있음 (boy 옆에 이미 baby가 있음)\n해결방법 : 벡터로 만든다. 자릿수를 늘려 표현할 수 있는 관계가 많아짐 (boy[13,14], girl[14,13]과 같이 정의하면 벡터간 거리가 같다)\n다양한 관계로 나타낼 수 있도록 벡터로 표현하게 됨 (Word Embedding) (참고로 GPT는 512차원이나 768차원정도 됨)\n\n처음엔 랜덤한 벡터였지만, 학습할수록 벡터가 변경되어 단어간의 관계를 표현\n\n\n\nWord2vec\n\n가지고 있는 단어(토큰)를, 문장에 빈 칸을 두어 맞추게 함\n\n예를 들어 5만 개의 단어 중 정답인 into가 들어오지 않으면 감점 후 다시 학습(5만가지의 Classification)\n\nWord2vec이 아닌, 단어(토큰)를 주고 주변의 단어를 맞추게 하는 방법도 있지만 요즘은 많이 쓰지 않는 추세\n언어모델의 학습은, 예를 들면 위키피디아의 문장을 가져와, 문장에 빈칸을 만들고 빈칸의 단어를 정답으로 만든 뒤, cross entropy 측정 등 진행\nWord2vec학습을 마치면 단어의 관계를 묘사하게 됨"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240630/index.html#rnn",
    "href": "posts/meta-dl-creditcard-20240630/index.html#rnn",
    "title": "[M_Study_6주차] 자연어처리 및 RNN관련 기초내용",
    "section": "RNN",
    "text": "RNN\n\nRNN을 활용한 감정분석 사례(Many-to-one)\n\nRNN(Recurrent Neural Networks)이라는 딥러닝으로 시계열을 처리하는 기초적인 architecture 적용 (이미지와 달리 문장은 길이가 항상 바뀐다)\n랜덤한 \\(h_0\\)을 시작으로, 현재의 단어 \\(x_1\\)부터 더해가면 최종적으로 \\(h_t\\)에 모든 문장의 정보가 들어있음 \n현재의 단어 \\(x_t\\)와, 이전까지의 단어들 \\(h_{t-1}\\) 중 어떤 것을 더 많이 반영할지를 정하는 파라미터 \\(W_hh\\), \\(W_xh\\) \n각 부분의 \\(W_hh\\), \\(W_xh\\)는 동일한 값 (Shared parameter) 예를 들어 \\(W_hh\\)값이 다르다면 단어의 수만큼 \\(W_hh\\)가 필요하며, 문장길이가 바뀔 때마다 학습을 다르게 해야함 \nSigmoid함수를 통과하여 0.5를 기준으로 긍정/부정 등을 평가하여 감정분석하는 방식 적용\nBackpropagation(Chain rule)을 사용해 학습\n\n\n\nRNN을 활용한 Many-to-many 적용케이스 (태풍의 날짜별 피해액 등)\n\nhidden state(\\(h_1\\))마다 예측(\\(\\hat{y}\\))을 하고, 예측에 대해 실제값과 cross entropy비교 및 Backpropagation \nMulti-layer RNN \nRNN의 장단점\n\n장점\n\nInput(문장의 길이 등)이 다른 문제에 대해 대처가 가능\nParameter share로 모델의 크기가 커지지 않게 됨\n(이론적으로) 마지막 step에는 모든 정보가 들어있게 됨\n\n단점\n\n느리다\nVanishing gradient(학습이 안됨) : 미분값이 0에 수렴하여 학습이 일어나지 않음 (↔︎ Exploding gradient)\nlong-range dependency(장기기억 소실, 학습은 되지만 초반부는 잊음) : 문장 초반부의 단어는 0에 수렴하며 잊게되고, 후반부의 내용만 잘 전달됨\n\n\n\n\n\nLSTM(Long Short Term Memory)\n\n장기기억을 잊는 문제를 해결하기 위해, 보존할 수 있는 (장기기억만을 담당하는)cell gate 추가 (RNN의 핵심적 아이디어를 조금 발전시킨 것)\n문제\n\n여전히 Vanishing gradient는 존재함\nlong-range dependency도 문장이 너무 길어지면 다시 발생\n\nTransformer가 등장하며 잘 안쓰이게 되었음\n\n\n\nGRU(Gated Recurrent Units)\n\nLSTM보다 간소화되었음\nTransformer가 등장하며 잘 안쓰이게 되었음\n다만 LSTM이나 GRU 모두 선박이나 태풍의 이동경로 등의 작은 규모는 적용할만 함 (Transformer는 상당히 큰 모델임)\n\n자연어는 Transformer 사용(위의 사례도 Transformer사용시 더 좋아짐)"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240823/index.html",
    "title": "[DE스터디/2주차과제1] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark (Dataframe/RDD API 실습)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823/index.html#dataframe-rdd-api사용해보기",
    "href": "posts/meta-de-spark_and_airflow-20240823/index.html#dataframe-rdd-api사용해보기",
    "title": "[DE스터디/2주차과제1] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "1. Dataframe, RDD API사용해보기",
    "text": "1. Dataframe, RDD API사용해보기\n\n(1) RDD\n\nSparkSession\n\nfrom pyspark.sql import SparkSession\n\n# SparkSession\nspark = (\n    SparkSession.builder\n        .appName(\"rdd-dataframe\")\n        .master(\"local\")\n        .getOrCreate()\n)\n\n# SparkContext를 SparkSession에서 빼두기\nsc = spark.sparkContext\n\n# 하단 메시지는 Jupyter 공식이미지에서 나오는 메시지로, 무시하기\n\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n24/08/28 14:33:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/08/28 14:33:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n\n\n\n# SparkContext 멈추기\nsc.stop()\n\n\n\nRange(0,10) & Collect()\n\n# collect()를 통해 파이썬 데이터셋으로 변경해야 사용할 수 있음\nrdd2 = sc.range(0, 10)\n\nprint(rdd2)\n\nPythonRDD[15] at RDD at PythonRDD.scala:53\n\n\n\nprint(rdd2.collect())\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\nparallelize로 변환[Python list → RDD] & take() [head()와 동일]\n\n# parallelize로 RDD로 만든 후 take로 위에서부터 3개 가져옴 (head/top/first와 같음)\ndata = [\"one\", \"two\", \"three\"]\nrdd3 = sc.parallelize(data)\n\nrdd3.take(3)\n\n['one', 'two', 'three']\n\n\n\n\ntextFile()\n\nsc.textFile(“../data/movie.csv”)만 단독 실행시 완료로 떠서 작업이 된 줄 알았는데, 다른 블럭에서 .count()를 실행하니 그제서야 그런 파일은 없다고 뜸. action이 있어야 데이터 처리가 이루어지는 Spark의 lazy한 특성을 확인함\n\n\nrdd1 = sc.textFile(\"../data/movies.csv\")\n\ncnt = rdd1.count()\nprint(cnt)\n\n9743\n\n\n\n# 첫번째 데이터는 컬럼명\narr2 = rdd1.take(10)\nprint(arr2)\n\n['movieId,title,genres', '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '2,Jumanji (1995),Adventure|Children|Fantasy', '3,Grumpier Old Men (1995),Comedy|Romance', '4,Waiting to Exhale (1995),Comedy|Drama|Romance', '5,Father of the Bride Part II (1995),Comedy', '6,Heat (1995),Action|Crime|Thriller', '7,Sabrina (1995),Comedy|Romance', '8,Tom and Huck (1995),Adventure|Children', '9,Sudden Death (1995),Action']\n\n\n\n\ntakeOrdered(5)\n\nSorting을 해서 가져오므로, 데이터가 큰 경우에는 사용X\nRDD는 컬럼이 없으므로 컬럼기준으로 Sorting은 불가함\n\n\narr3 = rdd1.takeOrdered(5)\nprint(arr3)\n\n['1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '10,GoldenEye (1995),Action|Adventure|Thriller', '100,City Hall (1996),Drama|Thriller', '100044,Human Planet (2011),Documentary', '100068,Comme un chef (2012),Comedy']\n\n\n\n\naggregate\n\n공식문서 코드로 이해하기\n\n문서링크 : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html\n\n샘플코드\n\nseqOp = (lambda x, y: (x[0] + y, x[1] + 1))\ncombOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\nsc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n&gt;&gt;&gt; (10, 4)\n\nseqOp로 각 파티션 계산 [주어진 항등원 (0,0)부터 시작]\n\n(0, 0) → (1, 1) → (3, 2)\n(0, 0) → (3, 1) → (7, 2)\n\ncombOp로 각 파티션 결과 취합\n\n(3, 2) + (7, 2) = (10, 4)\n\n\n\nrdd2 = sc.range(0, 1000, 1, 10)\n\nseqOp = lambda v1, v2: v1 + v2\ncombOp = lambda v1, v2: v1 + v2\n\nrdd2.aggregate(0, seqOp, combOp)\n\n499500\n\n\n\n항등원을 바꿔보니 항등원*11로 보여서 파티션의 수를 확인해보니 10개인 것을 확인함\n파티션의 갯수는 공식문서를 확인해보니 range에서 지정한 부분이었음\n\n문서링크 : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.range.html\nsc.range(start, end, step, number of partition)\n\n\n\nrdd2.aggregate(9, seqOp, combOp)\n\n499599\n\n\n\nrdd2.aggregate(50, seqOp, combOp)\n\n500050\n\n\n\nprint(rdd2.getNumPartitions())\n\n10\n\n\n\nreduce, fold\n\n\nrdd2.reduce(lambda v1, v2: v1 + v2)\n\n499500\n\n\n\nrdd2.fold(0, lambda v1, v2: v1 + v2)\n\n499500\n\n\n\n\nforeach\n\ndata = [\"one\", \"two\", \"three\"]\nrdd3 = sc.parallelize(data)\n\nrdd3.foreach(lambda v: print(v))\n\none\ntwo\nthree\n\n\n\n\nrepartition\n\n실습환경은 로컬이므로 파티션 1개인 상태여서 중요하지 않았음\n업무환경에서는 많이 쓰이는 함수임\n\n\n# 파티션 2개로 나눔\nrdd1 = sc.textFile(\"../data/movies.csv\")\n\nrdd4 = rdd1.repartition(2)\n\n\n\nforeachPartition\n\nreturn value는 파티션별 이터레이터\n파티션 별로 다른 함수를 호출해야하는 경우나, progress체크 등에 사용\n\n\nrdd4.foreachPartition(lambda it: print(it))\n\n&lt;itertools.chain object at 0x7f789b7bb100&gt;                          (0 + 1) / 1]\n&lt;itertools.chain object at 0x7f789b7bb9a0&gt;\n                                                                                \n\n\n\n# 파티션 항목 확인\n\n# 전체 확인 코드\n#rdd4.foreachPartition(lambda it: [print(x) for x in it])\n\n# 상위 5개만 확인\nrdd4.foreachPartition(lambda it: [print(x) for i, x in enumerate(it) if i &lt; 5])\n\nmovieId,title,genres\n1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\n2,Jumanji (1995),Adventure|Children|Fantasy\n3,Grumpier Old Men (1995),Comedy|Romance\n4,Waiting to Exhale (1995),Comedy|Drama|Romance\n10,GoldenEye (1995),Action|Adventure|Thriller\n11,\"American President, The (1995)\",Comedy|Drama|Romance\n12,Dracula: Dead and Loving It (1995),Comedy|Horror\n13,Balto (1995),Adventure|Animation|Children\n14,Nixon (1995),Drama\n\n\n\n# 어떤 파티션의 데이터인지까지 같이 출력 by ChatGPT\nrdd4.mapPartitionsWithIndex(\n    lambda partition_index, it: [(partition_index, x) for i, x in enumerate(it) if i &lt; 5]\n).foreach(lambda x: print(f\"Partition: {x[0]}, Value: {x[1]}\"))\n\nPartition: 0, Value: movieId,title,genres\nPartition: 0, Value: 1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\nPartition: 0, Value: 2,Jumanji (1995),Adventure|Children|Fantasy\nPartition: 0, Value: 3,Grumpier Old Men (1995),Comedy|Romance\nPartition: 0, Value: 4,Waiting to Exhale (1995),Comedy|Drama|Romance\nPartition: 1, Value: 10,GoldenEye (1995),Action|Adventure|Thriller\nPartition: 1, Value: 11,\"American President, The (1995)\",Comedy|Drama|Romance\nPartition: 1, Value: 12,Dracula: Dead and Loving It (1995),Comedy|Horror\nPartition: 1, Value: 13,Balto (1995),Adventure|Animation|Children\nPartition: 1, Value: 14,Nixon (1995),Drama\n\n\n\n\ncountByValue()\n\n값의 갯수를 계산\n\n\nrdd1.countByValue()\n\ndefaultdict(int,\n            {'movieId,title,genres': 1,\n             '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy': 1,\n             '2,Jumanji (1995),Adventure|Children|Fantasy': 1,\n             '3,Grumpier Old Men (1995),Comedy|Romance': 1,\n             '4,Waiting to Exhale (1995),Comedy|Drama|Romance': 1,\n             '5,Father of the Bride Part II (1995),Comedy': 1,\n             '6,Heat (1995),Action|Crime|Thriller': 1,\n             '7,Sabrina (1995),Comedy|Romance': 1,\n             '8,Tom and Huck (1995),Adventure|Children': 1,\n             '9,Sudden Death (1995),Action': 1,\n             '10,GoldenEye (1995),Action|Adventure|Thriller': 1,\n             '11,\"American President, The (1995)\",Comedy|Drama|Romance': 1,\n             '12,Dracula: Dead and Loving It (1995),Comedy|Horror': 1,\n             '13,Balto (1995),Adventure|Animation|Children': 1,\n             '14,Nixon (1995),Drama': 1,\n             '15,Cutthroat Island (1995),Action|Adventure|Romance': 1,\n             '16,Casino (1995),Crime|Drama': 1,\n             '17,Sense and Sensibility (1995),Drama|Romance': 1,\n             '18,Four Rooms (1995),Comedy': 1,\n             '19,Ace Ventura: When Nature Calls (1995),Comedy': 1,\n             '20,Money Train (1995),Action|Comedy|Crime|Drama|Thriller': 1,\n             '21,Get Shorty (1995),Comedy|Crime|Thriller': 1,\n             '22,Copycat (1995),Crime|Drama|Horror|Mystery|Thriller': 1,\n             '23,Assassins (1995),Action|Crime|Thriller': 1,\n             '24,Powder (1995),Drama|Sci-Fi': 1,\n             '25,Leaving Las Vegas (1995),Drama|Romance': 1,\n             '26,Othello (1995),Drama': 1,\n             '27,Now and Then (1995),Children|Drama': 1,\n             '28,Persuasion (1995),Drama|Romance': 1,\n             '29,\"City of Lost Children, The (Cité des enfants perdus, La) (1995)\",Adventure|Drama|Fantasy|Mystery|Sci-Fi': 1,\n             '30,Shanghai Triad (Yao a yao yao dao waipo qiao) (1995),Crime|Drama': 1,\n             '31,Dangerous Minds (1995),Drama': 1,\n             '32,Twelve Monkeys (a.k.a. 12 Monkeys) (1995),Mystery|Sci-Fi|Thriller': 1,\n             '34,Babe (1995),Children|Drama': 1,\n             '36,Dead Man Walking (1995),Crime|Drama': 1,\n             '38,It Takes Two (1995),Children|Comedy': 1,\n             '39,Clueless (1995),Comedy|Romance': 1,\n             '40,\"Cry, the Beloved Country (1995)\",Drama': 1,\n             '41,Richard III (1995),Drama|War': 1,\n             '42,Dead Presidents (1995),Action|Crime|Drama': 1,\n             '43,Restoration (1995),Drama': 1,\n             '44,Mortal Kombat (1995),Action|Adventure|Fantasy': 1,\n             '45,To Die For (1995),Comedy|Drama|Thriller': 1,\n             '46,How to Make an American Quilt (1995),Drama|Romance': 1,\n             '47,Seven (a.k.a. Se7en) (1995),Mystery|Thriller': 1,\n             '48,Pocahontas (1995),Animation|Children|Drama|Musical|Romance': 1,\n             '49,When Night Is Falling (1995),Drama|Romance': 1,\n             '50,\"Usual Suspects, The (1995)\",Crime|Mystery|Thriller': 1,\n             '52,Mighty Aphrodite (1995),Comedy|Drama|Romance': 1,\n             '53,Lamerica (1994),Adventure|Drama': 1,\n             '54,\"Big Green, The (1995)\",Children|Comedy': 1,\n             '55,Georgia (1995),Drama': 1,\n             '57,Home for the Holidays (1995),Drama': 1,\n             '58,\"Postman, The (Postino, Il) (1994)\",Comedy|Drama|Romance': 1,\n             '60,\"Indian in the Cupboard, The (1995)\",Adventure|Children|Fantasy': 1,\n             '61,Eye for an Eye (1996),Drama|Thriller': 1,\n             \"62,Mr. Holland's Opus (1995),Drama\": 1,\n             \"63,Don't Be a Menace to South Central While Drinking Your Juice in the Hood (1996),Comedy|Crime\": 1,\n             '64,Two if by Sea (1996),Comedy|Romance': 1,\n             '65,Bio-Dome (1996),Comedy': 1,\n             '66,Lawnmower Man 2: Beyond Cyberspace (1996),Action|Sci-Fi|Thriller': 1,\n             '68,French Twist (Gazon maudit) (1995),Comedy|Romance': 1,\n             '69,Friday (1995),Comedy': 1,\n             '70,From Dusk Till Dawn (1996),Action|Comedy|Horror|Thriller': 1,\n             '71,Fair Game (1995),Action': 1,\n             '72,Kicking and Screaming (1995),Comedy|Drama': 1,\n             '73,\"Misérables, Les (1995)\",Drama|War': 1,\n             '74,Bed of Roses (1996),Drama|Romance': 1,\n             '75,Big Bully (1996),Comedy|Drama': 1,\n             '76,Screamers (1995),Action|Sci-Fi|Thriller': 1,\n             '77,Nico Icon (1995),Documentary': 1,\n             '78,\"Crossing Guard, The (1995)\",Action|Crime|Drama|Thriller': 1,\n             '79,\"Juror, The (1996)\",Drama|Thriller': 1,\n             '80,\"White Balloon, The (Badkonake sefid) (1995)\",Children|Drama': 1,\n             \"81,Things to Do in Denver When You're Dead (1995),Crime|Drama|Romance\": 1,\n             \"82,Antonia's Line (Antonia) (1995),Comedy|Drama\": 1,\n             '83,Once Upon a Time... When We Were Colored (1995),Drama|Romance': 1,\n             '85,Angels and Insects (1995),Drama|Romance': 1,\n             '86,White Squall (1996),Action|Adventure|Drama': 1,\n             '87,Dunston Checks In (1996),Children|Comedy': 1,\n             '88,Black Sheep (1996),Comedy': 1,\n             '89,Nick of Time (1995),Action|Thriller': 1,\n             '92,Mary Reilly (1996),Drama|Horror|Thriller': 1,\n             '93,Vampire in Brooklyn (1995),Comedy|Horror|Romance': 1,\n             '94,Beautiful Girls (1996),Comedy|Drama|Romance': 1,\n             '95,Broken Arrow (1996),Action|Adventure|Thriller': 1,\n             '96,In the Bleak Midwinter (1995),Comedy|Drama': 1,\n             '97,\"Hate (Haine, La) (1995)\",Crime|Drama': 1,\n             '99,Heidi Fleiss: Hollywood Madam (1995),Documentary': 1,\n             '100,City Hall (1996),Drama|Thriller': 1,\n             '101,Bottle Rocket (1996),Adventure|Comedy|Crime|Romance': 1,\n             '102,Mr. Wrong (1996),Comedy': 1,\n             '103,Unforgettable (1996),Mystery|Sci-Fi|Thriller': 1,\n             '104,Happy Gilmore (1996),Comedy': 1,\n             '105,\"Bridges of Madison County, The (1995)\",Drama|Romance': 1,\n             '106,Nobody Loves Me (Keiner liebt mich) (1994),Comedy|Drama': 1,\n             '107,Muppet Treasure Island (1996),Adventure|Children|Comedy|Musical': 1,\n             '108,Catwalk (1996),Documentary': 1,\n             '110,Braveheart (1995),Action|Drama|War': 1,\n             '111,Taxi Driver (1976),Crime|Drama|Thriller': 1,\n             '112,Rumble in the Bronx (Hont faan kui) (1995),Action|Adventure|Comedy|Crime': 1,\n             '113,Before and After (1996),Drama|Mystery': 1,\n             '116,Anne Frank Remembered (1995),Documentary': 1,\n             '117,\"Young Poisoner\\'s Handbook, The (1995)\",Crime|Drama': 1,\n             '118,If Lucy Fell (1996),Comedy|Romance': 1,\n             '119,\"Steal Big, Steal Little (1995)\",Comedy': 1,\n             '121,\"Boys of St. Vincent, The (1992)\",Drama': 1,\n             '122,Boomerang (1992),Comedy|Romance': 1,\n             '123,Chungking Express (Chung Hing sam lam) (1994),Drama|Mystery|Romance': 1,\n             '125,Flirting With Disaster (1996),Comedy': 1,\n             '126,\"NeverEnding Story III, The (1994)\",Adventure|Children|Fantasy': 1,\n             \"128,Jupiter's Wife (1994),Documentary\": 1,\n             '129,Pie in the Sky (1996),Comedy|Romance': 1,\n             '132,Jade (1995),Thriller': 1,\n             '135,Down Periscope (1996),Comedy': 1,\n             '137,Man of the Year (1995),Documentary': 1,\n             '140,Up Close and Personal (1996),Drama|Romance': 1,\n             '141,\"Birdcage, The (1996)\",Comedy': 1,\n             '144,\"Brothers McMullen, The (1995)\",Comedy': 1,\n             '145,Bad Boys (1995),Action|Comedy|Crime|Drama|Thriller': 1,\n             '146,\"Amazing Panda Adventure, The (1995)\",Adventure|Children': 1,\n             '147,\"Basketball Diaries, The (1995)\",Drama': 1,\n             '148,\"Awfully Big Adventure, An (1995)\",Drama': 1,\n             '149,Amateur (1994),Crime|Drama|Thriller': 1,\n             '150,Apollo 13 (1995),Adventure|Drama|IMAX': 1,\n             '151,Rob Roy (1995),Action|Drama|Romance|War': 1,\n             '152,\"Addiction, The (1995)\",Drama|Horror': 1,\n             '153,Batman Forever (1995),Action|Adventure|Comedy|Crime': 1,\n             '154,Beauty of the Day (Belle de jour) (1967),Drama': 1,\n             '155,Beyond Rangoon (1995),Adventure|Drama|War': 1,\n             '156,Blue in the Face (1995),Comedy|Drama': 1,\n             '157,Canadian Bacon (1995),Comedy|War': 1,\n             '158,Casper (1995),Adventure|Children': 1,\n             '159,Clockers (1995),Crime|Drama|Mystery': 1,\n             '160,Congo (1995),Action|Adventure|Mystery|Sci-Fi': 1,\n             '161,Crimson Tide (1995),Drama|Thriller|War': 1,\n             '162,Crumb (1994),Documentary': 1,\n             '163,Desperado (1995),Action|Romance|Western': 1,\n             '164,Devil in a Blue Dress (1995),Crime|Film-Noir|Mystery|Thriller': 1,\n             '165,Die Hard: With a Vengeance (1995),Action|Crime|Thriller': 1,\n             '166,\"Doom Generation, The (1995)\",Comedy|Crime|Drama': 1,\n             '168,First Knight (1995),Action|Drama|Romance': 1,\n             '169,Free Willy 2: The Adventure Home (1995),Adventure|Children|Drama': 1,\n             '170,Hackers (1995),Action|Adventure|Crime|Thriller': 1,\n             '171,Jeffrey (1995),Comedy|Drama': 1,\n             '172,Johnny Mnemonic (1995),Action|Sci-Fi|Thriller': 1,\n             '173,Judge Dredd (1995),Action|Crime|Sci-Fi': 1,\n             '174,Jury Duty (1995),Comedy': 1,\n             '175,Kids (1995),Drama': 1,\n             '176,Living in Oblivion (1995),Comedy': 1,\n             '177,Lord of Illusions (1995),Horror': 1,\n             '178,Love & Human Remains (1993),Comedy|Drama': 1,\n             '179,Mad Love (1995),Drama|Romance': 1,\n             '180,Mallrats (1995),Comedy|Romance': 1,\n             '181,Mighty Morphin Power Rangers: The Movie (1995),Action|Children': 1,\n             '183,Mute Witness (1994),Comedy|Horror|Thriller': 1,\n             '184,Nadja (1994),Drama': 1,\n             '185,\"Net, The (1995)\",Action|Crime|Thriller': 1,\n             '186,Nine Months (1995),Comedy|Romance': 1,\n             '187,Party Girl (1995),Comedy': 1,\n             '188,\"Prophecy, The (1995)\",Fantasy|Horror|Mystery': 1,\n             '189,Reckless (1995),Comedy|Fantasy': 1,\n             '190,Safe (1995),Thriller': 1,\n             '191,\"Scarlet Letter, The (1995)\",Drama|Romance': 1,\n             '193,Showgirls (1995),Drama': 1,\n             '194,Smoke (1995),Comedy|Drama': 1,\n             '195,Something to Talk About (1995),Comedy|Drama|Romance': 1,\n             '196,Species (1995),Horror|Sci-Fi': 1,\n             '198,Strange Days (1995),Action|Crime|Drama|Mystery|Sci-Fi|Thriller': 1,\n             '199,\"Umbrellas of Cherbourg, The (Parapluies de Cherbourg, Les) (1964)\",Drama|Musical|Romance': 1,\n             '201,Three Wishes (1995),Drama|Fantasy': 1,\n             '202,Total Eclipse (1995),Drama|Romance': 1,\n             '203,\"To Wong Foo, Thanks for Everything! Julie Newmar (1995)\",Comedy': 1,\n             '204,Under Siege 2: Dark Territory (1995),Action': 1,\n             '205,Unstrung Heroes (1995),Comedy|Drama': 1,\n             '206,Unzipped (1995),Documentary': 1,\n             '207,\"Walk in the Clouds, A (1995)\",Drama|Romance': 1,\n             '208,Waterworld (1995),Action|Adventure|Sci-Fi': 1,\n             \"209,White Man's Burden (1995),Drama\": 1,\n             '210,Wild Bill (1995),Western': 1,\n             '211,\"Browning Version, The (1994)\",Drama': 1,\n             '212,Bushwhacked (1995),Adventure|Comedy|Crime|Mystery': 1,\n             '213,Burnt by the Sun (Utomlyonnye solntsem) (1994),Drama': 1,\n             '214,Before the Rain (Pred dozhdot) (1994),Drama|War': 1,\n             '215,Before Sunrise (1995),Drama|Romance': 1,\n             '216,Billy Madison (1995),Comedy': 1,\n             '217,\"Babysitter, The (1995)\",Drama|Thriller': 1,\n             '218,Boys on the Side (1995),Comedy|Drama': 1,\n             '219,\"Cure, The (1995)\",Drama': 1,\n             '220,Castle Freak (1995),Horror': 1,\n             '222,Circle of Friends (1995),Drama|Romance': 1,\n             '223,Clerks (1994),Comedy': 1,\n             '224,Don Juan DeMarco (1995),Comedy|Drama|Romance': 1,\n             '225,Disclosure (1994),Drama|Thriller': 1,\n             '227,Drop Zone (1994),Action|Thriller': 1,\n             '228,Destiny Turns on the Radio (1995),Comedy': 1,\n             '229,Death and the Maiden (1994),Drama|Thriller': 1,\n             '230,Dolores Claiborne (1995),Drama|Thriller': 1,\n             '231,Dumb & Dumber (Dumb and Dumber) (1994),Adventure|Comedy': 1,\n             '232,Eat Drink Man Woman (Yin shi nan nu) (1994),Comedy|Drama|Romance': 1,\n             '233,Exotica (1994),Drama': 1,\n             '234,Exit to Eden (1994),Comedy': 1,\n             '235,Ed Wood (1994),Comedy|Drama': 1,\n             '236,French Kiss (1995),Action|Comedy|Romance': 1,\n             '237,Forget Paris (1995),Comedy|Romance': 1,\n             '238,Far From Home: The Adventures of Yellow Dog (1995),Adventure|Children': 1,\n             '239,\"Goofy Movie, A (1995)\",Animation|Children|Comedy|Romance': 1,\n             '240,Hideaway (1995),Thriller': 1,\n             '241,Fluke (1995),Children|Drama': 1,\n             '242,Farinelli: il castrato (1994),Drama|Musical': 1,\n             '243,Gordy (1995),Children|Comedy|Fantasy': 1,\n             '246,Hoop Dreams (1994),Documentary': 1,\n             '247,Heavenly Creatures (1994),Crime|Drama': 1,\n             '248,Houseguest (1994),Comedy': 1,\n             '249,Immortal Beloved (1994),Drama|Romance': 1,\n             '250,Heavyweights (Heavy Weights) (1995),Children|Comedy': 1,\n             '251,\"Hunted, The (1995)\",Action': 1,\n             '252,I.Q. (1994),Comedy|Romance': 1,\n             '253,Interview with the Vampire: The Vampire Chronicles (1994),Drama|Horror': 1,\n             '254,Jefferson in Paris (1995),Drama': 1,\n             '255,\"Jerky Boys, The (1995)\",Comedy': 1,\n             '256,Junior (1994),Comedy|Sci-Fi': 1,\n             '257,Just Cause (1995),Mystery|Thriller': 1,\n             '258,\"Kid in King Arthur\\'s Court, A (1995)\",Adventure|Children|Comedy|Fantasy|Romance': 1,\n             '259,Kiss of Death (1995),Crime|Drama|Thriller': 1,\n             '260,Star Wars: Episode IV - A New Hope (1977),Action|Adventure|Sci-Fi': 1,\n             '261,Little Women (1994),Drama': 1,\n             '262,\"Little Princess, A (1995)\",Children|Drama': 1,\n             '263,Ladybird Ladybird (1994),Drama': 1,\n             '265,Like Water for Chocolate (Como agua para chocolate) (1992),Drama|Fantasy|Romance': 1,\n             '266,Legends of the Fall (1994),Drama|Romance|War|Western': 1,\n             '267,Major Payne (1995),Comedy': 1,\n             '269,My Crazy Life (Mi vida loca) (1993),Drama': 1,\n             '270,Love Affair (1994),Drama|Romance': 1,\n             '271,Losing Isaiah (1995),Drama': 1,\n             '272,\"Madness of King George, The (1994)\",Comedy|Drama': 1,\n             \"273,Mary Shelley's Frankenstein (Frankenstein) (1994),Drama|Horror|Sci-Fi\": 1,\n             '274,Man of the House (1995),Comedy': 1,\n             '275,Mixed Nuts (1994),Comedy': 1,\n             '276,Milk Money (1994),Comedy|Romance': 1,\n             '277,Miracle on 34th Street (1994),Drama': 1,\n             '278,Miami Rhapsody (1995),Comedy': 1,\n             '279,My Family (1995),Drama': 1,\n             '280,Murder in the First (1995),Drama|Thriller': 1,\n             \"281,Nobody's Fool (1994),Comedy|Drama|Romance\": 1,\n             '282,Nell (1994),Drama': 1,\n             '283,New Jersey Drive (1995),Crime|Drama': 1,\n             '284,New York Cop (Nyû Yôku no koppu) (1993),Action|Crime': 1,\n             '285,Beyond Bedlam (1993),Drama|Horror': 1,\n             '287,Nina Takes a Lover (1994),Comedy|Romance': 1,\n             '288,Natural Born Killers (1994),Action|Crime|Thriller': 1,\n             '289,Only You (1994),Comedy|Romance': 1,\n             '290,Once Were Warriors (1994),Crime|Drama': 1,\n             '291,Poison Ivy II (1996),Drama|Thriller': 1,\n             '292,Outbreak (1995),Action|Drama|Sci-Fi|Thriller': 1,\n             '293,Léon: The Professional (a.k.a. The Professional) (Léon) (1994),Action|Crime|Drama|Thriller': 1,\n             '294,\"Perez Family, The (1995)\",Comedy|Romance': 1,\n             '295,\"Pyromaniac\\'s Love Story, A (1995)\",Comedy|Romance': 1,\n             '296,Pulp Fiction (1994),Comedy|Crime|Drama|Thriller': 1,\n             '298,Pushing Hands (Tui shou) (1992),Drama': 1,\n             '299,Priest (1994),Drama': 1,\n             '300,Quiz Show (1994),Drama': 1,\n             '301,Picture Bride (Bijo photo) (1994),Drama|Romance': 1,\n             '302,\"Queen Margot (Reine Margot, La) (1994)\",Drama|Romance': 1,\n             '303,\"Quick and the Dead, The (1995)\",Action|Thriller|Western': 1,\n             '304,Roommates (1995),Comedy|Drama': 1,\n             '305,Ready to Wear (Pret-A-Porter) (1994),Comedy': 1,\n             '306,Three Colors: Red (Trois couleurs: Rouge) (1994),Drama': 1,\n             '307,Three Colors: Blue (Trois couleurs: Bleu) (1993),Drama': 1,\n             '308,Three Colors: White (Trzy kolory: Bialy) (1994),Comedy|Drama': 1,\n             '310,Rent-a-Kid (1995),Comedy': 1,\n             '311,Relative Fear (1994),Horror|Thriller': 1,\n             '312,Stuart Saves His Family (1995),Comedy': 1,\n             '313,\"Swan Princess, The (1994)\",Animation|Children': 1,\n             '314,\"Secret of Roan Inish, The (1994)\",Children|Drama|Fantasy|Mystery': 1,\n             '315,\"Specialist, The (1994)\",Action|Drama|Thriller': 1,\n             '316,Stargate (1994),Action|Adventure|Sci-Fi': 1,\n             '317,\"Santa Clause, The (1994)\",Comedy|Drama|Fantasy': 1,\n             '318,\"Shawshank Redemption, The (1994)\",Crime|Drama': 1,\n             '319,Shallow Grave (1994),Comedy|Drama|Thriller': 1,\n             '320,Suture (1993),Film-Noir|Thriller': 1,\n             '321,Strawberry and Chocolate (Fresa y chocolate) (1993),Drama': 1,\n             '322,Swimming with Sharks (1995),Comedy|Drama': 1,\n             '324,\"Sum of Us, The (1994)\",Comedy|Drama': 1,\n             \"325,National Lampoon's Senior Trip (1995),Comedy\": 1,\n             '326,To Live (Huozhe) (1994),Drama': 1,\n             '327,Tank Girl (1995),Action|Comedy|Sci-Fi': 1,\n             '328,Tales from the Crypt Presents: Demon Knight (1995),Horror|Thriller': 1,\n             '329,Star Trek: Generations (1994),Adventure|Drama|Sci-Fi': 1,\n             '330,Tales from the Hood (1995),Action|Crime|Horror': 1,\n             '331,Tom & Viv (1994),Drama': 1,\n             '332,Village of the Damned (1995),Horror|Sci-Fi': 1,\n             '333,Tommy Boy (1995),Comedy': 1,\n             '334,Vanya on 42nd Street (1994),Drama': 1,\n             '335,Underneath (1995),Mystery|Thriller': 1,\n             '336,\"Walking Dead, The (1995)\",Drama|War': 1,\n             \"337,What's Eating Gilbert Grape (1993),Drama\": 1,\n             '338,Virtuosity (1995),Action|Sci-Fi|Thriller': 1,\n             '339,While You Were Sleeping (1995),Comedy|Romance': 1,\n             '340,\"War, The (1994)\",Adventure|Drama|War': 1,\n             '341,Double Happiness (1994),Drama': 1,\n             \"342,Muriel's Wedding (1994),Comedy\": 1,\n             '343,\"Baby-Sitters Club, The (1995)\",Children': 1,\n             '344,Ace Ventura: Pet Detective (1994),Comedy': 1,\n             '345,\"Adventures of Priscilla, Queen of the Desert, The (1994)\",Comedy|Drama': 1,\n             '346,Backbeat (1993),Drama|Musical': 1,\n             '347,Bitter Moon (1992),Drama|Film-Noir|Romance': 1,\n             '348,Bullets Over Broadway (1994),Comedy': 1,\n             '349,Clear and Present Danger (1994),Action|Crime|Drama|Thriller': 1,\n             '350,\"Client, The (1994)\",Drama|Mystery|Thriller': 1,\n             '351,\"Corrina, Corrina (1994)\",Comedy|Drama|Romance': 1,\n             '352,Crooklyn (1994),Comedy|Drama': 1,\n             '353,\"Crow, The (1994)\",Action|Crime|Fantasy|Thriller': 1,\n             '354,Cobb (1994),Drama': 1,\n             '355,\"Flintstones, The (1994)\",Children|Comedy|Fantasy': 1,\n             '356,Forrest Gump (1994),Comedy|Drama|Romance|War': 1,\n             '357,Four Weddings and a Funeral (1994),Comedy|Romance': 1,\n             '358,Higher Learning (1995),Drama': 1,\n             '359,I Like It Like That (1994),Comedy|Drama|Romance': 1,\n             '360,I Love Trouble (1994),Action|Comedy': 1,\n             '361,It Could Happen to You (1994),Comedy|Drama|Romance': 1,\n             '362,\"Jungle Book, The (1994)\",Adventure|Children|Romance': 1,\n             '363,\"Wonderful, Horrible Life of Leni Riefenstahl, The (Macht der Bilder: Leni Riefenstahl, Die) (1993)\",Documentary': 1,\n             '364,\"Lion King, The (1994)\",Adventure|Animation|Children|Drama|Musical|IMAX': 1,\n             '365,Little Buddha (1993),Drama': 1,\n             '366,\"Wes Craven\\'s New Nightmare (Nightmare on Elm Street Part 7: Freddy\\'s Finale, A) (1994)\",Drama|Horror|Mystery|Thriller': 1,\n             '367,\"Mask, The (1994)\",Action|Comedy|Crime|Fantasy': 1,\n             '368,Maverick (1994),Adventure|Comedy|Western': 1,\n             '369,Mrs. Parker and the Vicious Circle (1994),Drama': 1,\n             '370,Naked Gun 33 1/3: The Final Insult (1994),Action|Comedy': 1,\n             '371,\"Paper, The (1994)\",Comedy|Drama': 1,\n             '372,Reality Bites (1994),Comedy|Drama|Romance': 1,\n             '373,Red Rock West (1992),Thriller': 1,\n             '374,Richie Rich (1994),Children|Comedy': 1,\n             '376,\"River Wild, The (1994)\",Action|Thriller': 1,\n             '377,Speed (1994),Action|Romance|Thriller': 1,\n             '378,Speechless (1994),Comedy|Romance': 1,\n             '379,Timecop (1994),Action|Sci-Fi|Thriller': 1,\n             '380,True Lies (1994),Action|Adventure|Comedy|Romance|Thriller': 1,\n             '381,When a Man Loves a Woman (1994),Drama|Romance': 1,\n             '382,Wolf (1994),Drama|Horror|Romance|Thriller': 1,\n             '383,Wyatt Earp (1994),Western': 1,\n             '384,Bad Company (1995),Action|Crime|Drama': 1,\n             '385,\"Man of No Importance, A (1994)\",Drama': 1,\n             '386,S.F.W. (1994),Drama': 1,\n             '387,\"Low Down Dirty Shame, A (1994)\",Action|Comedy': 1,\n             '388,Boys Life (1995),Drama': 1,\n             '389,\"Colonel Chabert, Le (1994)\",Drama|Romance|War': 1,\n             '390,Faster Pussycat! Kill! Kill! (1965),Action|Crime|Drama': 1,\n             \"391,Jason's Lyric (1994),Crime|Drama\": 1,\n             '393,Street Fighter (1994),Action|Adventure|Fantasy': 1,\n             '405,Highlander III: The Sorcerer (a.k.a. Highlander: The Final Dimension) (1994),Action|Fantasy': 1,\n             '406,Federal Hill (1994),Drama': 1,\n             '407,In the Mouth of Madness (1995),Horror|Thriller': 1,\n             '408,8 Seconds (1994),Drama': 1,\n             '409,Above the Rim (1994),Crime|Drama': 1,\n             '410,Addams Family Values (1993),Children|Comedy|Fantasy': 1,\n             '412,\"Age of Innocence, The (1993)\",Drama': 1,\n             '413,Airheads (1994),Comedy': 1,\n             '414,\"Air Up There, The (1994)\",Comedy': 1,\n             '415,Another Stakeout (1993),Comedy|Thriller': 1,\n             '416,Bad Girls (1994),Western': 1,\n             '417,Barcelona (1994),Comedy|Romance': 1,\n             '418,Being Human (1993),Drama': 1,\n             '419,\"Beverly Hillbillies, The (1993)\",Comedy': 1,\n             '420,Beverly Hills Cop III (1994),Action|Comedy|Crime|Thriller': 1,\n             '421,Black Beauty (1994),Adventure|Children|Drama': 1,\n             '422,Blink (1994),Thriller': 1,\n             '423,Blown Away (1994),Action|Thriller': 1,\n             '424,Blue Chips (1994),Drama': 1,\n             '425,Blue Sky (1994),Drama|Romance': 1,\n             '426,Body Snatchers (1993),Horror|Sci-Fi|Thriller': 1,\n             '427,Boxing Helena (1993),Drama|Mystery|Romance|Thriller': 1,\n             '428,\"Bronx Tale, A (1993)\",Drama': 1,\n             '429,Cabin Boy (1994),Comedy': 1,\n             '430,Calendar Girl (1993),Comedy|Drama': 1,\n             \"431,Carlito's Way (1993),Crime|Drama\": 1,\n             \"432,City Slickers II: The Legend of Curly's Gold (1994),Adventure|Comedy|Western\": 1,\n             '433,Clean Slate (1994),Comedy': 1,\n             '434,Cliffhanger (1993),Action|Adventure|Thriller': 1,\n             '435,Coneheads (1993),Comedy|Sci-Fi': 1,\n             '436,Color of Night (1994),Drama|Thriller': 1,\n             '437,Cops and Robbersons (1994),Comedy': 1,\n             '438,\"Cowboy Way, The (1994)\",Action|Comedy|Drama': 1,\n             '440,Dave (1993),Comedy|Romance': 1,\n             '441,Dazed and Confused (1993),Comedy': 1,\n             '442,Demolition Man (1993),Action|Adventure|Sci-Fi': 1,\n             '444,Even Cowgirls Get the Blues (1993),Comedy|Romance': 1,\n             '445,Fatal Instinct (1993),Comedy': 1,\n             '446,Farewell My Concubine (Ba wang bie ji) (1993),Drama|Romance': 1,\n             '448,Fearless (1993),Drama': 1,\n             '449,Fear of a Black Hat (1994),Comedy': 1,\n             '450,With Honors (1994),Comedy|Drama': 1,\n             '451,Flesh and Bone (1993),Drama|Mystery|Romance': 1,\n             \"452,Widows' Peak (1994),Drama\": 1,\n             '453,For Love or Money (1993),Comedy|Romance': 1,\n             '454,\"Firm, The (1993)\",Drama|Thriller': 1,\n             '455,Free Willy (1993),Adventure|Children|Drama': 1,\n             '456,Fresh (1994),Crime|Drama|Thriller': 1,\n             '457,\"Fugitive, The (1993)\",Thriller': 1,\n             '458,Geronimo: An American Legend (1993),Drama|Western': 1,\n             '459,\"Getaway, The (1994)\",Action|Adventure|Crime|Drama|Romance|Thriller': 1,\n             '460,Getting Even with Dad (1994),Comedy': 1,\n             '461,Go Fish (1994),Drama|Romance': 1,\n             '464,Hard Target (1993),Action|Adventure|Crime|Thriller': 1,\n             '466,Hot Shots! Part Deux (1993),Action|Comedy|War': 1,\n             '467,Live Nude Girls (1995),Comedy': 1,\n             '468,\"Englishman Who Went Up a Hill But Came Down a Mountain, The (1995)\",Comedy|Romance': 1,\n             '469,\"House of the Spirits, The (1993)\",Drama|Romance': 1,\n             '470,House Party 3 (1994),Comedy': 1,\n             '471,\"Hudsucker Proxy, The (1994)\",Comedy': 1,\n             \"472,I'll Do Anything (1994),Comedy|Drama\": 1,\n             '473,In the Army Now (1994),Comedy|War': 1,\n             '474,In the Line of Fire (1993),Action|Thriller': 1,\n             '475,In the Name of the Father (1993),Drama': 1,\n             '476,\"Inkwell, The (1994)\",Comedy|Drama': 1,\n             \"477,What's Love Got to Do with It? (1993),Drama|Musical\": 1,\n             '478,Jimmy Hollywood (1994),Comedy|Crime|Drama': 1,\n             '479,Judgment Night (1993),Action|Crime|Thriller': 1,\n             '480,Jurassic Park (1993),Action|Adventure|Sci-Fi|Thriller': 1,\n             '481,Kalifornia (1993),Drama|Thriller': 1,\n             '482,Killing Zoe (1994),Crime|Drama|Thriller': 1,\n             '484,Lassie (1994),Adventure|Children': 1,\n             '485,Last Action Hero (1993),Action|Adventure|Comedy|Fantasy': 1,\n             '486,Life with Mikey (1993),Comedy': 1,\n             '487,Lightning Jack (1994),Comedy|Western': 1,\n             '488,M. Butterfly (1993),Drama|Romance': 1,\n             '489,Made in America (1993),Comedy': 1,\n             '490,Malice (1993),Thriller': 1,\n             '491,\"Man Without a Face, The (1993)\",Drama': 1,\n             '492,Manhattan Murder Mystery (1993),Comedy|Mystery': 1,\n             '493,Menace II Society (1993),Action|Crime|Drama': 1,\n             '494,Executive Decision (1996),Action|Adventure|Thriller': 1,\n             '495,In the Realm of the Senses (Ai no corrida) (1976),Drama': 1,\n             '496,What Happened Was... (1994),Comedy|Drama|Romance|Thriller': 1,\n             '497,Much Ado About Nothing (1993),Comedy|Romance': 1,\n             '499,Mr. Wonderful (1993),Comedy|Romance': 1,\n             '500,Mrs. Doubtfire (1993),Comedy|Drama': 1,\n             '501,Naked (1993),Drama': 1,\n             '502,\"Next Karate Kid, The (1994)\",Action|Children|Romance': 1,\n             '504,No Escape (1994),Action|Drama|Sci-Fi': 1,\n             '505,North (1994),Comedy': 1,\n             '506,Orlando (1992),Drama|Fantasy|Romance': 1,\n             '507,\"Perfect World, A (1993)\",Crime|Drama|Thriller': 1,\n             '508,Philadelphia (1993),Drama': 1,\n             '509,\"Piano, The (1993)\",Drama|Romance': 1,\n             '510,Poetic Justice (1993),Drama': 1,\n             '511,\"Program, The (1993)\",Action|Drama': 1,\n             '512,\"Puppet Masters, The (1994)\",Horror|Sci-Fi': 1,\n             '513,Radioland Murders (1994),Comedy|Mystery|Romance': 1,\n             '514,\"Ref, The (1994)\",Comedy': 1,\n             '515,\"Remains of the Day, The (1993)\",Drama|Romance': 1,\n             '516,Renaissance Man (1994),Comedy|Drama': 1,\n             '517,Rising Sun (1993),Action|Drama|Mystery': 1,\n             '518,\"Road to Wellville, The (1994)\",Comedy': 1,\n             '519,RoboCop 3 (1993),Action|Crime|Drama|Sci-Fi|Thriller': 1,\n             '520,Robin Hood: Men in Tights (1993),Comedy': 1,\n             '521,Romeo Is Bleeding (1993),Crime|Thriller': 1,\n             '522,Romper Stomper (1992),Action|Drama': 1,\n             '523,Ruby in Paradise (1993),Drama': 1,\n             '524,Rudy (1993),Drama': 1,\n             '526,\"Savage Nights (Nuits fauves, Les) (1992)\",Drama': 1,\n             \"527,Schindler's List (1993),Drama|War\": 1,\n             '528,\"Scout, The (1994)\",Comedy|Drama': 1,\n             '529,Searching for Bobby Fischer (1993),Drama': 1,\n             '531,\"Secret Garden, The (1993)\",Children|Drama': 1,\n             '532,Serial Mom (1994),Comedy|Crime|Horror': 1,\n             '533,\"Shadow, The (1994)\",Action|Adventure|Fantasy|Mystery': 1,\n             '534,Shadowlands (1993),Drama|Romance': 1,\n             '535,Short Cuts (1993),Drama': 1,\n             '536,\"Simple Twist of Fate, A (1994)\",Drama': 1,\n             '537,Sirens (1994),Drama': 1,\n             '538,Six Degrees of Separation (1993),Drama': 1,\n             '539,Sleepless in Seattle (1993),Comedy|Drama|Romance': 1,\n             '540,Sliver (1993),Thriller': 1,\n             '541,Blade Runner (1982),Action|Sci-Fi|Thriller': 1,\n             '542,Son in Law (1993),Comedy|Drama|Romance': 1,\n             '543,So I Married an Axe Murderer (1993),Comedy|Romance|Thriller': 1,\n             '544,Striking Distance (1993),Action|Crime': 1,\n             '546,Super Mario Bros. (1993),Action|Adventure|Children|Comedy|Fantasy|Sci-Fi': 1,\n             '547,Surviving the Game (1994),Action|Adventure|Thriller': 1,\n             '548,Terminal Velocity (1994),Action|Mystery|Thriller': 1,\n             '549,Thirty-Two Short Films About Glenn Gould (1993),Drama|Musical': 1,\n             '550,Threesome (1994),Comedy|Romance': 1,\n             '551,\"Nightmare Before Christmas, The (1993)\",Animation|Children|Fantasy|Musical': 1,\n             '552,\"Three Musketeers, The (1993)\",Action|Adventure|Comedy|Romance': 1,\n             '553,Tombstone (1993),Action|Drama|Western': 1,\n             '555,True Romance (1993),Crime|Thriller': 1,\n             '556,\"War Room, The (1993)\",Documentary': 1,\n             '558,\"Pagemaster, The (1994)\",Action|Adventure|Animation|Children|Fantasy': 1,\n             '562,Welcome to the Dollhouse (1995),Comedy|Drama': 1,\n             '563,Germinal (1993),Drama|Romance': 1,\n             '564,Chasers (1994),Comedy': 1,\n             '567,Kika (1993),Comedy|Drama': 1,\n             '568,Bhaji on the Beach (1993),Comedy|Drama': 1,\n             '569,Little Big League (1994),Comedy|Drama': 1,\n             '573,\"Ciao, Professore! (Io speriamo che me la cavo) (1992)\",Drama': 1,\n             '574,Spanking the Monkey (1994),Comedy|Drama': 1,\n             '575,\"Little Rascals, The (1994)\",Children|Comedy': 1,\n             '577,Andre (1994),Adventure|Children|Drama': 1,\n             '579,\"Escort, The (Scorta, La) (1993)\",Crime|Thriller': 1,\n             '580,Princess Caraboo (1994),Drama': 1,\n             '581,\"Celluloid Closet, The (1995)\",Documentary': 1,\n             '583,Dear Diary (Caro Diario) (1994),Comedy|Drama': 1,\n             '585,\"Brady Bunch Movie, The (1995)\",Comedy': 1,\n             '586,Home Alone (1990),Children|Comedy': 1,\n             '587,Ghost (1990),Comedy|Drama|Fantasy|Romance|Thriller': 1,\n             '588,Aladdin (1992),Adventure|Animation|Children|Comedy|Musical': 1,\n             '589,Terminator 2: Judgment Day (1991),Action|Sci-Fi': 1,\n             '590,Dances with Wolves (1990),Adventure|Drama|Western': 1,\n             '592,Batman (1989),Action|Crime|Thriller': 1,\n             '593,\"Silence of the Lambs, The (1991)\",Crime|Horror|Thriller': 1,\n             '594,Snow White and the Seven Dwarfs (1937),Animation|Children|Drama|Fantasy|Musical': 1,\n             '595,Beauty and the Beast (1991),Animation|Children|Fantasy|Musical|Romance|IMAX': 1,\n             '596,Pinocchio (1940),Animation|Children|Fantasy|Musical': 1,\n             '597,Pretty Woman (1990),Comedy|Romance': 1,\n             '599,\"Wild Bunch, The (1969)\",Adventure|Western': 1,\n             '600,Love and a .45 (1994),Action|Comedy|Crime': 1,\n             '602,\"Great Day in Harlem, A (1994)\",Documentary': 1,\n             '605,One Fine Day (1996),Drama|Romance': 1,\n             '606,Candyman: Farewell to the Flesh (1995),Fantasy|Horror': 1,\n             '608,Fargo (1996),Comedy|Crime|Drama|Thriller': 1,\n             '609,Homeward Bound II: Lost in San Francisco (1996),Adventure|Children': 1,\n             '610,Heavy Metal (1981),Action|Adventure|Animation|Horror|Sci-Fi': 1,\n             '611,Hellraiser: Bloodline (1996),Action|Horror|Sci-Fi': 1,\n             '612,\"Pallbearer, The (1996)\",Comedy': 1,\n             '613,Jane Eyre (1996),Drama|Romance': 1,\n             '615,Bread and Chocolate (Pane e cioccolata) (1973),Comedy|Drama': 1,\n             '616,\"Aristocats, The (1970)\",Animation|Children': 1,\n             '617,\"Flower of My Secret, The (La flor de mi secreto) (1995)\",Comedy|Drama': 1,\n             '618,Two Much (1995),Comedy|Romance': 1,\n             '619,Ed (1996),Comedy': 1,\n             '626,\"Thin Line Between Love and Hate, A (1996)\",Comedy': 1,\n             '627,\"Last Supper, The (1995)\",Drama|Thriller': 1,\n             '628,Primal Fear (1996),Crime|Drama|Mystery|Thriller': 1,\n             '631,All Dogs Go to Heaven 2 (1996),Adventure|Animation|Children|Fantasy|Musical|Romance': 1,\n             '632,Land and Freedom (Tierra y libertad) (1995),Drama|War': 1,\n             '633,Denise Calls Up (1995),Comedy': 1,\n             '634,Theodore Rex (1995),Comedy': 1,\n             '635,\"Family Thing, A (1996)\",Comedy|Drama': 1,\n             '636,Frisk (1995),Drama': 1,\n             '637,Sgt. Bilko (1996),Comedy': 1,\n             '638,Jack and Sarah (1995),Romance': 1,\n             '639,Girl 6 (1996),Comedy|Drama': 1,\n             '640,Diabolique (1996),Drama|Thriller': 1,\n             '645,Nelly & Monsieur Arnaud (1995),Drama': 1,\n             '647,Courage Under Fire (1996),Action|Crime|Drama|War': 1,\n             '648,Mission: Impossible (1996),Action|Adventure|Mystery|Thriller': 1,\n             '649,Cold Fever (Á köldum klaka) (1995),Comedy|Drama': 1,\n             '650,Moll Flanders (1996),Drama': 1,\n             '653,Dragonheart (1996),Action|Adventure|Fantasy': 1,\n             '656,Eddie (1996),Comedy': 1,\n             '661,James and the Giant Peach (1996),Adventure|Animation|Children|Fantasy|Musical': 1,\n             '662,Fear (1996),Thriller': 1,\n             '663,Kids in the Hall: Brain Candy (1996),Comedy': 1,\n             '665,Underground (1995),Comedy|Drama|War': 1,\n             '667,Bloodsport 2 (a.k.a. Bloodsport II: The Next Kumite) (1996),Action': 1,\n             '668,Song of the Little Road (Pather Panchali) (1955),Drama': 1,\n             '670,\"World of Apu, The (Apur Sansar) (1959)\",Drama': 1,\n             '671,Mystery Science Theater 3000: The Movie (1996),Comedy|Sci-Fi': 1,\n             '673,Space Jam (1996),Adventure|Animation|Children|Comedy|Fantasy|Sci-Fi': 1,\n             '674,Barbarella (1968),Adventure|Comedy|Sci-Fi': 1,\n             '678,Some Folks Call It a Sling Blade (1993),Drama|Thriller': 1,\n             '679,\"Run of the Country, The (1995)\",Drama': 1,\n             '680,\"Alphaville (Alphaville, une étrange aventure de Lemmy Caution) (1965)\",Drama|Mystery|Romance|Sci-Fi|Thriller': 1,\n             \"685,It's My Party (1996),Drama\": 1,\n             '688,Operation Dumbo Drop (1995),Action|Adventure|Comedy|War': 1,\n             '691,Mrs. Winterbourne (1996),Comedy|Romance': 1,\n             '692,Solo (1996),Action|Sci-Fi|Thriller': 1,\n             '694,\"Substitute, The (1996)\",Action|Crime|Drama': 1,\n             '695,True Crime (1996),Mystery|Thriller': 1,\n             '697,Feeling Minnesota (1996),Drama|Romance': 1,\n             '698,Delta of Venus (1995),Drama': 1,\n             '700,Angus (1995),Comedy': 1,\n             '703,Boys (1996),Drama': 1,\n             '704,\"Quest, The (1996)\",Action|Adventure': 1,\n             '706,Sunset Park (1996),Drama': 1,\n             '707,Mulholland Falls (1996),Crime|Drama|Thriller': 1,\n             '708,\"Truth About Cats & Dogs, The (1996)\",Comedy|Romance': 1,\n             '709,Oliver & Company (1988),Adventure|Animation|Children|Comedy|Musical': 1,\n             '710,Celtic Pride (1996),Comedy': 1,\n             '711,Flipper (1996),Adventure|Children': 1,\n             '714,Dead Man (1995),Drama|Mystery|Western': 1,\n             '715,\"Horseman on the Roof, The (Hussard sur le toit, Le) (1995)\",Drama|Romance': 1,\n             '718,\"Visitors, The (Visiteurs, Les) (1993)\",Comedy|Fantasy|Sci-Fi': 1,\n             '719,Multiplicity (1996),Comedy': 1,\n             '720,Wallace & Gromit: The Best of Aardman Animation (1996),Adventure|Animation|Comedy': 1,\n             '722,\"Haunted World of Edward D. Wood Jr., The (1996)\",Documentary': 1,\n             '724,\"Craft, The (1996)\",Drama|Fantasy|Horror|Thriller': 1,\n             '725,\"Great White Hype, The (1996)\",Comedy': 1,\n             '726,Last Dance (1996),Drama': 1,\n             '728,Cold Comfort Farm (1995),Comedy': 1,\n             \"731,Heaven's Prisoners (1996),Crime|Thriller\": 1,\n             '733,\"Rock, The (1996)\",Action|Adventure|Thriller': 1,\n             '735,Cemetery Man (Dellamorte Dellamore) (1994),Horror': 1,\n             '736,Twister (1996),Action|Adventure|Romance|Thriller': 1,\n             '737,Barb Wire (1996),Action|Sci-Fi': 1,\n             '741,Ghost in the Shell (Kôkaku kidôtai) (1995),Animation|Sci-Fi': 1,\n             '742,Thinner (1996),Horror|Thriller': 1,\n             '743,Spy Hard (1996),Comedy': 1,\n             '745,Wallace & Gromit: A Close Shave (1995),Animation|Children|Comedy': 1,\n             '747,\"Stupids, The (1996)\",Comedy': 1,\n             '748,\"Arrival, The (1996)\",Action|Sci-Fi|Thriller': 1,\n             '750,Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964),Comedy|War': 1,\n             '757,Ashes of Time (Dung che sai duk) (1994),Drama': 1,\n             '759,Maya Lin: A Strong Clear Vision (1994),Documentary': 1,\n             '760,Stalingrad (1993),Drama|War': 1,\n             '761,\"Phantom, The (1996)\",Action|Adventure': 1,\n             '762,Striptease (1996),Comedy|Crime': 1,\n             '764,Heavy (1995),Drama|Romance': 1,\n             '765,Jack (1996),Comedy|Drama': 1,\n             '766,I Shot Andy Warhol (1996),Drama': 1,\n             '773,Touki Bouki (1973),Drama': 1,\n             '775,Spirits of the Dead (1968),Horror|Mystery': 1,\n             '778,Trainspotting (1996),Comedy|Crime|Drama': 1,\n             \"779,'Til There Was You (1997),Drama|Romance\": 1,\n             '780,Independence Day (a.k.a. ID4) (1996),Action|Adventure|Sci-Fi|Thriller': 1,\n             '781,Stealing Beauty (1996),Drama': 1,\n             '782,\"Fan, The (1996)\",Drama|Thriller': 1,\n             '783,\"Hunchback of Notre Dame, The (1996)\",Animation|Children|Drama|Musical|Romance': 1,\n             '784,\"Cable Guy, The (1996)\",Comedy|Thriller': 1,\n             '785,Kingpin (1996),Comedy': 1,\n             '786,Eraser (1996),Action|Drama|Thriller': 1,\n             '788,\"Nutty Professor, The (1996)\",Comedy|Fantasy|Romance|Sci-Fi': 1,\n             '790,\"Unforgettable Summer, An (Un été inoubliable) (1994)\",Drama': 1,\n             '791,\"Last Klezmer: Leopold Kozlowski, His Life and Music, The (1994)\",Documentary': 1,\n             '795,Somebody to Love (1994),Drama': 1,\n             '798,Daylight (1996),Action|Adventure|Drama|Thriller': 1,\n             '799,\"Frighteners, The (1996)\",Comedy|Horror|Thriller': 1,\n             '800,Lone Star (1996),Drama|Mystery|Western': 1,\n             '801,Harriet the Spy (1996),Children|Comedy': 1,\n             '802,Phenomenon (1996),Drama|Romance': 1,\n             '803,Walking and Talking (1996),Comedy|Drama|Romance': 1,\n             \"804,She's the One (1996),Comedy|Romance\": 1,\n             '805,\"Time to Kill, A (1996)\",Drama|Thriller': 1,\n             '806,American Buffalo (1996),Crime|Drama': 1,\n             '808,Alaska (1996),Adventure|Children': 1,\n             '809,Fled (1996),Action|Adventure': 1,\n             '810,Kazaam (1996),Children|Comedy|Fantasy': 1,\n             '813,Larger Than Life (1996),Comedy': 1,\n             '818,\"Very Brady Sequel, A (1996)\",Comedy': 1,\n             '823,\"Collector, The (La collectionneuse) (1967)\",Drama': 1,\n             '824,Kaspar Hauser (1993),Drama|Mystery': 1,\n             '828,\"Adventures of Pinocchio, The (1996)\",Adventure|Children': 1,\n             \"829,Joe's Apartment (1996),Comedy|Fantasy|Musical\": 1,\n             '830,\"First Wives Club, The (1996)\",Comedy': 1,\n             '832,Ransom (1996),Crime|Thriller': 1,\n             '833,High School High (1996),Comedy': 1,\n             '835,Foxfire (1996),Drama': 1,\n             '836,Chain Reaction (1996),Action|Adventure|Thriller': 1,\n             '837,Matilda (1996),Children|Comedy|Fantasy': 1,\n             '838,Emma (1996),Comedy|Drama|Romance': 1,\n             '839,\"Crow: City of Angels, The (1996)\",Action|Thriller': 1,\n             '840,House Arrest (1996),Children|Comedy': 1,\n             '841,\"Eyes Without a Face (Yeux sans visage, Les) (1959)\",Horror': 1,\n             '842,Tales from the Crypt Presents: Bordello of Blood (1996),Comedy|Horror': 1,\n             '848,\"Spitfire Grill, The (1996)\",Drama': 1,\n             '849,Escape from L.A. (1996),Action|Adventure|Sci-Fi|Thriller': 1,\n             '851,Basquiat (1996),Drama': 1,\n             '852,Tin Cup (1996),Comedy|Drama|Romance': 1,\n             '858,\"Godfather, The (1972)\",Crime|Drama': 1,\n             '861,Supercop (Police Story 3: Supercop) (Jing cha gu shi III: Chao ji jing cha) (1992),Action|Comedy|Crime|Thriller': 1,\n             '866,Bound (1996),Crime|Drama|Romance|Thriller': 1,\n             '867,Carpool (1996),Comedy|Crime': 1,\n             '869,Kansas City (1996),Crime|Drama|Musical|Thriller': 1,\n             \"870,Gone Fishin' (1997),Comedy\": 1,\n             '875,Nothing to Lose (1994),Action|Crime|Drama': 1,\n             '876,Supercop 2 (Project S) (Chao ji ji hua) (1993),Action|Comedy|Crime|Thriller': 1,\n             '879,\"Relic, The (1997)\",Horror|Thriller': 1,\n             '880,\"Island of Dr. Moreau, The (1996)\",Sci-Fi|Thriller': 1,\n             '881,First Kid (1996),Children|Comedy': 1,\n             '882,\"Trigger Effect, The (1996)\",Drama|Thriller': 1,\n             '885,Bogus (1996),Children|Drama|Fantasy': 1,\n             '886,Bulletproof (1996),Action|Comedy|Crime': 1,\n             '888,Land Before Time III: The Time of the Great Giving (1995),Adventure|Animation|Children|Musical': 1,\n             '889,1-900 (06) (1994),Drama|Romance': 1,\n             '891,Halloween: The Curse of Michael Myers (Halloween 6: The Curse of Michael Myers) (1995),Horror|Thriller': 1,\n             '892,Twelfth Night (1996),Comedy|Drama|Romance': 1,\n             '893,Mother Night (1996),Drama': 1,\n             '896,Wild Reeds (Les roseaux sauvages) (1994),Drama': 1,\n             '897,For Whom the Bell Tolls (1943),Adventure|Drama|Romance|War': 1,\n             '898,\"Philadelphia Story, The (1940)\",Comedy|Drama|Romance': 1,\n             \"899,Singin' in the Rain (1952),Comedy|Musical|Romance\": 1,\n             '900,\"American in Paris, An (1951)\",Musical|Romance': 1,\n             '901,Funny Face (1957),Comedy|Musical': 1,\n             \"902,Breakfast at Tiffany's (1961),Drama|Romance\": 1,\n             '903,Vertigo (1958),Drama|Mystery|Romance|Thriller': 1,\n             '904,Rear Window (1954),Mystery|Thriller': 1,\n             '905,It Happened One Night (1934),Comedy|Romance': 1,\n             '906,Gaslight (1944),Drama|Thriller': 1,\n             '907,\"Gay Divorcee, The (1934)\",Comedy|Musical|Romance': 1,\n             '908,North by Northwest (1959),Action|Adventure|Mystery|Romance|Thriller': 1,\n             '909,\"Apartment, The (1960)\",Comedy|Drama|Romance': 1,\n             '910,Some Like It Hot (1959),Comedy|Crime': 1,\n             '911,Charade (1963),Comedy|Crime|Mystery|Romance|Thriller': 1,\n             '912,Casablanca (1942),Drama|Romance': 1,\n             '913,\"Maltese Falcon, The (1941)\",Film-Noir|Mystery': 1,\n             '914,My Fair Lady (1964),Comedy|Drama|Musical|Romance': 1,\n             '915,Sabrina (1954),Comedy|Romance': 1,\n             '916,Roman Holiday (1953),Comedy|Drama|Romance': 1,\n             '917,\"Little Princess, The (1939)\",Children|Drama': 1,\n             '918,Meet Me in St. Louis (1944),Musical': 1,\n             '919,\"Wizard of Oz, The (1939)\",Adventure|Children|Fantasy|Musical': 1,\n             '920,Gone with the Wind (1939),Drama|Romance|War': 1,\n             '921,My Favorite Year (1982),Comedy': 1,\n             '922,Sunset Blvd. (a.k.a. Sunset Boulevard) (1950),Drama|Film-Noir|Romance': 1,\n             '923,Citizen Kane (1941),Drama|Mystery': 1,\n             '924,2001: A Space Odyssey (1968),Adventure|Drama|Sci-Fi': 1,\n             '926,All About Eve (1950),Drama': 1,\n             '927,\"Women, The (1939)\",Comedy': 1,\n             '928,Rebecca (1940),Drama|Mystery|Romance|Thriller': 1,\n             '929,Foreign Correspondent (1940),Drama|Film-Noir|Mystery|Thriller': 1,\n             '930,Notorious (1946),Film-Noir|Romance|Thriller': 1,\n             '931,Spellbound (1945),Mystery|Romance|Thriller': 1,\n             '932,\"Affair to Remember, An (1957)\",Drama|Romance': 1,\n             '933,To Catch a Thief (1955),Crime|Mystery|Romance|Thriller': 1,\n             '934,Father of the Bride (1950),Comedy': 1,\n             '935,\"Band Wagon, The (1953)\",Comedy|Musical': 1,\n             '936,Ninotchka (1939),Comedy|Romance': 1,\n             '937,Love in the Afternoon (1957),Comedy|Romance': 1,\n             '938,Gigi (1958),Musical': 1,\n             '940,\"Adventures of Robin Hood, The (1938)\",Action|Adventure|Romance': 1,\n             '941,\"Mark of Zorro, The (1940)\",Adventure': 1,\n             '942,Laura (1944),Crime|Film-Noir|Mystery': 1,\n             '943,\"Ghost and Mrs. Muir, The (1947)\",Drama|Fantasy|Romance': 1,\n             '944,Lost Horizon (1937),Drama': 1,\n             '945,Top Hat (1935),Comedy|Musical|Romance': 1,\n             '946,To Be or Not to Be (1942),Comedy|Drama|War': 1,\n             '947,My Man Godfrey (1936),Comedy|Romance': 1,\n             '948,Giant (1956),Drama|Romance|Western': 1,\n             '949,East of Eden (1955),Drama': 1,\n             '950,\"Thin Man, The (1934)\",Comedy|Crime': 1,\n             '951,His Girl Friday (1940),Comedy|Romance': 1,\n             '952,Around the World in 80 Days (1956),Adventure|Comedy': 1,\n             \"953,It's a Wonderful Life (1946),Children|Drama|Fantasy|Romance\": 1,\n             '954,Mr. Smith Goes to Washington (1939),Drama': 1,\n             '955,Bringing Up Baby (1938),Comedy|Romance': 1,\n             '956,Penny Serenade (1941),Drama|Romance': 1,\n             '959,Of Human Bondage (1934),Drama': 1,\n             '961,Little Lord Fauntleroy (1936),Drama': 1,\n             '963,\"Inspector General, The (1949)\",Musical': 1,\n             '965,\"39 Steps, The (1935)\",Drama|Mystery|Thriller': 1,\n             '968,Night of the Living Dead (1968),Horror|Sci-Fi|Thriller': 1,\n             '969,\"African Queen, The (1951)\",Adventure|Comedy|Romance|War': 1,\n             '970,Beat the Devil (1953),Adventure|Comedy|Crime|Drama|Romance': 1,\n             '971,Cat on a Hot Tin Roof (1958),Drama': 1,\n             '973,Meet John Doe (1941),Comedy|Drama': 1,\n             '976,\"Farewell to Arms, A (1932)\",Romance|War': 1,\n             '979,Nothing Personal (1995),Drama|War': 1,\n             '981,Dangerous Ground (1997),Drama': 1,\n             '982,Picnic (1955),Drama': 1,\n             '984,\"Pompatus of Love, The (1996)\",Comedy|Drama': 1,\n             '986,Fly Away Home (1996),Adventure|Children': 1,\n             '987,Bliss (1997),Drama|Romance': 1,\n             '988,Grace of My Heart (1996),Comedy|Drama': 1,\n             '990,Maximum Risk (1996),Action|Adventure|Thriller': 1,\n             '991,Michael Collins (1996),Drama': 1,\n             '993,Infinity (1996),Drama': 1,\n             '994,Big Night (1996),Comedy|Drama': 1,\n             '996,Last Man Standing (1996),Action|Crime|Drama|Thriller': 1,\n             '998,Set It Off (1996),Action|Crime': 1,\n             '999,2 Days in the Valley (1996),Crime|Film-Noir': 1,\n             '1003,Extreme Measures (1996),Drama|Thriller': 1,\n             '1004,\"Glimmer Man, The (1996)\",Action|Thriller': 1,\n             '1005,D3: The Mighty Ducks (1996),Children|Comedy': 1,\n             '1006,\"Chamber, The (1996)\",Drama': 1,\n             '1007,\"Apple Dumpling Gang, The (1975)\",Children|Comedy|Western': 1,\n             '1008,\"Davy Crockett, King of the Wild Frontier (1955)\",Adventure|Western': 1,\n             '1009,Escape to Witch Mountain (1975),Adventure|Children|Fantasy': 1,\n             '1010,\"Love Bug, The (1969)\",Children|Comedy': 1,\n             '1011,Herbie Rides Again (1974),Children|Comedy|Fantasy|Romance': 1,\n             '1012,Old Yeller (1957),Children|Drama': 1,\n             '1013,\"Parent Trap, The (1961)\",Children|Comedy|Romance': 1,\n             '1014,Pollyanna (1960),Children|Comedy|Drama': 1,\n             '1015,Homeward Bound: The Incredible Journey (1993),Adventure|Children|Drama': 1,\n             '1016,\"Shaggy Dog, The (1959)\",Children|Comedy': 1,\n             '1017,Swiss Family Robinson (1960),Adventure|Children': 1,\n             '1018,That Darn Cat! (1965),Children|Comedy|Mystery': 1,\n             '1019,\"20,000 Leagues Under the Sea (1954)\",Adventure|Drama|Sci-Fi': 1,\n             '1020,Cool Runnings (1993),Comedy': 1,\n             '1021,Angels in the Outfield (1994),Children|Comedy': 1,\n             '1022,Cinderella (1950),Animation|Children|Fantasy|Musical|Romance': 1,\n             '1023,Winnie the Pooh and the Blustery Day (1968),Animation|Children|Musical': 1,\n             '1024,\"Three Caballeros, The (1945)\",Animation|Children|Musical': 1,\n             '1025,\"Sword in the Stone, The (1963)\",Animation|Children|Fantasy|Musical': 1,\n             '1027,Robin Hood: Prince of Thieves (1991),Adventure|Drama': 1,\n             '1028,Mary Poppins (1964),Children|Comedy|Fantasy|Musical': 1,\n             '1029,Dumbo (1941),Animation|Children|Drama|Musical': 1,\n             \"1030,Pete's Dragon (1977),Adventure|Animation|Children|Musical\": 1,\n             '1031,Bedknobs and Broomsticks (1971),Adventure|Children|Musical': 1,\n             '1032,Alice in Wonderland (1951),Adventure|Animation|Children|Fantasy|Musical': 1,\n             '1033,\"Fox and the Hound, The (1981)\",Animation|Children|Drama': 1,\n             '1034,Freeway (1996),Comedy|Crime|Drama|Thriller': 1,\n             '1035,\"Sound of Music, The (1965)\",Musical|Romance': 1,\n             '1036,Die Hard (1988),Action|Crime|Thriller': 1,\n             '1037,\"Lawnmower Man, The (1992)\",Action|Horror|Sci-Fi|Thriller': 1,\n             '1040,\"Secret Agent, The (1996)\",Drama': 1,\n             '1041,Secrets & Lies (1996),Drama': 1,\n             '1042,That Thing You Do! (1996),Comedy|Drama': 1,\n             '1043,To Gillian on Her 37th Birthday (1996),Drama|Romance': 1,\n             '1046,Beautiful Thing (1996),Drama|Romance': 1,\n             '1047,\"Long Kiss Goodnight, The (1996)\",Action|Drama|Thriller': 1,\n             '1049,\"Ghost and the Darkness, The (1996)\",Action|Adventure': 1,\n             '1050,Looking for Richard (1996),Documentary|Drama': 1,\n             '1051,Trees Lounge (1996),Drama': 1,\n             '1053,Normal Life (1996),Crime|Drama|Romance': 1,\n             '1054,Get on the Bus (1996),Drama': 1,\n             '1055,Shadow Conspiracy (1997),Thriller': 1,\n             '1056,Jude (1996),Drama': 1,\n             '1057,Everyone Says I Love You (1996),Comedy|Musical|Romance': 1,\n             \"1059,William Shakespeare's Romeo + Juliet (1996),Drama|Romance\": 1,\n             '1060,Swingers (1996),Comedy|Drama': 1,\n             '1061,Sleepers (1996),Thriller': 1,\n             '1064,Aladdin and the King of Thieves (1996),Animation|Children|Comedy|Fantasy|Musical|Romance': 1,\n             '1066,Shall We Dance (1937),Comedy|Musical|Romance': 1,\n             '1068,Crossfire (1947),Crime|Film-Noir': 1,\n             '1073,Willy Wonka & the Chocolate Factory (1971),Children|Comedy|Fantasy|Musical': 1,\n             '1076,\"Innocents, The (1961)\",Drama|Horror|Thriller': 1,\n             '1077,Sleeper (1973),Comedy|Sci-Fi': 1,\n             '1078,Bananas (1971),Comedy|War': 1,\n             '1079,\"Fish Called Wanda, A (1988)\",Comedy|Crime': 1,\n             \"1080,Monty Python's Life of Brian (1979),Comedy\": 1,\n             '1081,Victor/Victoria (1982),Comedy|Musical|Romance': 1,\n             '1082,\"Candidate, The (1972)\",Drama': 1,\n             '1083,\"Great Race, The (1965)\",Comedy|Musical': 1,\n             '1084,Bonnie and Clyde (1967),Crime|Drama': 1,\n             '1085,\"Old Man and the Sea, The (1958)\",Adventure|Drama': 1,\n             '1086,Dial M for Murder (1954),Crime|Mystery|Thriller': 1,\n             '1088,Dirty Dancing (1987),Drama|Musical|Romance': 1,\n             '1089,Reservoir Dogs (1992),Crime|Mystery|Thriller': 1,\n             '1090,Platoon (1986),Drama|War': 1,\n             \"1091,Weekend at Bernie's (1989),Comedy\": 1,\n             '1092,Basic Instinct (1992),Crime|Mystery|Thriller': 1,\n             '1093,\"Doors, The (1991)\",Drama': 1,\n             '1094,\"Crying Game, The (1992)\",Drama|Romance|Thriller': 1,\n             '1095,Glengarry Glen Ross (1992),Drama': 1,\n             \"1096,Sophie's Choice (1982),Drama\": 1,\n             '1097,E.T. the Extra-Terrestrial (1982),Children|Drama|Sci-Fi': 1,\n             '1099,\"Christmas Carol, A (1938)\",Children|Drama|Fantasy': 1,\n             '1100,Days of Thunder (1990),Action|Drama|Romance': 1,\n             '1101,Top Gun (1986),Action|Romance': 1,\n             '1103,Rebel Without a Cause (1955),Drama': 1,\n             '1104,\"Streetcar Named Desire, A (1951)\",Drama': 1,\n             '1105,Children of the Corn IV: The Gathering (1996),Horror': 1,\n             '1107,Loser (1991),Comedy': 1,\n             \"1111,Microcosmos (Microcosmos: Le peuple de l'herbe) (1996),Documentary\": 1,\n             '1112,Palookaville (1996),Action|Comedy|Drama': 1,\n             '1114,\"Funeral, The (1996)\",Crime|Drama': 1,\n             '1116,\"Single Girl, A (Fille seule, La) (1995)\",Drama': 1,\n             '1117,\"Eighth Day, The (Huitième jour, Le) (1996)\",Drama': 1,\n             '1119,Drunks (1995),Drama': 1,\n             '1120,\"People vs. Larry Flynt, The (1996)\",Comedy|Drama': 1,\n             '1121,Glory Daze (1995),Drama': 1,\n             '1123,\"Perfect Candidate, A (1996)\",Documentary': 1,\n             '1124,On Golden Pond (1981),Drama': 1,\n             '1125,\"Return of the Pink Panther, The (1975)\",Comedy|Crime': 1,\n             '1126,Drop Dead Fred (1991),Comedy|Fantasy': 1,\n             '1127,\"Abyss, The (1989)\",Action|Adventure|Sci-Fi|Thriller': 1,\n             '1128,\"Fog, The (1980)\",Horror': 1,\n             '1129,Escape from New York (1981),Action|Adventure|Sci-Fi|Thriller': 1,\n             '1130,\"Howling, The (1980)\",Horror|Mystery': 1,\n             '1131,Jean de Florette (1986),Drama|Mystery': 1,\n             '1132,Manon of the Spring (Manon des sources) (1986),Drama': 1,\n             '1135,Private Benjamin (1980),Comedy': 1,\n             '1136,Monty Python and the Holy Grail (1975),Adventure|Comedy|Fantasy': 1,\n             '1137,Hustler White (1996),Romance': 1,\n             '1140,Entertaining Angels: The Dorothy Day Story (1996),Drama': 1,\n             '1144,\"Line King: The Al Hirschfeld Story, The (1996)\",Documentary': 1,\n             '1147,When We Were Kings (1996),Documentary': 1,\n             '1148,Wallace & Gromit: The Wrong Trousers (1993),Animation|Children|Comedy|Crime': 1,\n             '1150,\"Return of Martin Guerre, The (Retour de Martin Guerre, Le) (1982)\",Drama': 1,\n             '1151,Lesson Faust (1994),Animation|Comedy|Drama|Fantasy': 1,\n             '1156,\"Children Are Watching Us, The (Bambini ci guardano, I) (1944)\",Drama': 1,\n             '1161,\"Tin Drum, The (Blechtrommel, Die) (1979)\",Drama|War': 1,\n             '1162,\"Ruling Class, The (1972)\",Comedy|Drama': 1,\n             '1163,Mina Tannenbaum (1994),Drama': 1,\n             '1167,Dear God (1996),Comedy': 1,\n             '1170,Best of the Best 3: No Turning Back (1995),Action': 1,\n             '1171,Bob Roberts (1992),Comedy': 1,\n             '1172,Cinema Paradiso (Nuovo cinema Paradiso) (1989),Drama': 1,\n             '1173,\"Cook the Thief His Wife & Her Lover, The (1989)\",Comedy|Drama': 1,\n             '1175,Delicatessen (1991),Comedy|Drama|Romance': 1,\n             '1176,\"Double Life of Veronique, The (Double Vie de Véronique, La) (1991)\",Drama|Fantasy|Romance': 1,\n             '1177,Enchanted April (1992),Drama|Romance': 1,\n             '1178,Paths of Glory (1957),Drama|War': 1,\n             '1179,\"Grifters, The (1990)\",Crime|Drama|Film-Noir': 1,\n             '1180,Hear My Song (1991),Comedy': 1,\n             '1183,\"English Patient, The (1996)\",Drama|Romance|War': 1,\n             '1184,Mediterraneo (1991),Comedy|Drama': 1,\n             '1185,My Left Foot (1989),Drama': 1,\n             '1186,\"Sex, Lies, and Videotape (1989)\",Drama': 1,\n             '1187,Passion Fish (1992),Drama': 1,\n             '1188,Strictly Ballroom (1992),Comedy|Romance': 1,\n             '1189,\"Thin Blue Line, The (1988)\",Documentary': 1,\n             '1190,Tie Me Up! Tie Me Down! (¡Átame!) (1990),Crime|Drama|Romance': 1,\n             '1191,Madonna: Truth or Dare (1991),Documentary|Musical': 1,\n             '1192,Paris Is Burning (1990),Documentary': 1,\n             \"1193,One Flew Over the Cuckoo's Nest (1975),Drama\": 1,\n             \"1194,Cheech and Chong's Up in Smoke (1978),Comedy\": 1,\n             '1196,Star Wars: Episode V - The Empire Strikes Back (1980),Action|Adventure|Sci-Fi': 1,\n             '1197,\"Princess Bride, The (1987)\",Action|Adventure|Comedy|Fantasy|Romance': 1,\n             '1198,Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981),Action|Adventure': 1,\n             '1199,Brazil (1985),Fantasy|Sci-Fi': 1,\n             '1200,Aliens (1986),Action|Adventure|Horror|Sci-Fi': 1,\n             '1201,\"Good, the Bad and the Ugly, The (Buono, il brutto, il cattivo, Il) (1966)\",Action|Adventure|Western': 1,\n             '1202,Withnail & I (1987),Comedy': 1,\n             '1203,12 Angry Men (1957),Drama': 1,\n             '1204,Lawrence of Arabia (1962),Adventure|Drama|War': 1,\n             '1206,\"Clockwork Orange, A (1971)\",Crime|Drama|Sci-Fi|Thriller': 1,\n             '1207,To Kill a Mockingbird (1962),Drama': 1,\n             '1208,Apocalypse Now (1979),Action|Drama|War': 1,\n             \"1209,Once Upon a Time in the West (C'era una volta il West) (1968),Action|Drama|Western\": 1,\n             '1210,Star Wars: Episode VI - Return of the Jedi (1983),Action|Adventure|Sci-Fi': 1,\n             '1211,\"Wings of Desire (Himmel über Berlin, Der) (1987)\",Drama|Fantasy|Romance': 1,\n             '1212,\"Third Man, The (1949)\",Film-Noir|Mystery|Thriller': 1,\n             '1213,Goodfellas (1990),Crime|Drama': 1,\n             '1214,Alien (1979),Horror|Sci-Fi': 1,\n             '1215,Army of Darkness (1993),Action|Adventure|Comedy|Fantasy|Horror': 1,\n             '1216,\"Big Blue, The (Grand bleu, Le) (1988)\",Adventure|Drama|Romance': 1,\n             '1217,Ran (1985),Drama|War': 1,\n             '1218,\"Killer, The (Die xue shuang xiong) (1989)\",Action|Crime|Drama|Thriller': 1,\n             '1219,Psycho (1960),Crime|Horror': 1,\n             '1220,\"Blues Brothers, The (1980)\",Action|Comedy|Musical': 1,\n             '1221,\"Godfather: Part II, The (1974)\",Crime|Drama': 1,\n             '1222,Full Metal Jacket (1987),Drama|War': 1,\n             '1223,\"Grand Day Out with Wallace and Gromit, A (1989)\",Adventure|Animation|Children|Comedy|Sci-Fi': 1,\n             '1224,Henry V (1989),Action|Drama|Romance|War': 1,\n             '1225,Amadeus (1984),Drama': 1,\n             '1226,\"Quiet Man, The (1952)\",Drama|Romance': 1,\n             '1227,Once Upon a Time in America (1984),Crime|Drama': 1,\n             '1228,Raging Bull (1980),Drama': 1,\n             '1230,Annie Hall (1977),Comedy|Romance': 1,\n             '1231,\"Right Stuff, The (1983)\",Drama': 1,\n             '1232,Stalker (1979),Drama|Mystery|Sci-Fi': 1,\n             '1233,\"Boot, Das (Boat, The) (1981)\",Action|Drama|War': 1,\n             '1234,\"Sting, The (1973)\",Comedy|Crime': 1,\n             '1235,Harold and Maude (1971),Comedy|Drama|Romance': 1,\n             '1236,Trust (1990),Comedy|Drama|Romance': 1,\n             '1237,\"Seventh Seal, The (Sjunde inseglet, Det) (1957)\",Drama': 1,\n             '1238,Local Hero (1983),Comedy': 1,\n             '1240,\"Terminator, The (1984)\",Action|Sci-Fi|Thriller': 1,\n             '1241,Dead Alive (Braindead) (1992),Comedy|Fantasy|Horror': 1,\n             '1242,Glory (1989),Drama|War': 1,\n             '1243,Rosencrantz and Guildenstern Are Dead (1990),Comedy|Drama': 1,\n             '1244,Manhattan (1979),Comedy|Drama|Romance': 1,\n             \"1245,Miller's Crossing (1990),Crime|Drama|Film-Noir|Thriller\": 1,\n             '1246,Dead Poets Society (1989),Drama': 1,\n             '1247,\"Graduate, The (1967)\",Comedy|Drama|Romance': 1,\n             '1248,Touch of Evil (1958),Crime|Film-Noir|Thriller': 1,\n             '1249,\"Femme Nikita, La (Nikita) (1990)\",Action|Crime|Romance|Thriller': 1,\n             '1250,\"Bridge on the River Kwai, The (1957)\",Adventure|Drama|War': 1,\n             '1251,8 1/2 (8½) (1963),Drama|Fantasy': 1,\n             '1252,Chinatown (1974),Crime|Film-Noir|Mystery|Thriller': 1,\n             '1253,\"Day the Earth Stood Still, The (1951)\",Drama|Sci-Fi|Thriller': 1,\n             '1254,\"Treasure of the Sierra Madre, The (1948)\",Action|Adventure|Drama|Western': 1,\n             '1255,Bad Taste (1987),Comedy|Horror|Sci-Fi': 1,\n             '1256,Duck Soup (1933),Comedy|Musical|War': 1,\n             '1257,Better Off Dead... (1985),Comedy|Romance': 1,\n             '1258,\"Shining, The (1980)\",Horror': 1,\n             '1259,Stand by Me (1986),Adventure|Drama': 1,\n             '1260,M (1931),Crime|Film-Noir|Thriller': 1,\n             '1261,Evil Dead II (Dead by Dawn) (1987),Action|Comedy|Fantasy|Horror': 1,\n             '1262,\"Great Escape, The (1963)\",Action|Adventure|Drama|War': 1,\n             '1263,\"Deer Hunter, The (1978)\",Drama|War': 1,\n             '1264,Diva (1981),Action|Drama|Mystery|Romance|Thriller': 1,\n             '1265,Groundhog Day (1993),Comedy|Fantasy|Romance': 1,\n             '1266,Unforgiven (1992),Drama|Western': 1,\n             '1267,\"Manchurian Candidate, The (1962)\",Crime|Thriller|War': 1,\n             '1268,Pump Up the Volume (1990),Comedy|Drama': 1,\n             '1269,Arsenic and Old Lace (1944),Comedy|Mystery|Thriller': 1,\n             '1270,Back to the Future (1985),Adventure|Comedy|Sci-Fi': 1,\n             '1271,Fried Green Tomatoes (1991),Comedy|Crime|Drama': 1,\n             '1272,Patton (1970),Drama|War': 1,\n             '1273,Down by Law (1986),Comedy|Drama|Film-Noir': 1,\n             '1274,Akira (1988),Action|Adventure|Animation|Sci-Fi': 1,\n             '1275,Highlander (1986),Action|Adventure|Fantasy': 1,\n             '1276,Cool Hand Luke (1967),Drama': 1,\n             '1277,Cyrano de Bergerac (1990),Comedy|Drama|Romance': 1,\n             '1278,Young Frankenstein (1974),Comedy|Fantasy': 1,\n             '1279,Night on Earth (1991),Comedy|Drama': 1,\n             '1280,Raise the Red Lantern (Da hong deng long gao gao gua) (1991),Drama': 1,\n             '1281,\"Great Dictator, The (1940)\",Comedy|Drama|War': 1,\n             '1282,Fantasia (1940),Animation|Children|Fantasy|Musical': 1,\n             '1283,High Noon (1952),Drama|Western': 1,\n             '1284,\"Big Sleep, The (1946)\",Crime|Film-Noir|Mystery': 1,\n             '1285,Heathers (1989),Comedy': 1,\n             '1286,Somewhere in Time (1980),Drama|Romance': 1,\n             '1287,Ben-Hur (1959),Action|Adventure|Drama': 1,\n             '1288,This Is Spinal Tap (1984),Comedy': 1,\n             '1289,Koyaanisqatsi (a.k.a. Koyaanisqatsi: Life Out of Balance) (1983),Documentary': 1,\n             '1290,Some Kind of Wonderful (1987),Drama|Romance': 1,\n             '1291,Indiana Jones and the Last Crusade (1989),Action|Adventure': 1,\n             '1292,Being There (1979),Comedy|Drama': 1,\n             '1293,Gandhi (1982),Drama': 1,\n             '1295,\"Unbearable Lightness of Being, The (1988)\",Drama': 1,\n             '1296,\"Room with a View, A (1986)\",Drama|Romance': 1,\n             '1297,Real Genius (1985),Comedy': 1,\n             '1298,Pink Floyd: The Wall (1982),Drama|Musical': 1,\n             '1299,\"Killing Fields, The (1984)\",Drama|War': 1,\n             '1300,My Life as a Dog (Mitt liv som hund) (1985),Comedy|Drama': 1,\n             ...})\n\n\n\n\ncountApproxDistinct()\n\nSpark내부의 Approx알고리즘이 Count해줌\n\n데이터가 작을수록 알고리즘 효율이 떨어짐\n\n데이터가 큰 경우 Count만으로도 오래걸릴 수 있음\n\nAccumulator를 활용하는게 더 빠를 수 있음 (아래는 이해만을 위한 틀린 코드)\nrdd4.foreachPartition(\n    accumulator ac\n    ac.add(1)\n    )\n\n\n\n# Approx\nrdd1.countApproxDistinct()\n\n10275\n\n\n\n# Actual\nlen(rdd1.distinct().collect())\n\n9743\n\n\n\n\nCache\n\n중간에 캐싱을 해 둠\n캐싱을 해두었다면 메모리가 터졌을 때, 좀 더 빠르게 계산될 수 있음 (Optimization도움)\n캐싱할 때마다, Spark내부적으로 메타데이터 연산(Statistics)을 조금씩 해줌 (Optimization도움)\n데이터가 크다면 한번의 작업이 끝날때마다 캐싱을 해두는 것을 추천\n\n아래와 같이 작업마다 캐싱\n\nrdd2 = rdd1.filter(...).where().select()\nrdd2.cache()\n\n\n# cache\nrdd2.cache()\n\nPythonRDD[34] at RDD at PythonRDD.scala:53\n\n\n\n\nCheckPoint\n\n캐싱도 중간값을 저장해서 비슷하지만, CheckPoint는 명시적으로 어디에 저장할지 정의해야 함\n테스트해 본 결과, 아래와 같이 생성되었음\n\n_de-2024\\2024-08-23 072646.217657\\8f6fcbb9-d532-4e6e-a928-39b809364967\n\n\n\nfrom datetime import datetime\n\nsc.setCheckpointDir(\"../data/temp/checkpoint/\" + str(datetime.now()))\n\nrdd2.checkpoint()\n\n\n\nPersist\n\n어떤 StorageLevel에 쓸지 정의 가능 (Default는 메모리)\n\n무거운 연산을 해야한다면 일부러 디스크를 내릴 때 쓰기도 함 python     from pyspark import StorageLevel     rdd2.persist(StorageLevel.DISK_ONLY)\n메모리에 올라가지 않아서 아예 멈추는 상황도 생길 수 있어 사용(어떻게든 계산을 시키기 위함)\n\n\n\nfrom pyspark import StorageLevel\nrdd2.persist(StorageLevel.MEMORY_ONLY)\n\nprint(rdd2.getStorageLevel())\n\nMemory Serialized 1x Replicated\n\n\n\n\n파티션 관련\n\ngetNumPartitions() : 파티션의 수\n\n\nrdd1.getNumPartitions()\n\n3\n\n\n\npartitioner : 파티션을 어떻게 나눌지에 대한 알고리즘이 들어있음\n\n기본값은 Hash\n\n\n\nprint(rdd1.partitioner)\n\nNone\n\n\n\nrepartition() : 파티션의 수 지정\n\n기본 알고리즘인 Hashing이 사용되었을 것임\n\n\n\nrdd1 = rdd1.repartition(3)\n\nprint(rdd1.getNumPartitions())\n\n3\n\n\n\ncoalesce() : 파티션 수 줄이기\n\nrepartition()으로 줄이는 것보다 효율적\n\nrepartition으로 줄이면 Hashing, DiskI/O 등으로 Shuffle이 발생할 수 밖에 없지만\ncoalesce는 Hashing을 다시 하지 않도록, 최소한의 이동이 일어나도록 함\n\n\n\n\nrdd1 = rdd1.coalesce(2)\n\nprint(rdd1.getNumPartitions())\n\n2\n\n\n\n\n기타\n\nrdd1.isEmpty()\n\nFalse\n\n\n\ndata = []\nrdd5 = sc.parallelize(data)\n\nrdd5.isEmpty()\n\nTrue\n\n\n\n# Max, Min\nrdd2 = sc.range(0, 1000, 1, 10)\n\nprint(rdd2.max())\nprint(rdd2.min())\n\n999\n0\n\n\n\n# Meta\nrdd1 = sc.textFile(\"../data/movies.csv\")\n\nprint(rdd1.id) \nprint(rdd1.context) # 어떤 SparkContext를 사용중인지\nprint(rdd1.toDebugString)\n\n&lt;bound method RDD.id of ../data/movies.csv MapPartitionsRDD[33] at textFile at NativeMethodAccessorImpl.java:0&gt;\n&lt;SparkContext master=local appName=rdd-dataframe&gt;\n&lt;bound method RDD.toDebugString of ../data/movies.csv MapPartitionsRDD[33] at textFile at NativeMethodAccessorImpl.java:0&gt;\n\n\n\n\n\n(2) RDD Transformation\n\n# 실습을 위한 기초코드\ndata = [\"co1,tcol2,tA_B_C\", \"col,tcol3,tD_E_F\"]\nrdd1 = sc.parallelize(data)\n\nrdd1.take(10)\n\n['co1,tcol2,tA_B_C', 'col,tcol3,tD_E_F']\n\n\n\n# map\nrdd2 = rdd1.map(lambda v: v.upper())\nrdd2.take(10)\n\n['CO1,TCOL2,TA_B_C', 'COL,TCOL3,TD_E_F']\n\n\n\n# map\nrdd5 = rdd1.map(lambda v: v.split(','))\nrdd5.take(10)\n\n[['co1', 'tcol2', 'tA_B_C'], ['col', 'tcol3', 'tD_E_F']]\n\n\n\n# mapPartitions\nrdd3 = rdd1.mapPartitions(lambda it: map(lambda v: v.upper(), it))\nrdd3.take(10)\n\n['CO1,TCOL2,TA_B_C', 'COL,TCOL3,TD_E_F']\n\n\n\n# mapPartitionsWithIndex (mapPartitions + Index)\nrdd4 = rdd1.mapPartitionsWithIndex(lambda idx, it: map(lambda v: f\"idx:{idx}, value:{v}\", it))\nrdd4.take(10)\n\n['idx:0, value:co1,tcol2,tA_B_C', 'idx:0, value:col,tcol3,tD_E_F']\n\n\n\n# flatMap\nrdd6 = rdd1.flatMap(lambda v: v.split(\",\"))\nrdd6.take(10)\n\n['co1', 'tcol2', 'tA_B_C', 'col', 'tcol3', 'tD_E_F']\n\n\n\n# distinct\nrdd8 = sc.parallelize([1, 2, 3, 3, 5, 6, 8, 8, 10]).distinct()\nrdd8.take(10)\n\n[1, 2, 3, 5, 6, 8, 10]\n\n\n\n# subtract (겹치는 것을 제거)\nr1 = sc.parallelize([1, 2, 3, 4, 5])\nr2 = sc.parallelize([4, 5, 6, 7, 8])\nrdd10 = r1.subtract(r2)\nrdd10.take(10)\n\n[2, 1, 3]\n\n\n\n# glom\nrdd = sc.range(0, 50, 1, 5)\nrdd.glom().collect()\nrdd.take(10)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\n(3) RDD with key-value pairs\n\n# 실습을 위한 기초코드\ndata = [\"a\", \"b\", \"c\", \"b\", \"b\", \"d\"]\nrdd1 = sc.parallelize(data)\n\nrdd2 = rdd1.map(lambda v: (v, 1))\nrdd2.take(10)\n\n[('a', 1), ('b', 1), ('c', 1), ('b', 1), ('b', 1), ('d', 1)]\n\n\n\n# mapValues : rdd1은 key-value pair가 아님 [mapValues는 key-value pair를 위한 API이므로 오류발생]\nrdd1.mapValues(lambda v: v + 1).take(10) # error\n\n24/08/23 12:21:30 ERROR Executor: Exception in task 0.0 in stage 62.0 (TID 113)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2276, in &lt;lambda&gt;\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nIndexError: string index out of range\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator.foreach(Iterator.scala:941)\n    at scala.collection.Iterator.foreach$(Iterator.scala:941)\n    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n    at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n    at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\n24/08/23 12:21:30 WARN TaskSetManager: Lost task 0.0 in stage 62.0 (TID 113) (7b9e2a0ad7ba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2276, in &lt;lambda&gt;\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nIndexError: string index out of range\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator.foreach(Iterator.scala:941)\n    at scala.collection.Iterator.foreach$(Iterator.scala:941)\n    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n    at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n    at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\n\n24/08/23 12:21:30 ERROR TaskSetManager: Task 0 in stage 62.0 failed 1 times; aborting job\n\n\n\n---------------------------------------------------------------------------\n\nPy4JJavaError                             Traceback (most recent call last)\n\n/tmp/ipykernel_722/413939631.py in &lt;module&gt;\n\n      1 # rdd1은 key-pair value가 아님\n\n----&gt; 2 rdd1.mapValues(lambda v: v + 1).take(10) # error\n\n\n\n/usr/local/spark/python/pyspark/rdd.py in take(self, num)\n\n   1564 \n\n   1565             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))\n\n-&gt; 1566             res = self.context.runJob(self, takeUpToNumLeft, p)\n\n   1567 \n\n   1568             items += res\n\n\n\n/usr/local/spark/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)\n\n   1231         # SparkContext#runJob.\n\n   1232         mappedRDD = rdd.mapPartitions(partitionFunc)\n\n-&gt; 1233         sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n\n   1234         return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\n\n   1235 \n\n\n\n/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)\n\n   1302 \n\n   1303         answer = self.gateway_client.send_command(command)\n\n-&gt; 1304         return_value = get_return_value(\n\n   1305             answer, self.gateway_client, self.target_id, self.name)\n\n   1306 \n\n\n\n/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n\n    109     def deco(*a, **kw):\n\n    110         try:\n\n--&gt; 111             return f(*a, **kw)\n\n    112         except py4j.protocol.Py4JJavaError as e:\n\n    113             converted = convert_exception(e.java_exception)\n\n\n\n/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n\n    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n\n    325             if answer[1] == REFERENCE_TYPE:\n\n--&gt; 326                 raise Py4JJavaError(\n\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n\n    328                     format(target_id, \".\", name), value)\n\n\n\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 113) (7b9e2a0ad7ba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n\n    process()\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n\n    serializer.dump_stream(out_iter, outfile)\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n\n    vs = list(itertools.islice(iterator, batch))\n\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n\n    yield next(iterator)\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n\n    return f(*args, **kwargs)\n\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2276, in &lt;lambda&gt;\n\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n\nIndexError: string index out of range\n\n\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\n    at scala.collection.Iterator.foreach(Iterator.scala:941)\n\n    at scala.collection.Iterator.foreach$(Iterator.scala:941)\n\n    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\n    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\n    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\n    at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\n    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\n    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\n    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\n    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\n    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\n    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\n    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\n    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\n    at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\n    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\n    at java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\nDriver stacktrace:\n\n    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\n    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\n    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\n    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\n    at scala.Option.foreach(Option.scala:407)\n\n    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\n    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\n    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\n    at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\n    at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\n    at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n    at java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\n    at py4j.Gateway.invoke(Gateway.java:282)\n\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n\n    at java.base/java.lang.Thread.run(Thread.java:829)\n\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n\n    process()\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n\n    serializer.dump_stream(out_iter, outfile)\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n\n    vs = list(itertools.islice(iterator, batch))\n\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n\n    yield next(iterator)\n\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n\n    return f(*args, **kwargs)\n\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2276, in &lt;lambda&gt;\n\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n\nIndexError: string index out of range\n\n\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\n    at scala.collection.Iterator.foreach(Iterator.scala:941)\n\n    at scala.collection.Iterator.foreach$(Iterator.scala:941)\n\n    at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\n    at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\n    at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\n    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\n    at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\n    at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\n    at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\n    at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\n    at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\n    at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\n    at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\n    at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\n    at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\n    at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\n    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\n    ... 1 more\n\n\n\n\n\n# mapValues : rdd2는 key-value pair가 맞음\nrdd3 = rdd2.mapValues(lambda v: v + 1)\nrdd3.take(10)\n\n[('a', 2), ('b', 2), ('c', 2), ('b', 2), ('b', 2), ('d', 2)]\n\n\n\n# flatMapValues\ndata = [(\"a\", \"1,2,3\"), (\"b\", \"4,5,6\")]\nrdd = sc.parallelize(data)\nrdd.take(10)\n\n[('a', '1,2,3'), ('b', '4,5,6')]\n\n\n\nrdd4 = rdd.flatMapValues(lambda v: v.split(\",\"))\nrdd4.take(10)\n\n[('a', '1'), ('a', '2'), ('a', '3'), ('b', '4'), ('b', '5'), ('b', '6')]\n\n\n\n# reduceByKey\n\n# rdd2 = [('a', 1), ('b', 1), ('c', 1), ('b', 1), ('b', 1), ('d', 1)]\nrdd6 = rdd2.reduceByKey(lambda a, b: a + b)\nrdd6.take(10)\n\n[('a', 1), ('b', 3), ('c', 1), ('d', 1)]\n\n\n\n# groupByKey\nrdd7 = rdd6.groupByKey()\nrdd7.map(lambda x : (x[0], list(x[1]))).collect()\n\n[('a', [1]), ('b', [3]), ('c', [1]), ('d', [1])]\n\n\n\n# cogroup\n\n\n## group1 : rdd8\nkv1 = [(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k3\", \"v3\")]\nrdd8 = sc.parallelize(kv1)\nrdd8.take(10)\n\n[('k1', 'v1'), ('k2', 'v2'), ('k3', 'v3')]\n\n\n\n#3 group2 : rdd9\nkv2 = [(\"k1\", \"v4\"), (\"k2\", \"v5\"), (\"k3\", \"v6\")]\nrdd9 = sc.parallelize(kv2)\nrdd9.take(10)\n\n[('k1', 'v4'), ('k2', 'v5'), ('k3', 'v6')]\n\n\n\nrdd10 = rdd8.cogroup(rdd9)\n[(x, tuple(map(list, y))) for x, y in sorted(list(rdd10.collect()))]\n\n[('k1', (['v1'], ['v4'])), ('k2', (['v2'], ['v5'])), ('k3', (['v3'], ['v6']))]\n\n\n\n# join  실제로는 cogroup보다는 join을 많이 사용\nrdd11 = rdd8.join(rdd9)\nrdd11.take(10)\n\n[('k2', ('v2', 'v5')), ('k1', ('v1', 'v4')), ('k3', ('v3', 'v6'))]\n\n\n\n# combineByKey\nr1 = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n\ndef to_list(a):\n    return [a]\n\ndef append(a, b):\n    a.append(b)\n    return a\n\ndef extend(a, b):\n    a.extend(b)\n    return a\n\nrdd12 = r1.combineByKey(to_list, append, extend)\nrdd12.take(10)\n\n[('Math', [100, 50]), ('Eng', [80, 70, 90])]\n\n\n\n\n(4) DataFrame\n\nRow\n\n보통은 데이터를 읽어오지만, Row를 활용해 데이터를 정의해줄 수도 있음\n\n\nfrom pyspark.sql import Row\n\nr1 = Row(1, \"two\", True)\nr1\n\n&lt;Row(1, 'two', True)&gt;\n\n\n\nprint(r1[0])\nprint(r1[1])\nprint(r1[2])\n\n1\ntwo\nTrue\n\n\n\n\nDataFrame\n\n파이썬 list로 바로 DataFrame을 생성할 수는 없으며, Row 등으로 감싸준 경우는 가능\n컬럼명 미지정시 _1, _2와 같이 생성됨\n\n\n# DataFrame생성 불가능 예시 (TypeError 발생)\ndata1 = [1, 2, 3]\n\ndf = spark.createDataFrame(data1)\ndf.show()\n\nTypeError: Can not infer schema for type: &lt;class ‘int’&gt;\n\n# DataFrame생성 가능 예시 (컬럼 미지정)\nfrom pyspark.sql import Row\n\ndata2 = [Row(1), Row(2), Row(3)]\n\ndf = spark.createDataFrame(data2)\ndf.show()\n\n+---+\n| _1|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n\n\n\n# DataFrame생성 가능 예시 (컬럼 지정)\ndf = spark.createDataFrame(data2, ['num'])\ndf.show()\n\n+---+\n|num|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n\n\n\n# DataFrame생성 가능 예시 (Class를 활용)\nclass Person:\n    def __init__(self):\n        self.name = \"name\"\n        self.age = 20\n        self.job = \"student\"\n    def __init__(self, name, age, job):\n        self.name = name\n        self.age = age\n        self.job = job\n\np1 = Person(\"foo\", 30, \"programmer\")\np2 = Person(\"bar\", 10, \"student\")\n\nspark.createDataFrame([p1, p2]).show(3)\n\n+---+----------+----+\n|age|       job|name|\n+---+----------+----+\n| 30|programmer| foo|\n| 10|   student| bar|\n+---+----------+----+\n\n\n\n\n# DataFrame생성 가능 예시 (Tuple 활용)\nt1 = (\"foo\", 30, \"programmer\")\nt2 = (\"bar\", 10, \"student\")\n\nspark.createDataFrame([t1, t2]).show(3)\n\n+---+---+----------+\n| _1| _2|        _3|\n+---+---+----------+\n|foo| 30|programmer|\n|bar| 10|   student|\n+---+---+----------+\n\n\n\n\n# DataFrame생성 가능 예시 (Tuple 활용+컬럼지정)\nt1 = (\"foo\", 30, \"programmer\")\nt2 = (\"bar\", 10, \"student\")\n\nspark.createDataFrame([t1, t2],['name','age','job']).show(3)\n\n+----+---+----------+\n|name|age|       job|\n+----+---+----------+\n| foo| 30|programmer|\n| bar| 10|   student|\n+----+---+----------+\n\n\n\n\n# DataFrame생성 가능 예시 (스키마 정의)\n\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType\n\nschema = StructType(\n    [StructField(\"name\", StringType(), nullable = True),\n     StructField(\"age\", IntegerType(), nullable = True),\n     StructField(\"job\", StringType(), nullable = True)]\n)\n\nrowRDD = sc.parallelize([Row(\"foo\", 7, \"programmer\"), Row(\"bar\", 13, \"student\")])\nrowRDD.collect()\n\n[&lt;Row('foo', 7, 'programmer')&gt;, &lt;Row('bar', 13, 'student')&gt;]\n\n\n\nspark.createDataFrame(rowRDD, schema).show()\n\n+----+---+----------+\n|name|age|       job|\n+----+---+----------+\n| foo|  7|programmer|\n| bar| 13|   student|\n+----+---+----------+\n\n\n\n\n# SparkSession을 활용해 DataFrame으로 생성 [spark.read.text()]\nspark.read.text(\"../data/movies.csv\").show(3)\n\n# 참고 : sc.textFile() : SparkContext를 활용해 RDD로 생성\n\n+--------------------+\n|               value|\n+--------------------+\n|movieId,title,genres|\n|1,Toy Story (1995...|\n|2,Jumanji (1995),...|\n+--------------------+\nonly showing top 3 rows\n\n\n\n\n# SparkSession을 활용해 DataFrame으로 생성 [read.option()]\nspark.read.option(\"sep\", \",\").csv(\"../data/movies.csv\").show(3)\n\n+-------+----------------+--------------------+\n|    _c0|             _c1|                 _c2|\n+-------+----------------+--------------------+\n|movieId|           title|              genres|\n|      1|Toy Story (1995)|Adventure|Animati...|\n|      2|  Jumanji (1995)|Adventure|Childre...|\n+-------+----------------+--------------------+\nonly showing top 3 rows\n\n\n\n\n# SparkSession을 활용해 DataFrame으로 생성 [read.option(), 여러 데이터 소스]\nspark.read.option(\"basePath\", \"../data/\").csv(\"../data/*.csv\").show(3)\n\n+------+-------+------+---------+\n|   _c0|    _c1|   _c2|      _c3|\n+------+-------+------+---------+\n|userId|movieId|rating|timestamp|\n|     1|      1|   4.0|964982703|\n|     1|      3|   4.0|964981247|\n+------+-------+------+---------+\nonly showing top 3 rows\n\n\n\n\n# Table을 만들고 쿼리해서 사용 (영구적으로 사용할 수 없는 테이블임을 유의)\n## buckected_table이란 이름의 table이 없으면(count == 0)\nif (spark.sql(\"show tables\").where(\"tableName = 'buckected_table' \").count() == 0):\n    ## Data를 읽고\n    df = spark.read.csv(\"../data/movies.csv\")\n    ## created_at 기준으로 3개의 bucket으로 나누고 / buckected_table로 테이블 생성\n    df.repartition(1).write.mode(\"Overwrite\").bucketBy(3, \"id\").saveAsTable(\"buckected_table\")\n    \n    ## 테이블을 읽고 SQL 쿼리 사용\n    spark.read.table(\"buckected_table\")\n    spark.sql(\"select * from buckected_table limit 10\")    \n\n\n\nDataFrame API\n\n# .select()를 사용해 쿼리\n\ndf1 = spark.read.json(\"../data/*.json\")\ndf1.show(3)\n\n+--------------------+----------+--------------------+--------------------+------+--------------------+------------------+\n|               actor|created_at|                 org|             payload|public|                repo|              type|\n+--------------------+----------+--------------------+--------------------+------+--------------------+------------------+\n|{\"id\":35613825,\"l...|2024-05-19|{\"id\":126833237,\"...|{\"comment\":{\"url\"...|  true|{\"id\":749408001,\"...|CommitCommentEvent|\n|{\"id\":84257236,\"l...|2024-05-19|                null|{\"comment\":{\"url\"...|  true|{\"id\":430924902,\"...|CommitCommentEvent|\n|{\"id\":41898282,\"l...|2024-05-19|{\"id\":1040002,\"lo...|{\"comment\":{\"url\"...|  true|{\"id\":170801983,\"...|CommitCommentEvent|\n+--------------------+----------+--------------------+--------------------+------+--------------------+------------------+\nonly showing top 3 rows\n\n\n\n\ndf1.printSchema()\n\nroot\n |-- actor: string (nullable = true)\n |-- created_at: string (nullable = true)\n |-- org: string (nullable = true)\n |-- payload: string (nullable = true)\n |-- public: boolean (nullable = true)\n |-- repo: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n\ndf1.select(\"actor\", \"created_at\").show(3)\n\n+--------------------+----------+\n|               actor|created_at|\n+--------------------+----------+\n|{\"id\":35613825,\"l...|2024-05-19|\n|{\"id\":84257236,\"l...|2024-05-19|\n|{\"id\":41898282,\"l...|2024-05-19|\n+--------------------+----------+\nonly showing top 3 rows\n\n\n\n\n# SparkSQL (Expression) 활용\ndf1.selectExpr('actor').show(3, False)\n\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|actor                                                                                                                                                                                                                    |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|{\"id\":35613825,\"login\":\"vercel[bot]\",\"display_login\":\"vercel\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/vercel[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/35613825?\"}                        |\n|{\"id\":84257236,\"login\":\"DM-netizen\",\"display_login\":\"DM-netizen\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/DM-netizen\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/84257236?\"}                      |\n|{\"id\":41898282,\"login\":\"github-actions[bot]\",\"display_login\":\"github-actions\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/github-actions[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/41898282?\"}|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n\n\n\n\n# SparkSQL (Expression) 활용\ndf1.selectExpr('upper(actor) as upperName').show(3, False)\n\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|upperName                                                                                                                                                                                                                |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|{\"ID\":35613825,\"LOGIN\":\"VERCEL[BOT]\",\"DISPLAY_LOGIN\":\"VERCEL\",\"GRAVATAR_ID\":\"\",\"URL\":\"HTTPS://API.GITHUB.COM/USERS/VERCEL[BOT]\",\"AVATAR_URL\":\"HTTPS://AVATARS.GITHUBUSERCONTENT.COM/U/35613825?\"}                        |\n|{\"ID\":84257236,\"LOGIN\":\"DM-NETIZEN\",\"DISPLAY_LOGIN\":\"DM-NETIZEN\",\"GRAVATAR_ID\":\"\",\"URL\":\"HTTPS://API.GITHUB.COM/USERS/DM-NETIZEN\",\"AVATAR_URL\":\"HTTPS://AVATARS.GITHUBUSERCONTENT.COM/U/84257236?\"}                      |\n|{\"ID\":41898282,\"LOGIN\":\"GITHUB-ACTIONS[BOT]\",\"DISPLAY_LOGIN\":\"GITHUB-ACTIONS\",\"GRAVATAR_ID\":\"\",\"URL\":\"HTTPS://API.GITHUB.COM/USERS/GITHUB-ACTIONS[BOT]\",\"AVATAR_URL\":\"HTTPS://AVATARS.GITHUBUSERCONTENT.COM/U/41898282?\"}|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n\n\n\n\n# col을 사용해 쿼리\n\n## pyspark.sql.functions.* 의 모든 타입은 column을 input으로 받는다\n## split('id',', ')는 미동작 → split(col('id'),', ')해야 동작\nfrom pyspark.sql.functions import col\n\nc1 = col(\"actor\")\nc2 = col(\"org\")\n\ndf1.select(c1, c2).show(3)\n\n+--------------------+--------------------+\n|               actor|                 org|\n+--------------------+--------------------+\n|{\"id\":35613825,\"l...|{\"id\":126833237,\"...|\n|{\"id\":84257236,\"l...|                null|\n|{\"id\":41898282,\"l...|{\"id\":1040002,\"lo...|\n+--------------------+--------------------+\nonly showing top 3 rows\n\n\n\n\n# filter 사용\ndata2 = [Row(1), Row(2), Row(3)]\ndf = spark.createDataFrame(data2)\n\ndf.filter(\"_1 == 2\").show(3, False)\n\n+---+\n|_1 |\n+---+\n|2  |\n+---+\n\n\n\n\n# where 사용\ndata2 = [Row(1), Row(2), Row(3)]\ndf = spark.createDataFrame(data2)\n\ndf.where(\"_1 == 2\").show(3, False)\n\n+---+\n|_1 |\n+---+\n|2  |\n+---+\n\n\n\n\n# orderBy\ndata2 = [Row(3), Row(2), Row(4)]\ndf = spark.createDataFrame(data2)\n\ndf.orderBy(\"_1\").show(3)\n\n+---+\n| _1|\n+---+\n|  2|\n|  3|\n|  4|\n+---+\n\n\n\n\n# sort\ndata2 = [Row(3), Row(2), Row(4)]\ndf = spark.createDataFrame(data2)\n\ndf.sort(col(\"_1\")).show(3)\n\n+---+\n| _1|\n+---+\n|  2|\n|  3|\n|  4|\n+---+\n\n\n\n\n# groupBy\nimport pyspark.sql.functions as F\n\ndf = spark.read.json(\"../data/*.json\")\n\n# aggregations\ngroupedDF = df.groupBy(\"type\")\ngroupedDF.agg(F.max(F.col(\"created_at\"))).show()\n\n+------------------+---------------+\n|              type|max(created_at)|\n+------------------+---------------+\n|CommitCommentEvent|     2024-05-19|\n+------------------+---------------+\n\n\n\n\n# 중복제거\nprint(df.count())\nprint(df.distinct().count())\n\n228\n\n\n\n[Stage 218:=================================&gt;                   (128 + 1) / 200]\n\n[Stage 218:==============================================&gt;      (176 + 1) / 200]\n\n\n228\n\n\n\n                                                                                \n\n\n\n\n많이 사용하는 DataFrame API\n\nwithColumn (많이 쓰임)\n\npyspark.sql.functions.*로 transformation작업을 많이함\n\nimport pyspark.sql.functions as F 로 많이 사용\n\n필터링해서 새로운 컬럼에 넣고, 기존 컬럼을 Drop하는 식으로 많이 사용\n\n\n\nimport pyspark.sql.functions as F\n\n# actor컬럼을 trim해서, newColumn라는 컬럼을 새로 만들어 저장\ndf.withColumn(\"newColumn\", F.trim(F.col(\"actor\"))).show(3,False)\n\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|actor                                                                                                                                                                                                                    |created_at|org                                                                                                                                                                          |payload                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |public|repo                                                                                                            |type              |newColumn                                                                                                                                                                                                                |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|{\"id\":35613825,\"login\":\"vercel[bot]\",\"display_login\":\"vercel\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/vercel[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/35613825?\"}                        |2024-05-19|{\"id\":126833237,\"login\":\"boxlife-app\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/orgs/boxlife-app\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/126833237?\"}    |{\"comment\":{\"url\":\"https://api.github.com/repos/boxlife-app/calisto/comments/142178684\",\"html_url\":\"https://github.com/boxlife-app/calisto/commit/7651f70845e3313adf7d6c534ce90d1157e1daab#commitcomment-142178684\",\"id\":142178684,\"node_id\":\"CC_kwDOLKsPAc4IeXl8\",\"user\":{\"login\":\"vercel[bot]\",\"id\":35613825,\"node_id\":\"MDM6Qm90MzU2MTM4MjU=\",\"avatar_url\":\"https://avatars.githubusercontent.com/in/8329?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/vercel%5Bbot%5D\",\"html_url\":\"https://github.com/apps/vercel\",\"followers_url\":\"https://api.github.com/users/vercel%5Bbot%5D/followers\",\"following_url\":\"https://api.github.com/users/vercel%5Bbot%5D/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/vercel%5Bbot%5D/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/vercel%5Bbot%5D/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/vercel%5Bbot%5D/subscriptions\",\"organizations_url\":\"https://api.github.com/users/vercel%5Bbot%5D/orgs\",\"repos_url\":\"https://api.github.com/users/vercel%5Bbot%5D/repos\",\"events_url\":\"https://api.github.com/users/vercel%5Bbot%5D/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/vercel%5Bbot%5D/received_events\",\"type\":\"Bot\",\"site_admin\":false},\"position\":null,\"line\":null,\"path\":null,\"commit_id\":\"7651f70845e3313adf7d6c534ce90d1157e1daab\",\"created_at\":\"2024-05-19T14:00:00Z\",\"updated_at\":\"2024-05-19T14:00:00Z\",\"author_association\":\"NONE\",\"body\":\"Successfully deployed to the following URLs:\\n\\n## calisto – ./\\n\\n[calisto-git-main-boxlife.vercel.app](https://calisto-git-main-boxlife.vercel.app)  \\n[calisto-sigma.vercel.app](https://calisto-sigma.vercel.app)  \\n[calisto-boxlife.vercel.app](https://calisto-boxlife.vercel.app)  \\n[calisto.js.org](https://calisto.js.org)  \\n[calisto.boxlife.app](https://calisto.boxlife.app)\",\"reactions\":{\"url\":\"https://api.github.com/repos/boxlife-app/calisto/comments/142178684/reactions\",\"total_count\":0,\"+1\":0,\"-1\":0,\"laugh\":0,\"hooray\":0,\"confused\":0,\"heart\":0,\"rocket\":0,\"eyes\":0}}}                                                                                                                                                                                                                                                                                                                                                                                                                             |true  |{\"id\":749408001,\"name\":\"boxlife-app/calisto\",\"url\":\"https://api.github.com/repos/boxlife-app/calisto\"}          |CommitCommentEvent|{\"id\":35613825,\"login\":\"vercel[bot]\",\"display_login\":\"vercel\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/vercel[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/35613825?\"}                        |\n|{\"id\":84257236,\"login\":\"DM-netizen\",\"display_login\":\"DM-netizen\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/DM-netizen\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/84257236?\"}                      |2024-05-19|null                                                                                                                                                                         |{\"comment\":{\"url\":\"https://api.github.com/repos/joyhughes/Jen/comments/142178686\",\"html_url\":\"https://github.com/joyhughes/Jen/commit/2516172cb9b3efde69c236491e36b1ba7bc567c2#commitcomment-142178686\",\"id\":142178686,\"node_id\":\"CC_kwDOGa9kZs4IeXl-\",\"user\":{\"login\":\"DM-netizen\",\"id\":84257236,\"node_id\":\"MDQ6VXNlcjg0MjU3MjM2\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/84257236?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/DM-netizen\",\"html_url\":\"https://github.com/DM-netizen\",\"followers_url\":\"https://api.github.com/users/DM-netizen/followers\",\"following_url\":\"https://api.github.com/users/DM-netizen/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/DM-netizen/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/DM-netizen/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/DM-netizen/subscriptions\",\"organizations_url\":\"https://api.github.com/users/DM-netizen/orgs\",\"repos_url\":\"https://api.github.com/users/DM-netizen/repos\",\"events_url\":\"https://api.github.com/users/DM-netizen/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/DM-netizen/received_events\",\"type\":\"User\",\"site_admin\":false},\"position\":null,\"line\":null,\"path\":null,\"commit_id\":\"2516172cb9b3efde69c236491e36b1ba7bc567c2\",\"created_at\":\"2024-05-19T14:00:42Z\",\"updated_at\":\"2024-05-19T14:00:42Z\",\"author_association\":\"NONE\",\"body\":\"```\\r\\ntemplate&lt; class T &gt; void eff_kaleidoscope&lt; T &gt;::operator () ( any_buffer_pair_ptr& buf, element_context& context )\\r\\n{\\r\\n    vec2f old_center; float old_segments; float old_offset_angle; bool old_reflect;\\r\\n    center(context); segments(context); offset_angle(context); reflect(context); filled(context);\\r\\n    \\r\\n    if(*center!=old_center || *segments!=old_segments || *offset_angle!=old_offset_angle || *reflect!=old_reflect)\\r\\n    filled=false;\\r\\n    old_center = *center; old_segments=*segments; old_offset_angle=*offset_angle; old_reflect=*reflect;\\r\\n\\r\\n    if(!filled)\\r\\n    {\\r\\n        filled =true;\\r\\n        vftools tools( get_image&lt; T &gt;( buf ) );\\r\\n        tools.kaleidoscope(*center,*segments,*offset_angle,*reflect);\\r\\n    }  \\r\\n}\\r\\n\\r\\ntemplate class eff_kaleidoscope&lt; vec2f &gt;;\\r\\n```\\r\\n\\r\\nI think this should work now.\",\"reactions\":{\"url\":\"https://api.github.com/repos/joyhughes/Jen/comments/142178686/reactions\",\"total_count\":0,\"+1\":0,\"-1\":0,\"laugh\":0,\"hooray\":0,\"confused\":0,\"heart\":0,\"rocket\":0,\"eyes\":0}}}|true  |{\"id\":430924902,\"name\":\"joyhughes/Jen\",\"url\":\"https://api.github.com/repos/joyhughes/Jen\"}                      |CommitCommentEvent|{\"id\":84257236,\"login\":\"DM-netizen\",\"display_login\":\"DM-netizen\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/DM-netizen\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/84257236?\"}                      |\n|{\"id\":41898282,\"login\":\"github-actions[bot]\",\"display_login\":\"github-actions\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/github-actions[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/41898282?\"}|2024-05-19|{\"id\":1040002,\"login\":\"linuxfoundation\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/orgs/linuxfoundation\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/1040002?\"}|{\"comment\":{\"url\":\"https://api.github.com/repos/linuxfoundation/lfevents/comments/142178688\",\"html_url\":\"https://github.com/linuxfoundation/lfevents/commit/b2ef1fcd2f3488f79c7a3f74e5692e4305e4dc16#commitcomment-142178688\",\"id\":142178688,\"node_id\":\"CC_kwDOCi47P84IeXmA\",\"user\":{\"login\":\"github-actions[bot]\",\"id\":41898282,\"node_id\":\"MDM6Qm90NDE4OTgyODI=\",\"avatar_url\":\"https://avatars.githubusercontent.com/in/15368?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/github-actions%5Bbot%5D\",\"html_url\":\"https://github.com/apps/github-actions\",\"followers_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/followers\",\"following_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions\",\"organizations_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/orgs\",\"repos_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/repos\",\"events_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/github-actions%5Bbot%5D/received_events\",\"type\":\"Bot\",\"site_admin\":false},\"position\":null,\"line\":null,\"path\":null,\"commit_id\":\"b2ef1fcd2f3488f79c7a3f74e5692e4305e4dc16\",\"created_at\":\"2024-05-19T14:01:02Z\",\"updated_at\":\"2024-05-19T14:01:02Z\",\"author_association\":\"NONE\",\"body\":\"[![Visit Site](https://raw.githubusercontent.com/pantheon-systems/terminus-build-tools-plugin/master/assets/images/visit-site-36.png)](https://pr-903-lfasiallcci.pantheonsite.io/)\\n\\nCreated multidev environment [pr-903](https://dashboard.pantheon.io/sites/d9d89786-a031-4501-a11a-542662f8f8c1#pr-903) for lfasiallcci.\",\"reactions\":{\"url\":\"https://api.github.com/repos/linuxfoundation/lfevents/comments/142178688/reactions\",\"total_count\":0,\"+1\":0,\"-1\":0,\"laugh\":0,\"hooray\":0,\"confused\":0,\"heart\":0,\"rocket\":0,\"eyes\":0}}}                                                                                                                                                                                                                                                                                                                                                                          |true  |{\"id\":170801983,\"name\":\"linuxfoundation/lfevents\",\"url\":\"https://api.github.com/repos/linuxfoundation/lfevents\"}|CommitCommentEvent|{\"id\":41898282,\"login\":\"github-actions[bot]\",\"display_login\":\"github-actions\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/github-actions[bot]\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/41898282?\"}|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+----------------------------------------------------------------------------------------------------------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 3 rows\n\n\n\n\n\nColumn API\n\n# Seㅣect + Col로 DataFrame생성하기\ndf1 = spark.read.json(\"../data/*.json\")\ndf1.printSchema()\n\nroot\n |-- actor: string (nullable = true)\n |-- created_at: string (nullable = true)\n |-- org: string (nullable = true)\n |-- payload: string (nullable = true)\n |-- public: boolean (nullable = true)\n |-- repo: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n\nc1 = F.col(\"actor\")\nc2 = F.col(\"org\")\n\ndf2 = df1.select(c1, c2)\ndf2.printSchema()\n\nroot\n |-- actor: string (nullable = true)\n |-- org: string (nullable = true)\n\n\n\n\n# orderBy\ndf1.select(\"org\").orderBy(\"created_at\").show(5)\n\n+--------------------+\n|                 org|\n+--------------------+\n|{\"id\":126833237,\"...|\n|                null|\n|{\"id\":1040002,\"lo...|\n|                null|\n|                null|\n+--------------------+\nonly showing top 5 rows\n\n\n\n\n# F.split 예제 (데이터 불러오기)\n## F.split(F.col(\"genres\"), '[|]', 2) : 마지막 숫자는 최대로 나눌 수 있는 수. -1넣으면 무제한\n\nimport pyspark.sql.functions as F\ndf1 = spark.read.options(header='True').csv(\"../data/movies.csv\")\ndf1.show(3, False)\n\n+-------+-----------------------+-------------------------------------------+\n|movieId|title                  |genres                                     |\n+-------+-----------------------+-------------------------------------------+\n|1      |Toy Story (1995)       |Adventure|Animation|Children|Comedy|Fantasy|\n|2      |Jumanji (1995)         |Adventure|Children|Fantasy                 |\n|3      |Grumpier Old Men (1995)|Comedy|Romance                             |\n+-------+-----------------------+-------------------------------------------+\nonly showing top 3 rows\n\n\n\n\n# F.split (limit를 2로 지정)\ncc = F.split(F.col(\"genres\"), '[|]', 2)\ndf2 = df1.select(cc.alias(\"cc\"))\ndf2.show(5, False)\n\n+----------------------------------------------+\n|cc                                            |\n+----------------------------------------------+\n|[Adventure, Animation|Children|Comedy|Fantasy]|\n|[Adventure, Children|Fantasy]                 |\n|[Comedy, Romance]                             |\n|[Comedy, Drama|Romance]                       |\n|[Comedy]                                      |\n+----------------------------------------------+\nonly showing top 5 rows\n\n\n\n\n# F.split (limit를 -1로 지정, 무제한)\ncc = F.split(F.col(\"genres\"), '[|]', -1)\ndf2 = df1.select(cc.alias(\"cc\"))\ndf2.show(5, False)\n\n+-------------------------------------------------+\n|cc                                               |\n+-------------------------------------------------+\n|[Adventure, Animation, Children, Comedy, Fantasy]|\n|[Adventure, Children, Fantasy]                   |\n|[Comedy, Romance]                                |\n|[Comedy, Drama, Romance]                         |\n|[Comedy]                                         |\n+-------------------------------------------------+\nonly showing top 5 rows\n\n\n\n\n# F.expr & select(cols)\n\ncols = [F.expr(f\"cc[{idx}]\") for idx in range(0, 2)]\ndf3 = df2.select(cols)\ndf3.show(5, False)\n\n+---------+---------+\n|cc[0]    |cc[1]    |\n+---------+---------+\n|Adventure|Animation|\n|Adventure|Children |\n|Comedy   |Romance  |\n|Comedy   |Drama    |\n|Comedy   |null     |\n+---------+---------+\nonly showing top 5 rows\n\n\n\n\n# withColumn + F.lit : 컬럼 생성 후 값 채우기\ndf4 = df3.withColumn(\"c3\", F.lit(\"Korea\"))\ndf4.show(5, False)\n\n+---------+---------+-----+\n|cc[0]    |cc[1]    |c3   |\n+---------+---------+-----+\n|Adventure|Animation|Korea|\n|Adventure|Children |Korea|\n|Comedy   |Romance  |Korea|\n|Comedy   |Drama    |Korea|\n|Comedy   |null     |Korea|\n+---------+---------+-----+\nonly showing top 5 rows\n\n\n\n\ndf5 = df4.withColumn(\"c4\", F.lit(\"Test\"))\ndf5.show(5, False)\n\n+---------+---------+-----+----+\n|cc[0]    |cc[1]    |c3   |c4  |\n+---------+---------+-----+----+\n|Adventure|Animation|Korea|Test|\n|Adventure|Children |Korea|Test|\n|Comedy   |Romance  |Korea|Test|\n|Comedy   |Drama    |Korea|Test|\n|Comedy   |null     |Korea|Test|\n+---------+---------+-----+----+\nonly showing top 5 rows\n\n\n\n\n# explode : 각 행의 list-like값을, 모두 각자 다른 행으로 쪼개어 분리\n## 아래 예시로, df2의 첫 행의 데이터라 df6의 1~5행으로 나뉘고, 두번째 행이 6~8로 나뉨\ndf2.show(5,False)\n\n+-------------------------------------------------+\n|cc                                               |\n+-------------------------------------------------+\n|[Adventure, Animation, Children, Comedy, Fantasy]|\n|[Adventure, Children, Fantasy]                   |\n|[Comedy, Romance]                                |\n|[Comedy, Drama, Romance]                         |\n|[Comedy]                                         |\n+-------------------------------------------------+\nonly showing top 5 rows\n\n\n\n\ndf6 = df2.select(F.explode(col(\"cc\")))\ndf6.show(10, False)\n\n+---------+\n|col      |\n+---------+\n|Adventure|\n|Animation|\n|Children |\n|Comedy   |\n|Fantasy  |\n|Adventure|\n|Children |\n|Fantasy  |\n|Comedy   |\n|Romance  |\n+---------+\nonly showing top 10 rows\n\n\n\n\n# drop\ndf4.show(5)\n\n+---------+---------+-----+\n|    cc[0]|    cc[1]|   c3|\n+---------+---------+-----+\n|Adventure|Animation|Korea|\n|Adventure| Children|Korea|\n|   Comedy|  Romance|Korea|\n|   Comedy|    Drama|Korea|\n|   Comedy|     null|Korea|\n+---------+---------+-----+\nonly showing top 5 rows\n\n\n\n\ndf4.drop(\"cc[1]\", \"c3\").show(5, False)\n\n+---------+\n|cc[0]    |\n+---------+\n|Adventure|\n|Adventure|\n|Comedy   |\n|Comedy   |\n|Comedy   |\n+---------+\nonly showing top 5 rows\n\n\n\n\n# groupBy & countDistinct\n## genres기준으로, title의 Distinct값을 세서(unique)m  titleCount컬럼으로 저장\ndf7 = df1.groupBy(\"genres\").agg(F.countDistinct(\"title\").alias(\"titleCount\"))\ndf7.show(5, False)\n\n[Stage 253:================================================&gt;    (182 + 1) / 200]\n\n\n+------------------------------+----------+\n|genres                        |titleCount|\n+------------------------------+----------+\n|Comedy|Horror|Thriller        |17        |\n|Action|Drama|Horror           |1         |\n|Adventure|Sci-Fi|Thriller     |4         |\n|Action|Animation|Comedy|Sci-Fi|2         |\n|Action|Adventure|Drama|Fantasy|6         |\n+------------------------------+----------+\nonly showing top 5 rows\n\n\n\n\n[Stage 253:====================================================&gt;(198 + 1) / 200]\n\n                                                                                \n\n\n\n# sort & F.desc\ndf8 = df7.sort(F.desc(\"titleCount\"))\ndf8.show(10, False)\n\n[Stage 260:===================================================&gt; (195 + 1) / 200]\n\n\n+--------------------+----------+\n|genres              |titleCount|\n+--------------------+----------+\n|Drama               |1053      |\n|Comedy              |946       |\n|Comedy|Drama        |435       |\n|Comedy|Romance      |363       |\n|Drama|Romance       |349       |\n|Documentary         |339       |\n|Comedy|Drama|Romance|276       |\n|Drama|Thriller      |168       |\n|Horror              |167       |\n|Horror|Thriller     |135       |\n+--------------------+----------+\nonly showing top 10 rows\n\n\n\n\n                                                                                \n\n\n\n\nUDF(User Defined Function)\n\nimport pyspark.sql.functions as F\n\ndf1 = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"../data/movies.csv\")\ndf1.show(3, False)\n\n+-------+-----------------------+-------------------------------------------+\n|movieId|title                  |genres                                     |\n+-------+-----------------------+-------------------------------------------+\n|1      |Toy Story (1995)       |Adventure|Animation|Children|Comedy|Fantasy|\n|2      |Jumanji (1995)         |Adventure|Children|Fantasy                 |\n|3      |Grumpier Old Men (1995)|Comedy|Romance                             |\n+-------+-----------------------+-------------------------------------------+\nonly showing top 3 rows\n\n\n\n\nnewCol = F.udf(lambda v: v.upper())\ndf1.select(F.col(\"genres\"), newCol(F.col(\"genres\"))).show(1, False)\n\n+-------------------------------------------+-------------------------------------------+\n|genres                                     |&lt;lambda&gt;(genres)                           |\n+-------------------------------------------+-------------------------------------------+\n|Adventure|Animation|Children|Comedy|Fantasy|ADVENTURE|ANIMATION|CHILDREN|COMEDY|FANTASY|\n+-------------------------------------------+-------------------------------------------+\nonly showing top 1 row"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html",
    "href": "posts/prgms-sql-20240319/index.html",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#개요",
    "href": "posts/prgms-sql-20240319/index.html#개요",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#문제-조건에-맞는-도서-리스트-출력하기",
    "href": "posts/prgms-sql-20240319/index.html#문제-조건에-맞는-도서-리스트-출력하기",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "문제 : 조건에 맞는 도서 리스트 출력하기",
    "text": "문제 : 조건에 맞는 도서 리스트 출력하기\n\n문제 설명\n다음은 어느 한 서점에서 판매중인 도서들의 도서 정보(BOOK) 테이블입니다.\nBOOK 테이블은 각 도서의 정보를 담은 테이블로 아래와 같은 구조로 되어있습니다.\nColumn name Type Nullable Description BOOK_ID INTEGER FALSE 도서 ID CATEGORY VARCHAR(N) FALSE 카테고리 (경제, 인문, 소설, 생활, 기술) AUTHOR_ID INTEGER FALSE 저자 ID PRICE INTEGER FALSE 판매가 (원) PUBLISHED_DATE DATE FALSE 출판일\n\n\n문제\nBOOK 테이블에서 2021년에 출판된 ‘인문’ 카테고리에 속하는 도서 리스트를 찾아서 도서 ID(BOOK_ID), 출판일 (PUBLISHED_DATE)을 출력하는 SQL문을 작성해주세요. 결과는 출판일을 기준으로 오름차순 정렬해주세요.\n예시 예를 들어 BOOK 테이블이 다음과 같다면\nBOOK_ID CATEGORY AUTHOR_ID PRICE PUBLISHED_DATE 1 인문 1 10000 2020-01-01 2 경제 2 9000 2021-02-05 3 인문 2 11000 2021-04-11 4 인문 3 10000 2021-03-15 5 생활 1 12000 2021-01-10 조건에 속하는 도서는 도서 ID 가 3, 4인 도서이므로 다음과 같습니다.\nBOOK_ID PUBLISHED_DATE 3 2021-04-11 4 2021-03-15 그리고 출판일 오름차순으로 정렬하여야 하므로 다음과 같은 결과가 나와야 합니다.\nBOOK_ID PUBLISHED_DATE 4 2021-03-15 3 2021-04-11\n\n\n주의사항\nPUBLISHED_DATE의 데이트 포맷이 예시와 동일해야 정답처리 됩니다."
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#작성답안",
    "href": "posts/prgms-sql-20240319/index.html#작성답안",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT BOOK_ID, TO_CHAR(PUBLISHED_DATE, 'YYYY-MM-DD')\nFROM BOOK\nWHERE CATEGORY = '인문' AND EXTRACT(YEAR FROM PUBLISHED_DATE) = 2021\nORDER BY PUBLISHED_DATE ASC\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240319/index.html#정리",
    "href": "posts/prgms-sql-20240319/index.html#정리",
    "title": "[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기",
    "section": "정리",
    "text": "정리\n\nTO_CHAR(표시형식 변경)\n\n\nSELECT TO_CHAR(컬럼명, ‘표시형식’) FROM 테이블명 표시형식(연도 4자리) : TO_CHAR(컬럼명, ‘YYYY’) 표시형식(월) : TO_CHAR(컬럼명, ‘MM’) 표시형식(일, 연기준 1~366) : TO_CHAR(컬럼명, ‘DDD’) 표시형식(일, 일기준 1~31) : TO_CHAR(컬럼명, ‘DD’) 표시형식(일, 요일) : TO_CHAR(컬럼명, ‘D’) * 지역설정에 따라 시작요일이 달라짐  표시형식(분기) : TO_CHAR(컬럼명, ‘Q’) 표시형식(주, 연기준 1~53) : TO_CHAR(컬럼명, ‘WW’) 표시형식(주, 월기준 1~53) : TO_CHAR(컬럼명, ‘W’)  표시형식(요일, MON/월) : TO_CHAR(컬럼명, ‘DY’) 표시형식(요일, MONDAY/월요일) : TO_CHAR(컬럼명, ‘DAY’) 표시형식(월, JAN/1월) : TO_CHAR(컬럼명, ‘MON’) 표시형식(월, JANUARY/1월) : TO_CHAR(컬럼명, ‘MONTH’)  표시형식(시간, 12시간표기) : TO_CHAR(컬럼명, ‘HH12’) 표시형식(시간, 24시간표기) : TO_CHAR(컬럼명, ‘HH24’) 표시형식(분) : TO_CHAR(컬럼명, ‘MI’) 표시형식(초) : TO_CHAR(컬럼명, ‘SS’)\n\n\nAND(모두 만족) / OR(하나라도 만족) / NOT(조건과 맞지 않는)\n\n\n연산자 우선순위(참고용) 1 괄호 2 NOT 3 비교 (&gt; &lt; = !) 4 AND 5 OR\n\n\nORDER BY 컬럼명 ASC (오름차순)\nORDER BY 컬럼명 DESC (내림차순)"
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html",
    "href": "posts/coach-ds-20230506/index.html",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "",
    "text": "국민건강보험공단 건강검진정보를 활용한 간단한 분석 공공데이터포털 국민건강보험공단_건강검진정보\n실습 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html#개요",
    "href": "posts/coach-ds-20230506/index.html#개요",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "",
    "text": "국민건강보험공단 건강검진정보를 활용한 간단한 분석 공공데이터포털 국민건강보험공단_건강검진정보\n실습 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ds-20230506/index.html#데이터-불러오기",
    "href": "posts/coach-ds-20230506/index.html#데이터-불러오기",
    "title": "[간단분석] 공공데이터포털 건강검진정보 활용",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('국민건강보험공단_건강검진정보_20221231.CSV', encoding=\"cp949\")\n\n\ndf.columns\n\nIndex(['기준년도', '가입자일련번호', '시도코드', '성별', '연령대코드(5세단위)', '신장(5cm단위)',\n       '체중(5kg단위)', '허리둘레', '시력(좌)', '시력(우)', '청력(좌)', '청력(우)', '수축기혈압',\n       '이완기혈압', '식전혈당(공복혈당)', '총콜레스테롤', '트리글리세라이드', 'HDL콜레스테롤', 'LDL콜레스테롤',\n       '혈색소', '요단백', '혈청크레아티닌', '혈청지오티(AST)', '혈청지피티(ALT)', '감마지티피', '흡연상태',\n       '음주여부', '구강검진수검여부', '치아우식증유무', '치석'],\n      dtype='object')\n\n\n\n음주여부, 흡연상태, 연령대코드, 성별코드간의 상관관계\n\n상관계수 구하기\n\n\nfiltered = df[['음주여부','흡연상태','연령대코드(5세단위)','성별']].copy()\nfiltered.corr()\n\n\n\n\n\n\n\n\n음주여부\n흡연상태\n연령대코드(5세단위)\n성별\n\n\n\n\n음주여부\n1.000000\n0.252058\n-0.357367\n-0.254963\n\n\n흡연상태\n0.252058\n1.000000\n-0.073601\n-0.536833\n\n\n연령대코드(5세단위)\n-0.357367\n-0.073601\n1.000000\n0.042162\n\n\n성별\n-0.254963\n-0.536833\n0.042162\n1.000000\n\n\n\n\n\n\n\n\n상관계수 시각화 (Seaborn heatmap)\n\n\n#mask옵션 사용하고, annot옵션으로 수치표기, fmt옵션으로 소수자리 지정하여 heatmap으로 표현\nmask = np.triu(np.ones_like(filtered.corr(), dtype=bool))\nsns.heatmap(filtered.corr(), annot=True, fmt='.2f', mask=mask)\n\n\n\n\n\n\n\n\n\n상관관계가 강한 순으로 나열\n\n성별과 흡연, 음주와 연령의 뚜렷한 상관관계\n흡연과 음주의 약한 상관관계 &gt; [참고] -1.0과 -0.7 사이이면, 강한 음적 선형관계 -0.7과 -0.3 사이이면, 뚜렷한 음적 선형관계 -0.3과 -0.1 사이이면, 약한 음적 선형관계 -0.1과 +0.1 사이이면, 거의 무시될 수 있는 선형관계 +0.1과 +0.3 사이이면, 약한 양적 선형관계 +0.3과 +0.7 사이이면, 뚜렷한 양적 선형관계 +0.7과 +1.0 사이이면, 강한 양적 선형관계\n\n\n\n#상관계수로 표를 만들어, 강한 상관관계가 있는 순으로 조사해보았습니다 (절대값으로 내림차순)\ncon_index = []\ncon_value = []\ncon_value_abs= []\nfor i in filtered.corr().columns:\n    for j in filtered.corr().index:\n        con_index.append(i +\"/\" + j)\n        con_value.append(filtered.corr()[i][j])\n        con_value_abs.append(abs(filtered.corr()[i][j]))\npd.DataFrame({'상관계수':con_value,\n             '절대값':con_value_abs,\n             '항목':con_index}).sort_values(by=['절대값'],ascending=False)\n\n\n\n\n\n\n\n\n상관계수\n절대값\n항목\n\n\n\n\n0\n1.000000\n1.000000\n음주여부/음주여부\n\n\n5\n1.000000\n1.000000\n흡연상태/흡연상태\n\n\n10\n1.000000\n1.000000\n연령대코드(5세단위)/연령대코드(5세단위)\n\n\n15\n1.000000\n1.000000\n성별/성별\n\n\n7\n-0.536833\n0.536833\n흡연상태/성별\n\n\n13\n-0.536833\n0.536833\n성별/흡연상태\n\n\n2\n-0.357367\n0.357367\n음주여부/연령대코드(5세단위)\n\n\n8\n-0.357367\n0.357367\n연령대코드(5세단위)/음주여부\n\n\n3\n-0.254963\n0.254963\n음주여부/성별\n\n\n12\n-0.254963\n0.254963\n성별/음주여부\n\n\n1\n0.252058\n0.252058\n음주여부/흡연상태\n\n\n4\n0.252058\n0.252058\n흡연상태/음주여부\n\n\n6\n-0.073601\n0.073601\n흡연상태/연령대코드(5세단위)\n\n\n9\n-0.073601\n0.073601\n연령대코드(5세단위)/흡연상태\n\n\n11\n0.042162\n0.042162\n연령대코드(5세단위)/성별\n\n\n14\n0.042162\n0.042162\n성별/연령대코드(5세단위)\n\n\n\n\n\n\n\n\n\n흡연과 음주에 대한 추이\n\n앞서 약한 상관관계였던 흡연과 음주에 대한 인원추이\n\n\n# 코드를 실제값으로 변환\nsmoke = {1 : \"흡연안함\", 2: \"끊음\", 3: \"흡연중\"}\ndrink = {0: \"안마심\", 1: \"마심\"}\n\ndf['흡연상태'] = df['흡연상태'].replace(smoke)\ndf['음주여부'] = df['음주여부'].replace(drink)\n\n\n# '흡연상태','음주여부'만 가져와 crosstab\nfiltered_drink_smoke = df[['흡연상태','음주여부']]\ndf_crosstab = pd.crosstab(index=filtered_drink_smoke['음주여부'],columns=filtered_drink_smoke['흡연상태'])\ndf_crosstab\n\n\n\n\n\n\n\n흡연상태\n끊음\n흡연안함\n흡연중\n\n\n음주여부\n\n\n\n\n\n\n\n마심\n132750\n360530\n161804\n\n\n안마심\n33815\n280441\n30504\n\n\n\n\n\n\n\n\n# countplot 시각화\nsns.set_style('whitegrid')\nplt.rc('font',family=\"Malgun Gothic\")\nsns.countplot(data=filtered_drink_smoke, x='흡연상태', hue='음주여부', palette='Set1').set_title('흡연상태별 음주여부') # palette = 그래프테마\n\nText(0.5, 1.0, '흡연상태별 음주여부')\n\n\n\n\n\n\n\n\n\n\n흡연중이거나 했다가 끊은 경우에는 음주하는 사람의 비중이 높음\n흡연을 안하는 경우는 음주여부의 비중 차이가 크지 않음\n\n\n\n음주여부에 따른 콜레스테롤과 감마지티피의 관계\n\n감마지티피 : 알콜에 의한 간장애의 지표를 나타내는 검사항목\n각 항목을 산점도로 시각화하여 파악\n\n\ndf_temp = df[['총콜레스테롤','감마지티피','음주여부','흡연상태']]\n\n\n# lmplot 산점도\nplt.figure(figsize=(14,14))\nsns.lmplot(data=df_temp, x='총콜레스테롤', y='감마지티피', hue='음주여부', col='흡연상태',markers=['x','o'] # markers로 음주여부에 따라 o,x로\n           ,scatter_kws={'s':20}) # 뭉쳐진부분이 잘보이도록 scatter_kws로 점크기조정\n\n&lt;Figure size 1400x1400 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n대체로 특정 구역에 몰려있음, 체중 120kg 이상에 대해서 추가분석\n\n\nweight_over120 = df.loc[(df['체중(5kg단위)'] &gt;= 120), ['총콜레스테롤','감마지티피','음주여부','흡연상태']]\nplt.figure(figsize=(14,14))\nsns.lmplot(data=weight_over120, x='총콜레스테롤', y='감마지티피', hue='음주여부', col='흡연상태',markers=['x','o']\n           ,scatter_kws={'s':20})\n\n&lt;Figure size 1400x1400 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n흡연중인 경우, 흡연을 안하거나 끊은 경우보다 감마지피티의 값이 높게 분포\n음주여부에 따라 콜레스테롤의 차이는 크게 보이지 않으나, 감마지티피의 경우 모두 상승하는 경향\n\n\n\n연령대별 시력 확인\n\n# 별도로 정의해둔 연령코드 딕셔너리\nage_code = {1: '0~4세',\n 2: '5~9세',\n 3: '10~14세',\n 4: '15~19세',\n 5: '20~24세',\n 6: '25~29세',\n 7: '30~34세',\n 8: '35~39세',\n 9: '40~44세',\n 10: '45~49세',\n 11: '50~54세',\n 12: '55~59세',\n 13: '60~64세',\n 14: '65~69세',\n 15: '70~74세',\n 16: '75~79세',\n 17: '80~84세',\n 18: '85세+'}\n\n# 실명(시력 9.9) 제거\nsight_right = df.drop(df.loc[df['시력(우)']==9.9,].index)\nsight_left = df.drop(df.loc[df['시력(좌)']==9.9,].index)\n\n#숫자료 표기되는 연령대코드를 연령구간으로 표기할 index 생성 후 replace로 변경 (barplot의 index로 사용)\nage_code_right = []\nfor i in np.sort(sight_right['연령대코드(5세단위)'].unique()).tolist():\n    age_code_right.append(age_code[i])\nage_code_left = []\nfor i in np.sort(sight_left['연령대코드(5세단위)'].unique()).tolist():\n    age_code_left.append(age_code[i])\n\nsight_right['연령대코드(5세단위)'] = sight_right['연령대코드(5세단위)'].replace(age_code)\nsight_left['연령대코드(5세단위)'] = sight_left['연령대코드(5세단위)'].replace(age_code)\n\n\n#시력(좌), (우) 그래프를 식별하기 용이하도록 (좌)그래프를 회전\nfig, axs = plt.subplots(ncols=2, figsize=(12,5),\n                       gridspec_kw={'wspace':0.5},)\nfig.suptitle('연령별 시력 평균(좌, 우)').set_size(20) #제목\n\nsns.set_context('talk')\n\nsns.barplot(data = sight_left, x='시력(좌)', y='연령대코드(5세단위)', hue='성별', orient='h', errorbar=None, ax=axs[0],\n            order=age_code_left).invert_xaxis()\nsns.barplot(data = sight_right, x='시력(우)', y='연령대코드(5세단위)', hue='성별', orient='h', errorbar=None, ax=axs[1],\n            order=age_code_right)\n\nfor ax in axs:\n    ax.set_ylabel('연령대') # 코드가 아니므로 y축 이름을 연령대로 변경\n\n\n\n\n\n\n\n\n\n모든 연령대에서 특정 성별의 시력이 높음\n연령이 높아질수록 시력도 낮아지며, 좌우시력의 큰 차이는 보이지 않음"
  },
  {
    "objectID": "posts/dtcontest-ore-20240621/index.html",
    "href": "posts/dtcontest-ore-20240621/index.html",
    "title": "[공모전] 공공데이터 공모전-6(니켈기준 Plot짜보기)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(니켈기준 Plot짜보기)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240621/index.html#개요",
    "href": "posts/dtcontest-ore-20240621/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-6(니켈기준 Plot짜보기)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n각자 광물 하나씩 담당해서 러프하게 보고서 Plot짜보기"
  },
  {
    "objectID": "posts/dtcontest-ore-20240621/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240621/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-6(니켈기준 Plot짜보기)",
    "section": "내용정리",
    "text": "내용정리\n\n지난 회의정리\n\n국제거래 관련 데이터는 UN Comtrade database 활용하기\n아래 4개 광물에 대해 분석 진행하고, HSCODE기준 적용\n\n리튬 : 산화/수산화리튬(282520), 탄산리튬(283691)\n코발트 : 산화/수산화코발트(282200), 황산코발트(283329)\n망간 : 이산화망간(850610)\n니켈 : 산화/수산화니켈(282540), 황산니켈(283324)\n\n각자 광물을 1~2개 담당해서 공모전 보고서 양식에 맞게 고민해보기\n\n분석 배경\n어떤 Feature와 모델 사용\n기대효과\n\n\n\n\n회의내용에 대한 Self고찰 및 아이디어 Develope\n\n기존에 니켈에 대해 분석한게 있어서 해당 내용으로 정리해보기\n\n제안배경\n\n니켈의 용도\n니켈의 국가별 소비량 급증(공공데이터포털의 데이터 기준)\n\n2011년 대비 2배이상 전체 소비량 증가\n중국, 인도네시아를 중심으로 소비량이 대폭 증가함\n\n니켈의 국가별 생산량 편중(공공데이터포털의 데이터 기준)\n\n데이터에 Mine, Refined로 2종류가 있었음\n니켈은 해외에서 가공된 형태를 수입하여 사용, 국산화 진행중\n\nRefined만 추려서 산출하기로 함\n\n중국 34.3%, 인도네시아 20.3%로 2개 국가가 과반수\n\n\n제안배경 요약 : 니켈의 소비량 증가와 생산량 편중으로, 공급 리스크가 큰 상황에서 위기요소 탐지의 필요성 대두\n고려요소(사용하고자 한 Feature)\n\n생산국 수출량 : 광물의 특정 생산국 비중이 높은 점을 고려하여, 해당 국가의 광물 수출량의 이상탐지\n수입국 수입량 : 광물을 수입하는 타 국가의 수입량 증감을 확인하여 이상탐지\n생산국 판매액 : 주요 생산국의 대외 판매액을 기준으로 광물의 가격변동을 탐지\n\n기준 : FOB가격(물류나 보험료를 제외한 순수 물품가격)을 판매량으로 나누어 kg당 가격 산출\n\n다만 데이터를 보니 Null인 경우가 있어 primaryvalue라는 가격컬럼을 사용키로 함\n\n\nBDI(Baltic Dry Index) : 광물이 수입을 통해 조달된다는 점과, 광물운송은 벌크선을 통해 진행됨을 착안하여 물류문제에 대한 이상탐지 요소로 포함\n\n벌크선의 수요/가격에 대한 측면뿐 아니라 경기선행지표로도 사용되고 있음(원자재의 이동을 통해 생산증가 등의 예측)\n\n\n각 Feature의 전처리 방안\n\nPeriod를 기준으로 국가별 합산(groupby)\n사용하고자 하는 컬럼(가격, 수출입량)이 null인 경우에만 drop\nExponential smoothing 모델에 넣어 예측값 산출예정으로 yyyymmdd포맷으로 날짜 처리\n실제값+예측값을 합친 후 Anomality패키지(R의 AnomalyDetection 이상탐지 패키지)로 이상탐지\n각 Period별로 이상치가 발생한 Feature의 수를 합산하여 구간척도로 위기구간을 제안 (정상/경계/주의/위기 등)\n\n기대효과는 시간부족으로 향후 고민\n\n모델에서 예측한 미래구간에서 위기가 탐지되는 경우, 사전 구매 등 광물의 비축 등을 미리 진행\n위의 사항을 통한 공급망리스크 회피 및 자국 핵심산업 경쟁력 확보\n\n\n\n\n\n보고내용 Jupyter작성 초안\n\n제안배경\n\n작성시 참고사항 : 관련 현황 및 문제점 등 빅데이터 분석을 구상하고 제안한 배경에 대해서 작성 (활용 분야, 활용빈도, 중요성 등)\n\n\n니켈은 배터리 및 연료전지 소재, 다양한 촉매, 태양광 지지대의 도금, 내부식성이 필요한 해양구조물 등에 사용 (재생에너지, 친황경자동차 분야에 필수적)\n니켈의 국가별 소비량은 급증하고 있음 (보고서는 하단의 시각화 그래프만 사용)\n\n\n4차 핵심사업에 소요되는 광물로 2011년 대비 소비량 약 2배 증가\n\n\nimport requests\nimport json\nimport pandas as pd\n\n# 한국광해광업공단_광종별 소비현황에 대한 주소를 하단 `데이터 현황`에서 url부분에 붙여넣기\nurl = 'https://raw.githubusercontent.com/KR9268/db_datagokr/main/komir_consume.csv'\ndf_consume = pd.read_csv(url, encoding='cp949', low_memory=False)\ndf_consume_nickel = df_consume[df_consume['광종']=='니켈']\ndf_consume_nickel.head()\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n224\n9.334\n8.391\n7.965\n7.571\n6.667\n7.021\n8.423\n7.978\n9.052\n10.016\n6.101\n7.152\n8.386\n니켈\nAustria\n천톤\nEUROPE\nrefined\n\n\n225\n29.794\n18.827\n26.000\n29.129\n35.022\n31.415\n29.080\n34.137\n35.190\n33.392\n34.046\n25.826\n34.825\n니켈\nBelgium\n천톤\nEUROPE\nrefined\n\n\n226\n0.157\n0.125\n0.141\n0.137\n0.167\n0.120\n0.114\n0.134\n0.149\n0.179\n0.211\n0.185\n0.149\n니켈\nBulgaria\n천톤\nEUROPE\nrefined\n\n\n227\n3.595\n1.891\n0.410\n3.011\n2.882\n2.997\n2.870\n2.163\n2.223\n2.000\n2.297\n1.513\n1.827\n니켈\nCzech Republic\n천톤\nEUROPE\nrefined\n\n\n228\n0.046\n0.189\n0.063\n0.056\n0.242\n0.372\n0.346\n0.589\n0.544\n0.397\n0.269\n0.560\n0.517\n니켈\nDenmark\n천톤\nEUROPE\nrefined\n\n\n\n\n\n\n\n\n# 단위가 여럿인 경우 등 확인 후 통일\nfor each_column in df_consume_nickel.columns:\n    if '소비량' not in each_column:\n        print(f\"{each_column} : {df_consume_nickel[each_column].unique()}\")\n\n광종 : ['니켈']\n국가 : ['Austria' 'Belgium' 'Bulgaria' 'Czech Republic' 'Denmark' 'Finland'\n 'France' 'Germany' 'Greece' 'Hungary' 'Italy' 'Macedonia' 'Norway'\n 'Poland' 'Portugal' 'Romania' 'Russia' 'Serbia' 'Slovenia' 'Spain'\n 'Sweden' 'Switzerland' 'Ukraine' 'United Kingdom' 'South Africa' 'China'\n 'Hong Kong' 'India' 'Indonesia' 'Japan' 'Kazakhstan' 'Malaysia'\n 'North Korea' 'Philippines' 'Saudi Arabia' 'Singapore' 'South Korea'\n 'Taiwan' 'Thailand' 'Turkey' 'United Arab Emirates' 'Vietnam' 'Argentina'\n 'Brazil' 'Canada' 'Chile' 'Cuba' 'Mexico' 'U.S.A.' 'Australia'\n 'New Zealand']\n단위 : ['천톤']\n대륙 : ['EUROPE' 'AFRICA' 'ASIA' 'AMERICA' 'OCEANIA']\n품목 : ['refined']\n\n\n\n# 단일 항목인 광종, 단위, 품목 제거 (메모리확보 7.6+ KB → 6.4+ KB)\ndf_consume_nickel = df_consume_nickel[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량', '2023 소비량', '국가', '대륙','품목']]\ndf_consume_nickel.head()\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n국가\n대륙\n품목\n\n\n\n\n224\n9.334\n8.391\n7.965\n7.571\n6.667\n7.021\n8.423\n7.978\n9.052\n10.016\n6.101\n7.152\n8.386\nAustria\nEUROPE\nrefined\n\n\n225\n29.794\n18.827\n26.000\n29.129\n35.022\n31.415\n29.080\n34.137\n35.190\n33.392\n34.046\n25.826\n34.825\nBelgium\nEUROPE\nrefined\n\n\n226\n0.157\n0.125\n0.141\n0.137\n0.167\n0.120\n0.114\n0.134\n0.149\n0.179\n0.211\n0.185\n0.149\nBulgaria\nEUROPE\nrefined\n\n\n227\n3.595\n1.891\n0.410\n3.011\n2.882\n2.997\n2.870\n2.163\n2.223\n2.000\n2.297\n1.513\n1.827\nCzech Republic\nEUROPE\nrefined\n\n\n228\n0.046\n0.189\n0.063\n0.056\n0.242\n0.372\n0.346\n0.589\n0.544\n0.397\n0.269\n0.560\n0.517\nDenmark\nEUROPE\nrefined\n\n\n\n\n\n\n\n\n전체 소비량 증가\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport koreanize_matplotlib\n\nplt.figure(figsize=(20,4))\nsns.lineplot(df_consume_nickel.sum(numeric_only=True))\nplt.title('니켈 소비량 추이')\n\nText(0.5, 1.0, '니켈 소비량 추이')\n\n\n\n\n\n\n\n\n\n\n국가별 소비량 증가\n\n중국, 인도네시아를 중심으로 2011년 대비 소비량 대폭 증가\n\n\n\nnickel_by_country = df_consume_nickel.groupby('국가').sum(numeric_only=True)\nnickel_by_country.head(5)\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n\n\n국가\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArgentina\n0.974\n0.806\n0.892\n0.501\n0.528\n0.475\n0.535\n0.523\n0.526\n0.274\n0.544\n0.443\n0.452\n\n\nAustralia\n1.600\n1.600\n1.600\n1.600\n1.608\n1.600\n1.600\n1.600\n1.600\n1.600\n1.600\n1.600\n1.600\n\n\nAustria\n9.334\n8.391\n7.965\n7.571\n6.667\n7.021\n8.423\n7.978\n9.052\n10.016\n6.101\n7.152\n8.386\n\n\nBelgium\n29.794\n18.827\n26.000\n29.129\n35.022\n31.415\n29.080\n34.137\n35.190\n33.392\n34.046\n25.826\n34.825\n\n\nBrazil\n26.910\n22.455\n21.180\n26.616\n18.062\n25.506\n22.269\n22.376\n16.480\n12.062\n17.697\n17.782\n11.645\n\n\n\n\n\n\n\n\n# 표준편차가 상위 10개 국가 추출\ndescribed_nickel_by_country = nickel_by_country.transpose().describe()\nstd_described_nickel_by_country = described_nickel_by_country.loc['std'].sort_values(ascending=False)[0:10]\nstd_described_nickel_by_country\n\n국가\nChina           440.092028\nIndonesia       154.306424\nIndia            18.577287\nU.S.A.           16.972706\nTaiwan           16.607094\nSouth Africa     12.951598\nGermany          12.118787\nJapan             9.753820\nSouth Korea       9.739035\nItaly             9.129311\nName: std, dtype: float64\n\n\n\n# 표준편차 상위 10개국 그래프\nindex_country = std_described_nickel_by_country.index.tolist()\n\nplt.figure(figsize=(20,5))\nsns.lineplot(nickel_by_country.transpose()[index_country])\n\n\n\n\n\n\n\n\n\n# 표준편차 상위 10개국 중 중국제외한 그래프\nindex_country.remove('China')\n\nplt.figure(figsize=(20,5))\nsns.lineplot(nickel_by_country.transpose()[index_country])\n\n\n\n\n\n\n\n\n\n# 표준편차 상위 10개국 중 인도네시아 제외한 그래프\nindex_country.remove('Indonesia')\n\nplt.figure(figsize=(20,5))\nsns.lineplot(nickel_by_country.transpose()[index_country])\n\n\n\n\n\n\n\n\n\n니켈의 생산량\n\n\n주요 2개국 생산량이 58.3%로 편중(인도네시아, 중국, Refined니켈 기준)\n\n\n# 한국광해광업공단_광종별 소비현황에 대한 주소를 하단 `데이터 현황`에서 url부분에 붙여넣기\nurl = 'https://raw.githubusercontent.com/KR9268/db_datagokr/main/komir_product_country.csv'\ndf_product = pd.read_csv(url, encoding='cp949', low_memory=False)\ndf_product.head()\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n0\n561.354\n800.316\n657.1\n605.215\n787.404\n738.612\n740.380\n760.244\n1043.343\n619.748\n675.269\n669.926\n542.114\n알루미늄\nBosnia\n천톤\nEUROPE\nbauxite\n\n\n1\n0.000\n0.000\n0.0\n0.000\n11.900\n9.800\n12.200\n11.800\n14.300\n14.100\n14.500\n13.800\n13.800\n알루미늄\nCroatia\n천톤\nEUROPE\nbauxite\n\n\n2\n80.800\n90.129\n100.0\n71.100\n70.000\n110.000\n110.000\n110.000\n120.760\n123.496\n142.764\n120.000\n120.000\n알루미늄\nFrance\n천톤\nEUROPE\nbauxite\n\n\n3\n2324.000\n1815.328\n1844.0\n1876.000\n1831.270\n1880.000\n1927.145\n1559.360\n1379.123\n1428.639\n1227.000\n1173.000\n869.100\n알루미늄\nGreece\n천톤\nEUROPE\nbauxite\n\n\n4\n277.800\n255.100\n93.7\n14.400\n8.300\n16.700\n4.000\n5.000\n0.000\n0.000\n0.000\n0.000\n0.000\n알루미늄\nHungary\n천톤\nEUROPE\nbauxite\n\n\n\n\n\n\n\n\ndf_nickel_product = df_product[df_product['광종']=='니켈']\ndf_nickel_product.head(5)\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n442\n3.528\n0.728\n2.086\n4.889\n6.309\n3.952\n5.301\n4.204\n2.830\n3.764\n3.615\n1.423\n0.548\n니켈\nAlbania\n천톤\nEUROPE\nmine\n\n\n443\n19.081\n19.955\n19.579\n19.830\n10.643\n22.034\n36.201\n43.572\n38.530\n41.429\n42.098\n60.360\n69.568\n니켈\nFinland\n천톤\nEUROPE\nmine\n\n\n444\n21.100\n21.550\n19.350\n21.410\n19.750\n19.394\n19.080\n17.890\n13.715\n7.060\n4.755\n1.680\n0.000\n니켈\nGreece\n천톤\nEUROPE\nmine\n\n\n445\n7.632\n4.436\n7.607\n6.724\n6.650\n4.306\n7.120\n4.791\n3.314\n3.958\n4.735\n0.339\n0.360\n니켈\nKosovo\n천톤\nEUROPE\nmine\n\n\n446\n3.600\n1.704\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n니켈\nMacedonia\n천톤\nEUROPE\nmine\n\n\n\n\n\n\n\n\n# 단위가 여럿인 경우 등 확인 후 통일\nfor each_column in df_nickel_product.columns:\n    if '생산량' not in each_column:\n        print(f\"{each_column} : {df_nickel_product[each_column].unique()}\")\n\n광종 : ['니켈']\n국가 : ['Albania' 'Finland' 'Greece' 'Kosovo' 'Macedonia' 'Norway' 'Poland'\n 'Russia' 'Spain' 'Botswana' 'Ivory Coast' 'Madagascar' 'Morocco'\n 'South Africa' 'Zambia' 'Zimbabwe' 'China' 'Indonesia' 'Kazakhstan'\n 'Myanmar' 'Philippines' 'Turkey' 'Vietnam' 'Brazil' 'Canada' 'Colombia'\n 'Cuba' 'Dominican Republic' 'Guatemala' 'U.S.A.' 'Venezuela' 'Australia'\n 'New Caledonia' 'Papua New Guinea' 'Austria' 'France' 'Ukraine'\n 'United Kingdom' 'India' 'Japan' 'South Korea']\n단위 : ['천톤']\n대륙 : ['EUROPE' 'AFRICA' 'ASIA' 'AMERICA' 'OCEANIA']\n품목 : ['mine' 'refined']\n\n\n\n# 단일 항목인 광종, 단위, 품목 제거 (메모리확보 9.4+ KB → 8.4+ KB)\ndf_nickel_product = df_nickel_product[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량', '2023 생산량', '국가', '대륙','품목']]\ndf_nickel_product.head()\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n국가\n대륙\n품목\n\n\n\n\n442\n3.528\n0.728\n2.086\n4.889\n6.309\n3.952\n5.301\n4.204\n2.830\n3.764\n3.615\n1.423\n0.548\nAlbania\nEUROPE\nmine\n\n\n443\n19.081\n19.955\n19.579\n19.830\n10.643\n22.034\n36.201\n43.572\n38.530\n41.429\n42.098\n60.360\n69.568\nFinland\nEUROPE\nmine\n\n\n444\n21.100\n21.550\n19.350\n21.410\n19.750\n19.394\n19.080\n17.890\n13.715\n7.060\n4.755\n1.680\n0.000\nGreece\nEUROPE\nmine\n\n\n445\n7.632\n4.436\n7.607\n6.724\n6.650\n4.306\n7.120\n4.791\n3.314\n3.958\n4.735\n0.339\n0.360\nKosovo\nEUROPE\nmine\n\n\n446\n3.600\n1.704\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nMacedonia\nEUROPE\nmine\n\n\n\n\n\n\n\n\n# 니켈에 대한 국가별 생산량\ndf2_produce_country_mine = df_nickel_product[df_nickel_product['품목']=='mine'].groupby(by=['국가'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량', '2023 생산량']].sum()\ndf2_produce_country_refined = df_nickel_product[df_nickel_product['품목']=='refined'].groupby(by=['국가'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량', '2023 생산량']].sum()\n\n\n# 니켈에 대한 국가별 생산량의 총 합계(Mine)\ndf2_produce_country_total_mine = df2_produce_country_mine[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량', '2023 생산량']].sum(axis=1).sort_values(ascending=False)\ndf2_produce_country_total_mine[0:10]\n\n국가\nIndonesia        9543.539\nPhilippines      4762.154\nRussia           3057.635\nAustralia        2612.548\nNew Caledonia    2453.547\nCanada           2448.002\nChina            1274.216\nBrazil            988.699\nCuba              680.208\nSouth Africa      561.815\ndtype: float64\n\n\n\n# 니켈에 대한 국가별 생산량의 총 합계(Refined)\ndf2_produce_country_total_refined = df2_produce_country_refined[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량', '2023 생산량']].sum(axis=1).sort_values(ascending=False)\ndf2_produce_country_total_refined[0:10]\n\n국가\nChina            8813.411\nIndonesia        5204.761\nRussia           2458.088\nJapan            2295.233\nCanada           1742.140\nAustralia        1521.416\nNorway           1178.114\nNew Caledonia     936.948\nBrazil            809.897\nFinland           712.412\ndtype: float64\n\n\n\n# 니켈 총생산량 내림차순 기준 상위 10개국 Pie chart (Refined한정)\ntarget_country =df2_produce_country_total_refined[0:10][0:10].index.tolist()\ndf2_produce_country_total_refined.loc[target_country].plot(kind='pie',startangle=145, autopct='%.1f%%', pctdistance=0.8)\nplt.title('국가별 니켈 총생산량 비중(2011~2023,  Refined)')\n\nText(0.5, 1.0, '국가별 니켈 총생산량 비중(2011~2023,  Refined)')\n\n\n\n\n\n\n\n\n\n\n\n분석 내용 및 분석 결과\n\n작성시 참고사항 : 구체적인 내용을 자유롭게 기술하되, 세부적이고 구체적으로 작성\n\n\n고려사항(사용하고자 한 Feature)\n\n아래의 상황을 모델에 포함하여 고려하고자 하였습니다\n\n생산국 수출량 : 광물의 특정 생산국 비중이 높은 점을 고려하여, 해당 국가의 광물 수출량의 이상탐지\n수입국 수입량 : 광물을 수입하는 타 국가의 수입량 증감을 확인하여 이상탐지\n생산국 판매액 : 주요 생산국의 대외 판매액을 기준으로 광물의 가격변동을 탐지\n\n기준 : FOB가격(물류나 보험료를 제외한 순수 물품가격)을 판매량으로 나누어 kg당 가격 산출\n\nBDI(Baltic Dry Index) : 광물이 수입을 통해 조달된다는 점과, 광물운송은 벌크선을 통해 진행됨을 착안하여 물류문제에 대한 이상탐지 요소로 포함\n\n\n\nimport pandas as pd\nimport xlwings as xw\n\ndef filter_df(df:pd.DataFrame, from_country:str, export:bool, returnvalue:str=None, to_country:str=None):\n    '''\n        dataframe에서 특정 옵션 데이터를 필터링 하기 위한 함수\n    '''\n    flowCode = 'X' if export is True else 'M'\n    filter_to_country = (df['partnerDesc']==to_country) if to_country is not None else True\n\n    # DataFrame복사\n    df_copy = df[(df['reporterDesc']==from_country) & filter_to_country & (df['flowCode']==flowCode)].copy()\n\n    # DataFrame전처리 : 날짜값 변환(str로변환, 연/월 분리)\n    df_copy['period'] = df_copy['period'].astype('int').astype('str')\n    df_copy['period_year'] = df_copy['period'].str[:4]\n    df_copy['period_month'] = df_copy['period'].str[-2:]\n    df_copy['period_dateformat'] = pd.to_datetime(df_copy['refPeriodId'], format='%Y%m%d')\n\n    # 반환 컬럼\n    if returnvalue is not None:\n        df_copy = df_copy[returnvalue]\n    else:\n        pass\n\n    return df_copy\n\ndef preprocessing_kpi_uscomtrade(df:pd.DataFrame, type:str):\n    df[['netWgt','primaryValue']] = df[['netWgt','primaryValue']].dropna()\n\n    # 출발국가에 따라 기준값 세팅\n    if df['reporterDesc'].unique() in ['China','Indonesia']:\n        criteria_wt = 'netWgt'\n        criteria_value = 'primaryValue'\n    else:\n        criteria_wt = 'netWgt'\n        criteria_value = 'primaryValue'\n\n    # 기간(period출력)\n    #period = 'period'\n    period = 'period_dateformat'\n\n    # 산출\n    if type == 'Price':\n        df_return = df.groupby(period).sum(numeric_only=True)[criteria_value] / df.groupby(period).sum(numeric_only=True)[criteria_wt]\n    elif type == 'ExportQty':\n        df_return = df.groupby(period).sum(numeric_only=True)[criteria_wt]\n    elif type == 'ImportQty':\n        temp_dict = {}\n        for each_country in df['partnerDesc'].unique():\n            if each_country != 'World':\n                temp_df = df[df['partnerDesc']==each_country]\n                temp_dict[each_country] = temp_df.groupby(period).sum(numeric_only=True)['netWgt']\n        return temp_dict\n    \n    return df_return\n\n\n\n지표별 데이터 전처리(Null제거 및 기준별 Groupby 및 계산처리)\n\n중국과 인도네시아의 수출가격(니켈)\n중국과 인도네시아의 수출량(니켈)\n국가별 수입량(니켈)\n\n\nwb = xw.Book('Z:\\\\GoogleDrive\\\\공공데이터 활용 아이디어 공모전\\\\282540,283324_merged.csv')\nsht = wb.sheets[0]\nrange_df = sht.range('A2:BL'+str(sht.used_range.last_cell.row))\ndf_un_nickel_all = sht.used_range.options(pd.DataFrame,index=0).value\nwb.close()\n\ndf_un_nickel_all\n\n\n\n\n\n\n\n\ntypeCode\nfreqCode\nrefPeriodId\nrefYear\nrefMonth\nperiod\nreporterCode\nreporterISO\nreporterDesc\nflowCode\n...\nnetWgt\nisNetWgtEstimated\ngrossWgt\nisGrossWgtEstimated\ncifvalue\nfobvalue\nprimaryValue\nlegacyEstimationFlag\nisReported\nisAggregate\n\n\n\n\n0\nC\nM\n20110101.0\n2011.0\n1.0\n201101.0\n32.0\nARG\nArgentina\nM\n...\n1500.000\nFalse\n0.0\nFalse\n4.060111e+04\n0.000\n4.060111e+04\n0.0\nFalse\nTrue\n\n\n1\nC\nM\n20110101.0\n2011.0\n1.0\n201101.0\n36.0\nAUS\nAustralia\nM\n...\n120.000\nFalse\n0.0\nFalse\n1.844344e+04\n0.000\n1.844344e+04\n0.0\nFalse\nTrue\n\n\n2\nC\nM\n20110101.0\n2011.0\n1.0\n201101.0\n36.0\nAUS\nAustralia\nX\n...\n12000.000\nFalse\n0.0\nFalse\n0.000000e+00\n11942.675\n1.194267e+04\n0.0\nFalse\nTrue\n\n\n3\nC\nM\n20110101.0\n2011.0\n1.0\n201101.0\n40.0\nAUT\nAustria\nM\n...\n0.000\nFalse\n0.0\nFalse\n5.477600e+02\n0.000\n5.477600e+02\n0.0\nFalse\nTrue\n\n\n4\nC\nM\n20110101.0\n2011.0\n1.0\n201101.0\n56.0\nBEL\nBelgium\nM\n...\n6609.260\nFalse\n0.0\nFalse\n1.813610e+05\n0.000\n1.813610e+05\n0.0\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n105246\nC\nM\n20240401.0\n2024.0\n4.0\n202404.0\n392.0\nJPN\nJapan\nX\n...\n90547.000\nFalse\n0.0\nFalse\nNaN\n294094.768\n2.940948e+05\n0.0\nTrue\nFalse\n\n\n105247\nC\nM\n20240401.0\n2024.0\n4.0\n202404.0\n376.0\nISR\nIsrael\nM\n...\n3178.379\nTrue\n0.0\nFalse\n1.800000e+04\nNaN\n1.800000e+04\n6.0\nFalse\nTrue\n\n\n105248\nC\nM\n20240401.0\n2024.0\n4.0\n202404.0\n376.0\nISR\nIsrael\nM\n...\n529.730\nTrue\n0.0\nFalse\n3.000000e+03\nNaN\n3.000000e+03\n6.0\nTrue\nFalse\n\n\n105249\nC\nM\n20240401.0\n2024.0\n4.0\n202404.0\n376.0\nISR\nIsrael\nM\n...\n2648.649\nTrue\n0.0\nFalse\n1.500000e+04\nNaN\n1.500000e+04\n6.0\nTrue\nFalse\n\n\n105250\nC\nM\n20240401.0\n2024.0\n4.0\n202404.0\n392.0\nJPN\nJapan\nM\n...\n2502000.000\nFalse\n0.0\nFalse\n1.017666e+07\nNaN\n1.017666e+07\n0.0\nFalse\nTrue\n\n\n\n\n105251 rows × 47 columns\n\n\n\n\n# 인도네시아의 수출가격, 수출량, 수입량\nprice_indonesia_raw = filter_df(df=df_un_nickel_all, from_country='Indonesia', to_country=None, export=True, returnvalue=None)\n\nprice_indonesia = preprocessing_kpi_uscomtrade(price_indonesia_raw, 'Price')\nexport_qty_indonesia = preprocessing_kpi_uscomtrade(price_indonesia_raw, 'ExportQty')\nimport_qty_from_indonesia = preprocessing_kpi_uscomtrade(price_indonesia_raw, 'ImportQty')\n\n\n# 중국의 수출가격, 수출량, 수입량\nprice_china_raw = filter_df(df=df_un_nickel_all, from_country='China', to_country=None, export=True, returnvalue=None)\n\nprice_china = preprocessing_kpi_uscomtrade(price_china_raw, 'Price')\nexport_qty_china =  preprocessing_kpi_uscomtrade(price_china_raw, 'ExportQty')\nimport_qty_from_china = preprocessing_kpi_uscomtrade(price_china_raw, 'ImportQty')\n\n\n\n각 지표에 대해 예측수행\n\nExponential smoothing으로 예측\n\n타 팀원의 LSTM활용한 모델 등을 활용할 것으로 예상\n\n데이터를 시각화하여 계절성 등을 먼저 확인하고자 함\n데이터 확인용 시각화\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport koreanize_matplotlib\n\n\n%matplotlib inline\ndatasource = pd.DataFrame(price_china)\n\nplt.figure(figsize=(20,4))\nplt.xticks(rotation=45)\n#plt.gca().set_xticks(datasource.index)\nsns.lineplot(datasource)\n\n\n\n\n\n\n\n\n\nExponential Smoothing모델로 예측\n\n\n# ExponentialSmoothing 모델 활용\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\ndf = price_china\n\ndef predict_exponential(df:pd.DataFrame, month_to_expect:int):\n# 모델 학습\n    model = ExponentialSmoothing(df.values, trend='add', seasonal='add', seasonal_periods=12)\n    fit = model.fit()\n\n    # 예측 수행 (12개월)\n    forecast = fit.forecast(steps=month_to_expect)\n\n    # 날짜배열 생성\n    months= []\n    start_month = max(df.index)+ relativedelta(months=1)\n    end_month = max(df.index) + relativedelta(months=month_to_expect)\n    current_month = start_month\n    while current_month &lt;= end_month:\n        months.append(current_month)\n        current_month += relativedelta(months=1)\n\n    # 날짜+예측값 Series 생성\n    forecast = pd.Series(data=forecast, index=months)\n    return forecast\n\nforecast = predict_exponential(df=price_china, month_to_expect=12)\nforecast\n\n2024-01-01    14.324603\n2024-02-01    15.397100\n2024-03-01    14.578083\n2024-04-01    14.589412\n2024-05-01    15.382925\n2024-06-01    15.244014\n2024-07-01    15.578931\n2024-08-01    15.535253\n2024-09-01    15.241216\n2024-10-01    15.326029\n2024-11-01    15.416176\n2024-12-01    15.462405\ndtype: float64\n\n\n\n# 예측 결과 시각화\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df.values, label='실제 데이터')\nplt.plot(forecast.index, forecast, label='예측 데이터', linestyle='--')\nplt.xlabel('날짜')\nplt.ylabel('수급안정화지수')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n예측값을 데이터에 추가\n\nprice_china_plus_forecast = pd.concat([price_china, forecast])\nprice_indonesia_plus_forecast = pd.concat([price_indonesia, forecast])\n\n\n\n각 지표에 대해 이상탐지 (R Abnomality 패키지 활용)\n\n중국과 인도네시아의 수출가격(니켈) : 기간별 가격(예측치 포함)에 대한 이상탐지\n중국과 인도네시아의 수출량(니켈) : 기간별 수출량(예측치 포함)에 대한 이상탐지\n국가별 수입량(니켈) : 국가별 수입량(예측치 포함)에 대한 이상탐지\n수급안정화지수 : 예측치에 대한 이상탐지\nBDI : 예측치에 대한 이상탐지\nRStudio 등 설치했으나 구동문제가 있어, 팀원 상의 후에도 이 모델을 사용하는 경우 조치 예정\n\n\n\n이상탐지 결과 취합 및 위기요소 판단\n\n각 지표에 대해 아래와 같은 평가점수를 부여\n\n이상없음 0 / 이상탐지 1\n\n이상 탐지된 지표 수에 따라 구간별 위기상황 부여 (0정상 1경계 2주의 3위기 등)\n\n\n\n\n활용데이터\n\n분석 대상이 되는 데이터와 이에 대한 항목을 모두 작성\n\n공공데이터포털 수급안정화 지수\n\n외부 데이터를 활용하였다면 그에 대한 세부적인 내용 기재 및 활용 이유 제시\n\nInvesting.com BDI지수\nUN Comtrade 수출입현황"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark, Docker\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#데이터-엔지니어링시-각-단계별로-필요한-주요-플랫폼",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#데이터-엔지니어링시-각-단계별로-필요한-주요-플랫폼",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터 엔지니어링시 각 단계별로 필요한 주요 플랫폼",
    "text": "데이터 엔지니어링시 각 단계별로 필요한 주요 플랫폼\n\n수업에서 사용할 플랫폼은 Bold표기(데이터 분석/정제를 자동으로 수행하는 workflow파이프라인 제작 예정)\n\n언어 : SQL, Shell(터미널 사용), Java, Kotlin, Scala, Python\n대용량 데이터 인프라 (HDFS[Hadoop File System])\n데이터 수집 (Kafka, Logstash)\n데이터 저장/질의 (MySQL, Hive, ES[Elastic Search, 저장], MongoDB)\n\nElastic search는 검색엔진이어서 의아할 수 있으나, ELK스택(로그관리, 저장)할 때 많이 사용. kibana와 연동한 쉬운 시각화 가능\n\n데이터 처리 (Hadoop MR[MapReduce], Spark, Flink(실시간 처리 용도), Spark Stream)\nWorkflow (Airflow, Ooozie)\n데이터(모델) 서빙 (GraphQL, REST API, 웹 개발)\n데이터 시각화 (분석가가 더 많이해서 비중은 적음. 차트 종류와 특징, kibana)\n기타: 분석 도구 (Zeppelin), 데이터 설계, 머신러닝, 보안\n\n수업마다 왜 이 플랫폼을 선택했는지를 설명예정 \n스터디 방향\n\n나의 플랫폼 선택이유를 설명할 수 있어야 함\n\n수업마다 왜 이 플랫폼을 선택했는지를 설명예정\n\n대용량 데이터 처리 파이프라인에 대한 이해(대용량 분산처리를 위한 Spark이해)\n데이터에 따라 정제 과정, 데이터 구조 설계\n\nFancy하지 않고 고민 오래하여 만들기\n\nSpark의 기본구조에 대한 이해 (+이직 등 목표라면 이론도 함께)\nAirflow DAG구현\nES, Kibana : 위 항목보단 덜 중요하지만 기본구조는 이해 \n\n스터디 준비\n\nTerminal(윈도우라면 WSL설치)\nDocker(ubuntu 20.04기준), Docker-compose\nResource (메모리 등)\nIDE (Visual Studio Code 등) \n\n수업목표\n\n데이터를 주기적으로 수집하고 pyspark를 사용하여 정제\n정제한 데이터를 저장, kibana로 시각화\n자동 파이프라인 실행을 위한 Airflow job 생성\n수업파일 git주소 : : https://github.com/kmin-z/de-2024.git\n\n수업환경 관련 가이드사항\n\nJupyter 관련 문제 발생시, 이미 8888포트 사용중인지 확인하고, 사용중이라면 변경\n\n[향후 참고용] 문제에 대해 별도로 알아본 내역\n\nubuntu 내부에서 git을 설치한 후 clone할 수 있다\n  sudo snap install gh   # git 설치\n  git clone https://github.com/kmin-z/de-2024.git   # 레포 받기\ndocker 실행시 sudo를 계속 사용해야하는 경우 : docker user관련 설정\n\n공식문서 링크 : https://docs.docker.com/engine/install/linux-postinstall/\n\n\nWSL버전 문제 : 패키지 설치가 안되거나 symbolic link 등 많은 문제가 있었는데 WSL을 오래전에 깔아둔 문제였음 powershell에서 wsl --install 로 재설치하여 해결"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-개요",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-개요",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Spark 개요",
    "text": "Spark 개요\n\nData Warehouse vs Data Lake vs Data Lakehouse\n\nData Warehouse → Data Lake → Data Lakehouse로 발전\nData Warehouse : (구조가 정해져있는)Structured data를 저장. BI나 Report를 생성\nData Lake : (데이터가 다양해지며) Structured와 함께, Semi-structured, Unstructured(이미지나 사운드) Data도 저장\n\nStructured data만 정제해서 Data warehouse로 저장(정제된 테이블데이터 생성)\n기존처럼 BI, Report를 생성하기도 하고, Data Science를 위해 여러 유형의 데이터를 활용하기도 함. 또는 머신러닝 등도 진행\n\nData Lakehouse : 데이터를 덤프해두고, 따로 Warehouse 등을 저장해두지 않고, 사용자가 원하는 format으로 뽑아서 사용\n\nMetadata, Governance Layer 등으로 데이터를 가져가기 쉬운 장치를 해둠. (과거유형의 정제해두지 않으면 데이터를 사용하기 어려운 것과 달리)\n요즘의 트렌드(데이터를 저장해두고 사용하기 쉽게 해둠)\n\n과거에는 정제해야하는 사람이 있었다면, 이 방법은 데이터를 붓는 쪽과 사용하는 쪽으로 나뉨 \n\n\n\nDistributed System (Spark이해하기 위해 알아야 하는 개념)\n\n간단한 실습이나 데이터 사이언스는 Local에서 가능했겠지만,\n필요한 요소\n\nResource management : 데이터를 적절히 분배하며 저장하는 Resource관리\nScheduling : 여러 대의 서버가 Job을 어떻게 나누어 돌릴지\n\nHadoop이 위 두가지 요소의 역할을 함 : DFS(Distributed File System)을 구축한 프로젝트임\n\n큰 데이터를 어떻게 적절히 나누어 저장할지 등의 결정을 해줌\n\n기존에는 대용량 데이터 처리를 위해 MapReduce를 많이 사용했는데, 구현과 테스트가 매우 힘듦(+Hadoop)\n\n직접 해보면 MR Job은 잘 죽음\n\nSpark : Hadoop의 한계를 벗어나기 위한 프로젝트 \n\nSpark : unified analytics engine for large-scale data processing\n\n데이터 수집, 저장, 모니터링 등 여러 과정 중 주로 데이터 처리를 위해 사용\n(데이터 처리에 사용하는 만큼)다양한 데이터 포맷, 처리 방법, 저장소 사용가능\n\n다양한 high level tool 제공\n\nSQL, pandas API, MLlib(간단한 머신러닝)\nGraphX(그래프 알고리즘을 분산으로 돌려볼 수 있음)\nStructured Streaming(실시간 데이터 처리) \n\n\n\nSpark History\n\n2009년 캘리포니아대 버클리캠퍼스 AMP Lab에서 개발한, MapReduce 단점개선 프로젝트\n2013년 Apache 프로젝트에 등재됨\n자세히 설명된 위키피디아 내용 : https://en.wikipedia.org/wiki/Apache_Spark \n\nSpark Cons (장점, 선정한 이유로서 잘 기억하기)\n\n빠른 속도 (Logistics regression기준 Hadoop보다 100배 빠름)\n\n메모리에 데이터를 올려서 사용하는 플랫폼이므로, 무조건 디스크를 쓰는 Hadoop보다 빠름\n\n사용하기 편리함\n\n분산처리를 고려한 처리방식때문에 헷갈릴수는 있으나, SQL 등 친숙한 사용법\n\n다양한 기능 제공(Generality)\n\nSQL, Spark Streaming, MLlib 등 다양한 어플리케이션 제공\n\n다양한 Cluster, File system과 연동가능\n\nCluster마다 관리모듈이 다른데, Yarn, Mesos 등 다양한 Cluster를 붙여쓸 수 있는 플러그인 제공\n\nOption에 넣으면 해당 Cluster에 맞는 다양한 처리를 해줌\n\nFile system과 연동가능 : HDFS, Alluxio, Cassandra 등\n\n일반적인 작고 Local에서 돌리는 작업이라면 오히려 빠른 부분을 잘 못느낄 수 있음\n\n분산처리를 위한 추가적인 작업때문에 오히려 늦다고 느낄 수도 있음\n다만 대용량데이터를 다룰 때는 체감할 수 있다(다른 플랫폼은 돌아가지조차 않을 수 있음) \n\n\nSpark의 구조\n\nDriver(1개)와 Worker(n개)로 구성\nCluster manager는 Spark의 요소는 아님\n\nDriver와 Worker는 Cluster manager를 통해 소통하며 일을 함\n\nDriver\n\n전체 Job을 관장하는 Master의 역할\nSparkContext : Spark 전체의 환경(Environment)를 관장\n\n항상 Spark Application작성시에는 SparkContext를 먼저 생성하는 코드를 넣음(Driver에 SparkContext가 떴다면 initialization이 끝난 것)\n\n이유는 SparkContext가 떠야 Spark가 시작될 수 있음\n\n\n\nWorker\n\n실제로 작업을 하는 역할(Node). Executor(분산되어 할당된 여러 Task를 돌려줌) + Cache\n\nCluster manager\n\nSpark의 역할은 오로지 데이터의 처리인데, Task를 나누고 분배하려면 각각 Node의 상황 등을 파악해야 함\n가용성과 Task양 등은 Yarn resource 등의 Cluster manager가 알 수 있으며, Spark는 Resource management를 해주지 않음\nCluster의 종류를 옵션에 적으면, Cluster manager에 맞게 플러그인을 제공.\nCluster가 가용성 등의 정보를 제공해 Driver와 소통하여 Task를 나눌 수 있게한다  \n\n\nSpark Context\n\n처리의 역할만 하고, 데이터를 읽고 Cluster를 관리 등은 플러그인을 제공한 타 기능이 실행\n다양한 데이터 Source에 연결한 뒤, 데이터는 Job, Task가 나뉜 것에 따라 분배가 되어야하는데 Spark가 직접하지 않음\nHadoop이 이미 Distributed file system을 잘 구축해두었으므로(데이터 처리가 느리지만), 그대로 가져와서 사용\nSpark를 설치해보면, 자동으로 HDFS가 자동으로 함께 설치됨(Dependency가 걸려있음)\n데이터 처리용도 외의 데이터를 자르거나 Job을 분배하는 처리는 다른 플랫폼을 활용한다"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-environment-설정docker사용",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-environment-설정docker사용",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Spark Environment 설정(Docker사용)",
    "text": "Spark Environment 설정(Docker사용)\n\n클러스터의 무료 버전은 등록/제약(몇개월이내 데이터량) 등의 문제로 Local(Docker)에서 실습예정 \nDocker\n\n어떤 환경을 쓰더라도, 다른 OS나 패키지 등의 상황에 대응하기 어려움 (→ 어떤 환경에서든 Application이 돌아갈 수 있도록 도와주는 Docker)\nVirtual Machine이 OS단위 구현인 것과 달리, 하나의 OS에서 Docker엔진이 Software단위로 패키징\n\nOS커널은 다른 컨테이너와 공유할 수 있으며, 각 컨테이너는 격리된 프로세스로 실행\n\n수업은 Ubuntu를 활용하여 Docker Engine 설치\n\nhttps://docs.docker.com/engine/install/ubuntu/\nUbuntu설치 : https://ubuntu.com/download/desktop\nUbuntu설치 Tutorial : https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview\n\nDocker를 설치했다는 것은 Docker Engine을 설치했다는 것이고(하단그림 가로), 이제 위의 App(하단그림 세로)을 띄워야 함\n\n만들어져있는 App을 사용할 예정(Spark Driver, Spark Executer, Airflow, ES, Kibana, Jupyter 등)  \n\nDocker파일(컨테이너화 하는 파일) : 내가 원하는 소프트웨어를 어떤 환경/버전/라이브러리로 설치할 것을 정의하는 파일\n\n하단 그림을 해석하자면, airflow 2.7.1, Python 3.11버전을 사용하고 싶음\n\nFrom에서 제공하는 이미지를 가져와서 (airflow에서 Docker이미지를 제공함[Docker쓰는 경우가 많아서])\nroot 권한을 사용하여 apt-get을 update 후 python3-dev 등을 설치\nJAVA_Home세팅을 내가 설치한 java-11-openjdk-arm64로 해줌\nairflow 유저네임으로 바꿔서\nairflow에서 사용할 python패키지 (apache-airflow 등) 설치\nFrom의 이미지만을 사용하면 원하는 JDK를 사용하기 어렵고, 파이썬 패키지가 없는 상태임 \n\nDocker파일은 정의된 config파일로, 이것을 Docker이미지로 build하게 됨(build를 하면 이미지가 나옴)\n\ndocker build명령어로 config파일을 사용\n\n-t : 이름지정\n-f로 파일명을 지정하지 않고(-f Dockerfile 등), .을 찍으면 현재 디렉토리기준으로 Docker파일을 만듦\n\n  docker build -t airflow-python:20240805 .\n수업에서는 Docker-Compose를 사용할 예정으로 별도의 build를 진행하지 않음\n\n\nDocker이미지 : 컨테이너 App이 올라가기 전, 만들어져있는(올라가기 직전의) 명세\nDocker명령어 사용실습\n  docker images  # Docker 이미지를 볼 수 있음\n\n\n\nimage-3.png\n\n\n  docker container ls  # 올라와있는(up되어있는) Docker container들을 볼 수 있음\n                       # docker container까지만 치면 사용가능한 명령어를 보여줌\n\n\nDocker Compose\n\nDocker 컨테이너 설정들을 한번에 관리하기 위한 기술 (docker설치시 같이 설치됨)\ndocker에서 실행(설치)하도록 하는 airflow제공 가이드 : https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html\n\nairflow는 여러 기능이 있으므로 컨테이너도 여러개 있음\n\nspark를 기준으로 이해하자면, Driver를 뜨고 Worker(Node)를 뜨는 등 순서에 따라 진행하고, 모든 Node가 되는지 확인까지 compose로 진행\n하단 그림을 기준으로\n\nServices에 여러 컨테이너를 띄울 것(spark-master, spark-worker)\nspark-worker의 depends_on: spark-master : spark-master 뜬 다음에 진행하겠다는 의미 \n\ncompose를 사용하지 않는다면 컨테이너별로 명령어를 따로 쳐야하는 불편함. 이를 막을 수 있도록 docker-compose를 사용\n\ndocker-compose.yml 파일에 정의해두고 사용\n\n\nDocker Commands\n\n기본 커맨드\n\ndocker-compose up -d (start all the containers)\n\n-d : 백그라운드에서 실행(터미널이 끌 수 없는 상태로 실행되는 것을 방지)\n\ndocker-compose down (stop all the containers)\n\n참고 : stop되지 않은 컨테이너는 rm이 불가하다\n\ndocker ps (check the containers)\n\nstart된 컨테이너를 보여줌\n\ndocker container ls (stop된 컨테이너도 모두 뜸) \n\n\n\n디버깅이나 오류 상황등에서 사용하는 커맨드\n\ndocker logs  (check container logs)\n\n로그를 컨테이너별로 볼 수 있다\n\ndocker stop  (stop specific docker container)\ndocker rm  (remove specific docker container)\ndocker container ls -a (check all the containers including stopped ones)\ndocker images (docker 이미지 목록) \ndocker run –options  (run docker image)\n\ndocker run -it de-2024_scheduler scheduler (-it 옵션으로, de-2024_scheduler를 실행)\n\n이름을 지정하지 않으면 임의의 이름을 부여 (-name=)\n\ndocker run -it minz95/de2024:jupyter /bin/bash\n\ncontainer마다 /bin/bash가 존재함을 단순 참고\n\n\ndocker rmi  (remove docker image)\n\ndocker rm 등은 실제 이미지가 지워지는게 아니며, docker rmi로 삭제\n\ndocker system prune (remove all the orphans in docker system)\n\ncache 등 용량이 모자라면 사용(문제가 된 컨테이너가 계속 실행중인 경우 메모리가 쌓여있을 수 있음)\n\ndocker start \n\ncontainer 실행\n\ndocker exec -it  /bin/bash (execute docker container)\n\ncontainer에 들어가서 볼 수 있음\nexit를 입력하여 종료 가능\n\n\n\n\n\nDocker에서 Spark-Master를 잘 실행했다면, http://localhost:9090/ 를 입력하여 확인 가능\n\n하단 yml파일에 ports세팅을 해두었기 때문에 9090으로 접속 가능\n\ndocker-compose.yml 파일\n  services:\n    spark-master:\n      &lt;&lt;: *spark-common\n      hostname: spark-master\n      command: bin/spark-class org.apache.spark.deploy.master.Master\n      expose:\n        - \"7077\"\n      ports:\n        - \"9090:8080\" # UI\n        - \"7077:7077\" # Master와 직접연결\n        - \"4444:4040\"\n\n\n직접 Spark를 설치한다면 필요한 사항\n\nJava 8 이상이 설치된 Linux 서버\nSpark 다운로드\nSpark 홈페이지 -&gt; Download 페이지\n원하는 버젼 선택 후 다운로드\n스파크 설치, 압축 해제\n각자의 환경에 맞는 config 설정하기\n링크 : https://spark.apache.org/docs/latest/configuration.htm"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#submitting-spark-job",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#submitting-spark-job",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Submitting Spark Job",
    "text": "Submitting Spark Job\n\nSpark Application\n\nBuild-in Application : Spark에서 기본 제공\nUser-Built Application : User가 작성 \n\nSpark Application 작성순서\n\nSparkContext(SparkSession)생성\n    spark = SparkSession.builder.getOrCreate()\n데이터 모델 생성(DataFrame/Dataset, RDD)\n    df = spark.read.format(\"csv\").option(\"header\", True).load(csv_file)\n데이터 처리 : Where절 등 사용\n    result = df.withColumn('rate', col(\"rate\").cast(DoubleType()))\n결과 파일 처리\n    result.show(3, false)\n\n\nSpark Application 실습 (Word-Count)\n\n과정\n\n서버 준비\n파일을 나누어 서버에 분산시켜 전송 (HDFS가 진행)\n프로그램 빌드 (내가 진행)\n빌드한 프로그램을 서버에 전송 (Cluster Manager가 진행)\n서버 별로 실행 결과 기록 (Spark가 진행)\n모든 결과를 더해서 최종 결과 추출 (Spark가 진행)\n\n코드\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n        .appName(\"word-count\")\n        .master(\"local\")\n        .getOrCreate()\n)\nsc = spark.sparkContext\n\ntext = \"Hello Spark Hello Python Hello Docker Hello World\"\nwords = sc.parallelize(text.split(\" \"))\nwordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n\nfor wc in wordCounts.collect():\n    print(wc[0], wc[1])\n\nspark.stop()\n\n\nRDD(Resilient Distributed Dataset)\n\n위의 실습코드의 parallelize로 사용한 것이 RDD라는 모델\n분산 데이터 모델\nMapReduce와 비슷한 역할 (MR용도에 최적화된, 그러나 하둡보다는 발전한)\nHow를 기술 (데이터를 어떻게 조작할지를 기술)\n  rdd.map((_., 1L))\n  rdd.reduceByKey(_ + _)\n\n참고 : pandas DataFrame은 How가 아닌 What을 기술(어떤 작업을 할지 컬럼별로 기술)\n\nRDD는 컬럼이 없는 데이터 모델 \n\n\n\nCluster Manager\n\nNode 상태를 체크해서 작업을 분배, 죽은 Node를 제외하는 등 기능 필요\n\nDriver (Spark Context가 뜨는 Node)\n\n백그라운드 프로세스와 정보들 관리\nCluster Manager와 관계없이 동작 (Cluster Manager가 보는 상태는 Executor의 상태, Driver를 좌우하지 않음)\n\nCluster Manager와 정보는 공유함\n\n\nExecutor\n\n(Cluster manager의)Scheduler로부터 task를 할당받아 실행하는 역할 \n\nSpark-submit\n\nSpark실행때는 파이썬 명령어로 실행할 수 없음 (python test.py 등)\n\n빌드한 프로그램을 각 서버(띄운 Worker에서 실행하도록) 명령을 보내주어야 함\n이 내용이 Built-in application 중 spark-submit임\n  spark-submit --master=&lt;yarn|mesos…&gt; \n\nSpark-submit을 하면 전체 Worker node에 job을 분배해줌\n코드 샘플 (수업은 Local환경임을 참고)\n\nspark-master : yml파일 참고하여 작성함\n\nspark-master : hostname\n:7077 : master와 연결할 ports\n\nspark://spark-master:7077\ndocker-compose.yml\nservices:\n  spark-master:\n    &lt;&lt;: *spark-common\n    hostname: spark-master\n    command: bin/spark-class org.apache.spark.deploy.master.Master\n    expose:\n      - \"7077\"\n    ports:\n      - \"9090:8080\" # UI\n      - \"7077:7077\" # Master와 직접연결\n      - \"4444:4040\"\n\n\n\nSpark Context\n\nSpark Context를 생성해야 Spark Task를 실행할 수 있음\n\nInitialize Environment의 역할\n\nSpark Context는 매번 생성하므로 공통 함수가 있으면 편리함 \n\nSpark Session\n\nSpark Session은 Spark Context를 코딩하기 쉽게 래핑해둔 것으로 이해\n\nSpark Context가 Spark Session에 들어있다\n\nLow level 작업(RDD 등)을 할 때는 SparkContext를 사용\nSparkContext도 Spark Session을 통해 접근할 수 있음 \n\nSpark Submission Command\n\n[예시] 하단 코드를 기준으로\n\n-it option으로 spark-submit을 실행\n--master option으로 spark-master의, 7077로 지정\n&lt;python-app-path&gt;안에 파이썬 파일명\n파이썬 파일이 파라미터를 받는다면 &lt;data-path&gt;을 지정\n\ndocker exec -it de-2024-spark-master-1 spark-submit --master spark://spark-master:7077 &lt;python-app-path&gt; &lt;data-path&gt;\n[방법1] docker에서 실행 (실습환경 기준)\n\njobs에 대한 매핑이 이미 되어있기에 가능함을 참고 (yml파일의 x-spark-common의 volumns 부분)\n아래 파이썬 파일(word-count.py)은 파라미터를 받지 않음\n\ndocker exec -it de-2024-spark-master-1 spark-submit --master spark://spark-master:7077 jobs/word-count.py\n[방법2] 컨테이너에 들어간 후 실행 (실습환경 기준)\n\n아래 파이썬 파일(hello-world.py)은 파라미터를 받음\n\n# 해당 컨테이너에 들어감\ndocker exec -it de-2024-spark-master-1 /bin/bash\n# 파일이 있는 폴더로 들어감\ncd jobs\n# 해당 폴더에서 직접 실행 \nspark-submit --master spark://spark-master:7077 hello-world.py ..data/movies.csv"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#pyspark",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#pyspark",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Pyspark",
    "text": "Pyspark\n\nPyspark는 Main은 아님\n\nSpark는 Scala로 구현되어있어, 신기능은 Scala로 먼저 개발됨\nPython으로 포팅되기까지 시간이 걸림\n\nScala가 필수는 아님\n\nScala는 함수형 프로그래밍에 대한 부담이 큼\nSpark 자체가 다양한 언어를 지원하여, 간단한 처리로는 어떤 언어도 가능\n\n단 아래의 경우는 Scala가 필요함\n\n최신 Spark API가 필요하거나 Customizing이 필요한 경우\n성능최적화나 오픈소스 등을 기여하고자 할 때\nSpark 내부구조를 분석해서 디버깅해야할 Case가 있을 때"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#dataset",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#dataset",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Dataset",
    "text": "Dataset\n\n[참고]실습 데이터 폴더구조\n\ndags : airflow에서 사용\ndata : 데이터를 넣고 사용\nes-data : elastic search에서 사용\njobs : 주로 사용할 메인코드, spark 코드 사용\nlogs : airflow 로그용 디렉토리\nnotebooks : jupyernotebook 파일\nresources : elasticsearch jar파일\n\n과제 데이터셋 고민\n\nYahoo Finance API\n\n  import yfinance as yf \n  msft = yf.Ticker(\"MSFT\")\n  msft.info\n  # get historical market data \n  hist = msft.history(period=\"1mo\")\n  # show meta information about the history (requires history() to be called first) \n  msft.history_metadata\n데이터의 Schema 확인하는 샘플코드\ndf = spark.read.csv('')\ndf.printSchema()"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-materials",
    "href": "posts/meta-de-spark_and_airflow-20240812/index.html#spark-materials",
    "title": "[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Spark Materials",
    "text": "Spark Materials\n\nhttps://spark.apache.org/docs/latest/\nhttps://databricks.com/blog/category/engineering/spark\nspark 를 이루는 기반 기술에 대한 공부 (java I/O, python, Hadoop)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html",
    "href": "posts/meta-dl-creditcard-20240616/index.html",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 4주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240616/index.html#개요",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 4주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist-설명",
    "href": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist-설명",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "3주차 과제 설명(MNIST 설명)",
    "text": "3주차 과제 설명(MNIST 설명)\n\nBinary(Hypothesis : Sigmoid, CrossEntropy : Y, 1-Y)  → Multiclass(Hypothesis : Softmax, CrossEntropy : \\(Y_1\\), \\(Y_2\\)…)\nMNIST\n\n데이터사이언스, 딥러닝에서의 기초(코딩의 Hello world와 같음)\n사람들의 손글씨 데이터를 모아둔 유명한 데이터 셋\n\n  # tensorflow MNIST로딩 샘플코드\n  from tensorflow.keras.datasets import mnist\n\n  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n  print(x_train.shape, y_train.shape, y_train[0])\n\n  &gt;&gt;&gt;(60000, 28, 28) (60000,) 5 \n  # 6만건의 28*28형태의 데이터 (x_train.shape), 6만건의 0차원 형태 데이터(y_train.shape), 0번째 y_train 정답값(5)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist의-softmax구현",
    "href": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist의-softmax구현",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "3주차 과제 설명(MNIST의 Softmax구현)",
    "text": "3주차 과제 설명(MNIST의 Softmax구현)\n\nMNIST의 Softmax구현\n\nreshape을 사용하는 이유\n\n모델에 통과시키기 위해 행렬을 flatten(28*28 → 784*1)한 후, 다시 Visualize할 때 사용\n\n  img = x_train[i].reshape(28,28)\n  plt.imshow(img, cmap=plt.cm.binary)\nHyper parameter\n\nParameter : W와 b와 같은 학습을 통해 최적화하는 값\nHyper parameter : 모델의 선택(Linear vs Logistic regression), learning_rate, epoch, class(몇개로 나눌지), feature(input data) 등\n\n  # MNIST dataset parameters\n  num_classes = 10\n  num_features = 28 * 28 # 784\n\n  # Training parameters\n  learning_rate = 0.01\n  steps = 1000\n  batch_size = 256\nnp.array : list형태의 train data를 수학적 연산에 유리한 numpy로 변환하기 위해 사용\n\n권장하는 코딩습관 : 아래와 같이 데이터타입을 명시 (안적어도 float32로 기본지정되긴 함)\n\nnp.array(x_train) → np.array(x_train, np.float32)\n\nx_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n\n구조변경(flatten)\n\nreshape([-1, num_features]) : flatten 구현\n\n784 data에 reshape([-1, 28]) : reshape의 -1 부분은 28이 됨\n784 data에 reshape([-1, 784]) : reshape의 -1 부분은 1이 됨\n\n# data 구조를 통일\nx_train = x_train.reshape([-1, num_features])\nx_test = x_test.reshape([-1, num_features])\nprint(x_train.shape, x_test.shape)\n&gt;&gt;&gt; (60000, 784) (10000, 784)\n\n데이터 전처리(Normalize, Zero-centering)\n  x_train, x_test  = x_train / 255. , x_test / 255. # Normalizd\n  x_train, x_test = (x_train-np.mean(x_train)), (x_test-np.mean(x_test)) # Zero-Centering\nVariable 설정\n\nW와 b의 Shape은 결정해주어야 함 (W의 차원파악 중요)\n\nW : input dimesion, output dimesion\nb : number of classes(output dimesion)\n\n  W = tf.Variable(tf.ones([num_features, num_classes]), name = 'weight')\n  b = tf.Variable(tf.zeros([num_classes]), name='bias')\n\nHypothesis와 평가방법(accuracy) 등 설정\n  def softmax(x):\n    z = tf.matmul(x, W) + b\n    sm = tf.nn.softmax(z)\n    return sm\n\n  def cross_entropy(y_pred, y_true):\n    y_true = tf.one_hot(y_true, depth=num_classes)\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis = 1))\n\n  def accuracy(y_pred, y_true):\n    correct_prediction = tf.equal(tf.argmax(y_pred, axis=1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nOptimizer\n  optimizer = tf.optimizers.SGD(learning_rate)\n  def run_optimization(x,y):\n    with tf.GradientTape() as tape:\n      pred = softmax(x)\n      loss = cross_entropy(pred, y)\n\n    gradients = tape.gradient(loss, [W,b])\n\n    optimizer.apply_gradients(zip(gradients, [W,b]))\nBatch size구현\n\n아래 코드는 기초적 구현이며, 중복제거나 차례대로 진행되는 기능 등도 있음\n\n왜? 데이터가 시점 등에 따라 패턴이 있을 수 있다\n\n100개의 데이터를 10등분했을 때, 각 데이터마다 패턴이 있을 수 있음\n편향되지 않게 섞는 것이 batch_size의 기본적 컨셉 ```python n_train = x_train.shape[0] # 60000 n_test = x_test.shape[0] # 10000\n\n\ndef train_batch_maker(batch_size): random_idx = np.random.randint(n_train, size = batch_size) # (batch_size,) vector return x_train[random_idx], y_train[random_idx] # (batch_size, 28*28)\ndef test_batch_maker(batch_size): random_idx = np.random.randint(n_test, size = batch_size) # (batch_size,) vector return x_test[random_idx], y_test[random_idx] # (batch_size, 28*28) ```\n\n학습 및 Test\n\n  for step in range(steps):\n  batch_x, batch_y = train_batch_maker(batch_size)\n\n  # Run the optimization to update W and b values\n  run_optimization(batch_x, batch_y)\n\n  if step % 100 == 0:\n    pred = softmax(batch_x)\n    loss = cross_entropy(pred, batch_y)\n    acc = accuracy(pred, batch_y)\n    print(f\"step: {step} loss: {loss} accuracy: {acc}\")\n\n  pred = softmax(x_test)\n  print(f\"Test Accuracy: {accuracy(pred, y_test)}\")\n  &gt;&gt;&gt; Test Accuracy: 0.8712000250816345\n\n  #Predict 5 images from validaton set.\n  num_images = 5\n  test_images = x_test[:num_images]\n  predictions = softmax(test_images)\n\n  # Visualize image and model predcition\n  plt.figure(figsize = (5,5))\n  for i in range(num_images):\n    plt.subplot(1, num_images, i+1)\n    plt.imshow(np.reshape(test_images[i], [28,28]), cmap=plt.cm.binary)\n    plt.xlabel(np.argmax(predictions.numpy()[i]))\n\n  plt.show()\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist의-neural-network구현",
    "href": "posts/meta-dl-creditcard-20240616/index.html#주차-과제-설명mnist의-neural-network구현",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "3주차 과제 설명(MNIST의 Neural Network구현)",
    "text": "3주차 과제 설명(MNIST의 Neural Network구현)\n\nMNIST의 Neural Network구현(기초)\n\nSequential : Python의 list 선언과 유사함\n\n아래 코드의 경우, 사실상 Hidden layer가 없는 단층 Perceptron\nGPT의 경우 하단 모델표에서 Total params가 엄청 많음\n\n  from tensorflow.keras.models import Sequential\n  from tensorflow.keras.layers import Dense\n\n  model = Sequential()\n  model.add(Dense(2, activation = \"sigmoid\", input_dim=2)) # neuron=2, input_dim=2\n  model.add(Dense(1, activation = \"sigmoid\"))              # output_dim = 1\n  model.summary()\n\n  &gt;&gt;&gt;Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                Output Shape              Param #   \n    =================================================================\n    dense (Dense)               (None, 2)                 6         \n\n    dense_1 (Dense)             (None, 1)                 3         \n\n    =================================================================\n    Total params: 9 (36.00 Byte)\n    Trainable params: 9 (36.00 Byte)\n    Non-trainable params: 0 (0.00 Byte)\n    _________________________________________________________________\n\n라이브러리를 활용하여 아래와 같이 코드가 간단해짐 (verbose을 0이 아닌값 지정시 진행상태 확인가능)\n\n  model.compile(optimizer = \"SGD\",\n                loss = 'binary_crossentropy',\n                metrics=['accuracy'])\n\n  history = model.fit(x_data, y_data, epochs=1000, batch_size=32, verbose=0)\n\n모델평가\n\n아래와 같이 acc가 낮은 이유는, XOR문제처럼 현재와 같은 단층레이어로는 해결이 어렵기 때문\n\nloss, train_acc = model.evaluate(x_data, y_data, verbose=0)\nprint(f\"train_acc = {train_acc}\")\nprint(f\"loss = {loss}\")\npredict = model.predict(x_data)\nprint(predict)\n&gt;&gt;&gt;train_acc = 0.5\n  loss = 0.6908708810806274\n\n\nMNIST의 Neural Network구현(ANN, 인공신경망 Artificial Neural Network)\n\nDense(10, activation = “sigmoid”) : 뉴런이 10개이며 활성화함수가 Sigmoid\nDense(1, activation = “softmax”) : 활성화 함수로 Softmax 사용\n중요 포인트\n\ninput data의 shape와 일치시키기\noutput activation은 내가 풀고자 하는 문제에 맞춰 잘 지정해야함\n\nMulti-class인데 Sigmoid를 사용한다면 돌아가지 않을 것임 ```python from tensorflow.keras.layers import Flatten\n\n\nmodel = Sequential() model.add(Flatten(input_shape=(28,28))) model.add(Dense(10, activation = “sigmoid”)) model.add(Dense(10, activation = “sigmoid”)) model.add(Dense(1, activation = “softmax”)) model.summary() ```\n하단 모델표의 dense_2 (Dense)의 Param #의 의미\n\n784*10 + 10(bias) : fully-connected layer\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                Output Shape              Param #   \n=================================================================\nflatten (Flatten)           (None, 784)               0         \n\ndense_2 (Dense)             (None, 10)                7850      \n\ndense_3 (Dense)             (None, 10)                110       \n\ndense_4 (Dense)             (None, 1)                 11        \n\n=================================================================\nTotal params: 7971 (31.14 KB)\nTrainable params: 7971 (31.14 KB)\nNon-trainable params: 0 (0.00 Byte)\n\nNeural Network를 잘 사용하려면\n\n하이퍼 파라미터 튜닝 : Activation fuction 어떤 것, 뉴런/레이어 갯수 등 설정\n\n일반적으로 괜찮다고 알려진 값으로 지정해야 모델성능이 쓸만해짐"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240616/index.html#주차-정리",
    "href": "posts/meta-dl-creditcard-20240616/index.html#주차-정리",
    "title": "[M_Study_4주차] Convolutional Neural Network",
    "section": "4주차 정리",
    "text": "4주차 정리\n\nDeep Neural Network (Convolution NN)\n\n딥러닝은 비선형적 관계를 잘 묘사함\n\n과거에는 숫자데이터(금리, 고용률 등)만 다룰 수 있었음\n이미지 등 비정형데이터를 잘 다룰 수 있게됨\n\nCNN(Convolutional Neural Network)\n\n이미지를 분류, 처리하는 기술\n\n분류(Classification) : 대상이 어디에 속하는지 분류\n식별(Detection) : 사진에서의 어떤 객체를 분류하고, 그 객체가 어디에 속하는지 분류\n이러한 분류와 식별을 응용하여 자율주행 자동차, 얼굴인식App 등 구현 가능 (CNN Architecture영향을 받아 발전)\n\n\nCNN의 구성\n\n특성추출(feature learning)\n\n사진은 많은 정보를 포함해 계산량이 너무 많아지므로, 특성추출을 통해 압축이 필요함\nCNN은 모델이 이런 정보에서 어느 부분이 중요한지 찾게 만듦\n특성추출로 압축된 정보를 Neural Network에 넣어, 원본사진 넣은 것과 동일한 것처럼, 빠르고 가볍게 돌아감 (예를 들어 자율주행에 활용하기 위해서는 짧은 시간에 처리해야함)\nConvolution alyer(정보압축) / Pooling layer(정보버림) 등의 반복\n\nClassification\n\nFlatten / Fully-connected / Softmax 등 3주차까지 배운 내용\n\n\nConvolution layer\n\nSource(X)에 Kernel(W)행렬을 곱해 Result(Y)를 만드는것과 같음\n\n원본이미지(Source)의 손실을 최소화하며 잘 압축하는 좋은Kernel값을 찾고자 함\n아다마르곱 활용 : 매칭되는 칸의 숫자끼리 곱하여 더함 \n\nConvolution layer를 통과해 (28,28)이 (14,14)가 되어도 특징 식별 가능\n\nCNN은, 마스크 쓴 사람의 일부 얼굴 특징만으로 사람을 판단하는 것과 같은 방법을 차용\n\nCNN이 잘 작동하기 위한 가정\n\nSpatial Locality : 사진의 일부만 봐도 식별이 가능하다\nPositional invariance : 이미지에서 사과 등 객체가 어디에 있던 사과임\n\n예외 상황 : 엑스레이의 흰색이 위치에 따라 심장, 간 등 다름\n\n\n\nConvolution layer 추가설명\n\nimage(32,32,3)를 filter(3,3,3)가 움직이며 아다마르곱으로 처리\n\nkernel과 filter는 동일한 의미로 사용 \n(32X32X3)image에 (5X5X3)filter 적용시 output은 (28X28X1)\n\noutput은 activation map임(매핑을 했다는 것)\n\n(32X32X3)image에 (5X5X3)filter 4개를 적용하는 이유? \n\n필터가 필요했던 이유는 정보 압축 → 정보가 작아지며 손실 발생 (코끼리의 일부만을 보는 것과 같음)\n각 부분의 정보(4개의 filter)를 모두 모아서 정확히 알 수 있음 (4개의 filter라는 서로 다른 각도에서 본 코끼리를 합쳐서 보게되는 것)\n\n\n\nNested Conv-layers(Convolution layer에 대한 문제)\n\nLayer를 여러번 통과하여 너무 작아지는 문제(get smaller quickly) + 4k와 같은 큰 데이터의 계산량 문제\n이러한 문제를 방지하기 위해 stride와 padding이라는 parameter를 추가\n\nStride\n\n(Input - Filter) / stride + 1\n\n(7x7)image에 (3X3)filter, stride 1 → (7-3)/1 + 1 = (5X5)\n(7x7)image에 (3X3)filter, stride 2 → (7-3)/2 + 1 = (3X3)\n(7x7)image에 (3X3)filter, stride 3 → (7-3)/3 + 1 = (2.33X2.33)\n\noutput 소수인 것은 불가, padding(가장 자리에 0을 붙여줌)을 사용\n\n\n(Input - Filter + 2 \\(*\\) Padding) / stride + 1\n\nPadding을 2배하는 이유는 양쪽 테두리에 각 1개씩 추가되기 때문\n\n\n예시\n\nInput (32x32x3) / 10 Filter(5X5) / stride 1, pad 2\n\nOutput size(=32X32X10) : (32-5+2*2)/1 + 1 → (32X32) → 10개의 필터\nNumber of Parameter(=760) : (Filter (5X5) * Input의 차원 3 + Bias 1) → (553+1) → 필터 10개 → (553+1) * 10\nNumber of Parameter if fully-connected(=31,467,520) : (323210)(3232*3_1)\n\nFully-connected로 31,467,520개 파라미터가 필요했다면, CNN으로 760개 파라미터로 가능해짐\n\n\n\n관례적으로 filter size는 정사각형이지만 직사각형도 가능하다\nPooling Layer : 쓸모없는 정보를 버림\n\nPooling Layer의 종류\n\nMax pooling : 제일 큰 수만 가져옴\nAverage pooling : 평균 가져옴\nParameters 0개 (숫자를 찾는게 아닌 버림. 학습이 일어나지 않음)\n\nPooling Layer의 특징\n\n파라미터수가 0 (학습이 일어나지 않는다, Kernel[Filter]처럼 좋은 숫자를 찾는 작업이 없기 때문)\n\n\n\n\n\n\nFinal project 설명\n\n28만건의 신용카드 데이터로 만들기\n금융데이터는 privacy이슈가 있어 공개시 익명화되어있음\n\n단위가 다르면 연봉, 자산 여부를 역추적할 수 있어, PCA를 통해 익명화 가능\n\nPCA : 데이터 익명화기법은 아니고 처리기법\nV1, V2와 같은 세상에 존재하지 않는 축으로 변환하기때문에 익명화에 좋다\n원래는 상관관계를 0이 되게만드는 테크닉\n\n\nMissing value 처리\n\n평균,최빈,중앙 값 중 프로젝트에 가장 적합한 값 사용하여 대치\n\n상관관계 파악 (Heatmap 분석)\n\n과제의 금융데이터는 PCA로 인해 상관관계가 이미 사라져있음\n\n데이터의 구조 파악\n\nclass : 0정상 1이상거래 (0.17%만이 이상거래 → 모델평가시 고려)\n\n불필요한 컬럼 삭제\nTarget이 합쳐져있는 원본데이터이므로 X, Y로 나누는 작업 필요\n모델 평가\n\n99프로가 정상거래인데 99프로 정상임을 맞췄다면, 단순히 모델이 좋다고 보기 어려움\n\nPrecision, Recall, F1-Score → 이상거래를 몇건 맞췄는지 볼 수 있음\n\n\n평가지표는 f1-score를 사용 예정\n\n바꾸고싶다면 사유 명시(어떤 부분을 중요하게 생각하여 Recall을 기준으로 삼았다)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 3주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#과제-설명",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#과제-설명",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "과제 설명",
    "text": "과제 설명\n\n과제 : 월간 데이콘 신용카드 사용자 연체 예측 AI 경진대회\n\nhttps://dacon.io/competitions/official/235713/overview/description\n\nEDA, 전처리, 데이터마트(CSV파일 등)만들고, 그 과정이 담긴 ipynb제출"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#변수-설명-확인하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#변수-설명-확인하기",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "변수 설명 확인하기",
    "text": "변수 설명 확인하기\n\n원본링크\n변수별 설명\n\ngender: 성별\ncar: 차량 소유 여부\nreality: 부동산 소유 여부\nchild_num: 자녀 수\nincome_total: 연간 소득\nincome_type: 소득 분류\n\n[‘Commercial associate’, ‘Working’, ‘State servant’, ‘Pensioner’, ‘Student’]\n\nedu_type: 교육 수준\n\n[‘Higher education’ ,‘Secondary / secondary special’, ‘Incomplete higher’, ‘Lower secondary’, ‘Academic degree’]\n\nfamily_type: 결혼 여부\n\n[‘Married’, ‘Civil marriage’, ‘Separated’, ‘Single / not married’, ‘Widow’]\n\nhouse_type: 생활 방식\n\n[‘Municipal apartment’, ‘House / apartment’, ‘With parents’, ‘Co-op apartment’, ‘Rented apartment’, ‘Office apartment’]\n\nDAYS_BIRTH: 출생일\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미\n\nDAYS_EMPLOYED: 업무 시작일\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미\n양수 값은 고용되지 않은 상태를 의미함\n\nFLAG_MOBIL: 핸드폰 소유 여부\nwork_phone: 업무용 전화 소유 여부\nphone: 전화 소유 여부\nemail: 이메일 소유 여부\noccyp_type: 직업 유형\n\nfamily_size: 가족 규모\nbegin_month: 신용카드 발급 월\n\n데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미\n\ncredit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도\n\n낮을 수록 높은 신용의 신용카드 사용자를 의미함"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#데이터-읽고-기초적인-정보-확인하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#데이터-읽고-기초적인-정보-확인하기",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "데이터 읽고 기초적인 정보 확인하기",
    "text": "데이터 읽고 기초적인 정보 확인하기\n\n데이터 로딩하기\n\n\nfrom pkb_sqlite3 import DB_sqlite3\n\ndb_controller = DB_sqlite3('Dacon_creditcard_overdue.db')\ndf_train = db_controller.search_db_show_df('SELECT * FROM train')\ndf_test = db_controller.search_db_show_df('SELECT * FROM test')\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\n\n# 데이터 형태(잘 로딩되었는지 확인)\n\nprint(f\"\"\"{df_train.shape}\n{df_test.shape}\"\"\")\n\n(26457, 20)\n(10000, 19)\n\n\n\ndescribe()를 활용해 정보 확인하기\n\ncount값을 보면, occyp_type에만 Null(결측치)값이 있음\n\noccyp_type : 결측치가 8천여개로 Drop이 아닌 대체 필요\n\nUnique값이 있는 컬럼들은 범주형 변수로 추측\ncount/unique/freq값을 보면, 데이터가 편향된 컬럼 있음\n\n예를 들어, gender는 26,457 중 17,697로 약 65%로 F값이 많음(top)\n\nDAYS_BIRTH나 begin_month는 음수 변수로 로그변환 제외\n\n\n\ndf_train.describe(include='all').transpose().reset_index()\n\n\n\n\n\n\n\n\nindex\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n0\nindex\n26457.0\nNaN\nNaN\nNaN\n13228.0\n7637.622372\n0.0\n6614.0\n13228.0\n19842.0\n26456.0\n\n\n1\ngender\n26457\n2\nF\n17697\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\ncar\n26457\n2\nN\n16410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nreality\n26457\n2\nY\n17830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nchild_num\n26457.0\nNaN\nNaN\nNaN\n0.428658\n0.747326\n0.0\n0.0\n0.0\n1.0\n19.0\n\n\n5\nincome_total\n26457.0\nNaN\nNaN\nNaN\n187306.524493\n101878.367995\n27000.0\n121500.0\n157500.0\n225000.0\n1575000.0\n\n\n6\nincome_type\n26457\n5\nWorking\n13645\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\nedu_type\n26457\n5\nSecondary / secondary special\n17995\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\nfamily_type\n26457\n5\nMarried\n18196\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\nhouse_type\n26457\n6\nHouse / apartment\n23653\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10\nDAYS_BIRTH\n26457.0\nNaN\nNaN\nNaN\n-15958.053899\n4201.589022\n-25152.0\n-19431.0\n-15547.0\n-12446.0\n-7705.0\n\n\n11\nDAYS_EMPLOYED\n26457.0\nNaN\nNaN\nNaN\n59068.750728\n137475.427503\n-15713.0\n-3153.0\n-1539.0\n-407.0\n365243.0\n\n\n12\nFLAG_MOBIL\n26457.0\nNaN\nNaN\nNaN\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n13\nwork_phone\n26457.0\nNaN\nNaN\nNaN\n0.224742\n0.41742\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n14\nphone\n26457.0\nNaN\nNaN\nNaN\n0.294251\n0.455714\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n15\nemail\n26457.0\nNaN\nNaN\nNaN\n0.09128\n0.288013\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n16\noccyp_type\n18286\n18\nLaborers\n4512\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n17\nfamily_size\n26457.0\nNaN\nNaN\nNaN\n2.196848\n0.916717\n1.0\n2.0\n2.0\n3.0\n20.0\n\n\n18\nbegin_month\n26457.0\nNaN\nNaN\nNaN\n-26.123294\n16.55955\n-60.0\n-39.0\n-24.0\n-12.0\n0.0\n\n\n19\ncredit\n26457.0\nNaN\nNaN\nNaN\n1.51956\n0.702283\n0.0\n1.0\n2.0\n2.0\n2.0\n\n\n\n\n\n\n\n\n# 데이터 나누기\ny_train = df_train['credit'].copy()\nx_train = df_train.drop('credit',axis=1).copy()\n\nprint(f\"\"\"{x_train.shape}\n{y_train.shape}\"\"\")\n\n(26457, 19)\n(26457,)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#eda-수치형-변수-기초-통계량-확인",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#eda-수치형-변수-기초-통계량-확인",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "[EDA] 수치형 변수 : 기초 통계량 확인",
    "text": "[EDA] 수치형 변수 : 기초 통계량 확인\n\n왜도\n\n왜도를 구해 표로 정리했으며, 시각화를 통해 추가 확인하기로 함\n\n\nx_numerical = x_train.select_dtypes(include=['float64', 'int64'])\nnumeric_skew = x_numerical.skew().reset_index() # skew() 함수로 왜도 구하기\nnumeric_skew\n\n\n\n\n\n\n\n\nindex\n0\n\n\n\n\n0\nindex\n0.000000\n\n\n1\nchild_num\n2.852376\n\n\n2\nincome_total\n2.659271\n\n\n3\nDAYS_BIRTH\n-0.185986\n\n\n4\nDAYS_EMPLOYED\n1.777596\n\n\n5\nFLAG_MOBIL\n0.000000\n\n\n6\nwork_phone\n1.318953\n\n\n7\nphone\n0.903042\n\n\n8\nemail\n2.838422\n\n\n9\nfamily_size\n1.431759\n\n\n10\nbegin_month\n-0.290050\n\n\n\n\n\n\n\n\n\n첨도\n\n첨도를 구해 표로 정리했으며, 시각화를 통해 추가 확인하기로 함\n\n\nnumeric_kurt = x_numerical.kurtosis().reset_index()\nnumeric_kurt\n\n\n\n\n\n\n\n\nindex\n0\n\n\n\n\n0\nindex\n-1.200000\n\n\n1\nchild_num\n29.172394\n\n\n2\nincome_total\n16.359621\n\n\n3\nDAYS_BIRTH\n-1.046188\n\n\n4\nDAYS_EMPLOYED\n1.161734\n\n\n5\nFLAG_MOBIL\n0.000000\n\n\n6\nwork_phone\n-0.260383\n\n\n7\nphone\n-1.184604\n\n\n8\nemail\n6.057100\n\n\n9\nfamily_size\n10.578051\n\n\n10\nbegin_month\n-1.041906\n\n\n\n\n\n\n\n\n\n히스토그램 시각화\n\n히스토그램으로 시각화했고, 일부 분포의 판단이 힘든 값에 대해서는 재조정하여 시각화\n아래 변수들은 유무를 0,1로만 표현한 변수로, 추가적인 시각화에서 제외함\n\nwork_phone, phone, email\n\n\n\nimport matplotlib.pyplot as plt\nimport math\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(x_numerical.columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(x_numerical.columns):\n    ax = axes[var_idx]\n    ax.hist(x_numerical.iloc[:, var_idx], bins='auto', color='skyblue', edgecolor='black')\n    ax.set_title(f'{var_nm}')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n재조정하여 시각화한 내용을 기반으로, 아래와 같이 결정함\n\n뚜렷하게 쏠림(높은 왜도)을 보이는 ’DAYS_EMPLOYED’에 대해 로그변환 진행\n첨도가 3보다 낮은, ’DAYS_BIRTH’와 ’begin_month에 대해 표준화 진행\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 설정: 가로 그래프 개수와 세로 그래프 개수\nrows = 2\ncols = 2\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(15, 10))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nplot_count = 0\nfor var_idx, var_nm in enumerate(x_numerical.columns):\n    if var_nm in ['child_num', 'income_total', 'DAYS_EMPLOYED', 'family_size']:\n        ax = axes[plot_count]\n        data = x_numerical.iloc[:, var_idx]\n\n        # 이상치 제외를 위한 사분위수 계산\n        Q1 = np.percentile(data, 25)\n        Q3 = np.percentile(data, 75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        # 이상치를 제외한 데이터\n        filtered_data = data[(data &gt;= lower_bound) & (data &lt;= upper_bound)]\n\n        ax.hist(filtered_data, bins='auto', color='skyblue', edgecolor='black')\n        ax.set_title(f'{var_nm}')\n        ax.set_xlim(filtered_data.min(), filtered_data.max())  # x축의 최소값과 최대값 설정\n        plot_count += 1\n\n# 빈 서브플롯 숨기기\nfor i in range(plot_count, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n이상치\n\n전체 26,457 row중, 일부 이상치가 많아보이는 컬럼 있음\n\n대상 : child_num, income_total, DAYS_EMPLOYED, work_phone, email, family_size\n\n변수 설명 내용 중 ’소유 여부’인 변수(work_phone, email)를 확인 결과, 0 & 1만 있어 제외\n\n남은 대상 : child_num, income_total, DAYS_EMPLOYED, family_size\n\nBox plot을 그려보았으나, 시각화만으로는 판단이 어려워 추가적으로 확인하기로 함\n\n\n# 수업시간에 사용한 IQR기준 이상치함수 활용\n\ndef detect_outliers_fast(df):\n    # Calculate IQR without NaN values\n    Q1 = df.quantile(0.25, interpolation='midpoint')\n    Q3 = df.quantile(0.75, interpolation='midpoint')\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Vectorized outlier detection with NaN handling\n    outlier_flags = df.apply(\n        lambda col: ~col.between(lower_bound[col.name], upper_bound[col.name]) & col.notna()\n    )\n    return outlier_flags\n\n\n# 구한 이상치가 0인 경우를 제외하고 표기\nx_outliers = detect_outliers_fast(x_numerical)\noutliers_to_see = x_outliers.sum()[x_outliers.sum()&gt;0]\noutliers_to_see\n\nchild_num         369\nincome_total     1129\nDAYS_EMPLOYED    5726\nwork_phone       5946\nemail            2415\nfamily_size       350\ndtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\n\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(outliers_to_see.index)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 4))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(outliers_to_see.index):\n    ax = axes[var_idx]\n    sns.boxplot(x=x_numerical[var_nm], ax=ax, color='lightblue')\n    ax.set_title(f'{var_nm}')\n    ax.set_xlabel(var_nm)\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n각 변수에 대해 확인한 결과 아래와 같으며, 제외할지 등에 대한 판단이 어려움\n\nchild_num : 자녀가 19명인 경우는 확실히 이상치라는 느낌이지만, 3명의 경우는 이상치로 보기 어려워 보임\nfamily_size : 가족구성원이 5명인 경우도 이상치라고 해야할지 모호함\nDAYS_EMPLOYED는 추가로 확인해보니 미취업여부를 나타내기 위한 양수인 365243 다음으로 큰 수가 -17.\n\n불필요하게 큰 숫자로, 1로 대체\n\n\n각 변수의 이상치에 대해, 값 Drop이 아닌 변환 진행\n\n\ndef summarize_unique_outliers(x_numerical, x_outliers):\n    # 각 컬럼별로 이상치의 고유값을 추출하여 딕셔너리로 저장\n    unique_outliers = x_numerical[x_outliers].apply(lambda col: col.unique().tolist())\n\n    # 딕셔너리 형태로 반환\n    unique_outliers_dict = unique_outliers.to_dict()\n\n    return unique_outliers_dict\n\n\nunique_outliers = summarize_unique_outliers(x_numerical, x_outliers)\n\nprint(f\"\"\"* 이상치로 판명된 Unique값 현황\nchild_num : {unique_outliers['child_num']}\nincome_total : {unique_outliers['income_total']}\nDAYS_EMPLOYED : {unique_outliers['DAYS_EMPLOYED']}\nfamily_size : {unique_outliers['family_size']}\"\"\")\n\n* 이상치로 판명된 Unique값 현황\nchild_num : [nan, 3.0, 4.0, 5.0, 14.0, 19.0, 7.0]\nincome_total : [nan, 450000.0, 405000.0, 585000.0, 495000.0, 382500.0, 387000.0, 540000.0, 459000.0, 720000.0, 427500.0, 562500.0, 630000.0, 463500.0, 432000.0, 787500.0, 900000.0, 391500.0, 716323.5, 675000.0, 990000.0, 612000.0, 1575000.0, 652500.0, 945000.0, 418500.0, 634500.0, 423000.0, 742500.0, 517500.0, 560250.0, 810000.0, 445500.0, 616500.0, 765000.0, 396000.0, 472500.0, 594000.0, 468000.0, 661500.0, 441000.0, 494100.0, 531000.0, 607500.0, 414000.0, 697500.0, 1125000.0, 1350000.0]\nDAYS_EMPLOYED : [nan, 365243.0, -9391.0, -9404.0, -12332.0, -8491.0, -9988.0, -11940.0, -7400.0, -10993.0, -9870.0, -8091.0, -9957.0, -7310.0, -11062.0, -7679.0, -7514.0, -8671.0, -10773.0, -8284.0, -10121.0, -7369.0, -12278.0, -7415.0, -7401.0, -10050.0, -8553.0, -7593.0, -11494.0, -9052.0, -9269.0, -7835.0, -9683.0, -9858.0, -7346.0, -10490.0, -9925.0, -9029.0, -7838.0, -7553.0, -7379.0, -8171.0, -8375.0, -7536.0, -10600.0, -7851.0, -9575.0, -9422.0, -9255.0, -8298.0, -8163.0, -7734.0, -12179.0, -13245.0, -8172.0, -8325.0, -9258.0, -8100.0, -10384.0, -10821.0, -8538.0, -8772.0, -9359.0, -15072.0, -7733.0, -7979.0, -7471.0, -11951.0, -8412.0, -7824.0, -11589.0, -10094.0, -10758.0, -12169.0, -7840.0, -8022.0, -9181.0, -8316.0, -10290.0, -9485.0, -7624.0, -9751.0, -10149.0, -8953.0, -11451.0, -9325.0, -10454.0, -9479.0, -11183.0, -7591.0, -10079.0, -9866.0, -8109.0, -8684.0, -7465.0, -11157.0, -11083.0, -15038.0, -8643.0, -7371.0, -8479.0, -8469.0, -11906.0, -8760.0, -12490.0, -10689.0, -12455.0, -7752.0, -8737.0, -8855.0, -10236.0, -7627.0, -8794.0, -9136.0, -8063.0, -12423.0, -10437.0, -13102.0, -7288.0, -7718.0, -12148.0, -7866.0, -7827.0, -9947.0, -7341.0, -7830.0, -14810.0, -7804.0, -12949.0, -14473.0, -7614.0, -8808.0, -8377.0, -9046.0, -8131.0, -9447.0, -7513.0, -7404.0, -12827.0, -8649.0, -8369.0, -9178.0, -8405.0, -12621.0, -10780.0, -8175.0, -9225.0, -11884.0, -10936.0, -9748.0, -9745.0, -7698.0, -8497.0, -7364.0, -8624.0, -8083.0, -10364.0, -12870.0, -11389.0, -8460.0, -10994.0, -11907.0, -8348.0, -8206.0, -7888.0, -13800.0, -7778.0, -8448.0, -8862.0, -8015.0, -9379.0, -7551.0, -8714.0, -8509.0, -9482.0, -9975.0, -7347.0, -8803.0, -7811.0, -7413.0, -8157.0, -10110.0, -8143.0, -13735.0, -9195.0, -7747.0, -11693.0, -8040.0, -8691.0, -9076.0, -8033.0, -8875.0, -12647.0, -10762.0, -7557.0, -8838.0, -15661.0, -11398.0, -8601.0, -11552.0, -8756.0, -7522.0, -7738.0, -11542.0, -8290.0, -9152.0, -8987.0, -9239.0, -8859.0, -8140.0, -7949.0, -7622.0, -15713.0, -9349.0, -7278.0, -7500.0, -9260.0, -9724.0, -8036.0, -14887.0, -8767.0, -8443.0, -7871.0, -7566.0, -13415.0, -14018.0, -8966.0, -9044.0, -9240.0, -7276.0, -9564.0, -8044.0, -7900.0, -7494.0, -9385.0, -8376.0, -14413.0, -8647.0, -7786.0, -8254.0, -12995.0, -7676.0, -8995.0, -9756.0, -10155.0, -9389.0, -9236.0, -8710.0, -9419.0, -9559.0, -10909.0, -8071.0, -7343.0, -9581.0, -11954.0, -10475.0, -10217.0, -9363.0, -8861.0, -7373.0, -9131.0, -10361.0, -7756.0, -9508.0, -10147.0, -8535.0, -8870.0, -8664.0, -7640.0, -8152.0, -11202.0, -9830.0, -14536.0, -7765.0, -8507.0, -9194.0, -9698.0, -8801.0, -9794.0, -10843.0, -9094.0, -11555.0, -8230.0, -8618.0, -12591.0, -7280.0, -9270.0, -10629.0, -7916.0, -7563.0, -7504.0, -8923.0, -12032.0, -11272.0, -11785.0, -10353.0, -8072.0, -9311.0, -8757.0, -8411.0, -10647.0, -8892.0, -11061.0, -14338.0, -9574.0, -8011.0, -10047.0, -10370.0, -9320.0, -7314.0, -10692.0, -7864.0, -11332.0, -9527.0, -10305.0, -13010.0, -10746.0, -7953.0, -12917.0, -7706.0, -10052.0, -9316.0, -7677.0, -11603.0, -8130.0, -10688.0, -8410.0, -11692.0, -7518.0, -14775.0, -8894.0, -10202.0, -12513.0, -8256.0, -10259.0, -8342.0, -11683.0, -8510.0, -8189.0, -7704.0, -10232.0, -8863.0, -12462.0, -10627.0, -12163.0, -9989.0, -10452.0, -7736.0, -11448.0, -8205.0, -12253.0, -8066.0, -10657.0, -9019.0, -10777.0]\nfamily_size : [nan, 5.0, 6.0, 7.0, 15.0, 20.0, 9.0]"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#eda-범주형-변수-시각화-확인",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#eda-범주형-변수-시각화-확인",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "[EDA] 범주형 변수 : 시각화 확인",
    "text": "[EDA] 범주형 변수 : 시각화 확인\n\nedu_type(교육 수준)은 순서가 중요한 변수이므로 Label Encoder로 우선순위를 지정하는 것을 고려\n이외 변수는 모델의 혼동(상위관계 여부)을 방지하고자 One-hot인코딩 고려\n\n대다수의 변수가 2~6개의 unique값을 가져 크게 어려움은 없을 것으로 보임\n그러나 occyp_type(직업유형)의 경우는 값이 많은 관계로 좀 더 고민이 필요할 것으로 보임\n\n구간화는 도메인지식이 많지 않으므로 진행하지 않음\n\n\n# 범주형 변수 추리기\nx_categorical = x_train.select_dtypes(include=['object'])\nx_categorical.head(5)\n\n\n\n\n\n\n\n\ngender\ncar\nreality\nincome_type\nedu_type\nfamily_type\nhouse_type\noccyp_type\n\n\n\n\n0\nF\nN\nN\nCommercial associate\nHigher education\nMarried\nMunicipal apartment\nNone\n\n\n1\nF\nN\nY\nCommercial associate\nSecondary / secondary special\nCivil marriage\nHouse / apartment\nLaborers\n\n\n2\nM\nY\nY\nWorking\nHigher education\nMarried\nHouse / apartment\nManagers\n\n\n3\nF\nN\nY\nCommercial associate\nSecondary / secondary special\nMarried\nHouse / apartment\nSales staff\n\n\n4\nF\nY\nY\nState servant\nHigher education\nMarried\nHouse / apartment\nManagers\n\n\n\n\n\n\n\n\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# credit 데이터 합치기\ncombined_data = x_categorical.copy()\ncombined_data['credit'] = y_train.copy()  # 동일한 인덱스를 기준으로 credit 추가\n\n# 범주형 변수 설정\ncategorical_columns = ['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\ncols = 3\nnum_vars = len(categorical_columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 5))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor idx, var_nm in enumerate(categorical_columns):\n    ax = axes[idx]\n\n    # 데이터 그룹화 및 스택 데이터 계산\n    stacked_data = combined_data.groupby([var_nm, 'credit']).size().unstack(fill_value=0)\n\n    # 각 카테고리의 막대 그래프 그리기\n    bottom_values = np.zeros(len(stacked_data))\n    for credit_value in stacked_data.columns:\n        ax.bar(stacked_data.index, stacked_data[credit_value], bottom=bottom_values, label=f'Credit {credit_value}')\n        bottom_values += stacked_data[credit_value]\n\n    # 그래프 설정\n    ax.set_title(f'Stacked Bar Plot of {var_nm}')\n    ax.set_xlabel(var_nm)\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(title='Credit', loc='upper right')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\noccyp_type(직업유형)\n\noccyp_type의 경우는 앞서 확인했을 때, 유일하게 결측치가 있으면서 결측치가 많았음\n따라서 삭제가 아닌 대체가 필요한데, 결측치의 처리를 위해 결측치가 가지는 의미나 맥락을 고민해봄\n\n’무직’이라는 컬럼이 없는데, 결측치가 ’무직’을 의미하는 것은 아닐지 확인\n\nDAYS_EMPLOYED변수가 양수인(직업이 없는) 경우를 제외해도, 여전히 Null값이 존재\n무직이어서 Null인 값도 있지만, 무직이 아님에도 Null인 경우도 있음\nNull값에 따른 평균/최대/최소 임금 등을 시각해보았으나 별도의 인사이트는 얻지 못함\n\n\noccyp_type(직업유형)의 결측치 중, DAYS_EMPLOYED변수가 양수인 경우는 ‘NoJob’, 이외는 ’Missing’으로 대체하기로 함\n\nsklearn의 SimpleImputer라는 것도 있었으나, 최빈값 대체할 것은 아니므로 사용하지 않음\n\n\n\n\n# occyp_type(직업유형)의 값 확인\nx_categorical.occyp_type.value_counts()\n\noccyp_type\nLaborers                 4512\nCore staff               2646\nSales staff              2539\nManagers                 2167\nDrivers                  1575\nHigh skill tech staff    1040\nAccountants               902\nMedicine staff            864\nCooking staff             457\nSecurity staff            424\nCleaning staff            403\nPrivate service staff     243\nLow-skill Laborers        127\nWaiters/barmen staff      124\nSecretaries                97\nRealty agents              63\nHR staff                   62\nIT staff                   41\nName: count, dtype: int64\n\n\n\n# Null값에 따른 평균/최대/최소 임금 시각화. 별도의 인사이트는 얻지 못함\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 결측 여부 컬럼 추가\nx_train_temp = x_train[(x_train['DAYS_EMPLOYED']!=365243)].copy()\nx_train_temp['occyp_type_isnull'] = x_train_temp['occyp_type'].isnull()\n\n# 그룹별 income_total의 평균, 최대, 최소 계산\nsummary_stats = x_train_temp.groupby('occyp_type_isnull')['income_total'].agg(['mean', 'max', 'min']).reset_index()\nsummary_stats.columns = ['IsNull', 'Mean', 'Max', 'Min']\n\n# Boolean 값을 문자열로 변환\nsummary_stats['IsNull'] = summary_stats['IsNull'].astype(str)\n\n# 서브플롯 생성\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1행 2열의 서브플롯 생성\n\n# 왼쪽 플롯: 평균(Mean)\nsns.barplot(ax=axes[0], x='IsNull', y='Mean', data=summary_stats, alpha=0.7)\nfor i, row in summary_stats.iterrows():\n    axes[0].text(i, row['Mean'], f\"{row['Mean']:.2f}\", color='black', ha='center', va='bottom', fontsize=10)\naxes[0].set_title('Income Total - Mean by Null Status', fontsize=14)\naxes[0].set_xlabel('Occyp Type is Null', fontsize=12)\naxes[0].set_ylabel('Mean Income Total', fontsize=12)\naxes[0].grid(axis='y', linestyle='--', alpha=0.6)\n\n# 오른쪽 플롯: 최대(Max)와 최소(Min)\naxes[1].plot(summary_stats['IsNull'], summary_stats['Max'], color='red', marker='o', label='Max')\naxes[1].plot(summary_stats['IsNull'], summary_stats['Min'], color='blue', marker='o', label='Min')\nfor i, row in summary_stats.iterrows():\n    axes[1].text(i, row['Max'], f\"{row['Max']:.2f}\", color='red', ha='center', va='bottom', fontsize=10)\n    axes[1].text(i, row['Min'], f\"{row['Min']:.2f}\", color='blue', ha='center', va='top', fontsize=10)\naxes[1].set_title('Income Total - Max & Min by Null Status', fontsize=14)\naxes[1].set_xlabel('Occyp Type is Null', fontsize=12)\naxes[1].set_ylabel('Income Total', fontsize=12)\naxes[1].legend(title='Statistics', loc='upper right')\naxes[1].grid(axis='y', linestyle='--', alpha=0.6)\n\n# 레이아웃 조정 및 출력\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#y값-불균형도-확인",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#y값-불균형도-확인",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "y값 불균형도 확인",
    "text": "y값 불균형도 확인\n\n(변수 설명에 따르면)숫자가 낮을수록 신용도가 높은데, 신용도가 낮은 경우가 64%로 많은편임\n\n\ny_train.value_counts()\n\ncredit\n2.0    16968\n1.0     6267\n0.0     3222\nName: count, dtype: int64\n\n\n\ny_train.value_counts(normalize=True)\n\ncredit\n2.0    0.641343\n1.0    0.236875\n0.0    0.121783\nName: proportion, dtype: float64"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#전처리-결측이상치-처리-및-표준화정규화-등",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#전처리-결측이상치-처리-및-표준화정규화-등",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "[전처리] 결측/이상치 처리 및 표준화/정규화 등",
    "text": "[전처리] 결측/이상치 처리 및 표준화/정규화 등\n\n결측치 대체 (공통)\n\n모든(수치형/범주형) 변수를 통틀어 1개의 변수만 가지고 있던 결측치를 먼저 대체\n\n범주형 변수 중 occyp_type(직업유형)의 결측치를 조건에 따라 ‘NoJob’ 또는 ’Missing’으로 대체\n결측치 대체 후, Unique값 array 값이 아래와 같이 변경되었음\n\n\n\n# 결측치 대체 전\n\nprint(f\"Unique data수 : {len(x_train.occyp_type.unique())}\")\nx_train.occyp_type.unique()\n\nUnique data수 : 19\n\n\narray([None, 'Laborers', 'Managers', 'Sales staff',\n       'High skill tech staff', 'Core staff', 'Drivers', 'Medicine staff',\n       'Accountants', 'Realty agents', 'Security staff', 'Cleaning staff',\n       'Private service staff', 'Cooking staff', 'Secretaries',\n       'HR staff', 'IT staff', 'Low-skill Laborers',\n       'Waiters/barmen staff'], dtype=object)\n\n\n\n# 결측치 대체 후\n\nimport pandas as pd\n\ndef fill_occyp_type(row):\n    if pd.isna(row['occyp_type']):\n        if row['DAYS_EMPLOYED'] &gt; 0:\n            return 'NoJob'\n        else:\n            return 'Missing'\n    return row['occyp_type']\n\nx_train['occyp_type'] = x_train.apply(fill_occyp_type, axis=1)\n\n# 변화 확인\nprint(f\"Unique data수 : {len(x_train.occyp_type.unique())}\")\nx_train.occyp_type.unique()\n\nUnique data수 : 20\n\n\narray(['Missing', 'Laborers', 'Managers', 'Sales staff',\n       'High skill tech staff', 'Core staff', 'Drivers', 'Medicine staff',\n       'Accountants', 'NoJob', 'Realty agents', 'Security staff',\n       'Cleaning staff', 'Private service staff', 'Cooking staff',\n       'Secretaries', 'HR staff', 'IT staff', 'Low-skill Laborers',\n       'Waiters/barmen staff'], dtype=object)\n\n\n\n# 범주형, 수치형 변수로 나누어 작업\n\nx_categorical = x_train.select_dtypes(include=['object'])\nx_numerical = x_train.select_dtypes(include=['float64', 'int64'])\n\n\n\n수치형 변수 전처리\n\n값이 0, 1로 유무만을 나타내는 변수는 처리하지 않음\n\n대상 : FLAG_MOBIL, work_phone, phone, email\n\n값이 음수인 변수(역산하는 변수)는 아래와 같이 처리\n\n대상 : DAYS_BIRTH, DAYS_EMPLOYED, begin_month\n(공통) 표준화/정규화/로그변환 전, 최소값 조정 진행\n로그변환 : DAYS_EMPLOYED\n\n무직임을 표현하려면 양수이기만 하면 되므로, 기준의 365243을 1로 대체\nIQR기준 이상치가 있는 것으로 판명되긴하나, 값이 많고 처리기준을 정하기 어려워, 양수를 1로 대체하여 숫자 범위를 한번 줄인 후, 로그변환 진행\n\n표준화 : DAYS_BIRTH, begin_month\n\nIQR기준 이상치를 보지 못했으며, 첨도가 -1정도로 표준화 진행\n향후 사용할 모델에 따라 변경할 수 있음\n아래 기준은 GPT로 1차적으로 받아본 기준이며, 실제 진행 전에 좀 더 알아보고 결정\n\n표준화 : 선형모델, SVM, KNN\n정규화 : 거리기반 모델(클러스터링, KNN)\n\n\n\n이외의 변수는 정규화/표준화 중, 아래의 관점을 기준으로 정규화를 적용하기로 함\n\n대상 : child_num, income_total, family_size\n분포상 중심에서 떨어진 이상치를 보유하고있으며, 이상치 대체를 선행하기엔 영향을 알기 어려워 진행하지 않음\n표준화는 이상치의 영향을 많이 받을 수 있다는 점에서 정규화로 선택함\n\n\n\n# 수치형변수 현황 확인\nx_numerical.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nindex\n26457.0\n13228.000000\n7637.622372\n0.0\n6614.0\n13228.0\n19842.0\n26456.0\n\n\nchild_num\n26457.0\n0.428658\n0.747326\n0.0\n0.0\n0.0\n1.0\n19.0\n\n\nincome_total\n26457.0\n187306.524493\n101878.367995\n27000.0\n121500.0\n157500.0\n225000.0\n1575000.0\n\n\nDAYS_BIRTH\n26457.0\n-15958.053899\n4201.589022\n-25152.0\n-19431.0\n-15547.0\n-12446.0\n-7705.0\n\n\nDAYS_EMPLOYED\n26457.0\n59068.750728\n137475.427503\n-15713.0\n-3153.0\n-1539.0\n-407.0\n365243.0\n\n\nFLAG_MOBIL\n26457.0\n1.000000\n0.000000\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nwork_phone\n26457.0\n0.224742\n0.417420\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nphone\n26457.0\n0.294251\n0.455714\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\nemail\n26457.0\n0.091280\n0.288013\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nfamily_size\n26457.0\n2.196848\n0.916717\n1.0\n2.0\n2.0\n3.0\n20.0\n\n\nbegin_month\n26457.0\n-26.123294\n16.559550\n-60.0\n-39.0\n-24.0\n-12.0\n0.0\n\n\n\n\n\n\n\n\n이상치 대체(DAYS_EMPLOYED)\n\n# 이상치 대체 전\nx_numerical['DAYS_EMPLOYED'].max()\n\n365243\n\n\n\n# 이상치 대체 후\n\n# DAYS_EMPLOYED : 365243를 1로 대체\nx_numerical['DAYS_EMPLOYED'] = x_numerical['DAYS_EMPLOYED'].apply(lambda x: 1 if x == 365243.0 else x)\n\nx_numerical['DAYS_EMPLOYED'].max()\n\n1\n\n\n\n\n최소값 조정(음수변수들 : DAYS_BIRTH, DAYS_EMPLOYED, begin_month)\n\n# 최소값 변환 전\n \nx_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()\n\nDAYS_BIRTH      -25152.0\nDAYS_EMPLOYED   -15713.0\nbegin_month        -60.0\ndtype: float64\n\n\n\n# 최소값 변환 후\n\n# min() * -1  을 더하여 변환\nx_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] = x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] + x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()*-1\n\nx_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()\n\nDAYS_BIRTH       0.0\nDAYS_EMPLOYED    0.0\nbegin_month      0.0\ndtype: float64\n\n\n\n\n로그변환(‘DAYS_EMPLOYED’)\n\n# 로그변환 전\n\nx_numerical['DAYS_EMPLOYED'].head(5)\n\n0    11004.0\n1    14173.0\n2    11279.0\n3    13621.0\n4    13608.0\nName: DAYS_EMPLOYED, dtype: float64\n\n\n\n# 로그변환 후\n\nx_numerical['DAYS_EMPLOYED'] = x_numerical['DAYS_EMPLOYED'].apply(lambda x : np.log1p(x))\nx_numerical['DAYS_EMPLOYED'].head(5)\n\n0    9.306105\n1    9.559165\n2    9.330787\n3    9.519441\n4    9.518487\nName: DAYS_EMPLOYED, dtype: float64\n\n\n\n\n표준화 적용(‘DAYS_BIRTH’, ‘begin_month’)\n\n# 표준화 적용 전\n\nx_numerical[['DAYS_BIRTH', 'begin_month']].head(5)\n\n\n\n\n\n\n\n\nDAYS_BIRTH\nbegin_month\n\n\n\n\n0\n11253.0\n54.0\n\n\n1\n13772.0\n55.0\n\n\n2\n6065.0\n38.0\n\n\n3\n10064.0\n23.0\n\n\n4\n10115.0\n34.0\n\n\n\n\n\n\n\n\n# 표준화 적용 후\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nx_numerical[['DAYS_BIRTH', 'begin_month']] = std_scaler.fit_transform(x_numerical[['DAYS_BIRTH', 'begin_month']])\nx_numerical[['DAYS_BIRTH', 'begin_month']].head(5)\n\n\n\n\n\n\n\n\nDAYS_BIRTH\nbegin_month\n\n\n\n\n0\n0.490075\n1.215231\n\n\n1\n1.089621\n1.275620\n\n\n2\n-0.744719\n0.249003\n\n\n3\n0.207081\n-0.656836\n\n\n4\n0.219220\n0.007446\n\n\n\n\n\n\n\n\n\n정규화 적용(child_num, income_total, family_size)\n\n# 정규화 적용 전\n\nx_numerical[['child_num', 'income_total', 'family_size']].head(5)\n\n\n\n\n\n\n\n\nchild_num\nincome_total\nfamily_size\n\n\n\n\n0\n0\n202500.0\n2.0\n\n\n1\n1\n247500.0\n3.0\n\n\n2\n0\n450000.0\n2.0\n\n\n3\n0\n202500.0\n2.0\n\n\n4\n0\n157500.0\n2.0\n\n\n\n\n\n\n\n\n# 정규화 적용 후\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler()\nx_numerical[['child_num', 'income_total', 'family_size']]  = min_max_scaler.fit_transform(x_numerical[['child_num', 'income_total', 'family_size']] )\nx_numerical[['child_num', 'income_total', 'family_size']].head(5)\n\n\n\n\n\n\n\n\nchild_num\nincome_total\nfamily_size\n\n\n\n\n0\n0.000000\n0.113372\n0.052632\n\n\n1\n0.052632\n0.142442\n0.105263\n\n\n2\n0.000000\n0.273256\n0.052632\n\n\n3\n0.000000\n0.113372\n0.052632\n\n\n4\n0.000000\n0.084302\n0.052632\n\n\n\n\n\n\n\n\n\n\n범주형 변수 전처리\n\nOrdinal변수에 대해서는 Label인코딩 적용\n\n대상 : edu_type\n\nUnique값이 2~6개인 변수에 대해서는 One-hot Encoding 적용\n\n대상 : ‘gender’, ‘car’, ‘reality’, ‘income_type’, ‘family_type’, ‘house_type’\n22개 컬럼(변수) 추가될 것으로 예상\n\nUnique값이 20개인 변수에 대해서는, 아래의 방법을 적용\n\n대상 : ‘occyp_type’\n변수의 과도한 증가와, 상하관계 착각을 막기위해 다른 방법을 고민\nTarget인코딩이라는 것을 적용해보기로 함 (with LeaveOneOutEncoder)\n\n향후 적용여부에 따라 모델성능이 저하되거나 유의미하지 않다면 변수 제외를 고려(계산복잡성 등을 고려함)\n\n\n\n\nx_categorical.describe()\n\n\n\n\n\n\n\n\ngender\ncar\nreality\nincome_type\nedu_type\nfamily_type\nhouse_type\noccyp_type\n\n\n\n\ncount\n26457\n26457\n26457\n26457\n26457\n26457\n26457\n26457\n\n\nunique\n2\n2\n2\n5\n5\n5\n6\n20\n\n\ntop\nF\nN\nY\nWorking\nSecondary / secondary special\nMarried\nHouse / apartment\nLaborers\n\n\nfreq\n17697\n16410\n17830\n13645\n17995\n18196\n23653\n4512\n\n\n\n\n\n\n\n\nLabel인코딩 : Ordinal변수\n\n# Label인코딩 전\n\nx_categorical['edu_type'].unique()\n\narray(['Higher education', 'Secondary / secondary special',\n       'Incomplete higher', 'Lower secondary', 'Academic degree'],\n      dtype=object)\n\n\n\n# Label인코딩 후 (Order값은 낮은 것부터 입력)\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\norder = [\n    'Lower secondary',\n    'Secondary / secondary special',\n    'Incomplete higher',\n    'Higher education',\n    'Academic degree'\n]\n\no_encoder = OrdinalEncoder(categories=[order])\noe_df = o_encoder.fit_transform(x_categorical[['edu_type']])\nx_categorical['edu_type'] = oe_df.flatten()\n\n\nx_categorical['edu_type'].unique()\n\narray([3., 1., 2., 0., 4.])\n\n\n\n\nOne-hot인코딩 : Unique값이 2~6개인 변수\n\n# One-hot인코딩 전\n\nx_categorical.columns\n\nIndex(['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type',\n       'house_type', 'occyp_type'],\n      dtype='object')\n\n\n\n# One-hot 인코딩 데이터프레임 생성\n\nfrom sklearn.preprocessing import OneHotEncoder\n\none_hot_columns = ['gender', 'car', 'reality', 'income_type', 'family_type', 'house_type']\n\noh_encoder = OneHotEncoder(sparse_output=False)\nonehot_encoded = oh_encoder.fit_transform(x_categorical[one_hot_columns])\nencoded_col_names = oh_encoder.get_feature_names_out(input_features=one_hot_columns)\n\ndf_onehot_encoded = pd.DataFrame(onehot_encoded, columns=encoded_col_names, index=x_categorical.index)\ndf_onehot_encoded.columns\n\nIndex(['gender_F', 'gender_M', 'car_N', 'car_Y', 'reality_N', 'reality_Y',\n       'income_type_Commercial associate', 'income_type_Pensioner',\n       'income_type_State servant', 'income_type_Student',\n       'income_type_Working', 'family_type_Civil marriage',\n       'family_type_Married', 'family_type_Separated',\n       'family_type_Single / not married', 'family_type_Widow',\n       'house_type_Co-op apartment', 'house_type_House / apartment',\n       'house_type_Municipal apartment', 'house_type_Office apartment',\n       'house_type_Rented apartment', 'house_type_With parents'],\n      dtype='object')\n\n\n\n# Ont-hot인코딩 대상 컬럼 미리 Drop (별도로 생성한 인코딩된 데이터프레임을 다시 합칠 예정)\n\nx_categorical.drop(one_hot_columns, axis=1, inplace=True)\nx_categorical.columns\n\nIndex(['edu_type', 'occyp_type'], dtype='object')\n\n\n\n# One-hot인코딩 후\n\n# Ont-hot인코딩된 데이터프레임 Concat\nx_categorical = pd.concat([x_categorical, df_onehot_encoded], axis=1)\n\n# Ont-hot인코딩된 최종컬럼 확인\nx_categorical.columns\n\nIndex(['edu_type', 'occyp_type', 'gender_F', 'gender_M', 'car_N', 'car_Y',\n       'reality_N', 'reality_Y', 'income_type_Commercial associate',\n       'income_type_Pensioner', 'income_type_State servant',\n       'income_type_Student', 'income_type_Working',\n       'family_type_Civil marriage', 'family_type_Married',\n       'family_type_Separated', 'family_type_Single / not married',\n       'family_type_Widow', 'house_type_Co-op apartment',\n       'house_type_House / apartment', 'house_type_Municipal apartment',\n       'house_type_Office apartment', 'house_type_Rented apartment',\n       'house_type_With parents'],\n      dtype='object')\n\n\n\n\nTarget인코딩 : Ordinal이 아니고 Unique값이 많은 변수\n\n# Target인코딩 전\nx_categorical['occyp_type'].__len__\n\n&lt;bound method Series.__len__ of 0               Missing\n1              Laborers\n2              Managers\n3           Sales staff\n4              Managers\n              ...      \n26452        Core staff\n26453           Missing\n26454        Core staff\n26455          Laborers\n26456    Security staff\nName: occyp_type, Length: 26457, dtype: object&gt;\n\n\n\n# Target인코딩 후\n\nimport category_encoders as ce\nimport joblib\n\nx_column = 'occyp_type'\n\nleave_one_out_encoder = ce.LeaveOneOutEncoder(cols=x_column, sigma=0.1, return_df=True)\ndf_target_encoded = leave_one_out_encoder.fit_transform(x_categorical[x_column], y_train)\n\njoblib.dump(leave_one_out_encoder, 'encoder_leave_one_out.pkl')\n\ndf_target_encoded['occyp_type'].__len__\n\n&lt;bound method Series.__len__ of 0        1.570514\n1        1.621321\n2        1.552459\n3        1.461605\n4        1.727354\n           ...   \n26452    1.405981\n26453    1.556405\n26454    1.389829\n26455    1.376017\n26456    1.597510\nName: occyp_type, Length: 26457, dtype: float64&gt;\n\n\n\nx_categorical['occyp_type'] = df_target_encoded['occyp_type']"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#전처리된-데이터-저장",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#전처리된-데이터-저장",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "전처리된 데이터 저장",
    "text": "전처리된 데이터 저장\n\n# 범주형/수치형 변수로 나누어 작업한 파일 합치기\nx_train_preprocessed = pd.concat([x_categorical, x_numerical], axis=1)\nx_train_preprocessed\n\n\n\n\n\n\n\n\nedu_type\noccyp_type\ngender_F\ngender_M\ncar_N\ncar_Y\nreality_N\nreality_Y\nincome_type_Commercial associate\nincome_type_Pensioner\n...\nchild_num\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\n\n\n\n\n0\n3.0\n1.570514\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.000000\n0.113372\n0.490075\n9.306105\n1\n0\n0\n0\n0.052632\n1.215231\n\n\n1\n1.0\n1.621321\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.052632\n0.142442\n1.089621\n9.559165\n1\n0\n0\n1\n0.105263\n1.275620\n\n\n2\n3.0\n1.552459\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.273256\n-0.744719\n9.330787\n1\n0\n1\n0\n0.052632\n0.249003\n\n\n3\n1.0\n1.461605\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.000000\n0.113372\n0.207081\n9.519441\n1\n0\n1\n0\n0.052632\n-0.656836\n\n\n4\n3.0\n1.727354\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.084302\n0.219220\n9.518487\n1\n0\n0\n0\n0.052632\n0.007446\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26452\n1.0\n1.405981\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.105263\n0.127907\n0.923252\n9.527338\n1\n0\n0\n0\n0.157895\n1.456788\n\n\n26453\n3.0\n1.556405\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.052632\n0.098837\n0.158765\n9.490922\n1\n0\n0\n0\n0.052632\n-1.260729\n\n\n26454\n1.0\n1.389829\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n...\n0.000000\n0.171512\n1.398558\n9.525078\n1\n0\n0\n0\n0.052632\n0.067835\n\n\n26455\n2.0\n1.376017\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.093023\n1.383563\n9.655475\n1\n0\n0\n0\n0.000000\n-1.985400\n\n\n26456\n1.0\n1.597510\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.000000\n0.034884\n-0.859440\n9.595671\n1\n0\n0\n0\n0.052632\n1.034063\n\n\n\n\n26457 rows × 35 columns\n\n\n\n\n# CSV저장\nx_train_preprocessed.to_csv('train_preprocessed.csv')\n\n\n# DB저장\ndb_controller.df_to_table('train_pre', x_train_preprocessed)\n\n\n# DB저장된 내용 확인\ndb_controller.search_db_show_df(\"SELECT * FROM train_pre\")\n\n\n\n\n\n\n\n\nedu_type\noccyp_type\ngender_F\ngender_M\ncar_N\ncar_Y\nreality_N\nreality_Y\nincome_type_Commercial associate\nincome_type_Pensioner\n...\nchild_num\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\n\n\n\n\n0\n3.0\n1.570514\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.000000\n0.113372\n0.490075\n9.306105\n1\n0\n0\n0\n0.052632\n1.215231\n\n\n1\n1.0\n1.621321\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.052632\n0.142442\n1.089621\n9.559165\n1\n0\n0\n1\n0.105263\n1.275620\n\n\n2\n3.0\n1.552459\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.273256\n-0.744719\n9.330787\n1\n0\n1\n0\n0.052632\n0.249003\n\n\n3\n1.0\n1.461605\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.000000\n0.113372\n0.207081\n9.519441\n1\n0\n1\n0\n0.052632\n-0.656836\n\n\n4\n3.0\n1.727354\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.084302\n0.219220\n9.518487\n1\n0\n0\n0\n0.052632\n0.007446\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26452\n1.0\n1.405981\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.105263\n0.127907\n0.923252\n9.527338\n1\n0\n0\n0\n0.157895\n1.456788\n\n\n26453\n3.0\n1.556405\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.052632\n0.098837\n0.158765\n9.490922\n1\n0\n0\n0\n0.052632\n-1.260729\n\n\n26454\n1.0\n1.389829\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n...\n0.000000\n0.171512\n1.398558\n9.525078\n1\n0\n0\n0\n0.052632\n0.067835\n\n\n26455\n2.0\n1.376017\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.093023\n1.383563\n9.655475\n1\n0\n0\n0\n0.000000\n-1.985400\n\n\n26456\n1.0\n1.597510\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.000000\n0.034884\n-0.859440\n9.595671\n1\n0\n0\n0\n0.052632\n1.034063\n\n\n\n\n26457 rows × 35 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#향후-test데이터셋에-쓰기위한-전처리-코드-정리",
    "href": "posts/meta-cm-sql_and_ml_xai-20250107/index.html#향후-test데이터셋에-쓰기위한-전처리-코드-정리",
    "title": "[DA스터디/3주차/과제] EDA 및 전처리",
    "section": "향후 Test데이터셋에 쓰기위한 전처리 코드 정리",
    "text": "향후 Test데이터셋에 쓰기위한 전처리 코드 정리\n\nimport pandas as pd\nimport os\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder\nimport category_encoders as ce\nimport joblib\nfrom pkb_sqlite3 import DB_sqlite3\n\ndef preprocess_x_data(train_or_test:str, dataset:list[DataFrame], output_name:dict)-&gt;None:\n\n    # 파라메터 검증 및 데이터셋 복사\n    if train_or_test == 'train':\n        if len(dataset) == 2:\n            x_train = dataset[0].copy()\n            y_train = dataset[1].copy()\n        else:\n            return 'train인 경우 x,y 데이터가 필요합니다'\n    elif train_or_test == 'test':\n        if len(dataset) == 1:\n            x_train = dataset[0].copy()\n        else:\n            return 'test인 경우 x 데이터만 필요합니다'\n    else:\n        return 'train 또는 test만 입력 가능합니다'\n\n    \n    # (공통)결측치 대체 : occyp_type\n    def fill_occyp_type(row):\n            if pd.isna(row['occyp_type']):\n                if row['DAYS_EMPLOYED'] &gt; 0:\n                    return 'NoJob'\n                else:\n                    return 'Missing'\n            return row['occyp_type']\n\n    x_train['occyp_type'] = x_train.apply(fill_occyp_type, axis=1)\n\n    # 범주형/수치형 변수로 분할하여 작업\n    x_numerical = x_train.select_dtypes(include=['float64', 'int64'])\n    x_categorical = x_train.select_dtypes(include=['object'])\n\n    # (수치형변수 전처리) 이상치 대체   *** 주의 : 표준화 전에 수행되어야 함\n    x_numerical['DAYS_EMPLOYED'] = x_numerical['DAYS_EMPLOYED'].apply(lambda x: 1 if x == 365243.0 else x)\n\n    # (수치형변수 전처리) 최소값 조정\n    x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] = x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']] + x_numerical[['DAYS_BIRTH', 'DAYS_EMPLOYED', 'begin_month']].min()*-1\n\n    # (수치형변수 전처리) 로그변환\n    x_numerical['DAYS_EMPLOYED'] = x_numerical['DAYS_EMPLOYED'].apply(lambda x : np.log1p(x))\n\n    # (수치형변수 전처리) 표준화\n    std_scaler = StandardScaler()\n    x_numerical[['DAYS_BIRTH', 'begin_month']] = std_scaler.fit_transform(x_numerical[['DAYS_BIRTH', 'begin_month']])\n\n    # (수치형변수 전처리) 정규화\n    min_max_scaler = MinMaxScaler()\n    x_numerical[['child_num', 'income_total', 'family_size']]  = min_max_scaler.fit_transform(x_numerical[['child_num', 'income_total', 'family_size']] )\n\n    # (범주형변수 전처리) Label인코딩 (Ordinal변수)\n    order = [\n        'Lower secondary',\n        'Secondary / secondary special',\n        'Incomplete higher',\n        'Higher education',\n        'Academic degree'\n    ]\n\n    o_encoder = OrdinalEncoder(categories=[order])\n    oe_df = o_encoder.fit_transform(x_categorical[['edu_type']])\n    x_categorical['edu_type'] = oe_df.flatten()\n\n    # (범주형변수 전처리) One-hot인코딩 (Unique값 6개 이하)\n\n    ## One-hot 인코딩 데이터프레임 생성\n    one_hot_columns = ['gender', 'car', 'reality', 'income_type', 'family_type', 'house_type']\n\n    oh_encoder = OneHotEncoder(sparse_output=False)\n    onehot_encoded = oh_encoder.fit_transform(x_categorical[one_hot_columns])\n    encoded_col_names = oh_encoder.get_feature_names_out(input_features=one_hot_columns)\n\n    df_onehot_encoded = pd.DataFrame(onehot_encoded, columns=encoded_col_names, index=x_categorical.index)\n\n    ## Ont-hot인코딩 대상 컬럼 미리 Drop (별도로 생성한 인코딩된 데이터프레임을 다시 합칠 예정)\n    x_categorical.drop(one_hot_columns, axis=1, inplace=True)\n\n    ## Ont-hot인코딩된 데이터프레임 Concat\n    x_categorical = pd.concat([x_categorical, df_onehot_encoded], axis=1)\n\n    # (범주형변수 전처리) Target인코딩 (Ordinal이 아니면서 Unique값이 많은 경우)\n    x_column = 'occyp_type'\n                                \n    ## 파일로 저장해둔 인코더가 있으면 로딩, 아니면 생성\n    if os.path.isfile('encoder_leave_one_out.pkl'):\n        leave_one_out_encoder = joblib.load('encoder_leave_one_out.pkl')\n    else:\n        leave_one_out_encoder = ce.LeaveOneOutEncoder(cols=x_column, sigma=0.1, return_df=True)\n\n    ## 인코딩된 데이터 생성 및 인코더 업데이트\n    if train_or_test == 'train':\n        df_target_encoded = leave_one_out_encoder.fit_transform(x_categorical[x_column], y_train)\n    elif train_or_test == 'test':\n        df_target_encoded = leave_one_out_encoder.transform(x_categorical[x_column])\n    joblib.dump(leave_one_out_encoder, 'encoder_leave_one_out.pkl')\n\n    x_categorical['occyp_type'] = df_target_encoded['occyp_type']\n    \n\n    # 나누어 작업한 수치형/범주형 변수 합치고 저장\n    x_train_preprocessed = pd.concat([x_categorical, x_numerical], axis=1)\n\n    ## CSV저장 \n    x_train_preprocessed.to_csv(output_name['csv_file_name']) # 'train_preprocessed.csv'\n\n    ## DB저장 (아래 함수는 이미 테이블이 있는 경우 덮어씀)\n    db_controller.df_to_table(output_name['db_table_name'], x_train_preprocessed) # 'train_pre'\n\n    return '작업이 완료되었습니다'\n\n\n# DB로딩 및 데이터 가져오기\ndb_controller = DB_sqlite3('Dacon_creditcard_overdue.db')\ndf_train = db_controller.search_db_show_df('SELECT * FROM train')\ndf_test = db_controller.search_db_show_df('SELECT * FROM test')\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\n\n# train 데이터 전처리\ny_train = df_train['credit'].copy()\nx_train = df_train.drop('credit',axis=1).copy()\n\npreprocess_x_data(train_or_test='train', \n                  dataset=[x_train, y_train], \n                  output_name={'csv_file_name':'train_preprocessed.csv', \n                               'db_table_name':'train_pre'}\n                  )\n\n'작업이 완료되었습니다'\n\n\n\n# 전처리된 데이터 확인\ndb_controller.search_db_show_df(\"SELECT * FROM train_pre\").head(3)\n\n\n\n\n\n\n\n\nedu_type\noccyp_type\ngender_F\ngender_M\ncar_N\ncar_Y\nreality_N\nreality_Y\nincome_type_Commercial associate\nincome_type_Pensioner\n...\nchild_num\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\n\n\n\n\n0\n3.0\n1.477037\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.000000\n0.113372\n0.556662\n11004.0\n1\n0\n0\n0\n0.052632\n1.215231\n\n\n1\n1.0\n1.417380\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.052632\n0.142442\n0.870181\n14173.0\n1\n0\n0\n1\n0.105263\n1.275620\n\n\n2\n3.0\n1.452349\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.000000\n0.273256\n-0.402621\n11279.0\n1\n0\n1\n0\n0.052632\n0.249003\n\n\n\n\n3 rows × 35 columns\n\n\n\n\n# test 데이터 전처리\ndf_test = db_controller.search_db_show_df('SELECT * FROM test')\n\npreprocess_x_data(train_or_test='test', \n                  dataset=[df_test], \n                  output_name={'csv_file_name':'test_preprocessed.csv', \n                               'db_table_name':'test_pre'}\n                  )\n\n'작업이 완료되었습니다'\n\n\n\n# 전처리된 데이터 확인\ndb_controller.search_db_show_df(\"SELECT * FROM test_pre\").head(3)\n\n\n\n\n\n\n\n\nedu_type\noccyp_type\ngender_F\ngender_M\ncar_N\ncar_Y\nreality_N\nreality_Y\nincome_type_Commercial associate\nincome_type_Pensioner\nincome_type_State servant\nincome_type_Student\nincome_type_Working\nfamily_type_Civil marriage\nfamily_type_Married\nfamily_type_Separated\nfamily_type_Single / not married\nfamily_type_Widow\nhouse_type_Co-op apartment\nhouse_type_House / apartment\nhouse_type_Municipal apartment\nhouse_type_Office apartment\nhouse_type_Rented apartment\nhouse_type_With parents\nindex\nchild_num\nincome_total\nDAYS_BIRTH\nDAYS_EMPLOYED\nFLAG_MOBIL\nwork_phone\nphone\nemail\nfamily_size\nbegin_month\n\n\n\n\n0\n1.0\n1.514196\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n26457\n-0.596243\n-0.714468\n0.179018\n15662.0\n1\n0\n1\n0\n-0.225667\n0.000000\n\n\n1\n3.0\n1.502646\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n26458\n-0.596243\n-0.492869\n0.350337\n6990.0\n1\n0\n1\n0\n-0.225667\n0.400000\n\n\n2\n1.0\n1.500443\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n26459\n-0.596243\n-1.139229\n0.524543\n15444.0\n1\n1\n1\n0\n-0.225667\n0.333333\n\n\n\n\n\n\n\n\n# DB연결해제\ndb_controller.close()"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html",
    "href": "posts/coach-ml-kaggle-20230506/index.html",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "",
    "text": "Kaggle Korea - House price prediction 실습 기록용으로 남깁니다.\nKaggle 원문 링크\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#개요",
    "href": "posts/coach-ml-kaggle-20230506/index.html#개요",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "",
    "text": "Kaggle Korea - House price prediction 실습 기록용으로 남깁니다.\nKaggle 원문 링크"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#개념",
    "href": "posts/coach-ml-kaggle-20230506/index.html#개념",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "개념",
    "text": "개념\nRMSE(Root Mean Squeare Error)\nRoot    (4)\nMean    (3)\nSquare  (2)\nError   (1)\n(1) 실제값에서 예측값을 뺀 '오차'를\n(2) 합했을 때 음수의 영향을 제거하기 위해 '제곱'하고\n(3) '평균'오차로 만든 후\n(4) '루트'를 씌워 값의 크기를 작게 한다 (값을 작게하여 연산속도에 이점이 있다)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#파일-다운로드-및-알아보기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#파일-다운로드-및-알아보기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "파일 다운로드 및 알아보기",
    "text": "파일 다운로드 및 알아보기\nFile descriptions\ntrain.csv - 예측 모델을 만들기 위해 사용하는 학습 데이터입니다. \n    집의 정보와 예측할 변수인 가격(Price) 변수를 가지고 있습니다.\ntest.csv - 학습셋으로 만든 모델을 가지고 예측할 가격(Price) 변수를 제외한 집의 정보가\n    담긴 테스트 데이터 입니다.\nsample_submission.csv - 제출시 사용할 수 있는 예시 submission.csv 파일입니다.\nData fields\nID : 집을 구분하는 번호\ndate : 집을 구매한 날짜\nprice : 집의 가격(Target variable)\nbedrooms : 침실의 수\nbathrooms : 화장실의 수\nsqft_living : 주거 공간의 평방 피트(면적)\nsqft_lot : 부지의 평방 피트(면적)\nfloors : 집의 층 수\nwaterfront : 집의 전방에 강이 흐르는지 유무 (a.k.a. 리버뷰)\nview : 집이 얼마나 좋아 보이는지의 정도\ncondition : 집의 전반적인 상태\ngrade : King County grading 시스템 기준으로 매긴 집의 등급\nsqft_above : 지하실을 제외한 평방 피트(면적)\nsqft_basement : 지하실의 평방 피트(면적)\nyr_built : 지어진 년도\nyr_renovated : 집을 재건축한 년도\nzipcode : 우편번호\nlat : 위도\nlong : 경도\nsqft_living15 : 2015년 기준 주거 공간의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)\nsqft_lot15 : 2015년 기준 부지의 평방 피트(면적, 집을 재건축했다면, 변화가 있을 수 있음)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#패키지-및-데이터-불러오기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#패키지-및-데이터-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "패키지 및 데이터 불러오기",
    "text": "패키지 및 데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data_path = './data/train.csv'\ntest_data_path = './data/test.csv'\n\ndata = pd.read_csv(train_data_path)\ntest = pd.read_csv(test_data_path)\nprint('train data : {}'.format(data.shape))\nprint('test data : {}'.format(test.shape))\n\ntrain data : (15035, 21)\ntest data : (6555, 20)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#데이터-전처리",
    "href": "posts/coach-ml-kaggle-20230506/index.html#데이터-전처리",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\n정답컬럼 분리\n\ntest데이터와 달리 train data에는 컬럼이 1개 더 있음 (정답컬럼인 price)\n별도의 정답 데이터(y)로 분리\n\n\nprint('컬럼 분리 전')\nprint(data.columns)\nprint(test.columns)\n\n컬럼 분리 전\nIndex(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\n\n\n\n# y라는 변수에 price(정답)을 옮기고, 전체데이터를 백업(data_backup에 할당)하고 price컬럼 삭제\ny = data['price'] \ndata_backup = data.copy()\ndata.drop('price',axis=1, inplace=True)\n\n\nprint('컬럼 분리 후')\nprint(data.columns)\nprint(test.columns)\nprint(y.name)\n\n컬럼 분리 후\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nIndex(['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15'],\n      dtype='object')\nprice\n\n\n\n\n결측치 확인 및 제거\n\ntrain, test 데이터를 합쳐서 한번에 확인\n\n\n# 합치기\ndf_chk_missing = pd.concat((data, test), axis=0)\n\n# 향후 분할을 대비한 행 수 저장\ntrain_length = len(data)\ntest_length = len(test)\n\nprint(train_length, test_length)\n\n15035 6555\n\n\n\n결측치 확인방법1(pandas)\n\nisna()로 결측치를 확인\n\n\nprint(df_chk_missing.isna().sum())\n\nid               0\ndate             0\nbedrooms         0\nbathrooms        0\nsqft_living      0\nsqft_lot         0\nfloors           0\nwaterfront       0\nview             0\ncondition        0\ngrade            0\nsqft_above       0\nsqft_basement    0\nyr_built         0\nyr_renovated     0\nzipcode          0\nlat              0\nlong             0\nsqft_living15    0\nsqft_lot15       0\ndtype: int64\n\n\n\n\n결측치 확인방법2(missingno)\n\nmissingno 패키지로 컬럼별 결측치 시각화\n\n\nimport missingno\n\nmissingno.matrix(df_chk_missing)\n\n\n\n\n\n\n\n\n\n\n결측치 확인방법3(ydata_profiling)\n\nydata_profiling 패키지로 결측치 및 다양한 값 확인 가능\n렌더링 용량 문제로 실행결과는 이미지로 대체(RangeError: Maximum call stack size exceeded)\n\n\nfrom ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df_chk_missing, title=\"Profiling Report\")\nprofile\n\n\n\n\n실행결과 샘플\n\n\n\n결측치가 없으므로 다음 과정을 진행\n\n\n\n\n불필요한 변수 제거, 데이터 변환 등\n\n단순식별용 데이터 삭제\n\n가격과 관계없는 단순식별용 데이터인 id 삭제\n\n\nmain_id = df_chk_missing['id'][:train_length]\ntest_id = df_chk_missing['id'][train_length:]\ndel df_chk_missing['id']\n\n\n\n불필요한 데이터 삭제\n\n날짜 뒤에 T00000과 같이 시간데이터(로 추정됨)가 있는데, 모두 T00000으로만 되어있으므로 삭제\n\n\n# T000000으로 되어있는 값 세기\ndf_chk_missing['date'].str.contains('T000000').value_counts()\n\ndate\nTrue    21590\nName: count, dtype: int64\n\n\n\n# apply로 lambda함수를 사용하여, date컬럼의 앞자리만 저장\ndf_chk_missing['date'] = df_chk_missing['date'].apply(lambda x : str(x[:6]))\ndf_chk_missing.head()\n\n\n\n\n\n\n\n\ndate\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n0\n201410\n3\n1.00\n1180\n5650\n1.0\n0\n0\n3\n7\n1180\n0\n1955\n0\n98178\n47.5112\n-122.257\n1340\n5650\n\n\n1\n201502\n2\n1.00\n770\n10000\n1.0\n0\n0\n3\n6\n770\n0\n1933\n0\n98028\n47.7379\n-122.233\n2720\n8062\n\n\n2\n201502\n3\n2.00\n1680\n8080\n1.0\n0\n0\n3\n8\n1680\n0\n1987\n0\n98074\n47.6168\n-122.045\n1800\n7503\n\n\n3\n201406\n3\n2.25\n1715\n6819\n2.0\n0\n0\n3\n7\n1715\n0\n1995\n0\n98003\n47.3097\n-122.327\n2238\n6819\n\n\n4\n201501\n3\n1.50\n1060\n9711\n1.0\n0\n0\n3\n7\n1060\n0\n1963\n0\n98198\n47.4095\n-122.315\n1650\n9711\n\n\n\n\n\n\n\n\n\n로그변환\n\n치우친 분포를 정규분포에 가깝게 만들기\n\n\n분포가 치우쳐져 있는 항목 찾기(시각화)\n\nrow_plot = 5\ncol_plot = 4\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(24, 35)) \n\ncolumns = df_chk_missing.columns\ncolumns_idx = 1 # 첫 컬럼인 date(날짜)는 제외하기 위해 0이 아닌 1부터 시작\nfor row in range(row_plot):\n    for col in range(col_plot):\n        sns.kdeplot(data=df_chk_missing[columns[columns_idx]], ax=ax[row][col])\n        ax[row][col].set_title(columns[columns_idx])\n        columns_idx += 1\n        if columns_idx == len(columns) :\n            break\n\n\n\n\n\n\n\n\n\n아래의 항목들이 치우쳐져 있음\n\nsqft_living\nsqft_lot\nwaterfront (→유/무 지표로 0,1만 있는게 정상이므로 제외)\nsqft_above\nsqft_basement\nsqft_living15\nsqft_lot15\n\n\n\n# 변환대상 리스트에 저장\nskewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n\n# 그래프로 그리기 (변환 전/후 그래프를 함께 그릴 예정이므로 plot의 수는 두배)\nrow_plot = 6\ncol_plot = 2\nfig, ax = plt.subplots(row_plot, col_plot, figsize=(15, 35)) \n\ncolumns = skewed_columns\ncolumns_idx = 0\n\n\nfor row in range(row_plot):\n    # 로그변환 대상만 식별 후 진행\n    if columns[row] in skewed_columns:\n        # 기존 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][0])\n        ax[row][0].set_title(columns[row])\n\n        # 로그변환\n        df_chk_missing[columns[row]] = np.log1p(df_chk_missing[columns[row]])\n\n        # 변환된 그래프 그리기\n        sns.kdeplot(data=df_chk_missing[columns[row]], ax=ax[row][1])\n        ax[row][1].set_title(columns[row]+'_log')\n\n\n\n\n\n\n\n\n\n\n\ntrain, test 데이터로 정리\n\npreprocessed_train = df_chk_missing[:train_length].copy()\npreprocessed_test = df_chk_missing[train_length:].copy()\nprice_train = y.copy()\n\n# date(날짜)의 타입을 int로 변경 (변경하지 않는 경우 object타입으로 인한 오류 발생)\npreprocessed_train['date'] = preprocessed_train['date'].astype(int)\npreprocessed_test['date'] = preprocessed_test['date'].astype(int)\n\nprint(preprocessed_train.shape)\nprint(preprocessed_test.shape)\n\n(15035, 19)\n(6555, 19)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#scikit-learn-등-관련-패키지-불러오기",
    "href": "posts/coach-ml-kaggle-20230506/index.html#scikit-learn-등-관련-패키지-불러오기",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "Scikit-learn 등 관련 패키지 불러오기",
    "text": "Scikit-learn 등 관련 패키지 불러오기\n\n본래 사용하는 패키지는 모두 최상단에서 불러오는게 맞음!\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n모델 불러오고 Cross Validation으로 모델성능 측청\n\ngboost = GradientBoostingRegressor(random_state=1210)\nxgboost = xgb.XGBRegressor(random_state=1210)\nlightgbm = lgb.LGBMRegressor(random_state=1210)\n\nmodel_dict = {'GradientBoosting':gboost,\n              'XGBoost':xgboost,\n              'LigntGBM':lightgbm}\n\n# LightGBM의 메시지가 나오지 않도록 별도로 저장 후 출력\nmodel_cv_score = dict()\nfor model in model_dict.keys():\n    model_cv_score[model] = np.mean(cross_val_score(model_dict[model], X=preprocessed_train, y=price_train))\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001070 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2296\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 540497.991270\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2327\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542956.681826\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2331\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 543149.529265\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2332\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 542032.619305\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2298\n[LightGBM] [Info] Number of data points in the train set: 12028, number of used features: 19\n[LightGBM] [Info] Start training from score 534776.444047\n\n\n\nfor model in model_dict.keys():\n    print(f'{model} : {model_cv_score[model]}')\n\nGradientBoosting : 0.8613647608814923\nXGBoost : 0.8762617283884332\nLigntGBM : 0.8818569800403846\n\n\n\n\n모델학습 및 예측\n\nScore가 가장 높았던 lightGBM으로 진행해보기\n\n\nmodel_dict['LigntGBM'].fit(preprocessed_train.values, y)\nprediction = model_dict['LigntGBM'].predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n\n\narray([1296687.09405506,  311847.90404507,  806735.28228208, ...,\n       1726006.82963994,  395020.94053356,  333594.29000994])\n\n\n\n\n제출용 DataFrame 및 csv파일 생성\n\ndf_submission = pd.DataFrame({'id' : test_id, \n                              'price' : prediction})\ndf_submission\n\n\n\n\n\n\n\n\nid\nprice\n\n\n\n\n0\n15208\n1.296687e+06\n\n\n1\n15209\n3.118479e+05\n\n\n2\n15210\n8.067353e+05\n\n\n3\n15211\n2.098083e+05\n\n\n4\n15212\n4.343237e+05\n\n\n...\n...\n...\n\n\n6550\n21758\n4.230647e+05\n\n\n6551\n21759\n5.111171e+05\n\n\n6552\n21760\n1.726007e+06\n\n\n6553\n21761\n3.950209e+05\n\n\n6554\n21762\n3.335943e+05\n\n\n\n\n6555 rows × 2 columns\n\n\n\n\ndf_submission.to_csv('submission.csv', index=False)"
  },
  {
    "objectID": "posts/coach-ml-kaggle-20230506/index.html#gridsearch",
    "href": "posts/coach-ml-kaggle-20230506/index.html#gridsearch",
    "title": "[Scikit-learn] Kaggle 집값예측 실습",
    "section": "GridSearch",
    "text": "GridSearch\n\nLightGBM에 Grid Search 적용해보기\n\nfit 후 결과값 부연설명\n\n5 folds : cv = 5\n4 candidates : 2(max_depth) X 2(n_estimators)\n20 fits : 5 folds X 4 candidates\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_param = {\n    'n_estimators': [50, 100], #트리수\n    'max_depth': [1, 10], #트리깊이\n}\n\ngrid_model = GridSearchCV(lgb.LGBMRegressor(random_state=1210), \n                          param_grid=grid_param, \n                          scoring='neg_mean_squared_error',\n                           cv=5, verbose=1, n_jobs=5)\n\n\ngrid_model.fit(preprocessed_train.values, y)\n\n\ngrid_model.cv_results_\nparams = grid_model.cv_results_['params']\n\ndf_grid_result = pd.DataFrame(params)\ndf_grid_result['score'] = grid_model.cv_results_['mean_test_score']\n\n\ndf_grid_result\n\n\n\n\n\n\n\n\nmax_depth\nn_estimators\nscore\n\n\n\n\n0\n1\n50\n-4.787553e+10\n\n\n1\n1\n100\n-3.851269e+10\n\n\n2\n10\n50\n-1.723322e+10\n\n\n3\n10\n100\n-1.636420e+10\n\n\n\n\n\n\n\n\nGridSearch 기준 Score가 가장 좋은 파라메터로 진행해보기\n\n\nmodel = lgb.LGBMRegressor(max_depth=10, n_estimators=100, random_state=1210)\nmodel.fit(preprocessed_train.values, y)\nprediction = model.predict(preprocessed_test.values)\nprediction\n\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2338\n[LightGBM] [Info] Number of data points in the train set: 15035, number of used features: 19\n[LightGBM] [Info] Start training from score 540682.653143\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n\n\narray([1291842.41370212,  314132.92290945,  817260.44452776, ...,\n       1713971.79620206,  389405.58625426,  332109.32763046])"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html",
    "href": "posts/prgms-sql-20240317/index.html",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#개요",
    "href": "posts/prgms-sql-20240317/index.html#개요",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#문제-평균-일일-대여-요금-구하기",
    "href": "posts/prgms-sql-20240317/index.html#문제-평균-일일-대여-요금-구하기",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "문제 : 평균 일일 대여 요금 구하기",
    "text": "문제 : 평균 일일 대여 요금 구하기\n\n\n\n문제 이미지"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#작성답안",
    "href": "posts/prgms-sql-20240317/index.html#작성답안",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT ROUND(AVG(DAILY_FEE)) AS AVERAGE_FEE\nFROM CAR_RENTAL_COMPANY_CAR\nWHERE CAR_TYPE = 'SUV'\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240317/index.html#정리",
    "href": "posts/prgms-sql-20240317/index.html#정리",
    "title": "[프로그래머스SQL] 평균 일일 대여 요금 구하기",
    "section": "정리",
    "text": "정리\n\nROUND : 반올림\nAVG : 평균\nAS ??? : 컬럼명을 ???으로 가져온다 (Alias 라고 함)\nWHERE : 작성한 조건을 기준으로 가져온다\n\nWHERE의 여러 형태예시\n\nWHERE CAR_TYPE = 'SUV'\nWHERE CAR_TYPE != 'SUV'\nWHERE DAILY_FEE &gt; 14000\nWHERE DAILY_FEE BETWEEN 14000 AND 16000\nWHERE DAILY_FEE IN (14000, 16000)\nWHERE CAR_TYPE IN ('SUV', '세단')"
  },
  {
    "objectID": "posts/dtcontest-ore-20240616/index.html",
    "href": "posts/dtcontest-ore-20240616/index.html",
    "title": "[공모전] 공공데이터 공모전-5(UN Comtrade API)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(UNComtradeAPI)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240616/index.html#개요",
    "href": "posts/dtcontest-ore-20240616/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-5(UN Comtrade API)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\nUN Comtrade 사이트로 데이터 받기가 너무 번거로워서 API쪽 확인 후 Jupyter제작"
  },
  {
    "objectID": "posts/dtcontest-ore-20240616/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240616/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-5(UN Comtrade API)",
    "section": "내용정리",
    "text": "내용정리\n\n도입목적\n\nUS Comtrade Database 사이트에서 직접 데이터를 받는 경우, 오류가 여러번 발생\n\n정확한 사유없이 다시 시도하라는 메시지여서 해결방법 찾기가 어려움\n\nAPI를 제공하는 것을 확인\n\n비용없이 사용가능한 API도 있는 것으로 확인하여 활용\n\n\n\n\n필수사항\n\ncomtradeapicall 파이썬 패키지 설치\nAPI Subscription Key 발급 : UN Comtrade Developer에서 발급 (https://comtradedeveloper.un.org/)\n\n가입 후 사용하고자 하는 API를 Subscription\nProfile 메뉴에서 Subscription내역이 보이고, Show로 Key를 확인할 수 있음\n\n\n\n\n사용방법\n\n첫번째 칸에서 필수정보 입력 후 실행\n\n    subscription_key = 'your api key'\n    directory = 'your download directory'\n\n두번째 칸에서 조회정보 입력 후 실행\n\n    hscode = '288512,281325'\n    flow_code = 'M,X' # M : Import / X : Export\n\n원하는 기능으로 이동하여 진행\n\n저장없이 데이터 일부 확인 → Subscription key 필요없는 일부 데이터 확인용 코드로 이동\n\n조회조건(월)입력 후 바로 아래 칸 실행\n\n특정 연도 데이터 저장 필요 → Subscription key 필요한 다운로드 코드로 이동\n\n조회조건 입력 (연단위)에서 4자리 연도 입력하거나, 조회조건 입력 (월단위)에서 6자리 연월 입력\n입력 후 CSV파일 저장(1개월씩 저장) 부분 실행\n\n특정 월 데이터 저장 필요 → Subscription key 필요한 다운로드 코드 (반복문없이 1건 실행)`로 이동\n\n# 조회조건 입력 (월단위)에서 6자리 연월 입력\n입력 후 CSV파일 저장(1개월씩 저장) 부분 실행\n\n\n받아진 데이터의 Row 확인 (한번에 너무 많은 행 받으면 Block당하므로 점검용)\n\n실행하면 저장된 파일들에 대해 정보 제공\n\n  The file 282540,283324_202404.csv has 136 rows.\n  The total : 970.\n데이터 프레임 합치기\n\n실행하면 저장되어있는 파일을 하나로 합친다\n\n\n\n\ngithub reposiroty주소\nhttps://github.com/KR9268/UnComtradeAPI\n\n\n샘플코드(패키지 및 함수)"
  },
  {
    "objectID": "posts/dtcontest-ore-20240616/index.html#기본실행코드-api-key-및-조회조건-등-입력",
    "href": "posts/dtcontest-ore-20240616/index.html#기본실행코드-api-key-및-조회조건-등-입력",
    "title": "[공모전] 공공데이터 공모전-5(UN Comtrade API)",
    "section": "기본실행코드 (API Key 및 조회조건 등 입력)",
    "text": "기본실행코드 (API Key 및 조회조건 등 입력)\n\nimport pandas as pd\nimport requests\nimport comtradeapicall\nimport time\nimport random\nfrom IPython.display import clear_output\nimport os\n\nsubscription_key = 'your api key' # comtrade api subscription key (from comtradedeveloper.un.org)\n\ndirectory = 'your download directory'  # output directory for downloaded files \nproxy_url = '&lt;PROXY URL&gt;'  # optional if you need proxy url\n\n\n조회조건 입력\n\nhscode = '282540,283324'\nflow_code = 'M,X' # M : Import / X : Export\n\n\n\nSubscription key 필요없는 일부 데이터 확인용 코드\n\n# 조회조건 입력\ntotal_period = '202401,202402,202403,202404'\n\n\nmydf = comtradeapicall.previewFinalData(typeCode='C', freqCode='M', clCode='HS', period=total_period,\n                                        reporterCode=None, cmdCode=hscode, flowCode=flow_code, partnerCode=None,\n                                        partner2Code=None,\n                                        customsCode=None, motCode=None, maxRecords=50000, format_output='JSON',\n                                        aggregateBy=None, breakdownMode='classic', countOnly=None, includeDesc=True)\nmydf\n\n\n\n\n\n\n\n\ntypeCode\nfreqCode\nrefPeriodId\nrefYear\nrefMonth\nperiod\nreporterCode\nreporterISO\nreporterDesc\nflowCode\n...\nnetWgt\nisNetWgtEstimated\ngrossWgt\nisGrossWgtEstimated\ncifvalue\nfobvalue\nprimaryValue\nlegacyEstimationFlag\nisReported\nisAggregate\n\n\n\n\n0\nC\nM\n20240101\n2024\n1\n202401\n36\nAUS\nAustralia\nM\n...\n20000.00\nFalse\n20600.0\nFalse\n291960.098\n290906.941\n291960.098\n0\nFalse\nTrue\n\n\n1\nC\nM\n20240101\n2024\n1\n202401\n36\nAUS\nAustralia\nM\n...\n20000.00\nFalse\n20600.0\nFalse\n291960.098\n290906.941\n291960.098\n0\nFalse\nTrue\n\n\n2\nC\nM\n20240101\n2024\n1\n202401\n76\nBRA\nBrazil\nM\n...\n1576.83\nTrue\n0.0\nFalse\n47901.000\n47267.000\n47901.000\n6\nFalse\nTrue\n\n\n3\nC\nM\n20240101\n2024\n1\n202401\n76\nBRA\nBrazil\nM\n...\n1520.00\nFalse\n0.0\nFalse\n45083.000\n44570.000\n45083.000\n0\nFalse\nTrue\n\n\n4\nC\nM\n20240101\n2024\n1\n202401\n76\nBRA\nBrazil\nM\n...\n55.83\nTrue\n0.0\nFalse\n1696.000\n1580.000\n1696.000\n6\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\nC\nM\n20240101\n2024\n1\n202401\n842\nUSA\nUSA\nM\n...\n23000.00\nFalse\n0.0\nFalse\n384800.000\n381720.000\n384800.000\n0\nTrue\nFalse\n\n\n496\nC\nM\n20240101\n2024\n1\n202401\n792\nTUR\nTürkiye\nM\n...\n133887.00\nFalse\n0.0\nFalse\n599898.000\n448243.000\n599898.000\n0\nFalse\nTrue\n\n\n497\nC\nM\n20240101\n2024\n1\n202401\n792\nTUR\nTürkiye\nM\n...\n19887.00\nFalse\n0.0\nFalse\n89176.000\n65031.000\n89176.000\n0\nFalse\nTrue\n\n\n498\nC\nM\n20240101\n2024\n1\n202401\n792\nTUR\nTürkiye\nM\n...\n30000.00\nFalse\n0.0\nFalse\n140402.000\n101488.000\n140402.000\n0\nFalse\nTrue\n\n\n499\nC\nM\n20240101\n2024\n1\n202401\n792\nTUR\nTürkiye\nM\n...\n48000.00\nFalse\n0.0\nFalse\n217920.000\n157511.000\n217920.000\n0\nFalse\nTrue\n\n\n\n\n500 rows × 47 columns\n\n\n\n\n\nSubscription key 필요한 다운로드 코드\n\n너무 자주, 많이 호출하면 밴당할 수 있음\n조회조건 입력은 연단위, 월단위 중 하나만 실행\n\n\n# 조회조건 입력 (연단위) - Year Total Period생성기\nyear_txt = '2017'\ntotal_period = []\nfor i in range(12, 0, -1):\n    total_period.append(f\"{year_txt}{str(i).zfill(2)}\")\n\n['201712',\n '201711',\n '201710',\n '201709',\n '201708',\n '201707',\n '201706',\n '201705',\n '201704',\n '201703',\n '201702',\n '201701']\n\n\n\n# 조회조건 입력 (월단위)\ntotal_period = ['201706', '201705']\n\n\n# CSV파일 저장(1개월씩 저장)\nfor each_period in total_period:\n    mydf = comtradeapicall.getFinalData(subscription_key, typeCode='C', freqCode='M', clCode='HS', period=each_period,\n                                        reporterCode=None, cmdCode=hscode, flowCode=flow_code, partnerCode=None,\n                                        partner2Code=None,\n                                        customsCode=None, motCode=None, maxRecords='250000', format_output='JSON',\n                                        aggregateBy=None, breakdownMode='classic', countOnly=None, includeDesc=True)\n    mydf.to_csv(f'{hscode}_{each_period}.csv', index=False)\n    clear_output()\n    print(f'{hscode}_{each_period}.csv')\n    time.sleep(random.randint(1,2))\n\n282540,283324_201705.csv\n\n\n\n\nSubscription key 필요한 다운로드 코드 (반복문없이 1건 실행)\n\n# 조회조건 입력 (월단위)\nperiod_manual = '202404'\n\n\n# CSV파일 저장(1개월씩 저장)\nmydf = comtradeapicall.getFinalData(subscription_key, typeCode='C', freqCode='M', clCode='HS', period=period_manual,\n                                        reporterCode=None, cmdCode=hscode, flowCode=flow_code, partnerCode=None,\n                                        partner2Code=None,\n                                        customsCode=None, motCode=None, maxRecords='250000', format_output='JSON',\n                                        aggregateBy=None, breakdownMode='classic', countOnly=None, includeDesc=True)\nmydf.to_csv(f'{hscode}_{period_manual}.csv', index=False)\nmydf\n\n\n\n\n\n\n\n\ntypeCode\nfreqCode\nrefPeriodId\nrefYear\nrefMonth\nperiod\nreporterCode\nreporterISO\nreporterDesc\nflowCode\n...\nnetWgt\nisNetWgtEstimated\ngrossWgt\nisGrossWgtEstimated\ncifvalue\nfobvalue\nprimaryValue\nlegacyEstimationFlag\nisReported\nisAggregate\n\n\n\n\n0\nC\nM\n20240401\n2024\n4\n202404\n757\nCHE\nSwitzerland\nM\n...\n130.000\nFalse\n0.0\nFalse\n1866.447\nNaN\n1866.447\n0\nFalse\nTrue\n\n\n1\nC\nM\n20240401\n2024\n4\n202404\n757\nCHE\nSwitzerland\nX\n...\n11.638\nTrue\n0.0\nFalse\nNaN\n1203.627\n1203.627\n6\nFalse\nTrue\n\n\n2\nC\nM\n20240401\n2024\n4\n202404\n757\nCHE\nSwitzerland\nX\n...\n11.000\nFalse\n0.0\nFalse\nNaN\n1137.675\n1137.675\n0\nFalse\nTrue\n\n\n3\nC\nM\n20240401\n2024\n4\n202404\n757\nCHE\nSwitzerland\nX\n...\n0.638\nTrue\n0.0\nFalse\nNaN\n65.952\n65.952\n6\nFalse\nTrue\n\n\n4\nC\nM\n20240401\n2024\n4\n202404\n842\nUSA\nUSA\nM\n...\n217523.000\nFalse\n0.0\nFalse\n4915370.000\n4866083.000\n4915370.000\n0\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n131\nC\nM\n20240401\n2024\n4\n202404\n300\nGRC\nGreece\nM\n...\n24.000\nFalse\n0.0\nFalse\n26.819\nNaN\n26.819\n0\nFalse\nTrue\n\n\n132\nC\nM\n20240401\n2024\n4\n202404\n300\nGRC\nGreece\nM\n...\n24.000\nFalse\n0.0\nFalse\n26.819\nNaN\n26.819\n0\nFalse\nTrue\n\n\n133\nC\nM\n20240401\n2024\n4\n202404\n36\nAUS\nAustralia\nM\n...\n495415.000\nFalse\n502653.0\nFalse\n303606.306\n276999.633\n303606.306\n0\nFalse\nTrue\n\n\n134\nC\nM\n20240401\n2024\n4\n202404\n36\nAUS\nAustralia\nM\n...\n600.000\nFalse\n1038.0\nFalse\n2845.630\n2153.520\n2845.630\n0\nFalse\nTrue\n\n\n135\nC\nM\n20240401\n2024\n4\n202404\n36\nAUS\nAustralia\nM\n...\n494815.000\nFalse\n501615.0\nFalse\n300760.676\n274846.113\n300760.676\n0\nFalse\nTrue\n\n\n\n\n136 rows × 47 columns\n\n\n\n\n\n받아진 데이터의 Row수 확인\n\n한번에 너무 많이 받으면 Block당할 수 있으니 받은 데이터 Row수 확인 필요\n\n\ntotal_rows = 0\n\n# Iterate over all files in the directory\nfor filename in os.listdir(directory):\n    if filename.startswith((hscode)) and filename.endswith('.csv'):  # Check if the file is a CSV file and starts with the specified prefixes\n        filepath = os.path.join(directory, filename)\n        df = pd.read_csv(filepath)\n        num_rows = len(df)\n        total_rows += num_rows\n        print(f'The file {filename} has {num_rows} rows.')\n\n# Print the total number of rows\nprint(f'The total : {total_rows}.')\n\nThe file 282540,283324_201705.csv has 136 rows.\nThe file 282540,283324_201706.csv has 698 rows.\nThe file 282540,283324_202404.csv has 136 rows.\nThe total : 970.\n\n\n\n\n데이터프레임합치기\n\n# Initialize an empty list to store the dataframes\ndfs = []\n\n# Iterate over all files in the directory\nfor filename in os.listdir(directory):\n    if filename.startswith((hscode)) and filename.endswith('.csv'):  # Check if the file is a CSV file and starts with the specified prefixes\n        filepath = os.path.join(directory, filename)\n        df = pd.read_csv(filepath)\n        dfs.append(df)\n\n# Concatenate all the dataframes in the list into a single dataframe\nmerged_df = pd.concat(dfs, ignore_index=True)\nmerged_df.to_csv(f'{hscode}_merged.csv', index=False)"
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html",
    "href": "posts/dtcontest-ore-20240624/index.html",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(광물 전체 제안배경 작성)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html#지난-회의정리",
    "href": "posts/dtcontest-ore-20240624/index.html#지난-회의정리",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "지난 회의정리",
    "text": "지난 회의정리\n\n내가 진행할 과업\n\nUN Comtreade API개발한 것을 기반으로 광물별 데이터 저장 및 바로 사용할수 있도록 db화\n\n기존에 만들어둔 공공데이터포털 데이터 공유용 git을 활용하여 모든 데이터 관리 예정\n\n링크 : https://github.com/KR9268/db_datagokr\n\n\n분석을 구상하고 제안한 배경에 대해 작성\n\n기존의 니켈뿐 아니라 분석 대상 광석 전체에 대해 분석 진행\n해당 내용 기반으로 분석의 당위성 부여 예정\n\n활용데이터에 대한 정리\n\n활용한 외부데이터와 내용, 활용이유 정리\n\n\n타 팀원들 진행할 과업\n\nLSTM 등 예측 모델에 대한 설명\nProphet 등 이상탐지 모델에 대한 설명\n만들어둔 db활용하여 데이터 추가가공 및 전처리\n모델링 및 모델학습\n\n다같이 진행할 과업\n\n분석으로 인한 기대(파급)효과"
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html#제안배경",
    "href": "posts/dtcontest-ore-20240624/index.html#제안배경",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "제안배경",
    "text": "제안배경\n\n현황 및 제안목적\n\n탄소중립 등 환경에 대한 관심과, 전기차 배터리 등 미래 유망산업의 핵심광물 수요 증가\n\n리튬, 니켈, 코발트, 망간은 전기차 이차전지의 원료로, 배터리 양극재 등에 필요한 핵심금속\n세계은행 예상에 따르면 배터리 시장은 2050년까지 크게 성장할 전망 (2050년에는 2018년 기준 450%이상 수요증가할 것으로 예상)  출처 : Minerals for Climate Action: The Mineral Intensity of the Clean Energy Transition(2020), The World Bank\n\n\n\n핵심광물 매장이 특정 국가에 심하게 편중되어 있어 상당한 공급망 리스크 존재\n\n광물/국가별 매장량 추이\n\n니켈 : 인도네시아(42%), 호주(18.3%), 브라질(12.2%) 등 \n코발트 : 콩고(57%), 호주(16.1%) 등 \n리튬 : 칠레(33.6%), 호주(22.4%), 아르헨티나(13%) 등 \n망간 : 남아프리카(31.4%), 호주(26.2%), 중국(14.7%) 등  출처 : USGS Mineral Commodity Summaries 2024 데이터로 가공 https://www.sciencebase.gov/catalog/item/65a6e45fd34e5af967a46749\n\n\n\n\n핵심광물 조달에 대한 수입경쟁 심화\n\n각 광물에 대한 국가별 수입량은 2014년 대비 상승세를 보이고 있음\n\n다만, 리튬은 전기차 캐즘이나 배터리시장 경쟁심화 등 여러 요소로 감소된 것으로 추정\n\n배터리시장의 한국업체 점유율 전년비 5.3%하락\n전기차 캐즘 : Chasm, 대중화 전 수요가 감소하는 것\n\n\n\n\n\n\nimage.png\n\n\n출처 : USGS Mineral Commodity Summaries 2024 데이터로 가공 https://www.sciencebase.gov/catalog/item/65a6e45fd34e5af967a46749\n\n각 광물에 대한 국가별 수입량은 2014년 대비 상승세를 보이고 있음\n\n한국 : 희소금속 산업 발전대책 2.0 추진 (확보-비축-순환 수급에 대한 3중 안정망 등)\n미국 : 중요 광물 공급을 위한 연방정부 전략 추진 (수입의존 저감 및 공급망 확보 등)\n일본 : 신 국제자원 전략 추진 (비축제도 재검토, 광종별 확보책 구축, 확보를 위한 국제협력 도출 등)\nEU : EIP Raw materials 추진 (자원의 채광,재활용 등에 대한 시험적 대응, 희소자원에 대한 대체 이용처 도출 등)\n\n\n출처 : 한눈에 보는 6대 핵심광물 이슈분석, 한국지질자원연구원\n\n광물별 상위 수입국 현황(하단 참조)\n\n광물별 상위 수입국 현황 - 니켈 (단위 : kt[킬로톤])\n\nIndex 0~4 : 1~5순위 / Index 5 : 합계\n\n\n\n\n\nShow the code\ntarget_ore = '니켈'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nBrazil\n29,012\nBrazil\n34,127\nBrazil\n19,720\nJapan\n112,033\nJapan\n135,167\nJapan\n191,793\nJapan\n200,127\nJapan\n206,021\nJapan\n173,627\nChina\n256,142\n\n\n1\nGermany\n11,705\nRep. of Korea\n27,394\nChina\n15,197\nRep. of Korea\n53,085\nRep. of Korea\n52,446\nBelgium\n34,111\nBelgium\n31,329\nChina\n98,853\nChina\n119,071\nJapan\n124,779\n\n\n2\nTurkiye\n2,203\nCanada\n16,517\nGermany\n14,054\nChina\n21,639\nAustralia\n21,776\nRep. of Korea\n31,817\nCanada\n21,353\nBelgium\n41,121\nBelgium\n43,286\nBelgium\n41,815\n\n\n3\nSwitzerland\n1,344\nGermany\n12,654\nCanada\n13,500\nGermany\n13,939\nBelgium\n20,963\nCanada\n25,360\nChina\n16,340\nCanada\n21,410\nCanada\n18,103\nMalaysia\n26,280\n\n\n4\nSpain\n1,292\nThailand\n4,923\nRep. of Korea\n8,951\nIndia\n11,978\nChina\n20,569\nAustralia\n19,640\nAustralia\n13,834\nMalaysia\n10,150\nRep. of Korea\n14,469\nCanada\n13,364\n\n\n5\n45,555\n95,615\n71,421\n212,674\n250,921\n302,721\n282,983\n377,554\n368,556\n462,380\n\n\n\n\n\n\n광물별 상위 수입국 현황 - 코발트 (단위 : kt[킬로톤])\n\n\n\nShow the code\ntarget_ore = '코발트'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nGermany\n360,060\nGermany\n366,375\nUSA\n444,837\nUSA\n496,572\nUSA\n471,603\nUnited Kingdom\n546,036\nGermany\n571,008\nGermany\n660,773\nGermany\n488,970\nGermany\n529,288\n\n\n1\nSweden\n213,809\nSweden\n220,380\nGermany\n347,106\nUnited Kingdom\n345,329\nGermany\n329,296\nGermany\n380,888\nUSA\n472,193\nUSA\n295,225\nItaly\n250,075\nUnited Kingdom\n395,371\n\n\n2\nBrazil\n154,625\nBrazil\n124,744\nSweden\n201,729\nGermany\n299,495\nFinland\n277,965\nUSA\n348,198\nUnited Kingdom\n371,668\nItaly\n232,667\nBrazil\n191,783\nUSA\n219,089\n\n\n3\nNetherlands\n124,798\nCanada\n93,406\nBrazil\n137,948\nItaly\n195,547\nUnited Kingdom\n264,720\nItaly\n217,586\nItaly\n217,804\nUnited Kingdom\n198,261\nAustria\n186,431\nJapan\n189,547\n\n\n4\nBelgium\n90,674\nRep. of Korea\n76,266\nNetherlands\n101,162\nSweden\n182,634\nItaly\n228,860\nFrance\n176,344\nSweden\n159,068\nSweden\n187,343\nSweden\n164,849\nSweden\n180,823\n\n\n5\n943,966\n881,171\n1,232,782\n1,519,578\n1,572,444\n1,669,052\n1,791,741\n1,574,269\n1,282,108\n1,514,119\n\n\n\n\n\n\n광물별 상위 수입국 현황 - 리튬 (단위 : kt[킬로톤])\n\n\n\nShow the code\ntarget_ore = '리튬'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nBelgium\n11,844\nRep. of Korea\n35,014\nChina\n47,227\nJapan\n69,609\nRep. of Korea\n91,029\nRep. of Korea\n124,094\nRep. of Korea\n130,051\nRep. of Korea\n189,821\nChina\n278,358\nChina\n324,683\n\n\n1\nSpain\n6,852\nBelgium\n18,782\nUSA\n33,806\nChina\n61,411\nJapan\n86,273\nJapan\n118,216\nChina\n101,259\nChina\n169,226\nRep. of Korea\n238,878\nJapan\n109,344\n\n\n2\nGermany\n3,900\nCanada\n7,502\nBelgium\n24,300\nRep. of Korea\n55,620\nChina\n46,158\nChina\n59,437\nJapan\n97,427\nJapan\n108,999\nJapan\n120,350\nUSA\n34,261\n\n\n3\nTurkiye\n3,313\nGermany\n6,237\nRussian Federation\n11,620\nUSA\n34,820\nUSA\n36,507\nUSA\n26,740\nUSA\n26,229\nUSA\n27,766\nNetherlands\n17,332\nNetherlands\n12,724\n\n\n4\nNetherlands\n1,376\nSpain\n5,638\nCanada\n10,842\nBelgium\n17,840\nBelgium\n21,817\nFrance\n16,984\nBelgium\n16,554\nRussian Federation\n19,591\nUSA\n15,192\nUnited Kingdom\n6,761\n\n\n5\n27,285\n73,173\n127,794\n239,299\n281,784\n345,471\n371,519\n515,402\n670,112\n487,772\n\n\n\n\n\n\n광물별 상위 수입국 현황 - 망간 (단위 : kt[킬로톤])\n\n\n\nShow the code\ntarget_ore = '망간'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nGermany\n1,762,753\nChina, Hong Kong SAR\n6,596,207\nChina, Hong Kong SAR\n6,430,363\nChina, Hong Kong SAR\n4,445,408\nUSA\n5,646,705\nUSA\n5,337,518\nUSA\n5,522,054\nUSA\n6,726,526\nGermany\n4,296,848\nUSA\n7,890,263\n\n\n1\nBrazil\n1,139,607\nGermany\n2,047,818\nUSA\n3,899,931\nUSA\n4,356,834\nChina, Hong Kong SAR\n5,317,808\nChina, Hong Kong SAR\n5,014,159\nChina, Hong Kong SAR\n4,943,913\nGermany\n4,675,819\nJapan\n2,842,379\nGermany\n3,032,223\n\n\n2\nBelgium\n847,525\nBelgium\n1,255,255\nGermany\n3,361,475\nGermany\n2,858,660\nGermany\n2,722,953\nGermany\n3,163,188\nGermany\n4,058,559\nChina, Hong Kong SAR\n4,648,238\nChina, Hong Kong SAR\n2,556,882\nJapan\n2,457,672\n\n\n3\nSpain\n498,741\nBrazil\n1,015,417\nRussian Federation\n1,509,007\nJapan\n2,302,316\nChina\n2,504,820\nChina\n2,789,753\nChina\n2,812,012\nTunisia\n2,484,856\nPoland\n1,565,781\nChina, Hong Kong SAR\n2,360,582\n\n\n4\nTurkiye\n457,040\nRep. of Korea\n443,342\nChina\n1,213,518\nRussian Federation\n1,732,776\nJapan\n2,433,654\nJapan\n2,398,831\nJapan\n2,636,763\nJapan\n2,484,354\nChina\n1,529,201\nPoland\n1,547,787\n\n\n5\n4,705,666\n11,358,039\n16,414,293\n15,695,994\n18,625,940\n18,703,449\n19,973,301\n21,019,792\n12,791,091\n17,288,526\n\n\n\n\n\n출처 : UN COMTRADE DB의 자료를 가공  참고 : https://www.sedaily.com/NewsView/2D7TKJ4UE9\n\n핵심광물을 확보하기 위해 경쟁국들은 다양한 전략을 추진중\n\n한국 : 희소금속 산업 발전대책 2.0 추진 (확보-비축-순환 수급에 대한 3중 안정망 등)\n미국 : 중요 광물 공급을 위한 연방정부 전략 추진 (수입의존 저감 및 공급망 확보 등)\n일본 : 신 국제자원 전략 추진 (비축제도 재검토, 광종별 확보책 구축, 확보를 위한 국제협력 도출 등)\nEU : EIP Raw materials 추진 (자원의 채광,재활용 등에 대한 시험적 대응, 희소자원에 대한 대체 이용처 도출 등)\n\n\n출처 : 한눈에 보는 6대 핵심광물 이슈분석, 한국지질자원연구원\n\n\n현황요약 및 제안목적\n\n아래와 같은 상황에서 한국도 핵심광물 비축이나 국산화 노력 등이 이루어지고 있음\n\n핵심광물의 수요증가(전기차 등 환경에 대한 관심과 배터리 시장의 성장)\n공급망 위기(생산지 편중)\n수급경쟁 심화(경쟁국의 확보전략 추진, 수입량 증대)\n\n그럼에도 생산지 편중, 경쟁국은 통제 불가능 요소로, 여전히 공급망 리스크가 상당함\n이러한 상황에서 미래 핵심산업의 경쟁력 확보를 이끌어낼 핵심 원재료(광물)의 확보를 위해, 각 상황에 대한 예측과 위기요소를 탐지할, 위기요소 탐지모델을 도입하고자 함\n\n\n\n\n도입 방법\n\n분석대상 정립\n\n핵심광물로 니켈, 코발트, 리튬, 망간을 지정하고, 국제기준인 6자리 HSCODE를 기준으로 함\n\n산화코발트(282200), 황산코발트(283329), 산화/수산화리튬(282520), 탄산리튬(283691)\n산화/수산화니켈(282540), 황산니켈(283324), 이산화망간(850610)\n\n\n\n\n고려요소(Feature) 선정\n\n국가별 자원수입량을 통해 경쟁자의 활동(광물이동의 편중 등)을 관찰\n\n위의 HSCODE로 전세계 수출입현황을 알 수 있는 UN Comtrade DB API로 데이터 확보\n\n중량당 가격(Price per Weight)을 가격변수로 고려\n\n수출입데이터의 Value(Price)와 Netweight로 연월 데이터 산출\n\n핵심광물 생산지가 해외에 편중되어 국제운송이 필수이므로, 물류에 대해 BDI지수로 관찰\n\nBDI : Baltic Dry Index. 원자재 운송에 사용되는 건화물선에 대한 운임비용 지수\n원자재 이동이라는 점에서 경기선행지표로도 인식되고 있음(향후 시장상황에 대한 지표)\n\n\n\n\n예측 및 이상탐지\n\n고려요소에 대해 ARIMA, 홀트윈터스 지수평활법을 시작으로 LSTM 등 AI모델로 예측 진행\n예측 데이터 기반으로 AnomalyDetection(R, 파이썬의 Prophet) 등을 활용한 이상탐지 진행"
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html#usgs-자료분석-광물별-매장량-산출",
    "href": "posts/dtcontest-ore-20240624/index.html#usgs-자료분석-광물별-매장량-산출",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "USGS 자료분석 (광물별 매장량 산출)",
    "text": "USGS 자료분석 (광물별 매장량 산출)\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom IPython.display import display, HTML\nimport plotly.express as px\n\n\n\n\nShow the code\nores = {'니켈':\n        {'csv_name':'mcs2024-nicke_world.csv',\n        'csv_df':''},\n         '코발트':\n         {'csv_name':'mcs2024-cobal_world.csv',\n        'csv_df':''},\n         '리튬':\n         {'csv_name':'mcs2024-lithi_world.csv',\n        'csv_df':''},\n         '망간':\n         {'csv_name':'mcs2024-manga_world.csv',\n        'csv_df':''}}\n\n\n\n\nShow the code\n# CSV리딩\nfor ore in ores.keys():\n    # CSV리딩\n    ores[ore]['csv_df'] = pd.read_csv(ores[ore]['csv_name'])\n\n    # 데이터 전처리\n    ## Null처리\n    ores[ore]['csv_df'] = ores[ore]['csv_df'].filter(regex='^(?!Unnamed)').dropna(subset=['Country'])\n\n    ## 킬로톤인 경우 톤으로 변환(컬럼 추가)\n    if 'Reserves_kt' in ores[ore]['csv_df'].columns:\n        ores[ore]['csv_df']['Reserves_t'] = ores[ore]['csv_df']['Reserves_kt'] * 1000\n\n    ## 숫자에 섞인 기호 처리 후 변환\n    if ores[ore]['csv_df']['Reserves_t'].dtype=='O':\n        ores[ore]['csv_df']['Reserves_t'] = ores[ore]['csv_df']['Reserves_t'].str.replace('&gt;','')\n        ores[ore]['csv_df']['Reserves_t'] = ores[ore]['csv_df']['Reserves_t'].str.replace(',','')\n        ores[ore]['csv_df']['Reserves_t'] = ores[ore]['csv_df']['Reserves_t'].str.strip().astype(int)\n    \n    ## 국가명 변환(튀르키예 등)\n    ores[ore]['csv_df']['Country'] = ores[ore]['csv_df']['Country'].str.replace('T체rkiye','Turkiye')\n\n    # 매장량 (실제값)\n    ores[ore]['reserve_df_actual'] = ores[ore]['csv_df'].copy()\n    ores[ore]['reserve_df_actual'] = pd.DataFrame(ores[ore]['csv_df'].groupby('Country').sum()['Reserves_t'])\n    ores[ore]['reserve_df_actual']['Reserves_t'].astype(int)\n    ores[ore]['reserve_df_actual'] = ores[ore]['reserve_df_actual'].drop('World total (rounded)')\n\n    ores[ore]['reserve_df_actual'] = ores[ore]['reserve_df_actual'].copy()\n    ores[ore]['reserve_df_actual']['Reserves_t_total'] = int(ores[ore]['reserve_df_actual'].sum().item())\n    ores[ore]['reserve_df_actual']['Reserves_portion'] = ores[ore]['reserve_df_actual']['Reserves_t']/ores[ore]['reserve_df_actual']['Reserves_t_total']\n    ores[ore]['reserve_df_actual'] = ores[ore]['reserve_df_actual'].sort_values(by='Reserves_portion', ascending=False)\n\n\n\n\nShow the code\n# 시각화용 DataFrame\nfor ore in ores.keys():\n    ores[ore]['plotly_df'] = ores[ore]['reserve_df_actual'].copy()\n    ores[ore]['plotly_df'] = ores[ore]['plotly_df'].reset_index()\n    for i, _ in enumerate(ores[ore]['plotly_df'].index):\n        value = f\"{ores[ore]['plotly_df']['Country'][i]}&lt;br&gt;{int(ores[ore]['plotly_df']['Reserves_t'][i]/1000):,} kt &lt;br&gt;{ores[ore]['plotly_df']['Reserves_portion'][i]*100:.1f}%\"\n        ores[ore]['plotly_df'].loc.__setitem__((i, 'Country'), value)  \n\n\n\n\nShow the code\nfor ore in ores.keys():\n    # Data\n    data = ores[ore]['plotly_df']\n\n    # Create treemap\n    fig = px.treemap(data, path=['Country'], values='Reserves_portion', title=ore,\n                        labels=ores[ore]['plotly_df']['Reserves_t'], width=700, height=380)\n    fig.update_traces(textposition='middle center')\n    fig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html#un-comtrade-자료분석광물별-top5수입국",
    "href": "posts/dtcontest-ore-20240624/index.html#un-comtrade-자료분석광물별-top5수입국",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "UN Comtrade 자료분석(광물별 Top5수입국)",
    "text": "UN Comtrade 자료분석(광물별 Top5수입국)\n\n\nShow the code\nores_uncomtrade_url = {'니켈':\n        {'csv_url':['https://raw.githubusercontent.com/KR9268/db_datagokr/main/282540,283324_merged(since2011).csv',\n                    'https://raw.githubusercontent.com/KR9268/db_datagokr/main/282540,283324_merged(since2021).csv'],\n        'csv_df':''},\n         '코발트':\n         {'csv_url':['https://raw.githubusercontent.com/KR9268/db_datagokr/main/282200,283329_merged(since2011).csv',\n                    'https://raw.githubusercontent.com/KR9268/db_datagokr/main/282200,283329_merged(since2021).csv'],\n        'csv_df':''},\n         '리튬':\n         {'csv_url':['https://raw.githubusercontent.com/KR9268/db_datagokr/main/282520,283691_merged(since2011).csv',\n                    'https://raw.githubusercontent.com/KR9268/db_datagokr/main/282520,283691_merged(since2021).csv'],\n        'csv_df':''},\n         '망간':\n         {'csv_url':['https://raw.githubusercontent.com/KR9268/db_datagokr/main/850610_merged(since2011).csv',\n                    'https://raw.githubusercontent.com/KR9268/db_datagokr/main/850610_merged(since2021).csv'],\n        'csv_df':''}}\n\n\n\n\nShow the code\n# CSV리딩\nfor ore in ores_uncomtrade_url.keys():\n    temp_list_for_df_concat = []\n    for each_url in ores_uncomtrade_url[ore]['csv_url']:\n        temp_list_for_df_concat.append(pd.read_csv(each_url, encoding='cp949', low_memory=False))\n    ores_uncomtrade_url[ore]['csv_df'] = pd.concat(temp_list_for_df_concat)\n\n\n\n\nShow the code\n# 기간데이터 전처리 & 데이터 가공 및 순위산출\n\ndef preprocess_df(df_copy):\n    # 날짜변환\n    df_copy['period'] = df_copy['period'].astype('int').astype('str')\n    df_copy['period_year'] = df_copy['period'].str[:4]\n    df_copy['period_month'] = df_copy['period'].str[-2:]\n    df_copy['period_dateformat'] = pd.to_datetime(df_copy['refPeriodId'], format='%Y%m%d')\n\n    # 단위당 가격\n    df_copy['value_per_qty'] = df_copy['primaryValue']/df_copy['qty']\n    df_copy['value_per_qty'] = df_copy['value_per_qty'].replace([np.inf, -np.inf], 0)\n\n    # 국가명 변환(튀르키예 등)\n    df_copy['reporterDesc'] = df_copy['reporterDesc'].str.replace('T체rkiye','Turkiye')\n    df_copy['partnerDesc'] = df_copy['partnerDesc'].str.replace('T체rkiye','Turkiye')\n    return df_copy\n\nfor ore in ores_uncomtrade_url.keys():\n    ## 기간데이터 전처리\n    ores_uncomtrade_url[ore]['csv_df'] = preprocess_df(ores_uncomtrade_url[ore]['csv_df'])\n    ## 데이터 가공 및 순위산출(groupby)\n    ores_uncomtrade_url[ore]['df_groupby_value'] =ores_uncomtrade_url[ore]['csv_df'][ores_uncomtrade_url[ore]['csv_df']['flowCode']=='M'].groupby(['refYear','reporterDesc']).sum(numeric_only=True)['primaryValue']\n    ores_uncomtrade_url[ore]['df_groupby_value'] =ores_uncomtrade_url[ore]['csv_df'][ores_uncomtrade_url[ore]['csv_df']['flowCode']=='M'].groupby(['refYear','reporterDesc']).sum(numeric_only=True)['qty']\n    #ores_uncomtrade_url[ore]['df_groupby_value'] =ores_uncomtrade_url[ore]['csv_df'][ores_uncomtrade_url[ore]['csv_df']['flowCode']=='M'].groupby(['refYear','reporterDesc']).sum(numeric_only=True)['value_per_qty']\n\n\n\n\nShow the code\n# 상위 5개국 산출\n\ndef return_top5(df_groupby, list_removing_year):\n    dict_top_all_year = {}\n    dict_top_all_year_value = {}\n    dict_top_all_year_total = {}\n    dict_top_all_year_value_sum = {}\n\n    year_index = list(set(df_groupby.index.get_level_values(0)))\n    for _ in list_removing_year:\n        year_index.remove(_)\n    year_index.sort()\n\n    for each_year in year_index:\n        top5 = df_groupby[each_year].sort_values(ascending=False)\n        if 'World' in top5.index:\n            top5 = top5.drop('World')\n        top5_index = top5.index.tolist()[:5]\n\n        top5_value = []\n        top5_index_and_value=[]\n        for index_country in top5_index:\n            # top5_value.append(top5[index_country])\n            # top5_index_and_value.append(f\"{index_country}\\n{top5[index_country]:,.0f}\")\n\n            top5_value.append(top5[index_country]/1000)\n            top5_index_and_value.append(f\"{index_country}\\n{top5[index_country]/1000:,.0f}\")\n        \n        dict_top_all_year[each_year] = top5_index\n        dict_top_all_year_value[each_year] = top5_value\n\n        dict_top_all_year_total[each_year] = top5_index_and_value\n        dict_top_all_year_value_sum[each_year] = f\"{sum(top5_value):,.0f}\"\n\n    return pd.concat([pd.DataFrame(dict_top_all_year_total), \n                      pd.DataFrame(dict_top_all_year_value_sum, \n                                   index=[5])]) \n\nfor ore in ores_uncomtrade_url.keys():\n    ores_uncomtrade_url[ore]['df_top5'] = return_top5(ores_uncomtrade_url[ore]['df_groupby_value'], [2011,2012,2013,2024])\n\n\n\n\nShow the code\ntarget_ore = '코발트'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nGermany\n360,060\nGermany\n366,375\nUSA\n444,837\nUSA\n496,572\nUSA\n471,603\nUnited Kingdom\n546,036\nGermany\n571,008\nGermany\n660,773\nGermany\n488,970\nGermany\n529,288\n\n\n1\nSweden\n213,809\nSweden\n220,380\nGermany\n347,106\nUnited Kingdom\n345,329\nGermany\n329,296\nGermany\n380,888\nUSA\n472,193\nUSA\n295,225\nItaly\n250,075\nUnited Kingdom\n395,371\n\n\n2\nBrazil\n154,625\nBrazil\n124,744\nSweden\n201,729\nGermany\n299,495\nFinland\n277,965\nUSA\n348,198\nUnited Kingdom\n371,668\nItaly\n232,667\nBrazil\n191,783\nUSA\n219,089\n\n\n3\nNetherlands\n124,798\nCanada\n93,406\nBrazil\n137,948\nItaly\n195,547\nUnited Kingdom\n264,720\nItaly\n217,586\nItaly\n217,804\nUnited Kingdom\n198,261\nAustria\n186,431\nJapan\n189,547\n\n\n4\nBelgium\n90,674\nRep. of Korea\n76,266\nNetherlands\n101,162\nSweden\n182,634\nItaly\n228,860\nFrance\n176,344\nSweden\n159,068\nSweden\n187,343\nSweden\n164,849\nSweden\n180,823\n\n\n5\n943,966\n881,171\n1,232,782\n1,519,578\n1,572,444\n1,669,052\n1,791,741\n1,574,269\n1,282,108\n1,514,119\n\n\n\n\n\n\n\nShow the code\ntarget_ore = '리튬'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nBelgium\n11,844\nRep. of Korea\n35,014\nChina\n47,227\nJapan\n69,609\nRep. of Korea\n91,029\nRep. of Korea\n124,094\nRep. of Korea\n130,051\nRep. of Korea\n189,821\nChina\n278,358\nChina\n324,683\n\n\n1\nSpain\n6,852\nBelgium\n18,782\nUSA\n33,806\nChina\n61,411\nJapan\n86,273\nJapan\n118,216\nChina\n101,259\nChina\n169,226\nRep. of Korea\n238,878\nJapan\n109,344\n\n\n2\nGermany\n3,900\nCanada\n7,502\nBelgium\n24,300\nRep. of Korea\n55,620\nChina\n46,158\nChina\n59,437\nJapan\n97,427\nJapan\n108,999\nJapan\n120,350\nUSA\n34,261\n\n\n3\nTurkiye\n3,313\nGermany\n6,237\nRussian Federation\n11,620\nUSA\n34,820\nUSA\n36,507\nUSA\n26,740\nUSA\n26,229\nUSA\n27,766\nNetherlands\n17,332\nNetherlands\n12,724\n\n\n4\nNetherlands\n1,376\nSpain\n5,638\nCanada\n10,842\nBelgium\n17,840\nBelgium\n21,817\nFrance\n16,984\nBelgium\n16,554\nRussian Federation\n19,591\nUSA\n15,192\nUnited Kingdom\n6,761\n\n\n5\n27,285\n73,173\n127,794\n239,299\n281,784\n345,471\n371,519\n515,402\n670,112\n487,772\n\n\n\n\n\n\n\nShow the code\ntarget_ore = '망간'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nGermany\n1,762,753\nChina, Hong Kong SAR\n6,596,207\nChina, Hong Kong SAR\n6,430,363\nChina, Hong Kong SAR\n4,445,408\nUSA\n5,646,705\nUSA\n5,337,518\nUSA\n5,522,054\nUSA\n6,726,526\nGermany\n4,296,848\nUSA\n7,890,263\n\n\n1\nBrazil\n1,139,607\nGermany\n2,047,818\nUSA\n3,899,931\nUSA\n4,356,834\nChina, Hong Kong SAR\n5,317,808\nChina, Hong Kong SAR\n5,014,159\nChina, Hong Kong SAR\n4,943,913\nGermany\n4,675,819\nJapan\n2,842,379\nGermany\n3,032,223\n\n\n2\nBelgium\n847,525\nBelgium\n1,255,255\nGermany\n3,361,475\nGermany\n2,858,660\nGermany\n2,722,953\nGermany\n3,163,188\nGermany\n4,058,559\nChina, Hong Kong SAR\n4,648,238\nChina, Hong Kong SAR\n2,556,882\nJapan\n2,457,672\n\n\n3\nSpain\n498,741\nBrazil\n1,015,417\nRussian Federation\n1,509,007\nJapan\n2,302,316\nChina\n2,504,820\nChina\n2,789,753\nChina\n2,812,012\nTunisia\n2,484,856\nPoland\n1,565,781\nChina, Hong Kong SAR\n2,360,582\n\n\n4\nTurkiye\n457,040\nRep. of Korea\n443,342\nChina\n1,213,518\nRussian Federation\n1,732,776\nJapan\n2,433,654\nJapan\n2,398,831\nJapan\n2,636,763\nJapan\n2,484,354\nChina\n1,529,201\nPoland\n1,547,787\n\n\n5\n4,705,666\n11,358,039\n16,414,293\n15,695,994\n18,625,940\n18,703,449\n19,973,301\n21,019,792\n12,791,091\n17,288,526\n\n\n\n\n\n\n\nShow the code\ntarget_ore = '니켈'\n\nores_uncomtrade_url[target_ore]['df_top5_html'] = ores_uncomtrade_url[target_ore]['df_top5'].copy()\nfor each_year in ores_uncomtrade_url[target_ore]['df_top5'].columns:\n    ores_uncomtrade_url[target_ore]['df_top5_html'][each_year] = ores_uncomtrade_url[target_ore]['df_top5_html'][each_year].str.replace('\\n','&lt;br&gt;')\n\n# HWP파일 작성용 CSV저장, 자동 줄바꿈 적용시 아래 표기처럼 복사/붙여넣기 가능\nores_uncomtrade_url[target_ore]['df_top5'].to_excel(f\"Top5Country_{target_ore}.xlsx\")\ndisplay(HTML(ores_uncomtrade_url[target_ore]['df_top5_html'].to_html(escape=False)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nBrazil\n29,012\nBrazil\n34,127\nBrazil\n19,720\nJapan\n112,033\nJapan\n135,167\nJapan\n191,793\nJapan\n200,127\nJapan\n206,021\nJapan\n173,627\nChina\n256,142\n\n\n1\nGermany\n11,705\nRep. of Korea\n27,394\nChina\n15,197\nRep. of Korea\n53,085\nRep. of Korea\n52,446\nBelgium\n34,111\nBelgium\n31,329\nChina\n98,853\nChina\n119,071\nJapan\n124,779\n\n\n2\nTurkiye\n2,203\nCanada\n16,517\nGermany\n14,054\nChina\n21,639\nAustralia\n21,776\nRep. of Korea\n31,817\nCanada\n21,353\nBelgium\n41,121\nBelgium\n43,286\nBelgium\n41,815\n\n\n3\nSwitzerland\n1,344\nGermany\n12,654\nCanada\n13,500\nGermany\n13,939\nBelgium\n20,963\nCanada\n25,360\nChina\n16,340\nCanada\n21,410\nCanada\n18,103\nMalaysia\n26,280\n\n\n4\nSpain\n1,292\nThailand\n4,923\nRep. of Korea\n8,951\nIndia\n11,978\nChina\n20,569\nAustralia\n19,640\nAustralia\n13,834\nMalaysia\n10,150\nRep. of Korea\n14,469\nCanada\n13,364\n\n\n5\n45,555\n95,615\n71,421\n212,674\n250,921\n302,721\n282,983\n377,554\n368,556\n462,380"
  },
  {
    "objectID": "posts/dtcontest-ore-20240624/index.html#un-comtrade-자료분석연도별-광물수입량-산출",
    "href": "posts/dtcontest-ore-20240624/index.html#un-comtrade-자료분석연도별-광물수입량-산출",
    "title": "[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)",
    "section": "UN Comtrade 자료분석(연도별 광물수입량 산출)",
    "text": "UN Comtrade 자료분석(연도별 광물수입량 산출)\n\n\nShow the code\nplt.rcParams['font.family'] = 'Nanum Gothic'\n\nfig, ax = plt.subplots(figsize=(20, 4), ncols=3)\n\nfor ore in ores_uncomtrade_url.keys():\n    sns_index = ores_uncomtrade_url[ore]['df_top5'].columns.tolist()\n    sns_value = ores_uncomtrade_url[ore]['df_top5'].values[5].tolist()\n    sns_df = pd.DataFrame(sns_value, index=sns_index, columns=[ore])\n    sns_df = sns_df[ore].str.replace(',','').astype(int)\n\n    if ore in ['망간']:\n        target_ax = ax[0]\n    elif ore in ['니켈','리튬']:\n        target_ax = ax[1]\n    elif ore in ['니켈','코발트']:\n        target_ax = ax[2]\n\n    sns_df.plot(ax=target_ax)\n\n    target_ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,}'.format(int(x))))\n\nfor each_ax in ax:\n    each_ax.legend()"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Elasticsearch\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#정제코드-작성해보기",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#정제코드-작성해보기",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "정제코드 작성해보기",
    "text": "정제코드 작성해보기\n\njobs\n\nmain.py\n\nSpark initialization + filter(정제) + 저장\n공통기능\n추후 자동화될 부분 고려되어있는 코드\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\"--target_date\", default=None, help=\"optional:target date(yyyy-mm-dd)\")\n  args = parser.parse_args()\n\n  # ...(생략)\n\n  if args.target_date is None: # 정제실패시 다시돌릴 때 편의를 위함\n      args.target_date = (datetime.now() - timedelta(1)).strftime('%Y-%m-%d')\n  args.input_path = f\"/opt/bitnami/spark/data/{args.target_date}-*.json\" # 데이터명이 날짜일 예정\n\nfilter.py\n\n정제코드(BaseFilter + TopRepoFilter)\n\nfrom base import BaseFilter # base.py에서 가져옴\n\nclass TopRepoFilter(BaseFilter):\n    def filter(self, df):\n        # Top 10 Repo: Push Count\n        repo_cnt_df = df.groupBy('repository_id', 'repo_name').pivot('type').count()\n        repo_cnt_df.where((F.col('repository_id').isNotNull())) \\\n                    .orderBy(F.desc('PushEvent')) \\\n                    .limit(10)\n        return repo_cnt_df\n\n정제코드(BaseFilter + DailyStatFilter)\n\nfrom base import BaseFilter # base.py에서 가져옴\n\nclass DailyStatFilter(BaseFilter):\n    def hit_count(self, df, cond, col_name):\n        # is_cond 컬럼을 만든 후, \n        # F.col('type') == 'PushEvent' 와 같은 조건을 F.when(cond, 1)에 넣어 값이 조건에 맞으면(PushEvent) 1, 아니면(otherwise) 0\n        # .agg(F.sum('is_cond').alias(col_name) 으로 합산 후 alias지정\n        return df.withColumn('is_cond', F.when(cond, 1).otherwise(0)).agg(F.sum('is_cond').alias(col_name))\n\n\n    def filter(self, df):\n        # daily stats\n        ## 유니크한 유저  :  d_user_count  | 1000\n        stat_df = df.agg(F.countDistinct('user_name').alias('d_user_count'))\n        ## d_repo_count | 500 데이터프레임을 만든 후, crossjoin으로 합침  :  d_user_count, d_repo_count | 1000, 500\n        stat_df = stat_df.crossJoin(df.agg(F.countDistinct('repository_id').alias('d_repo_count')))\n\n        # push counts\n        ## where절로 숫자를 구한 후 데이터프레임 생성작업이 필요없도록, hit_count함수로 구현\n        ## \n        push_cnt_df = self.hit_count(df, F.col('type') == 'PushEvent', 'push_count')\n        push_cnt_df = push_cnt_df.cache() # 캐시를 호출하여 최적화에 도움\n        stat_df = stat_df.crossJoin(push_cnt_df)\n\n        pr_cnt_df = self.hit_count(df, F.col('type') == 'PullRequestEvent', 'pr_count')\n        pr_cnt_df = pr_cnt_df.cache() # 캐시를 호출하여 최적화에 도움\n        stat_df = stat_df.crossJoin(pr_cnt_df)\n\n        fork_cnt_df = self.hit_count(df, F.col('type') == 'ForkEvent', 'fork_count')\n        fork_cnt_df = fork_cnt_df.cache() # 캐시를 호출하여 최적화에 도움\n        stat_df = stat_df.crossJoin(fork_cnt_df)\n\n        commit_comment_cnt_df = self.hit_count(df, F.col('type') == 'CommitCommentEvent', 'commit_comment_count')\n        commit_comment_cnt_df = commit_comment_cnt_df.cache()\n        stat_df = stat_df.crossJoin(commit_comment_cnt_df)\n\n        stat_df.show(10, False)\n        return stat_df\nbase.py\n\nexecutor세팅 관련코드\n\n  def read_input(spark, input_path):\n    def _input_exists(input_path):\n        return glob.glob(input_path)\n\n    if _input_exists(input_path):\n        df = spark.read.json(input_path)\n        df.printSchema()\n\n        # 파티션의 수가 EXECUTOR보다 적으면 리파티션 (노는 EXECUTOR방지)\n        max_executor_num = 3\n        if df.rdd.getNumPartitions() &lt; max_executor_num:\n            df = df.repartition(max_executor_num)\n        return df\n    else:\n        return None\n\n메타정보 추가코드\n\ndef df_with_meta(df, datetime):\n    df = df.withColumn(\"@timestamp\", F.lit(datetime)) \n    return df"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#spark-submit",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#spark-submit",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "spark-submit",
    "text": "spark-submit\n\n참고사항\n\nmemory옵션(driver, executor), num-executors, executor-cores를 만져보기\ngh archive데이터를 기준으로 메모리 3기가 정도가 원활해보임\n\nspark-submit \\\n--name main.py \\\n--master spark://spark-master:7077 \\\n--jars \"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\" \\\n--conf spark.dynamicAllocation.enabled=true \\\n--conf spark.dynamicAllocation.executorIdleTimeout=2m \\\n--conf spark.dynamicAllocation.minExecutors=1 \\\n--conf spark.dynamicAllocation.maxExecutors=3 \\\n--conf spark.dynamicAllocation.initialExecutors=1 \\\n--conf spark.memory.offHeap.enabled=true \\\n--conf spark.memory.offHeap.size=2G \\\n--conf spark.shuffle.service.enabled=true \\\n--conf spark.executor.memory=2G \\\n--conf spark.driver.memory=2G \\\n--conf spark.driver.maxResultSize=0 \\\n--num-executors 2 \\\n--executor-cores 1 \\\n${SCRIPT}"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#elasticsearch",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#elasticsearch",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Elasticsearch",
    "text": "Elasticsearch\n\nElasticsearch은 검색엔진인데 왜 저장용도로 사용하는가\n\nELK스택 : Elasticsearch + Logstash + Kibana를 같이 사용. 로그데이터 관리/모니터링 등에 많이 사용(스타트업부터~대기업까지도)\n검색엔진이 아닌 ELK스택과 같은 활용으로 이해\n키바나와 같이 사용할 수 있는 매력(편리함)\n\n데이터의 저장/커넥션/export 등의 과정없이, ES의 호스트네임 입력만으로 쉽게 연동 가능\n\n\n\nElasticsearch\n\ndistributed, RESTful search and analytics engine\n\nRESTful : REST API형태로 호출해서 검색가능(서버에 쿼리를 날리면 검색이 가능)\n\nApache Lucene을 기반으로 만든 분산 검색 엔진\nAnalyzer가 내장되어있어 document를 저장하게되면, 검색되도록 해줌 \n\nElasticsearch 특징(Characteristics)\n\ntransaction, join 지원되지 않음\n\njoin을 사용해야하는 경우는 ES를 쓰지 않도록 해야 함\n\n(검색을 위한)색인작업 시간이 필요\n\n색인을 위한 Scoring 알고리즘(제일 적합한 문서임을 점수를 매김) : TF-IDF→BM25로 변경됨\n\nBM25는 대기업 검색엔진에도 들어있음\n\n\n버전업이 빠르고/잦고/많음. 이에 따라 (버전별) UI나 API차이가 클 수 있음 \n\nElasticsearch 구조 (Structure)\n\ndocument : 하나의 기본 구조. json 형태로된 문서\nfield : document의 필드 (jon의 필드, dataframe에서의 컬럼과 같음)\nindex : document가 모인 단위, 여러 노드에 분산 저장됨(분산 검색엔진이므로)\n\nRDBMS의 index와는 다른 역할. index를 만들어 빠르게 쿼리를 가능하게 하지 않음\n\n_id : index내 문서에 부여된 unique id (ES가 unique함을 보장하지는 않으며 사용자의 책임 하에 사용)\n\n다른 index의 _id라면 같을 수 있음\n같은 index 내의 문서라고, 다른 shard에 있어 동일 _id 를 가질 가능성 있음\n\n강제하고, 체크하고 있지 않기 때문 \n\n\n\nShard\n\nPrimay와 Replica가 있음. 아래 그림기준 같은 색 칠해진 것들이 같은 Shard  \n\nSearch과정(저장쿼리결과도출까지 ES내부적으로 일어나는 일) 및 용어\n\ndocument를 분석해서 Lucene(루신)이 inverted index(역색인)를 생성\n\n아파치 루신이 (메모리에서) 역색인을 생성\n\ndocument의 insert/update/delete 변경을 메모리에 들고 있다가 주기적으로(batch단위로) disk에 flush\n\nrefresh: 디스크에 내려 검색가능해짐\n\ndisk에 flush(들어간)된 segment들이 중간중간 적절히 segment merge됨\n\nsegment: 디스크에 기록된 파일들의 단위\n\n(여러 segment가 모인) lucene index\n\n루신엔진의 역할은 모인 segment에서 결과를 검색하여 결과를 줌(Lucene은 index 내에서만 검색 가능)\n루신엔진이 준 결과를 ES가 모아 merge에서 리턴함\n\nshard(lucene index를 모은[wrapping] 단위)\neleasticsearch index : 여러 shard 가 모인 단위.\n\nES에서 높은 highlevel에서 부르는 index\nES검색을 하면 각 shard에 명령이 내려가 내부에서 검색한 후, 결과를 merge해서 줌\n\ncommit : fsync()를 통해, 커널시스템 캐시내용 ↔︎ 실제 디스크내용의 sync를 맞춤 (⇒ 비싼 작업)\n\n커널시스템 캐시내용 : lucene flush 시킨 것\ncommit주기는 옵션으로 지정 가능\n\nElasticsearch의 flush는 내부적으로 Lucene commit을 수행\n\nflush가 일어나면 commit을 수행 (디스크를 내리고 싱크를 맞추는 작업의 반복)\n\ntranslog : commit 되기 전의 작업 기록 (로딩실패 등의 경우 shard recovery 에 이용)\n\ncommit되었다면 fail해도 내용을 잃어버리지 않겠지만, commit되지 않은 기록은 translog로 보존\n\n참조 이미지  \n\nnodes\n\nmaster node: 데이터의 저장보다는 관리의 역할\n\n클러스터를 관리 - 인덱스 생성이나 삭제, 샤드 할당\nmaster-eligible node : master node 가 될 수 있는 후보들. 지정할 수도 있음\n\nmaster-eligible node끼리 voting해서 master node를 뽑게 됨\n\n\ndata node: 데이터가 저장되는 노드. CRUD/Search/Aggregation 등이 일어남\ningest node: 색인 전처리(ingest pipeline) 수행\ncoordinating node: role이 전부 지정되지 않은 노드(master/data/ingest 모두 아님)\n\n클라이언트 요청만 처리(coordinating만 함). data nodes에 요청을 포워딩하고 결과를 모아서 클라이언트에게 응답\n\n기본적으로는 모든 노드가 coordinating 역할을 수행 \n\nES Install\n\n설치링크 : https://www.elastic.co/kr/downloads/elasticsearch\n최신 ES에는 이미 JDK 내장되어 있음\n\nES_JAVA_HOME 옵션을 지정해서 JDK 변경 가능 \n\n\nES 실습환경 설정관련 유의사항(사용중인 docker-compose.yml 수정하는 경우)\n\nES의 버전에 맞춰 Kibana의 버전도 맞추어야 함 (같이 사용하려면)\n    es:\n      image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3 # Kibana연동시 이 버전과 맞춰야 함\n      networks:\n        - default-network\n      environment:\n        - node.name=es # 노드이름 설정\n        - discovery.type=single-node\n        - discovery.seed_hosts=es\n        - xpack.security.enabled=false\n        - xpack.security.enrollment.enabled=false\n        - xpack.security.http.ssl.enabled=false\n        - xpack.security.transport.ssl.enabled=false\n        - cluster.routing.allocation.disk.threshold_enabled=false\n        - ELASTIC_PASSWORD=password\n      mem_limit: 1073741824 # 환경에 따라 미지원될 수 있음. 삭제해도 무방\n      ulimits:\n        memlock:\n          soft: -1\n          hard: -1\n      volumes:\n        - ./es-data:/usr/share/es/data # es-data폴더로 지정. 로그작성 등에 사용\n      ports:\n        - 9200:9200\nlocalhost:9200/?query=’키워드’와 같은 느낌으로도 실행가능 (REST API이므로)"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#elasticsearch-index",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#elasticsearch-index",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Elasticsearch index",
    "text": "Elasticsearch index\n\nElasticsearch index\n\nLucene index와 elasticsearch index는 다름을 참고\n\nSettings\n\nnumber_of_replicas(동적으로 변경 가능) : primary shard 1개 당 replica shard 개수\nnumber_of_shard(reindex없이 변경 불가) : index 에 지정된 shards 의 수\n\nRDBMS의 index와 달리 어렵고 비용이 많이 드는 작업(Scoring알고리즘 등의 재실행)\nreindex는 서비스가 돌고 있으면 서비스를 내려야 하는 등 하기 어려움 \n\n\nset index\n\nPUT &lt;실제index-name&gt; : index 만들기\nGET &lt;실제index-name&gt; : 만든 index확인하기\nrefresh_interval : 검색대상이 되도록 얼마나 자주 refresh(새로운 데이터의 색인 생성에 대한 주기)\n\nindex.search.idle.after (default는 30)\n\n-1일 경우 refresh하지 않음 (현재 실습처럼 검색하지 않고 쓰는 경우에 사용가능) \n\n\n\nfield type\n\nfield의 타입\n\nsimple(기본) 타입 : text, keyword, date, long, double, boolean, ip\n\ntext vs keyword\n\ntext : analyzer가 적용된 후 색인\n\nanalyzer가 각 단어를 분석했기 때문에 full-text search 가능\n\nkeyword : analyzer가 적용되지 않고 색인\n\nanalyzer가 토큰별 분석을 하지 않아 전체문장 일치해야 검색 가능(즉 search용도가 아님)\nsearch대상이 아닌 sorting/aggregation대상인 경우 많이 사용\n(analyzer가 적용되지 않으므로) text보다 빠르다\n\n\n\n계층 구조있는 타입 : object, nested\n그 외 타입 : geo_point, geo_shape, completion\n\n한번 지정되면 reindex하지 않으면 변경 불가. (설계시 신중하게) \n\nindex mapping\n\ndynamic mapping : 새로운 field name으로 데이터가 들어오면 적당한 타입을 부여\nexplicit mapping : 명시적으로 mapping을 사전에 지정 (실제 서비스 등에서 주로 사용) \n\ncustom analyzer\n\n사용자가 원하는 analyzer(서드파티 포함) 지정 가능\nhtml_strip, standard, lowercase, stop-token(욕설 등 특정 토큰 색인X) 등\n여러 analyzer 등을 완료하면 색인이 가능해짐 \n\nmultifields\n\n한 필드의 데이터를 여러 설정으로 색인 가능\n\n(서비스 요구사항이 fix되어)매핑이 지정되었는데 추가 요구사항이 있는 경우, 반영하기 위해 사용\nreindex는 비용이 많이 듦 \n\n\n옵션 index:false\n\n해당 필드 역색인을 만들지 여부(default:true)\n\n주의 : index를 만들지 않으므로 검색대상이 되지 않음\n\nES 8.1 부터 필드가 doc_values를 사용한 경우는 index없이도 검색 가능(검색 성능은 떨어짐)\n역색인을 생성하지 않을 뿐 정렬이나 집계 가능\n\ntext와 annotated_text 타입을 제외하고 지원\nsorting, aggregations, script 이 필요없으면 disable 가능 \n\n\n옵션 enabled:false(object 필드에 적용)\n\nfalse지정시, 파싱도 수행하지 않음\n_source에 들어가지만 검색/저장되지 않음\n\n데이터 명세상 잘 오고 있는지 확인하는 정도로만 갖고 싶을 때(_source에서 확인만)\nAPI로 받은 데이터를 모두 사용하지 않고, 특정 데이터만 확인하고 싶을 때 \n\n\nfielddata(잘쓰지 않음)\n\ntext 필드에 대해 sorting, aggregations, 혹은 script 작업을 수행해야할 때 사용\ntext 필드를 sorting하고 싶을때 fielddata:enable로 적용\n\ntext필드이므로 analyzer 적용되고 색인이 생성됨 (예를 들면 문장단위의 sorting 실행됨)\nsorting을 하려면 메모리에 올려야 하므로, heap메모리 사용이 순간적으로 커진다\n\n기본적으로 disable \n\nanalyzer\n\n원하는 여러 필터를 순서대로 적용 가능\n적용순서 : character filters → tokenizer → token filters\n종류 : HTML Strip Character Filter, Mapping Character Filter, Pattern Replace Character Filter \n\ntokenizer\n\ncharacter stream에서 stream of tokens로 바꿈\n1개의 tokenizer만 지정 가능 (토큰 생성방법에 따라 후속이 많이 달라지므로)\n기본제공 토크나이저 중 한글전용 토크나이저는 없음 (별도 형태소 분석기 등을 붙여 사용해야 함)\n종류 : standard tokenizer, etter tokenizer, whitespace tokenizer, ngram tokenizer 등\n\nngram tokenizer예시) min 2, max 3: hello → he, hel, el, ell, ll, llo \n\n\ntoken filter\n\ntoken stream에서 token을 추가/변경/삭제\n0개 이상(미지정 가능)\n종류 : lowercase/uppercase, stop words, synonym, pattern_replace, stemmer(어간 추출, 한글불가), trim, truncate \n\nnormalizer\n\nkeyword 필드(text필드와 달리 analyzer미적용)에만 적용. analyzer와 비슷\nanalyzer는 stream of charactor를 stream of token으로 만들어줌\n\ncharacter filters → tokenizer → token filters\n\nnormalizer는 단일 토큰만 생성.\n\ncharacter filters, token filters를 적용가능하게 해줌\n\n예를 들어, 특정 문자만 지우는 등의 기능으로 사용 \n\n\n\n_routing\n\ndocument가 어떤 shard에 배정될지 지정\nshard number를 결정할 때 보통 hash알고리즘으로 되어있음\n\nshard_num = hash(_routing) % num_primary_shards\n\n기본값으로 _id를 사용(유니크값이므로 각 데이터별로 어느 shard에 넣을지 결정하기에 용이)\n색인시에 routing지정했다면 update, delete, get, search 시에도 지정해야 함\n\nupdate 할 때 등도 shard를 찾아 갈 수 있도록 지정해야 함"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#queryes에서-쿼리하는법",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#queryes에서-쿼리하는법",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Query(ES에서 쿼리하는법)",
    "text": "Query(ES에서 쿼리하는법)\n\nElasticsearch Query\n\nget/put/delete 등 Restful API같은 구조 \n\nget API\n\n_doc\n\nGET /_doc/&lt;_id&gt;\n\n그냥 _id만 넣으면 어느 index에서 찾아야할지 ES가 판단 불가\ntranslog에만 기록이 된 것도 조회가 가능하다(get은 검색은 아니라는 뜻)\n\nHEAD /_doc/&lt;_id&gt;\n\n존재여부를 bool로 반환\n\n\n_source/ (object필드에 enabled:false 적용한 경우 rawdata저장되는 곳)\n\nGET /_source/&lt;_id&gt;\nHEAD /_source/&lt;_id&gt;\n\n존재여부 확인 가능 \n\n\n\n_source 추가 설명\n\n원본 JSON document body를 저장\n색인되지 않으므로 검색대상이 아닌 원본 데이터임\n주의사항 : _source: disable 할 경우 update, reindex 등을 사용할 수 없다\n\n다른 데이터는 analyzer를 통과한(색인된) 데이터이므로, 재색인할 때는 활용불가하며 원본 있어야함\n원본으로 _source에 넣어두므로, disable해두면 reindex나 update를 통한 버전업 불가 \n\n\ndelete API\n\nDELETE /_doc/&lt;_id&gt;\n\n삭제flag만 올려두고, 실제 삭제는 Licene segment의 주기적으로 merge될 때 진행됨\n내부적으로는 최근 삭제된 문서를 일정기간 보존(shard복구시 삭제 작업을 재처리하기 위함) \n\n\nupdate API\n\nPOST /_update/&lt;_id&gt;\n\n일부 필드 업데이트(doc_as_upsert가능)\ndetect_noop: 실질적으로 어떠한 값도 변경하지 않을 경우 수행 X\n\n아무값 변경이 없는데 POST명령이 들어간 경우, 업데이트를 수행하지 않도록 하는 기능 \n\n\n\nmultiget API\n\nGET /_mget { “docs”: [ ] } \n\nbulk API\n\nPOST /_bulk\n\nindex, create, delete, update를 한번에 모아보낼 수 있음\n으로 데이터 구분\nContent-Type: application/x-ndjson, 마지막 라인 으로 종결\n대량 색인 작업 시에 필수로 사용(처음 ES구축시 대량색인해야할 때)\n실제 적용 순서가 보장되지 않음 (여러 bulk 작업 간의 순서 보장 X)\n\n분산 작업이므로(여러 노드가 작업하므로)\n\n한 bulk request 안의 같은 _index + _id 조합은 순서 보장 \n\n\nDelete by query\n\n조건에 맞는 document 삭제\n주의사항\n\n쿼리 조건에 맞는 document가 여러개일 경우, 삭제하려던 것이 삭제되어있거나 업데이터 되어 쿼리에 맞지 않을 수 있음\n\nindex 스냅샷을 찍고 스냅샷을 기준으로 작업 진행\n스냅샷 이후 변경이 확인되면, 삭제 실패로 끝남\n\nwait_for_completion: false : async 작업 가능\n\ndelete될 쿼리를 날려두고 다른 작업ㅇ르 할 수 있음\n\n작업 결과가 .tasks index에 document 로 남음\n\nDelete by query가 잘못되었을 때 복구할 수 있도록\n\n\nupdate by query도 있음 \n\nsearch (중요)\n\nGET &lt;index&gt;/_search or POST &lt;index&gt;/_search\n특징\n\n색인 생성한 것에 대해 맞는 문서를 리턴(scoring순서에 맞게 나옴)\n(index 여러 곳에서 검색하기 위해)index는 ,로 구분하여 지정 가능\nindex 이름에 wildcard * 지정 가능\nquery string 인자를 이용하는 검색도 존재\n\nex) GET /test/_search?q=fieldName:queryText\n\n\nmatch_all(모두 만족) / match(하나의 필드가 만족)\n  {“query”: {\n      “match_all”: {}\n      “match”: {       “mytext”: “Hello, World!”     }\n    } }\nterm / terms\n\nquery string 을 analyze하지 않음(exact term을 포함한 문서를 찾음)\nkeyword 필드의 경우 normalizer 가 적용되었으면 쿼리에도 적용\nterms: 여러 query string (조건) 에 의해 검색 \n\n\nprefix query\n\n필드가 특정한 prefix로 시작하는 document 검색\n명시적 매핑(explicit mapping)시 index_prefixes옵션을 미리 준 경우,\n\n속도가 높일 수 있으나(analyzer와 관계없이), 그렇지 않았다면 expensive query\n\n많은 DB에서는 사실상 prefix query는 expensive query\n\n\nES는 비싼 쿼리를 가능하게할지 말지도 옵션 지정 가능\n\nsearch.allow_expensive_queries: false라면 expensive query불가 \n\n\nrange/exists\n\nrange: 필드가 특정 range 이내의 값인 document\n\ngt(greater than), gte, lt, lte로 range를 지정\ndate field 에 대해서도 지원\ntext / keyword 필드에 대한 range query는 expensive query\n\nsearch.allow_expensive_queries: false라면 사용불가\n\n\nexists: 필드에 값이 색인되어 있는 document\n\nempty string 도 값이 있는 것으로 판단\nJSON 이 null 이거나 [], [null, null] 등일 경우 값이 없는 것으로 판단\nmapping 에 index: false 여서 색인되지 않은 경우에도 값이 없는 것으로 판단bool query \n\n\nbool query(여러 쿼리를 조합하는 쿼리)\n\nmust: 지정된 모든 쿼리 조건을 만족해야 함\nmust_not: 지정된 모든 쿼리 조건을 만족하지 않아야 함\nshould: 지정된 쿼리 중 적어도 minimum_should_match 개수 만큼 만족해야 함\n\nfilter는 모두 만족해야 함\n\nfilter : 쿼리 외의 true/false로 답이 나오는 것들\n\nquery는 relevance score를 계산(리턴값이 score)해 얼마나 문서와 잘 매칭되는지 판단\nfilter와 query 중 우선순위는 없음\n\nshould query(bool)라면, filter는 만족하는 query중에서 minimum_should_match를 count \n\n\n\n\n\nsort\n\n필드를 지정해서 검색 결과를 정렬(보통 score는 계산하지 않고 정렬만 함)\n정렬 중에는 필드 값이 메모리에 올라감\n\ntext 타입은 일반적으로 정렬 대상이 될 수 없음 (성능이슈, 비싼작업) \n\n\nfrom / size\n\n기초적인 pagination.\n1만개를 초과한 document를 pagination할 수 없으며, 이로 인해 잘 사용되지 않음\n\nindex.max_result_window settings로 기본값이 1만개로 되어있음\n\n메모리 & 시간이 from + size 의 값에 비례 \n\nscroll / search_after\n\nfrom, size 대신 사용되는 pagination방법\nscroll: search context를 유지하며 대량 데이터를 처리\n\n검색이 수행된 그 상태를 스냅샷처럼 찍어 검색 결과를 스크롤링\n(스냅샷 쓰므로)스크롤링 도중 변화되는 문서 내용은 반영되지 않음\n옵션\n\nsize: 배치에 반환할 문서의 수\n_scroll_id: 다음 배치의 검색 값 받을 수 있음\n\nscroll 인자로 넘겨준 시간만큼 search context를 유지\n기준값이 동일한 document 사이에서는 일관된 순서를 보장하지는 않음(sort되어 나오지 않음)\n\nsearch_after(제일 많이 사용): 결과 문서 중 가장 마지막 문서의 sort 기준값을 사용\n\n_id 필드는 doc value가 disabled되어 있기 때문에 이를 기준으로 하는 정렬은 많은 메모리를 사용\n\n_id필드로 정렬하지는 않는다는 뜻\n\nscroll과 다른점 : 실시간 변경 데이터 반영 가능"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240902/index.html#optimization",
    "href": "posts/meta-de-spark_and_airflow-20240902/index.html#optimization",
    "title": "[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Optimization",
    "text": "Optimization\n\nreindex\n\n비싼 작업\n원본 index내 문서의 _source를 읽어서 대상 index에 새로 색인\nquery를 지정하여 일부 문서만 reindex 가능\nscript 이용하여 throttling, slicing, 비동기 작업 가능\n다른 Elasticsearch 클러스터에서 데이터를 가져와 reindex 가능\n\n다른 Elasticsearch를 연결하려면 elasticsearch.yml에 reindex.remote.whitelist: “host:XXXX” 추가\n\nPOST _reindex\n{“source”: …\n   “dest”: …}\n\n\nIndex life cycle\n\n종류\n\nhot: 현재 업데이트가 수행되고 있고 읽기 작업도 가장 많은 상태\nwarm: 인덱스에 더 이상 업데이트가 수행되지는 않지만 읽기 작업은 들어오는 상태\ncold: 인덱스에 더 이상 업데이트가 수행되지 않고 읽기 작업도 가끔씩만 들어오는 상태.\n\n검색은 되어야 하나 속도가 느려도 괜찮은 상황.\n\nfrozen: 인덱스에 더 이상 업데이트가 수행 되지 않고 읽기 작업도 거의 들어오지 않는 상태.\n\n검색은 되어야 하나 속도가 상당히 느려도 괜찮은 상황.\n\ndelete : 인덱스가 더 이상 필요없고 삭제되어도 무방한 상태\n\nindex가 어느 cycle에 있는지 보면서 자원을 분배\n\n이에 따라 검색속도 차이가 많이 남\n검색이 느리다면 ’cold’로 간 것은 아닌지 유추 \n\n\nIndex strategies\n\ntime series로 만들기 (보통 추천)\n\n(RDBMS처럼)데이터 종류별로 만드는 것이 아닌, 2024.01, 2024.02 등\n보통은 최근 데이터로 분석하는 경우 많음\n검색 범위를 좁히기 + index life cycle전략짜기 에 용이함\n오래된 데이터 삭제에 편리\n\n데이터가 커질 경우 index mapping 직접 만들기\nindex template 활용하기 (비슷한 index mapping 사용하는 경우)\nrouting활용하기(어떤 shard에 데이터를 저장할지 지정)\n\n성능 향상\nindex mappings _routing meta field 에 required: true 옵션을 부여해 강제할 수 있다\n\nfield type은 기본적으로 변경 불가능함을 염두에 두기\n\n같은 필드의 type 이 계속 변경될 수 있다면 ES특성상 ES가 적합하지 않음\n해당 데이터가 들어와야 한다면 enable: false로 색인포기하고 에러발생하지 않게 할 수 있음\n\n운영도중 analyze 변경 필요성이 있다면, multi field를 고려한다 \n\nShard Strategies\n\nshard의 개수\n\nshard 갯수를 노드 수의 n배로 둘 필요는 없음\n\nhot/warm 등 index에 따라 리소스가 분배되므로, 성능향상에 꼭 도움이 되지 않음\n\n활용중인 index가 많은 경우, 단일 index작업에 모든 노드가 참여하지 않음(고르게 분배되지 않음)\n\n\n추후 추가 서버가 투입될 가능성이 있음"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240716/index.html",
    "href": "posts/meta-dl-creditcard-20240716/index.html",
    "title": "[MStudy_과제개선2] 신용카드 이상거래 탐지 모델링",
    "section": "",
    "text": "Kaggle CreditCard Fraud Detection (개선2 : Keras tuner, Hyperband search)\n\n개요\n\n딥러닝 스터디에 제출했던 과제에 대한 피드백 반영 및 개선(지속 개선예정)\n원본데이터 : Kaggle CreditCard Fraud Detection\n적용한 피드백\n\nKeras tuner 사용시 처음부터 Learning rate를 Test대상에 포함\nEarly stopping이 아닌 전체 epoch 수행 후 최적포인트를 적용하는 방법도 해보려했으나, 학습자체에 시간이 오래걸려 제외\n\n추가 테스트\n\n다른 Adam계열 Optimizer를 Keras tuner후보군에 추가\nEarly stopping에 대해 Keras callback의 ReduceLROnPlateau도 사용해봄\nRandom search, Grid Search 를 알아보다가 Hyperband라는 것도 있어서 사용해봄\n\nRandom search는 무작위 탐색으로 효율적이지만 최적일수도 있고 아닐수도 있음\nGrid search는 가능한 모든 조합을 탐색하여 오래 걸림\n효율적이지만 최적이 아닐 확률이 있다는 점과 가정용 컴퓨터에서 계산효율적이지 못한 방법론이 맞는지에 대한 고민\nHyperband가 정해진 시간내에 중간 결과를 기준으로 조합 후보를 버리는 등 결합된 방법론으로 보여 적용해봄\n\n수업에 나온 Leaky ReLU도 적용해보고 싶었지만, Tuner호환 등 여러 문제가 있어 제외함\n학습한 모델은 저장해두고 향후 다른 것도 해보기\n\n결과\n\n초보 수준에서는 Tuner가 좋은 파라미터를 정해줄 것을 기대하여 막연히 점수가 개선될 것을 기대했으나, 실험 결과로는 유의미한 개선점이 보이지는 않았음\n\n모델평가(Stratified Fold만 적용)\n\n[Train]7121/7121 - 4s - 573us/step - f1_score: 0.0071 - loss: 0.0024 - precision_7: 0.9479 - recall_7: 0.8782\n[Test]891/891 - 1s - 598us/step - f1_score: 0.0072 - loss: 0.0033 - precision_7: 0.9000 - recall_7: 0.7347\n\n모델평가(Tuner + Learning rate scheduler + Early stopping 추가 적용)\n\n[Train]8011/8011 - 6s - 741us/step - f1_score: 0.0035 - loss: 0.0151 - precision: 0.8372 - recall: 0.8126\n[Test]891/891 - 1s - 772us/step - f1_score: 0.0034 - loss: 0.0216 - precision: 0.7600 - recall: 0.7755\n\n\nOptimizer나 Hyperband 등 세부적인 내용의 고려 없이 적용되어 긍정적인 결과가 나오지 않은 것으로 추정\n\n\n\n\n개선과제 진행\n\nKeras Tuner 사용한 딥러닝 모델 개선\n\nLearning rate를 처음 튜닝부터 포함\nOptimizer를 다양하게 고려 (Adam계열)\nLeaky ReLU를 Activation 후보군에 포함하려했으나, Tuner지원하지않는 것으로 보여 제외\n\nChoice 함수에는 int, float, str, bool만 가능하여 아래와 같이 추가했음\n\n  from keras.utils import get_custom_objects\n  from keras.layers import LeakyReLU\n\n  get_custom_objects().update({'leaky-relu': LeakyReLU()}) # Add LeakyReLU\n  activation = hp.Choice('activation'+str(i),values=['relu','elu','leaky-relu']) # For Activation\n\n그러나 아래의 오류가 계속 발생하여 제외함\n\n  &gt;&gt;&gt; ValueError: Could not interpret activation function identifier: leaky-relu\n\n\nimport keras_tuner as kt\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Precision, Recall, F1Score\nimport matplotlib.pyplot as plt\n\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # Input & Flatten\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n\n    # Hidden Layers\n    for i in range(hp.Int('num_layers',min_value=1,max_value=20)):\n\n        # For Dense\n        units = hp.Int('units',min_value=10,max_value=150,step=5) # For Neurons\n        activation = hp.Choice('activation'+str(i),values=['relu','elu']) # For Activation\n\n        model.add(tf.keras.layers.Dense(units, activation=activation,\n                                        # 기본값은 glorot_uniform(Xavier), He는 Kaiming\n                                        kernel_initializer=tf.keras.initializers.HeNormal())) \n        \n        # Add Batch Normalization\n        model.add(tf.keras.layers.BatchNormalization()) # Layer통과후 & Activation 전\n        \n        # For Dropout\n        dropout_rate = hp.Choice('dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n    \n    # Model setting\n    hp_learning_rate = hp.Choice('learning_rate', values = [0.05, 0.01, 0.001]) \n    dict_optimizers_with_lr = {\n        'Adam': tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n        'Nadam': tf.keras.optimizers.Nadam(learning_rate=hp_learning_rate),\n        'AdamW': tf.keras.optimizers.AdamW(learning_rate=hp_learning_rate),\n        'Adamax': tf.keras.optimizers.Adamax(learning_rate=hp_learning_rate)\n        }\n    hp_optimizer=hp.Choice('optimizer',values=['Adam','Nadam','AdamW','Adamax'])\n\n    model.compile(optimizer=dict_optimizers_with_lr[hp_optimizer], \n                  loss='binary_crossentropy',\n                  metrics=[F1Score(average=\"weighted\"), Precision(), Recall()])\n    \n    return model\n\n\nwith tf.device('/device:GPU:0'):    \n    tuner = kt.Hyperband(build_model,\n                     objective = kt.Objective('val_f1_score', direction='max'), \n                     max_epochs = 10,\n                     project_name = 'test_hyperband')\n    \n    for idx in kfold_dataset.keys():\n    ## 모델 학습\n        tuner.search(kfold_dataset[idx]['x_train'],  kfold_dataset[idx]['y_train'],\n                                            epochs=10,\n                                            validation_data=(kfold_dataset[idx]['x_validation'], kfold_dataset[idx]['y_validation']))\n\nTrial 30 Complete [00h 02m 29s]\nval_f1_score: 0.00343498052097857\n\nBest val_f1_score So Far: 0.4999999403953552\nTotal elapsed time: 00h 45m 46s\n\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'num_layers': 4,\n 'units': 105,\n 'activation0': 'elu',\n 'dropout0': 0.5,\n 'learning_rate': 0.05,\n 'optimizer': 'Nadam',\n 'activation1': 'relu',\n 'dropout1': 0.6,\n 'activation2': 'relu',\n 'dropout2': 0.9,\n 'activation3': 'elu',\n 'dropout3': 0.3,\n 'activation4': 'relu',\n 'dropout4': 0.9,\n 'activation5': 'elu',\n 'dropout5': 0.4,\n 'activation6': 'elu',\n 'dropout6': 0.3,\n 'activation7': 'elu',\n 'dropout7': 0.2,\n 'activation8': 'elu',\n 'dropout8': 0.5,\n 'activation9': 'elu',\n 'dropout9': 0.6,\n 'activation10': 'elu',\n 'dropout10': 0.5,\n 'activation11': 'relu',\n 'dropout11': 0.9,\n 'activation12': 'elu',\n 'dropout12': 0.6,\n 'activation13': 'elu',\n 'dropout13': 0.3,\n 'activation14': 'relu',\n 'dropout14': 0.5,\n 'activation15': 'elu',\n 'dropout15': 0.8,\n 'activation16': 'elu',\n 'dropout16': 0.2,\n 'activation17': 'elu',\n 'dropout17': 0.1,\n 'activation18': 'elu',\n 'dropout18': 0.2,\n 'activation19': 'elu',\n 'dropout19': 0.5,\n 'tuner/epochs': 10,\n 'tuner/initial_epoch': 0,\n 'tuner/bracket': 0,\n 'tuner/round': 0}\n\n\n\nmodel_by_tuner= tuner.get_best_models(num_models=1)[0]\nmodel_by_tuner.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 29)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 105)            │         3,150 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (None, 105)            │           420 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 105)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 105)            │        11,130 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (None, 105)            │           420 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 105)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 105)            │        11,130 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (None, 105)            │           420 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 105)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 105)            │        11,130 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ (None, 105)            │           420 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 105)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 1)              │           106 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 38,326 (149.71 KB)\n\n\n\n Trainable params: 37,486 (146.43 KB)\n\n\n\n Non-trainable params: 840 (3.28 KB)\n\n\n\n\n\n딥러닝 모델에 Learning rate scheduler와 Early stopping 적용\n\nLearning rate scheduler와 Early stopping 모두 weighted f1-score는 지원하지 않아, val_f1-score로 사용\n\n    UserWarning: Learning rate reduction is conditioned on metric `&lt;F1Score name=f1_score&gt;` \n    which is not available. Available metrics are\n    : f1_score,loss,precision,recall,val_f1_score,val_loss,val_precision,val_recall,learning_rate.\n    UserWarning: Early stopping conditioned on metric `&lt;F1Score name=f1_score&gt;` \n    which is not available. Available metrics are\n    : f1_score,loss,precision,recall,val_f1_score,val_loss,val_precision,val_recall,learning_rate\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\n\n\ntotal_epoch_value = 1000\n\nwith tf.device('/device:GPU:0'):\n    reduce_lr = ReduceLROnPlateau(monitor='val_f1_score',\n                factor=0.2, # lr감소치. 현재 lr이 0.01이라면 0.01*0.2를 적용\n                patience=10, # 개선에 대한 허용치. 10 epoch까지 개선이 없다면 적용\n                mode='max', # auto, max, min 옵션 있음\n                min_lr=0.001)\n    es = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=200)\n\n    # 모델 학습\n    for idx in kfold_dataset.keys():\n        history_tuner = model_by_tuner.fit(kfold_dataset[idx]['x_train'],  kfold_dataset[idx]['y_train'], \n                                        epochs=total_epoch_value,\n                                        callbacks=[reduce_lr, es],\n                                        validation_data=(kfold_dataset[idx]['x_validation'], kfold_dataset[idx]['y_validation']))\n\n\n# 시각화\nfig, axs = plt.subplots(nrows=4, ncols=1, figsize=(10,20))\n\nfor idx, key in enumerate(history_tuner.history.keys()):\n    if idx == 4:\n        break\n    axs[idx].plot(history_tuner.history[key], label=f\"train_{key}\")\n    axs[idx].plot(history_tuner.history[f\"val_{key}\"], label=f\"val_{key}\")\n    axs[idx].legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# 모델 평가\nprint('* 모델평가')\nmodel_by_tuner.evaluate(x_train, y_train, verbose=2)\nmodel_by_tuner.evaluate(x_test, y_test, verbose=2)\n\n* 모델평가\n8011/8011 - 6s - 741us/step - f1_score: 0.0035 - loss: 0.0151 - precision: 0.8372 - recall: 0.8126\n891/891 - 1s - 772us/step - f1_score: 0.0034 - loss: 0.0216 - precision: 0.7600 - recall: 0.7755\n\n\n[0.02163444086909294,\n 0.00343498052097857,\n 0.7599999904632568,\n 0.7755101919174194]\n\n\n\n모델평가(Stratified Fold만 적용)\n\n7121/7121 - 4s - 573us/step - f1_score: 0.0071 - loss: 0.0024 - precision_7: 0.9479 - recall_7: 0.8782\n891/891 - 1s - 598us/step - f1_score: 0.0072 - loss: 0.0033 - precision_7: 0.9000 - recall_7: 0.7347\n\n모델평가(Tuner + Learning rate scheduler + Early stopping 추가 적용)\n\n8011/8011 - 6s - 741us/step - f1_score: 0.0035 - loss: 0.0151 - precision: 0.8372 - recall: 0.8126\n891/891 - 1s - 772us/step - f1_score: 0.0034 - loss: 0.0216 - precision: 0.7600 - recall: 0.7755\n\n\n\n\n학습한 모델 저장\n\nmodel_by_tuner.save('model_by_tuner_fitted.keras')\n\n\nloadedm_model = keras.models.load_model('model_by_tuner.keras')\nprint('* 모델평가')\nloadedm_model.evaluate(x_train, y_train, verbose=2)\nloadedm_model.evaluate(x_test, y_test, verbose=2)\n\n* 모델평가\n8011/8011 - 8s - 1ms/step - f1_score: 0.0035 - loss: 0.0151 - precision: 0.8372 - recall: 0.8126\n891/891 - 1s - 786us/step - f1_score: 0.0034 - loss: 0.0216 - precision: 0.7600 - recall: 0.7755\n\n\n[0.02163444086909294,\n 0.00343498052097857,\n 0.7599999904632568,\n 0.7755101919174194]\n\n\n\n\n\n\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240715/index.html",
    "href": "posts/meta-dl-creditcard-20240715/index.html",
    "title": "[MStudy_과제개선1] 신용카드 이상거래 탐지 모델링",
    "section": "",
    "text": "Kaggle CreditCard Fraud Detection (개선1 : Stratify 및 fold 적용에 따른 비교, weighted f1-score)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240715/index.html#데이터-확인",
    "href": "posts/meta-dl-creditcard-20240715/index.html#데이터-확인",
    "title": "[MStudy_과제개선1] 신용카드 이상거래 탐지 모델링",
    "section": "데이터 확인",
    "text": "데이터 확인\n\n전체 데이터 : Null값 없음을 확인\n\nTime 컬럼 : 첫번째 거래와의 단순 시간차이이므로 삭제 예정\nV1 ~ V28 컬럼 : 익명화된 데이터. 전부 사용\nAmount 컬럼\n\n특이사항 : 결제액이 0인 데이터가 있어, 월정액 등록 전 Validation용 결제가 아닐까 추정\n이상거래(Class 1)도 데이터로 포함되어있지만, 실질적 돈 이동이 없어 의미가 있는지 의문\n\n위 이유로 처음에는 결제액 0인 데이터를 제거할까 고민\n몇 없는 이상거래 데이터이므로 유지하는 것으로 결정 (전체 492개 이상거래 중 27건 제거시 약 5%의 데이터 손실)\n\n\nClass 컬럼\n\n특이사항 : 98%가 정상거래(Class 0)인 데이터로 편중이 심함\n데이터가 Imbalance한 경우, Accuracy는 성능측정에 한계가 있으므로, F1-Score를 사용\n\n\n\n\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('creditcard.db')\ndf = pd.read_sql_query(\"SELECT * FROM creditcard\", conn)\nconn.close()\ndf\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284802\n172786.0\n-11.881118\n10.071785\n-9.834783\n-2.066656\n-5.364473\n-2.606837\n-4.918215\n7.305334\n1.914428\n...\n0.213454\n0.111864\n1.014480\n-0.509348\n1.436807\n0.250034\n0.943651\n0.823731\n0.77\n0\n\n\n284803\n172787.0\n-0.732789\n-0.055080\n2.035030\n-0.738589\n0.868229\n1.058415\n0.024330\n0.294869\n0.584800\n...\n0.214205\n0.924384\n0.012463\n-1.016226\n-0.606624\n-0.395255\n0.068472\n-0.053527\n24.79\n0\n\n\n284804\n172788.0\n1.919565\n-0.301254\n-3.249640\n-0.557828\n2.630515\n3.031260\n-0.296827\n0.708417\n0.432454\n...\n0.232045\n0.578229\n-0.037501\n0.640134\n0.265745\n-0.087371\n0.004455\n-0.026561\n67.88\n0\n\n\n284805\n172788.0\n-0.240440\n0.530483\n0.702510\n0.689799\n-0.377961\n0.623708\n-0.686180\n0.679145\n0.392087\n...\n0.265245\n0.800049\n-0.163298\n0.123205\n-0.569159\n0.546668\n0.108821\n0.104533\n10.00\n0\n\n\n284806\n172792.0\n-0.533413\n-0.189733\n0.703337\n-0.506271\n-0.012546\n-0.649617\n1.577006\n-0.414650\n0.486180\n...\n0.261057\n0.643078\n0.376777\n0.008797\n-0.473649\n-0.818267\n-0.002415\n0.013649\n217.00\n0\n\n\n\n\n284807 rows × 31 columns\n\n\n\n\n# df.info()를 통한 null값 및 컬럼 확인\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n\n\n\n# Class데이터 : 일반거래(0)와 이상거래(1) 건수\ndf['Class'].value_counts()\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\n\n# Class데이터 : 일반거래(0)와 이상거래(1) 비중\ndf[df['Amount']==0]['Class'].value_counts(normalize=True)\n\nClass\n0    0.985205\n1    0.014795\nName: proportion, dtype: float64\n\n\n\n# Amount = 0인 데이터\ndf[df['Amount']==0]['Class'].value_counts()\n\nClass\n0    1798\n1      27\nName: count, dtype: int64\n\n\n\n# t-sne 구현 샘플 (분류 분포를 확인해보고자 했으나, 차원 및 데이터가 많아서인지 너무 오래걸려서 코드만 보존)\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assume we have a high-dimensional dataset X\nX = df[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n\n# Create a t-SNE model with 2 components (for 2D visualization)\ntsne = TSNE(n_components=2, random_state=0)\n\n# Fit and transform the data\nX_tsne = tsne.fit_transform(X)\n\n# Visualize the results\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.show()"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240715/index.html#데이터셋-구성",
    "href": "posts/meta-dl-creditcard-20240715/index.html#데이터셋-구성",
    "title": "[MStudy_과제개선1] 신용카드 이상거래 탐지 모델링",
    "section": "데이터셋 구성",
    "text": "데이터셋 구성\n\n불필요한 컬럼 제거 및 X, Y 분할\n\ndf_x = df.drop(['Time', 'Class'], axis=1).copy()\ndf_y = df['Class'].copy()\n\ndf_x.shape, df_y.shape\n\n((284807, 29), (284807,))\n\n\n\n\nTrain, Validation, Test 데이터 나누기(stratify옵션 비교, 학습에 미사용)\n\nLabel 비율을 맞추는 stratify옵션에 따른 Label값 수 확인\n단순히 Train, Validation, Test를 나누는 것과, kfold로 나누고 모델링에서 비교해볼 예정\n\n비율은 Train, Validation, Test 8:1:1 가깝게 설정\n\nScikit learn의 train_test_split 사용\nTest data에서 약간의 차이가 있었다 (미적용시 40, 적용시 49)\n\n\n# stratify 미적용\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, ty_train, y_test = train_test_split(df_x, df_y, test_size=0.1)\n\nprint(f\"{x_train.shape}, {x_test.shape}\")\nprint(f\"{y_train.shape}, {y_test.shape}\")\nprint()\nprint(f\"y_train {y_train.value_counts()}\")\nprint(f\"y_test {y_test.value_counts()}\")\n\n(256326, 29), (28481, 29)\n(256326,), (28481,)\n\ny_train Class\n0    255883\n1       443\nName: count, dtype: int64\ny_test Class\n0    28441\n1       40\nName: count, dtype: int64\n\n\n\n# stratify 적용\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, stratify=df_y)\n\nprint(f\"{x_train.shape}, {x_test.shape}\")\nprint(f\"{y_train.shape}, {y_test.shape}\")\nprint()\nprint(f\"y_train {y_train.value_counts()}\")\nprint(f\"y_test {y_test.value_counts()}\")\n\n(256326, 29), (28481, 29)\n(256326,), (28481,)\n\ny_train Class\n0    255883\n1       443\nName: count, dtype: int64\ny_test Class\n0    28432\n1       49\nName: count, dtype: int64\n\n\n\n\nTrain, Validation, Test 나누기(학습에 사용)\n\nTest set 먼저 분리하여, 이후 평가에서 공통적으로 사용\nTrain, Validation은 각각 다른 방법으로 나누어 사용\n\n\nTest set 나누기(공통)\n\n단순히 비율로 나눈 데이터셋의 변수명\n\nx_train_2, x_validation_2, x_test\ny_train_2, y_validation_2, y_test\n\n\n\n# Train, Test 나누기\nfrom sklearn.model_selection import train_test_split\n\n# stratify 적용\nx_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, stratify=df_y)\n\nprint(f\"{x_train.shape}, {x_test.shape}\")\nprint(f\"{y_train.shape}, {y_test.shape}\")\nprint()\nprint(f\"y_train {y_train.value_counts()}\")\nprint(f\"y_test {y_test.value_counts()}\")\n\n(256326, 29), (28481, 29)\n(256326,), (28481,)\n\ny_train Class\n0    255883\n1       443\nName: count, dtype: int64\ny_test Class\n0    28432\n1       49\nName: count, dtype: int64\n\n\n\n\nTrain, Validation 나누기(단순히 비율만 고려)\n\n단순히 비율로 나눈 데이터셋의 변수명\n\nx_train_2, x_validation_2, x_test\ny_train_2, y_validation_2, y_test\n\n\n\n# Train, Validation 나누기\nx_train_2, x_validation_2, y_train_2, y_validation_2 = train_test_split(x_train, y_train, test_size=1/9, stratify=y_train)\nprint(f\"{x_train_2.shape}, {x_validation_2.shape}\")\nprint(f\"{y_train_2.shape}, {y_validation_2.shape}\")\nprint()\nprint(f\"y_train {y_train_2.value_counts()}\")\nprint(f\"validation {y_validation_2.value_counts()}\")\n\n(227845, 29), (28481, 29)\n(227845,), (28481,)\n\ny_train Class\n0    227451\n1       394\nName: count, dtype: int64\nvalidation Class\n0    28432\n1       49\nName: count, dtype: int64\n\n\n\n\nTrain, Validation 나누기(StratifiedKFold적용)\n\n분류문제인 경우 StratifiedKFold 적용, 회귀인 경우에는 사용하지 않음\nStratifiedKFold 적용 데이터셋의 변수명\n\nkfold_dataset[idx][‘x_train’], kfold_dataset[idx][‘x_validation’], x_test\nkfold_dataset[idx][‘y_train’], kfold_dataset[idx][‘y_validation’], y_test\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# Train, Validation 나누기\nkfold_dataset = {}\n\ns_kfold = StratifiedKFold(n_splits=9)\nfor i, (train_index, validation_index) in enumerate(s_kfold.split(x_train, y_train)):\n    kfold_dataset[i] = {'x_train': x_train.iloc[train_index], # Scaler 적용시 .iloc삭제\n                        'y_train': y_train.iloc[train_index], # Scaler 적용시 .iloc삭제\n                        'x_validation' : x_train.iloc[validation_index], # Scaler 적용시 .iloc삭제\n                        'y_validation' : y_train.iloc[validation_index]  # Scaler 적용시 .iloc삭제\n                        }\n    \nfor idx in kfold_dataset.keys():\n    print(f\"Train set{idx} : {kfold_dataset[idx]['x_train'].shape}, {kfold_dataset[idx]['y_train'].shape}\")\n    print(f\"Validation set{idx} : {kfold_dataset[idx]['x_validation'].shape}, {kfold_dataset[idx]['y_validation'].shape}\")\n    print()\n\nTrain set0 : (227845, 29), (227845,)\nValidation set0 : (28481, 29), (28481,)\n\nTrain set1 : (227845, 29), (227845,)\nValidation set1 : (28481, 29), (28481,)\n\nTrain set2 : (227845, 29), (227845,)\nValidation set2 : (28481, 29), (28481,)\n\nTrain set3 : (227845, 29), (227845,)\nValidation set3 : (28481, 29), (28481,)\n\nTrain set4 : (227845, 29), (227845,)\nValidation set4 : (28481, 29), (28481,)\n\nTrain set5 : (227845, 29), (227845,)\nValidation set5 : (28481, 29), (28481,)\n\nTrain set6 : (227846, 29), (227846,)\nValidation set6 : (28480, 29), (28480,)\n\nTrain set7 : (227846, 29), (227846,)\nValidation set7 : (28480, 29), (28480,)\n\nTrain set8 : (227846, 29), (227846,)\nValidation set8 : (28480, 29), (28480,)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240715/index.html#모델-구성",
    "href": "posts/meta-dl-creditcard-20240715/index.html#모델-구성",
    "title": "[MStudy_과제개선1] 신용카드 이상거래 탐지 모델링",
    "section": "모델 구성",
    "text": "모델 구성\n\n기존에 f1 score를 위해 사용하려던 tensorflow_addons는 개발종료되어 다른 패키지로 대체\n단순 f1 score가 아닌 weighted f1 score사용\n\n\n기초 딥러닝 모델(비율로 단순히 나눈 데이터셋 vs StratifiedKFold 비교)\n\n딥러닝 모델(StratifiedKFold 미적용)\n\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Precision, Recall, F1Score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 모델링\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Input((29,1)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid') # 또는 2, Softmax 사용가능\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[F1Score(average=\"weighted\"), Precision(), Recall()])\n\n# 학습 및 평가\n## 모델 학습\nmodel_history = model.fit(x_train_2, y_train_2, \n                    epochs=10, verbose=0,\n                    validation_data=(x_validation_2, y_validation_2))\n\n## 모델 평가\nprint('* 모델평가')\nresult_train = model.evaluate(x_train_2, y_train_2, verbose=2)\nresult_validate = model.evaluate(x_test, y_test, verbose=2)\n\n* 모델평가\n7121/7121 - 4s - 562us/step - f1_score: 0.0035 - loss: 0.0041 - precision_6: 0.9545 - recall_6: 0.7462\n891/891 - 1s - 568us/step - f1_score: 0.0035 - loss: 0.0037 - precision_6: 0.9211 - recall_6: 0.7143\n\n\n\n# 시각화 (Train, Validation score)\nfig, axs = plt.subplots(nrows=4, ncols=1, figsize=(10,20))\n\nfor idx, key in enumerate(model_history.history.keys()):\n    if idx == 4:\n        break\n    axs[idx].plot(model_history.history[key], label=f\"train_{key}\")\n    axs[idx].plot(model_history.history[f\"val_{key}\"], label=f\"val_{key}\")\n    axs[idx].legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n딥러닝 모델(StratifiedKFold 적용)\n\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Precision, Recall, F1Score\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 모델링\nmodel_2 = tf.keras.models.Sequential([\n    tf.keras.layers.Input((29,1)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid') # 또는 2, Softmax 사용가능\n])\n\nmodel_2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[F1Score(average=\"weighted\"), Precision(), Recall()])\n\n# 학습 및 평가\nkfold_result_train = {}\nkfold_result_validate = {}\nkfold_model_history = {}\n\nfor idx in kfold_dataset.keys():\n    ## 모델 학습\n    kfold_model_history[idx] = model_2.fit(kfold_dataset[idx]['x_train'],  kfold_dataset[idx]['y_train'], \n                        epochs=10, verbose=0,\n                        validation_data=(kfold_dataset[idx]['x_validation'], kfold_dataset[idx]['y_validation']))\n\n    ## 모델 평가\n    print(f'* 모델평가_{idx}')\n    kfold_result_train[idx] = model_2.evaluate(kfold_dataset[idx]['x_train'],kfold_dataset[idx]['y_train'], verbose=2)\n    kfold_result_validate[idx] = model_2.evaluate(x_test, y_test, verbose=2)\n\n* 모델평가_0\n7121/7121 - 4s - 574us/step - f1_score: 0.0037 - loss: 0.0081 - precision_7: 0.8941 - recall_7: 0.7284\n891/891 - 1s - 583us/step - f1_score: 0.0037 - loss: 0.0059 - precision_7: 0.8421 - recall_7: 0.6531\n* 모델평가_1\n7121/7121 - 4s - 570us/step - f1_score: 0.0039 - loss: 0.0065 - precision_7: 0.9078 - recall_7: 0.7995\n891/891 - 1s - 590us/step - f1_score: 0.0038 - loss: 0.0053 - precision_7: 0.9000 - recall_7: 0.7347\n* 모델평가_2\n7121/7121 - 4s - 579us/step - f1_score: 0.0042 - loss: 0.0053 - precision_7: 0.9050 - recall_7: 0.8223\n891/891 - 1s - 603us/step - f1_score: 0.0042 - loss: 0.0047 - precision_7: 0.9024 - recall_7: 0.7551\n* 모델평가_3\n7121/7121 - 4s - 574us/step - f1_score: 0.0046 - loss: 0.0048 - precision_7: 0.9440 - recall_7: 0.8122\n891/891 - 1s - 595us/step - f1_score: 0.0045 - loss: 0.0054 - precision_7: 0.8974 - recall_7: 0.7143\n* 모델평가_4\n7121/7121 - 4s - 574us/step - f1_score: 0.0048 - loss: 0.0066 - precision_7: 0.9493 - recall_7: 0.8092\n891/891 - 1s - 601us/step - f1_score: 0.0048 - loss: 0.0055 - precision_7: 0.9000 - recall_7: 0.7347\n* 모델평가_5\n7121/7121 - 4s - 565us/step - f1_score: 0.0042 - loss: 0.0027 - precision_7: 0.9149 - recall_7: 0.8753\n891/891 - 1s - 580us/step - f1_score: 0.0042 - loss: 0.0027 - precision_7: 0.8636 - recall_7: 0.7755\n* 모델평가_6\n7121/7121 - 4s - 571us/step - f1_score: 0.0054 - loss: 0.0050 - precision_7: 0.9016 - recall_7: 0.8604\n891/891 - 1s - 598us/step - f1_score: 0.0054 - loss: 0.0047 - precision_7: 0.8444 - recall_7: 0.7755\n* 모델평가_7\n7121/7121 - 4s - 567us/step - f1_score: 0.0061 - loss: 0.0047 - precision_7: 0.9387 - recall_7: 0.8553\n891/891 - 1s - 599us/step - f1_score: 0.0060 - loss: 0.0047 - precision_7: 0.9231 - recall_7: 0.7347\n* 모델평가_8\n7121/7121 - 4s - 573us/step - f1_score: 0.0071 - loss: 0.0024 - precision_7: 0.9479 - recall_7: 0.8782\n891/891 - 1s - 598us/step - f1_score: 0.0072 - loss: 0.0033 - precision_7: 0.9000 - recall_7: 0.7347\n\n\n\n# 시각화\nfig, axs = plt.subplots(nrows=4, ncols=1, figsize=(10,20))\n\nfor idx, key in enumerate(kfold_model_history[8].history.keys()):\n    if idx == 4:\n        break\n    axs[idx].plot(kfold_model_history[8].history[key], label=f\"train_{key}\")\n    axs[idx].plot(kfold_model_history[8].history[f\"val_{key}\"], label=f\"val_{key}\")\n    axs[idx].legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStratifiedKFold 적용유무에 따른 수치 비교\n\n적용 후 Precision을 제외한 Test지표 향상됨\n\n모델평가(적용X)\n\n7121/7121 - 4s - 562us/step - f1_score: 0.0035 - loss: 0.0041 - precision_6: 0.9545 - recall_6: 0.7462\n891/891 - 1s - 568us/step - f1_score: 0.0035 - loss: 0.0037 - precision_6: 0.9211 - recall_6: 0.7143\n\n모델평가(적용O)\n\n7121/7121 - 4s - 573us/step - f1_score: 0.0071 - loss: 0.0024 - precision_7: 0.9479 - recall_7: 0.8782\n891/891 - 1s - 598us/step - f1_score: 0.0072 - loss: 0.0033 - precision_7: 0.9000 - recall_7: 0.7347\n\n\n적용 후 Precision을 제외한 Train, Validation지표 향상\n\n\nimport koreanize_matplotlib\n\n# 시각화\nfig, axs = plt.subplots(nrows=4, ncols=2, figsize=(16,30))\n\n# 그래프간 비교를 위해, y축 고정을 위한 최대/최소값 계산\ngraph_min = {'f1_score':0.003,\n'loss':0.002,\n'precision':0.3,\n'recall':0.5}\ngraph_max = {'f1_score':0.009,\n'loss':0.04,\n'precision':1,\n'recall':0.9}\n\n\n# 그래프1\nfor idx, key in enumerate(model_history.history.keys()):\n    if idx == 4:\n        break\n    axs[idx, 0].plot(model_history.history[key], label=f\"train_{key}\")\n    axs[idx, 0].plot(model_history.history[f\"val_{key}\"], label=f\"val_{key}\")\n    axs[idx, 0].set_title(f\"fold적용전_{key}\")\n    axs[idx, 0].set_ylim([graph_min[key.replace('_6','').replace('_7','')], graph_max[key.replace('_6','').replace('_7','')] ])\n    axs[idx, 0].legend()\n\n# 그래프2\nfor idx, key in enumerate(kfold_model_history[8].history.keys()):\n    if idx == 4:\n        break\n\n    axs[idx, 1].plot(kfold_model_history[8].history[key], label=f\"train_{key}\")\n    axs[idx, 1].plot(kfold_model_history[8].history[f\"val_{key}\"], label=f\"val_{key}\")\n    axs[idx, 1].set_title(f\"fold적용후_{key}\")\n    axs[idx, 1].set_ylim([graph_min[key.replace('_6','').replace('_7','')], graph_max[key.replace('_6','').replace('_7','')]])\n    axs[idx, 1].legend()\n\nplt.show()"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 5주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#metric",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#metric",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "Metric",
    "text": "Metric\n\nMetric : 모델을 평가하기 위해 사용하는 수치 지표\n\n\n분류 문제 Metric\n\nAccuracy : 전체 예측 중 옳게 예측한 비율\n\n해석이 용이함. 불균형 데이터에서 과대평가 문제가 있음\n\\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\n\nF1-score : Precision과 Recall의 조화평균\n\nTP중심(TP Oriented)이고 직관적이지 못한 단점\n불균형 데이터의 Accuracy의 과대평가를 피할 수 있는 장점\n\\(F1 \\text{ Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right)\\)\n\\(\\text{Precision} = \\frac{TP}{TP + FP}\\)\n\\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\n\nAUC-ROC : ROC커브의 하단 면적\n\n전반적인 분류 성능평가에 용이 (상대적으로)불균형데이터에서도 유용\nROC커브에 대한 설명이 필요하여 직관적이지 못함\n\nLog loss : Loss의 로그값으로, 0 이상이며 낮을수록 높은 신뢰도\n\n다른 Metric과 달리 예측 확률의 신뢰도에 대한 평가\n\n예측이 얼마나 적중했는지를 보는 다른 Metric vs 예측확률이 얼마나 믿을만한지 보는 Log loss\n분류 결과만으로 평가하는 다른 Metric vs 확률까지 반영하는 Log loss\n틀린 확률이 극단적인 경우 큰 패널티 부여\n\n예를 들어, 모델이 확인을 가지고 1일 확률 99%로 예측했으나 0인 경우 Log loss값 커짐\n이러한 특성으로 모델의 잘못된 예측을 방지하는데 유용\n\n\n다른 metric과 달리 다중분류에서도 사용 가능\n\nclass불균형 영향을 받으므로 가중치 조절 필요\n\n\\(\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\\)\n\n\n\n\n다중 분류 문제 Metric\n\nOverall Accuracy : 전반적인 정확도(전체 예측 중 올바른 예측 비율)\n\n불균형데이터에서는 부적합\n3*3 Matrix기준으로 대각선이 정답이며, 이 대각선이 하단의 \\(\\text{Number of Correct Predictions}\\)에 해당\n\\(\\text{Overall Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Number of Total Predictions}}\\)\n\nMacro Precision/Recall/F1-Score : 각 class(y)별로 Precision/Recall/F1-Score를 계산한 후 평균\n\n숫자가 많은 class를 잘맞추는게 중요한게 아니라, 중요도를 균등하게 반영, 전반적인 예측을 중요시\n위와 같은 이유로 불균형에 취약함\n\\(\\text{Macro Precision, Recall, F1-Score} = -\\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\text{Precision, Recall, F1-Score} \\right]_i\\)\n\nMicro Precision/Recall/F1-Score : 각 class(y)에 대해 맞게 예측한 수(TP)/ 모든 class(y)에 대해 예측한 수\n\n(단순평균이 아니므로)상대적으로 불균형에 강함, 거시적으로 보기는 어려움\n\\(\\text{Micro Precision} = \\frac{\\sum_{i=1}^{M} \\left[ TP \\right]_i}{\\sum_{i=1}^{M} \\left[ TP + FP \\right]_i}\\)\n\n다중분류문제의 경우, 보통 Macro와 Micro를 함께 병기하여 사용\n\n불균형한 경우, 한가지만 보면 문제될 수 있음\n\n\n\n\n회귀 문제 Metric\n\nRMSE(평균제곱오차), MSE(평균제곱근오차)\n\n제곱을 취하므로 상대적으로 이상치에 민감 (이상치에 따라 크게 오차값이 튄다)\nRMSE로 단위를 통일하여 사용하는 경우 많음 *(단위가 통일되어 scale왜곡이 되지 않음)\n\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\\)\n\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\)\n\nMAE(평균절대오차), MAPE(평균절대비오차, Percentage error)\n\n절대값으로 계산하므로 SE(제곱오차)보다 이상치에 덜 민감 (수학적으로는 깔끔하지 않음)\nMAPE는 백분율로 나타내기에 (예측하고자 하는 값이 일반적 범우에 있는 경우)해석이 용이함\n\n분류문제 대비, 회귀문제 Metric은 설명이 어려움(모델의 scale이나 분산등을 설명해야 함)\ny로 나누기 때문에, y가 0에 가까워질수록 값이 튀는 문제(MAPE는 이 문제로 사용하기 어려움)\n\n\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\\)\n\\(\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\)\n\n\n\n\nMetric 선정시 고려사항\n\nMetric 선정시 고려사항\n\n문제 유형\n데이터의 특성(불균형도, 이상치의 비중, 스케일 차이)\n모델의 목표, 비즈니스 맥락\n비교 가능성\n결과 해석 및 소통 용이성\n\n\n\n\nMetric 선정관련 참고사항\n\n과제의 성격에 맞는 Metric의 설정이 중요함\n\n물동량 예측문제라면, Under estimate가 더 치명적임\n\nOver estimate : 차를 더 부르고 끝\nUnder estimate : 급하게 차를 불러야 하므로 비용이 더 큼\n\n위와 같은 상황에서 Pinball loss라는 것을 사용하기도 함\n\nPinball loss : 과소예측과 과대예측에 대해 각각 다른 가중치를 부여\n분위수(τ) 0.5(균형)을 기준으로, τ&lt;0.5는 과대예측에 큰 패널티, τ&gt;0.5는 과소예측에 큰 패널티\n재고관리, 금융리스크모델링 등에서 활용\n\n\n학습하기 위한 Main metric과, 소통을 위한 Sub metric을 함께 설정"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#xai",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#xai",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "XAI",
    "text": "XAI\n\nXAI와 금융데이터\n\nXAI(Explainable AI) : 기존의 Black box였던 모델 부분을 설명하는 방법론들을 통칭\n(예시로 이해하는) XAI가 금융 데이터 분석에서 중요한 이유(예시)\n\n편향탐지 : AI기반 대출 승인 모델을 돌렸는데 특정 인종(흑인 등)이 거절되는 경우를 탐지\n\nFeature importance는 영향도를 보여줄 뿐, 어떤 방향으로 작용할지 확인이 어려움\nXAI로 미리 확인하여, 흑인이면 대출을 안해준다는 사실을 알았다면 문제를 방지할 수 있었음\nXAI로 어떤 변수가 얼마나 어떻게 중요한지 설명/적용하는 것과, 단순히 변수가 중요하다는 것은 다름\n\n비즈니스 의사결정 지원 : 변수가 어떻게 작용하는지 이해하여 사기 탐지 모델을 적용\n\n모델링하여 단순히 Batch를 태우기도 하지만, 분석과제로 인사이트를 얻고 커뮤니케이션을 할 수 있음\n\n데이터 처리 신뢰성 확보 및 손실 최소화\n\n모델 오류가 발생하기 전에, XAI를 통해 도메인 지식에서 어긋나는 변수나 과대대표되는 변수를 확인\n단위가 큰 금융 데이터에게는 특히 중요한 작업\n\n\n\n\n\nSHAP (XAI with SHAP)\n\nSHAP : 변수가 예측에 기여하는 정도를 계산 (Shapley Value에 기반하여 설명)\n\n계산방식\n\n모든 가능한 Feature 조합을 생성한 뒤\n각 조합에서 특정 Feature가 추가되었을 때 예측값의 변화를 계산\n기여도 계산 후 평균을 내는 방식으로 최종 산출\n\n\nSHAP의 해석\n\n해당 변수가 추가되었을 때 예측 값 변화의 평균이므로, 회귀계수와는 다름\n\n변수만큼 증가하는 회귀계수 vs 변수의 단위만큼 증가하는 Shapley value\n\nShapley value는 단위의 영향을 받는 값\n\n파산확률 추론 분류모델 예시로 이해하기\n\n변수 A의 Shapley value가 0.1 → 변수 A의 추가로 파산 확률이 10%p 증가\n변수 B의 Shapley value가 -0.2 → 변수 B의 추가로 파산 확률이 20%p 감소\n\n대출한도 추론 회귀모델 예시로 이해하기\n\n변수 C의 Shapley value가 2백만 → 변수 C가 고객의 대출 한도를 2백만원만큼 감소시키는데 기여함\n\n\n\n\n\n\nSHAP의 해석분해\n\nSHAP의 해석분해 (해석분해 : 모델의 예측 값을 구성 요소(특성)별로 분리하여 설명)\n\n$예측값 = SHAP기준값 + _{i=1}^{n} (_i) $\n\n예측값 = SHAP기준값 + SHAP값 전체의 합\nSHAP 기준값 : 모델이 아무런 정보가 없을 때의 예측값\n\n회귀문제 : 모든 데이터의 평균 / 분류문제 : 전체 데이터 확률 평균의 로그 오즈(에 근사한 값)\n\n기준값에서 시작해, 각 변수의 Shapley value를 더해 예측값을 나타냄\n\nSHAP는 각각의 예측(Row)에 종속적 (100명을 예측한다면 변수별로 100개의 SHAP)\n절대값의 평균 = SHAP을 통해 구한 Feature importance\n\n여러가지 방법으로 Global / Local하게 해석 가능\n\n요약\n\nSHAP은 row단위의 값이며, 평균냈을 때 feature importance, 변수 유무에 따른 값의 변화를 나타냄\n예를 들어, MxN개의 관측값이 있는 표라면, MxN개의 Shapley value가 나옴\n\n컬럼단위로 절대값의 평균을 내면, 해당 컬럼의 feature importance\nROW단위로 더해 SHAP기준값까지 더해주면 예측값이 됨\n\n\n\n추가사항\n\nSHAP은 트리모델과 달리 feature importance의 일관성이 있음\n\n트리모델은 feature importance가 변수의 추가/삭제에 따라 크게 바뀜\n\n트리모델이 feature importance구하는 방법이, 변수가 분기에서 얼마나 사용되고, 많은 데이터를 나눌 수 있는지이기 때문\n\nSHAP은 변수의 예측력 차이이므로 크게 달라지지 않음(일관성있음)\n\nSHAP는 각 변수의 설명력을 나타냄\n\n\n\n\nSHAP의 Global 해석 (Column)\n\nSHAP Summary Plot\n\nTop변수들(SHAP 절대값의 합 Top)의 SHAP value를 시각화한 것\nSHAP Summary Plot샘플을 통한 이해 \n\n색이 푸를수록 값이 낮고 / 붉을수록 값이 높음\nEXT_SOURCE_3 변수 : 모델에 추가될 때, 값이 낮을수록 모델OUTPUT증가(X축)\nAMT_CREDIT 변수 : 모델에 추가될 때, 값이 높을수록 모델OUTPUT증가(X축)\nEXT_SOURCE_3 변수처럼 색이 일관될수록, 변수의 영향도 일관된 방향으로 작동\n\n\nSHAP Feature Importance Plot\n\nSHAP 절대값의 평균을 시각화한 것(Feature Importance를 SHAP으로 나타낸 것)\n\nSHAP Summary Plot에서 일관되지 못한 변수였다면, 상위에 나타나지 않음\n\n절대적인 크기를 구하기 위해 방향성을 삭제했다는 한계\nSHAP Feature Importance Plot샘플 \n\nSHAP Dependence Plot\n\nRow별 SHAP value와 변수값을 시각화한 것\nSHAP Dependence Plot샘플을 통한 이해 \n\nAMD_GOODS_PRICE변수는, 값이 증가할수록 예측(y)에 양의 영향을 끼침\n\n5만~15만쯤에 위치한 ’임계값’부터 방향이 바뀜\n만약 이 변수에 대해 조치(행동)한다면 이 값(threshold, 임계값)을 참고해야 함\n\n\nSHAP Dependence Plot샘플을 통한 이해2 \n\nAMT_CREDIT변수는, 값이 작을때는 음의 영향\n\n5만~25만의 임계점부터 양의 방향으로 바뀜\n\n\n\n\n\n\nSHAP의 Local 해석 (Row)\n\nSHAP Force Plot\n\n특정 데이터 하나(row)에서, 각 변수가 끼치는 영향을 1차원 평면에 정렬하여 시각화한 것\n`SHAP Force Plot샘플을 통한 이해 \n\nBase value를 기준으로 파란색은 부정적 / 빨간색은 긍정적 영향을 끼친 변수\n전체적으로 파란색이 더 많아, Base value에 비해 예측(y)값이 줄어든 것 (로그오즈 기준)\n\n\n\n\n\nFeature Selection with SHAP\n\nFeature Selection with SHAP\n\n각 feature의 절대값의 평균을 기반으로 계산\n\n값이 클수록 중요한 기여\n\n분할 횟수나 분산 감소를 기준으로 계산되는 트리모델의 Feature importance과 다른 점\n\nSHAP Feature importance는 실제 y와 연관성이 명확히 보이고 설명하기 쉬움\n\nSHAP값의 합 = 모델의 예측값 이기 때문.\n\n\n뚜렷한 방향성을 갖거나, 변수유무에 따른 변동폭이 클수록 Feature importance 증가\n\n\n\n\n참고사항\n\nSHAP가 가장 많이 사용되고 직관적이며, 다른 XAI로는 LIME 등도 있음\nSHAP은 변수의 관계(a변수 증가시 어느정도 y증가)를 직관적으로 설명 가능\n\n분할 횟수나 분산 감소로 계산되는 트리모델의 Feature importance로는 단위통일 등 되어있지 않고 해석이 어려움\n\n분류문제의 SHAP는 로그오즈 형태로 나오지만, 회귀문제의 SHAP는 숫자로 나옴\nSHAP Dependence Plot을 통해, 현재까지 몰랐던 모델의 input/output 사이의 작용을 알게됨\n\n모델이 어떻게 반영해왔는지에 대한 경향을 알 수 있음\nXAI시각화를 통해, 모델링으로 해결할만한 과제가 아님을 인지하게되어 추가로 DA적인 분석 등 조치가능"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#week4-보완-xgboost의-rf-w-gpu",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#week4-보완-xgboost의-rf-w-gpu",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "Week4 보완 : XGBoost의 RF w/ GPU",
    "text": "Week4 보완 : XGBoost의 RF w/ GPU\n\nXGBoost와 SKlearn은 목적함수에 차이가 있어, 독립적 트리 기반의 모델인 RF에서 성능차이가 있을 수 있음\n\n목적함수의 차이\n\nXGBoost : (목적합수의 정확한 값 대신) 2차 근사 (1차gradient미분, 2차hessian미분). 근사는 계산량을 줄이는데 효과적\nSKlearn : 목적함수의 정확한 실제값 (정확한 손실함수 값)\n\nSKlearn은 분할기준을 정확히 최적화하지만, XGBoost는 근사 최적화여서 개별 트리 성능이 저하될 수 있음\n\nXGBoost는 중복샘플링을 수행하지 않음(subsample시 replacement를 수행하지 않음, 각 트리가 고유한 서브셋을 사용함)\n\n중복샘플링은 각 트리가 원본데이터와 다른 구조/분포를 갖게 해 각 트리의 독립성, 다양성이 높아져 일반화 성능이 높아짐(Bagging에 적합)\nBoosting은 순차적 학습이므로, 중복허용시 특정 샘플이 과도하게 중복되거나 포함되지 않아 안정성이 낮아짐\n위의 이유로 XGBoost와 SKlearn은 다르게 학습되며, XGBoost의 RF는 XGB알고리즘에 따라 성능상의 Penalty가 있음\n\n\n\n더미데이터를 활용한 비교 실습(XGBoost vs Sklearn)\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Dummy 분류데이터 생성\n## Generate a large synthetic dataset\nX, y = make_classification(n_samples=100000, n_features=100, n_informative=25,\n                           n_redundant=10, n_classes=2, random_state=42)\n\n## Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nprint(X.shape)\npd.DataFrame(y_test).value_counts()\n\n(100000, 100)\n\n\n0\n0    10039\n1     9961\nName: count, dtype: int64\n\n\n\n# 공통 하이퍼파라미터 설정 (설정 후 Sklearn과 XGBoost에서 사용)\nparams = {\n    # \"n_estimators\": 100,      # 트리 개수\n    # \"max_depth\": 10,          # 최대 깊이\n    \"random_state\": 42,         # 재현성을 위한 random state\n    \"n_jobs\": -1                # 사용하는 Thread수, -1은 최대한 사용\n}\n\n\n# Train sklearn Random Forest\nsklearn_rf = RandomForestClassifier(**params)\n\n## fit\nsklearn_fit_start = time.time()\nsklearn_rf.fit(X_train, y_train)\nsklearn_fit_time = time.time() - sklearn_fit_start\n\n## predict\nsklearn_predict_start = time.time()\nsklearn_rf_preds = sklearn_rf.predict(X_test)\nsklearn_predict_time = time.time() - sklearn_predict_start\n\nsklearn_rf_accuracy = accuracy_score(y_test, sklearn_rf_preds)\n\n\n# Train XGBoost Random Forest\nxgb_rf = xgb.XGBRFClassifier(\n    # n_estimators=params[\"n_estimators\"],\n    # max_depth=params[\"max_depth\"],\n    random_state=params[\"random_state\"],\n    verbosity=0,\n    n_jobs=params[\"n_jobs\"],\n    tree_method='gpu_hist',      # GPU 사용\n    subsample=1.0,               # 전체 샘플 사용\n    colsample_bynode=1.0         # 전체 피처 사용\n)\n\n## fit\nxgb_fit_start = time.time()\nxgb_rf.fit(X_train, y_train)\nxgb_fit_time = time.time() - xgb_fit_start\n\n## predict\nxgb_predict_start = time.time()\nxgb_rf_preds = xgb_rf.predict(X_test)\nxgb_predict_time = time.time() - xgb_predict_start\n\nxgb_rf_accuracy = accuracy_score(y_test, xgb_rf_preds)\n\n\n# Summarize results\nprint(f'Sklearn Random Forest Accuracy: {sklearn_rf_accuracy:.4f}')\nprint(f'Sklearn Random Forest Fit Time: {sklearn_fit_time:.4f} seconds')\nprint(f'Sklearn Random Forest Predict Time: {sklearn_predict_time:.4f} seconds')\nprint()\n\nprint(f'XGBoost Random Forest Accuracy: {xgb_rf_accuracy:.4f}')\nprint(f'XGBoost Random Forest Fit Time: {xgb_fit_time:.4f} seconds')\nprint(f'XGBoost Random Forest Predict Time: {xgb_predict_time:.4f} seconds')\n\nSklearn Random Forest Accuracy: 0.9443\nSklearn Random Forest Fit Time: 16.6355 seconds\nSklearn Random Forest Predict Time: 0.0863 seconds\n\nXGBoost Random Forest Accuracy: 0.7635\nXGBoost Random Forest Fit Time: 1.3666 seconds\nXGBoost Random Forest Predict Time: 0.0480 seconds\n\n\n\n\n실습데이터를 활용한 비교 실습(XGBoost vs Sklearn)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\n# data split\n\n## 전처리해둔 데이터 로딩\ndata = pd.read_csv(\"data_preprocessed.csv\")\n\n## 데이터셋 나누기\ntrain , test = train_test_split(data, test_size = 0.1, random_state = 42, stratify = data['TARGET'])\n\nX_train = train.drop(['TARGET'], axis = 1)\ny_train = train['TARGET']\nX_test = test.drop(['TARGET'], axis = 1)\ny_test = test['TARGET']\n\n\n# Train sklearn Random Forest\nsklearn_rf = RandomForestClassifier(**params)\n\n## fit\nsklearn_fit_start = time.time()\nsklearn_rf.fit(X_train, y_train)\nsklearn_fit_time = time.time() - sklearn_fit_start\n\n## predict\nsklearn_predict_start = time.time()\nsklearn_rf_preds = sklearn_rf.predict(X_test)\nsklearn_predict_time = time.time() - sklearn_predict_start\n\nsklearn_rf_accuracy = accuracy_score(y_test, sklearn_rf_preds)\n\n\n# Train XGBoost Random Forest\nxgb_rf = xgb.XGBRFClassifier(\n    # n_estimators=params[\"n_estimators\"],\n    # max_depth=params[\"max_depth\"],\n    random_state=params[\"random_state\"],\n    verbosity=0,\n    n_jobs=params[\"n_jobs\"],\n    tree_method='gpu_hist',      # GPU 사용\n    subsample=1.0,               # 전체 샘플 사용\n    colsample_bynode=1.0         # 전체 피처 사용\n)\n\n## fit\nxgb_fit_start = time.time()\nxgb_rf.fit(X_train, y_train)\nxgb_fit_time = time.time() - xgb_fit_start\n\n## predict\nxgb_predict_start = time.time()\nxgb_rf_preds = xgb_rf.predict(X_test)\nxgb_predict_time = time.time() - xgb_predict_start\n\nxgb_rf_accuracy = accuracy_score(y_test, xgb_rf_preds)\n\n\n# Summarize results\nprint(f'Sklearn REAL DATA Random Forest Accuracy: {sklearn_rf_accuracy:.4f}')\nprint(f'Sklearn REAL DATA Random Forest Fit Time: {sklearn_fit_time:.4f} seconds')\nprint(f'Sklearn REAL DATA Random Forest Predict Time: {sklearn_predict_time:.4f} seconds')\nprint()\n\nprint(f'XGBoost REAL DATA Random Forest Accuracy: {xgb_rf_accuracy:.4f}')\nprint(f'XGBoost REAL DATA Random Forest Fit Time: {xgb_fit_time:.4f} seconds')\nprint(f'XGBoost REAL DATA Random Forest Predict Time: {xgb_predict_time:.4f} seconds')\n\nSklearn REAL DATA Random Forest Accuracy: 0.9193\nSklearn REAL DATA Random Forest Fit Time: 24.7601 seconds\nSklearn REAL DATA Random Forest Predict Time: 0.2899 seconds\n\nXGBoost REAL DATA Random Forest Accuracy: 0.9176\nXGBoost REAL DATA Random Forest Fit Time: 2.6202 seconds\nXGBoost REAL DATA Random Forest Predict Time: 0.0680 seconds"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#metrics",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#metrics",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "Metrics",
    "text": "Metrics\n\n분류 Metrics\n\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# Generate example data\nnp.random.seed(42)  # For reproducibility\nn_samples = 1000  # Number of samples\n\n# True labels and predictions\ny_true = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])  # Balanced true labels\ny_pred = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])  # Predictions with different distribution\n\n# Calculate metrics\naccuracy = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\n# Display results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\nAccuracy: 0.5050\nF1 Score: 0.4660\n\n\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nimport numpy as np\n\n# Generate example data\nnp.random.seed(42)\nn_samples = 1000\n\n# True labels and predicted probabilities\ny_true = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])  # Balanced true labels\ny_pred_proba = np.random.uniform(0, 1, size=n_samples)  # Predicted probabilities\n\n# Calculate AUC-ROC\nauc_roc = roc_auc_score(y_true, y_pred_proba)\n\n# Calculate Log Loss\nlog_loss_value = log_loss(y_true, y_pred_proba)\n\n# Display results\nprint(f\"AUC-ROC: {auc_roc:.4f}\")\nprint(f\"Log Loss: {log_loss_value:.4f}\")\n\nAUC-ROC: 0.5060\nLog Loss: 0.9939\n\n\n\ny_pred_proba[:10]\n\narray([0.18513293, 0.54190095, 0.87294584, 0.73222489, 0.80656115,\n       0.65878337, 0.69227656, 0.84919565, 0.24966801, 0.48942496])\n\n\n\nfrom sklearn.metrics import roc_curve\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\nauc_roc = roc_auc_score(y_true, y_pred_proba)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC-ROC = {auc_roc:.4f}\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")  # Dashed diagonal\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n다중분류 Metrics (Multi Label)\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Example data: true labels and predicted labels\nnp.random.seed(42)\nn_samples = 1000\nn_classes = 4  # Number of classes\n\n# Generate true labels and predictions\ny_true = np.random.randint(0, n_classes, size=n_samples)  # True labels\ny_pred = np.random.randint(0, n_classes, size=n_samples)  # Predicted labels\n\n\n# Overall Accuracy\noverall_accuracy = accuracy_score(y_true, y_pred)\n\n# Macro Precision, Recall, F1-Score\nmacro_precision = precision_score(y_true, y_pred, average='macro')\nmacro_recall = recall_score(y_true, y_pred, average='macro')\nmacro_f1 = f1_score(y_true, y_pred, average='macro')\n\n# Micro Precision, Recall, F1-Score\nmicro_precision = precision_score(y_true, y_pred, average='micro')\nmicro_recall = recall_score(y_true, y_pred, average='micro')\nmicro_f1 = f1_score(y_true, y_pred, average='micro')\n\n# Display results\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")\nprint(f\"Macro Precision: {macro_precision:.4f}\")\nprint(f\"Macro Recall: {macro_recall:.4f}\")\nprint(f\"Macro F1-Score: {macro_f1:.4f}\")\nprint(f\"Micro Precision: {micro_precision:.4f}\")\nprint(f\"Micro Recall: {micro_recall:.4f}\")\nprint(f\"Micro F1-Score: {micro_f1:.4f}\")\n\nOverall Accuracy: 0.2400\nMacro Precision: 0.2445\nMacro Recall: 0.2402\nMacro F1-Score: 0.2405\nMicro Precision: 0.2400\nMicro Recall: 0.2400\nMicro F1-Score: 0.2400\n\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plot the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"Class {i}\" for i in range(n_classes)])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n회귀 Metrics\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\nimport numpy as np\n\n# Generate example data\nnp.random.seed(42)\nn_samples = 1000\n\n# True values (y) and predicted values (y_hat)\ny_true = np.random.uniform(10, 10000, size=n_samples)  # True values\ny_pred = y_true + np.random.normal(0, 10, size=n_samples)  # Predicted values with noise\n\n# Calculate MSE and RMSE\nmse = mean_squared_error(y_true, y_pred)\nrmse = root_mean_squared_error(y_true, y_pred)\n# rmse = np.sqrt(mse)\n\n\n# Calculate MAE and MAPE\nmae = mean_absolute_error(y_true, y_pred)\nmape = mean_absolute_percentage_error(y_true, y_pred)\n# mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n# Display results\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MAPE: {mape:.4f}%\")\n\nMSE: 98.6791\nRMSE: 9.9337\nMAE: 7.8999\nMAPE: 0.0046%\n\n\n\nMAPE값이 튀는 경우에 대한 샘플Case (y가 0에 가까울 때)\n\n이런 문제가 있을 때는 사용할 수 없고, 설명력을 갖지 못함을 유의\n예를 들어, 주기적으로 0이 발생하는 택배물동량의 경우도 사용할 수 없음\n\n\n\n# Example of small true value causing high MAPE\ny_true_example = np.array([0.01, 50, 100])  # Small true value included\ny_pred_example = np.array([0.05, 55, 90])  # Predicted values\n\n# Calculate MAPE\nmape_example = np.mean(np.abs((y_true_example - y_pred_example) / y_true_example)) * 100\n\nprint(f\"True values: {y_true_example}\")\nprint(f\"Predicted values: {y_pred_example}\")\nprint(f\"MAPE: {mape_example:.4f}%\")\n\nTrue values: [1.e-02 5.e+01 1.e+02]\nPredicted values: [5.0e-02 5.5e+01 9.0e+01]\nMAPE: 140.0000%"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#xai-with-shap",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#xai-with-shap",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "XAI With SHAP",
    "text": "XAI With SHAP\n\nSHAP 설치\n\n\n!pip install SHAP\n\n\n트리기반 모델로 SHAP 실습하기\n\n실습을 위한 모델 설정\n\n\nimport shap\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# 데이터 로딩 및 나누기\ndata = pd.read_csv('data_preprocessed.csv')\n\nX = data.drop(columns=[\"TARGET\"])\ny = data[\"TARGET\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a LightGBM classifier\nlgb_model = lgb.LGBMClassifier(\n    objective=\"binary\",\n    random_state=42,\n)\n\nlgb_model.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_test, y_test)],\n    eval_metric=\"logloss\",\n    callbacks=[lgb.early_stopping(stopping_rounds=10)],\n)\n\n# Evaluate the model\ny_pred = lgb_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018294 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 11686\n[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 125\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080794 -&gt; initscore=-2.431606\n[LightGBM] [Info] Start training from score -2.431606\nTraining until validation scores don't improve for 10 rounds\nDid not meet early stopping. Best iteration is:\n[100]   valid_0's binary_logloss: 0.24629\nAccuracy: 0.9198\n\n\n\nTreeExplainer 설정 및 SHAP value 확인\n\ntrain에 대해서도, test에 대해서도 SHAP value확인가능\n\n다만, 기본적으로 test데이터는 unseen이어야 하므로 train에 대해서 본다\n\n\n\n\n# Calculate SHAP values\nexplainer = shap.TreeExplainer(lgb_model)\n\n# 트레인 셋에서의 SHAP value\nshap_values_train = explainer.shap_values(X_train)\n\n# 테스트 셋에서의 SHAP value\nshap_values_test = explainer.shap_values(X_test)\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\explainers\\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\explainers\\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')\n\n\n\nSHAP value 확인하기\n\n\ndf_shap_train = pd.DataFrame(shap_values_train, columns = X_train.columns)\ndf_shap_test  = pd.DataFrame(shap_values_test, columns = X_test.columns)\n\nprint(f\"\"\"* 값의 shape 확인하기\n  X_train           : {X_train.shape}\n  shap_values_train : {shap_values_train.shape}\"\"\")\n\ndf_shap_train.head(5)\n\n* 값의 shape 확인하기\n  X_train           : (246008, 131)\n  shap_values_train : (246008, 131)\n\n\n\n\n\n\n\n\n\nSK_ID_CURR\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\nDAYS_REGISTRATION\n...\nFLAG_OWN_CAR_Y\nFLAG_OWN_REALTY_N\nFLAG_OWN_REALTY_Y\nHOUSETYPE_MODE_block of flats\nHOUSETYPE_MODE_specific housing\nHOUSETYPE_MODE_terraced house\nHOUSETYPE_MODE_nan\nEMERGENCYSTATE_MODE_No\nEMERGENCYSTATE_MODE_Yes\nEMERGENCYSTATE_MODE_nan\n\n\n\n\n0\n0.001434\n-0.000103\n-0.002647\n0.040973\n-0.134860\n-0.037693\n-0.008186\n-0.036237\n0.046080\n-0.005266\n...\n0.0\n-0.001680\n0.000372\n0.007599\n-0.000594\n-0.000023\n0.0\n0.0\n0.0\n0.000039\n\n\n1\n0.035705\n0.000411\n-0.002086\n-0.239073\n-0.055616\n0.194492\n-0.003785\n0.052061\n0.060291\n0.004833\n...\n0.0\n0.000848\n-0.000106\n-0.000023\n-0.000032\n-0.000005\n0.0\n0.0\n0.0\n-0.000012\n\n\n2\n0.003548\n-0.000015\n-0.001610\n0.109762\n-0.025149\n-0.002882\n0.113585\n0.014116\n-0.008174\n0.003287\n...\n0.0\n0.000834\n-0.000061\n0.001993\n-0.000078\n-0.000005\n0.0\n0.0\n0.0\n0.000370\n\n\n3\n0.000996\n-0.000067\n0.003211\n0.106862\n0.091010\n-0.258358\n0.018849\n-0.011067\n-0.080588\n-0.009850\n...\n0.0\n0.000839\n-0.000180\n0.001513\n-0.000029\n0.003533\n0.0\n0.0\n0.0\n-0.000012\n\n\n4\n0.001159\n0.000270\n-0.002707\n0.005237\n-0.021219\n-0.062577\n0.005898\n0.041537\n-0.065084\n-0.003805\n...\n0.0\n0.000812\n-0.000052\n0.001695\n-0.000027\n-0.000005\n0.0\n0.0\n0.0\n0.000019\n\n\n\n\n5 rows × 131 columns\n\n\n\n\nBase value(기준값), 특정 row의 SHAP value구하기\n\n\n# Get base value\nbase_value = explainer.expected_value  # For the positive class (1)\nprint(f\"Base value (expected value): {base_value:.4f}\")\n\n\n# Select a specific row and feature\nrow_index = 10  # Index of the row\nfeature_name = \"EXT_SOURCE_3\"  # Name of the specific feature\n\n## Get SHAP value for the selected feature and row\nspecific_shap_value = shap_values_train[row_index, X_train.columns.get_loc(feature_name)]\n\n## Print results\nprint(f\"SHAP value for row {row_index}, feature '{feature_name}': {specific_shap_value:.4f}\")\n\nBase value (expected value): -2.7856\nSHAP value for row 10, feature 'EXT_SOURCE_3': 1.5446\n\n\n\nSHAP value와 Base value의 합이 예측값과 근사함을 보여주는 예제\n\n\nimport numpy as np\nimport pandas as pd\nimport shap\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# 사용자 데이터 예제 (사용자 데이터를 여기에서 불러옵니다)\n# 예: 데이터프레임 `data`가 존재한다고 가정합니다\n# data = pd.read_csv(\"your_data.csv\")\n# X = data.drop(\"target\", axis=1)\n# y = data[\"target\"]\n\n# 예제 데이터 생성 (사용자 데이터를 여기에 적용하세요)\nnp.random.seed(42)\nX = pd.DataFrame({\n    \"feature1\": np.random.rand(100),\n    \"feature2\": np.random.rand(100),\n    \"feature3\": np.random.rand(100)\n})\ny = 3 * X[\"feature1\"] + 2 * X[\"feature2\"] + X[\"feature3\"] + np.random.randn(100) * 0.1\n\n# 데이터 분리\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 모델 학습\nmodel = lgb.LGBMRegressor(silent = True)\nmodel.fit(X_train, y_train)\n\n# SHAP value의 합과 base value를 더한 값 계산\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap_sum = np.sum(shap_values, axis=1) + explainer.expected_value\n\n\n# 예측값\npredictions = model.predict(X_test)\n# 각 변수별 SHAP 값, 예측값, SHAP 합, Expected Value를 포함한 데이터프레임 생성\ndf_shap = pd.DataFrame(shap_values, columns=X_test.columns)\n\ndf_shap[\"Expected Value\"] = explainer.expected_value\ndf_shap[\"SHAP Sum\"] = shap_sum\ndf_shap[\"Prediction\"] = predictions\n\n\n# feature1, feature2, feature3 : 변수 3종\n# Expected Value : Base Value\n# SHAP Sum : 위 4가지 값의 합\n# Prediction : 예측값\ndf_shap.head(5)\n\n\n\n\n\n\n\n\nfeature1\nfeature2\nfeature3\nExpected Value\nSHAP Sum\nPrediction\n\n\n\n\n0\n-1.116524\n0.727300\n-0.301556\n2.953377\n2.262596\n2.262596\n\n\n1\n1.175684\n-0.358968\n-0.228965\n2.953377\n3.541127\n3.541127\n\n\n2\n1.059330\n0.362629\n0.215773\n2.953377\n4.591108\n4.591108\n\n\n3\n0.430526\n-0.687566\n0.238544\n2.953377\n2.934881\n2.934881\n\n\n4\n-0.660002\n-0.460247\n-0.347192\n2.953377\n1.485935\n1.485935\n\n\n\n\n\n\n\n\n# 검증: 두 값의 차이가 거의 0인지 확인\ndifference = np.abs(df_shap[\"Prediction\"] - df_shap[\"SHAP Sum\"])\nprint(f\"Difference Mean: {difference.mean()}, Max Difference: {difference.max()}\")\n\nDifference Mean: 1.5931700403370996e-15, Max Difference: 4.440892098500626e-15\n\n\n\n\nSHAP시각화\n\n대표적으로 많이 사용되는 3가지 예시\n\nSummary Plot, Feature Importance Plot, Dependence Plot\nSHAP를 Global하게 해석(컬럼 기준)\n\n\n\nSHAP Global해석 : SHAP Summary Plot(Violin Plot)\n\n아래 그래프에서의 높이(너비) : 점의 수가 많고 적음을 알 수 있음\n\n\n# Visualize SHAP summary plot for the positive class\nshap.summary_plot(shap_values_train, X_train, feature_names=X.columns)\n\n\n\n\n\n\n\n\n\n\nSHAP Global해석 : SHAP Feature Importance Plot(Bar Plot)\n\n절대값이므로 방향성이 제거됨\n위의 Summary Plot보다는 청중을 이해시키기 쉬움\n\n\nshap.summary_plot(shap_values_train,\n                  X_train,\n                  plot_type='bar')\n\n\n\n\n\n\n\n\n\n\nSHAP Global해석 : SHAP Dependence Plot(Scatter Plot)\n\n임계점을 확인하여 변수에 조치 등을 취할 수 있음\n\n\n# AMT_GOODS_PRICE의 TAGRGET에 끼치는 영향\n## 해석 예시 : 하단 그래프는 5만(0.5)을 기점으로 음의 영향이 있다\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"AMT_GOODS_PRICE\", shap_values_train, X_train, interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\n# AMT_CREDIT이 TAGRGET에 끼치는 영향\n## 해석예시 : 5만(0.5)~25만(2.5)까지 양의 영향을 끼친다\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"AMT_CREDIT\", shap_values_train, X_train, interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\n\nSHAP Local해석 : SHAP Force Plot\n\n특정 row에서의 각 변수가 미치는 영향을 시각화\n\n\n!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n# Colab으로 실행시에만 위의 코드 실행\n\n\nshap.force_plot(explainer.expected_value, shap_values_train[0, :], X_train.iloc[0, :],     matplotlib=True)\n\n\n\n\n\n\n\n\n\n\n\nSHAP Feature Importance구하기 & Feature Selection\n\nFeature Selection : 사용하지 않을 변수를 제외해, 메모리와 학습시간 등을 절약\n\nFeature Selection은 트리기반의 부스팅/배깅모델에서는 성능이 좋아지는 경우가 많지 않음\n\n떨어지는 경우가 많으며, 데이터 많이 넣는 쪽이 오히려 성능이 좋기도 함 (Trade-off)\n그러나, 메모리 등 리소스적인 한계로 활용하게 될 가능성이 큼(+학습시간 줄이기 등)\n\n\n\n\n# Train데이터의 Feature Importance(절대값의 평균)\nshap_fi_train = df_shap_train.apply(lambda x : np.mean(np.abs(x)), axis = 0).sort_values()[::-1]\n\n# Feature Importance가 0인 값을 제외\nselected_features = shap_fi_train[shap_fi_train &gt; 0].index.tolist()\n\n# Feature Importance가 0이상인 값\nprint(f\"* 전체 feature의 수 : {len(shap_fi_train)}\")\nprint(f\"* 추려진 feature의 수 : {len(selected_features)}\")\nprint(\"* 추려진 feature 목록\\n\", selected_features)\n\n* 전체 feature의 수 : 131\n* 추려진 feature의 수 : 109\n* 추려진 feature 목록\n ['EXT_SOURCE_3', 'EXT_SOURCE_2', 'AMT_GOODS_PRICE', 'EXT_SOURCE_1', 'AMT_CREDIT', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE_Encoded', 'AMT_ANNUITY', 'DAYS_BIRTH', 'CODE_GENDER_F', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_OWN_CAR_N', 'CODE_GENDER_M', 'NAME_CONTRACT_TYPE_Cash loans', 'NAME_FAMILY_STATUS', 'FLAG_DOCUMENT_3', 'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE', 'REGION_RATING_CLIENT_W_CITY', 'DEF_30_CNT_SOCIAL_CIRCLE', 'NAME_INCOME_TYPE', 'FLAG_WORK_PHONE', 'AMT_REQ_CREDIT_BUREAU_QRT', 'OCCUPATION_TYPE', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'REGION_POPULATION_RELATIVE', 'DAYS_REGISTRATION', 'REG_CITY_NOT_LIVE_CITY', 'FLOORSMAX_AVG', 'DEF_60_CNT_SOCIAL_CIRCLE', 'AMT_INCOME_TOTAL', 'TOTALAREA_MODE', 'WEEKDAY_APPR_PROCESS_START', 'APARTMENTS_MEDI', 'AMT_REQ_CREDIT_BUREAU_DAY', 'OBS_60_CNT_SOCIAL_CIRCLE', 'HOUR_APPR_PROCESS_START', 'FLAG_PHONE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'APARTMENTS_AVG', 'SK_ID_CURR', 'FLAG_DOCUMENT_16', 'YEARS_BEGINEXPLUATATION_MEDI', 'APARTMENTS_MODE', 'ENTRANCES_MEDI', 'FLAG_DOCUMENT_18', 'YEARS_BEGINEXPLUATATION_MODE', 'NAME_HOUSING_TYPE', 'LIVINGAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LANDAREA_MEDI', 'ELEVATORS_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'COMMONAREA_MEDI', 'WALLSMATERIAL_MODE', 'ELEVATORS_AVG', 'FLAG_OWN_REALTY_N', 'NONLIVINGAREA_MEDI', 'BASEMENTAREA_AVG', 'FLOORSMIN_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'COMMONAREA_AVG', 'LANDAREA_AVG', 'YEARS_BUILD_MODE', 'FLOORSMAX_MEDI', 'COMMONAREA_MODE', 'FLOORSMAX_MODE', 'LIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE', 'ENTRANCES_MODE', 'FLOORSMIN_AVG', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE_block of flats', 'BASEMENTAREA_MODE', 'YEARS_BUILD_MEDI', 'FLAG_DOCUMENT_13', 'LANDAREA_MODE', 'CNT_FAM_MEMBERS', 'REG_REGION_NOT_WORK_REGION', 'LIVINGAREA_MODE', 'NAME_TYPE_SUITE', 'AMT_REQ_CREDIT_BUREAU_MON', 'REG_CITY_NOT_WORK_CITY', 'NONLIVINGAPARTMENTS_MEDI', 'FLAG_DOCUMENT_14', 'ENTRANCES_AVG', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'NONLIVINGAREA_MODE', 'BASEMENTAREA_MEDI', 'NONLIVINGAREA_AVG', 'FLOORSMIN_MEDI', 'REG_REGION_NOT_LIVE_REGION', 'FLAG_DOCUMENT_5', 'NONLIVINGAPARTMENTS_MODE', 'YEARS_BUILD_AVG', 'FLAG_OWN_REALTY_Y', 'CNT_CHILDREN', 'FONDKAPREMONT_MODE', 'LIVE_REGION_NOT_WORK_REGION', 'FLAG_EMAIL', 'LIVINGAPARTMENTS_MEDI', 'REGION_RATING_CLIENT', 'FLAG_DOCUMENT_8', 'HOUSETYPE_MODE_specific housing', 'LIVE_CITY_NOT_WORK_CITY', 'EMERGENCYSTATE_MODE_nan', 'FLAG_DOCUMENT_6', 'HOUSETYPE_MODE_terraced house']\n\n\n\nSHAP으로 Feature Selection한 뒤의 결과 비교\n\n아래 모델/데이터를 기준으로, 성능에서 크게 차이나지 않음\n\n\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\n# 전체 변수 데이터\nprint(\"전체 데이터로 학습\")\n\n## Full Model\nfull_model = LGBMClassifier(class_weight=\"balanced\", random_state=42)\nfull_model.fit(X_train, y_train)\n\n## Full Pred\ny_pred_full = full_model.predict(X_test)\ny_proba_full = full_model.predict_proba(X_test)\n\n## Full Results\naccuracy_full = accuracy_score(y_test, y_pred_full)\nauc_full = roc_auc_score(y_test, y_proba_full[:, 1])\ncf_full = confusion_matrix(y_test, y_pred_full)\n\n\n# Feature selection된 데이터\nprint(\"\\nFeature selection된 데이터로 학습\")\n\nX_train_filtered = X_train.loc[:, selected_features]\nX_test_filtered = X_test.loc[:, selected_features]\n\n## Filtered Model\nfiltered_model  = LGBMClassifier(class_weight=\"balanced\", random_state=42)\nfiltered_model.fit(X_train_filtered, y_train)\n\n## Filtered Pred\ny_pred_filtered = filtered_model.predict(X_test_filtered)\ny_proba_filtered = filtered_model.predict_proba(X_test_filtered)\n\n## Filtered Results\naccuracy_filtered = accuracy_score(y_test, y_pred_filtered)\nauc_filtered = roc_auc_score(y_test, y_proba_filtered[:, 1])\ncf_filtered = confusion_matrix(y_test, y_pred_filtered)\n\n전체 데이터로 학습\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020035 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 11686\n[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 125\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n\nFeature selection된 데이터로 학습\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 19876, number of negative: 226132\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017355 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 11610\n[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 109\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n\n\n\nprint(f'모든 변수를 사용한 정확도 : {round(accuracy_full,4)}')\nprint(f'무의미한 변수를 삭제한 정확도 : {round(accuracy_filtered,4)}')\nprint(cf_full)\nprint('-'*100)\nprint(f'모든 변수를 사용한 AUC : {round(auc_full,4)}')\nprint(f'무의미한 변수를 삭제한 AUC : {round(auc_filtered,4)}')\nprint(cf_filtered)\n\n모든 변수를 사용한 정확도 : 0.703\n무의미한 변수를 삭제한 정확도 : 0.7028\n[[39914 16640]\n [ 1629  3320]]\n----------------------------------------------------------------------------------------------------\n모든 변수를 사용한 AUC : 0.7578\n무의미한 변수를 삭제한 AUC : 0.7577\n[[39908 16646]\n [ 1634  3315]]"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#기타-참고사항",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#기타-참고사항",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "기타 참고사항",
    "text": "기타 참고사항\n\n실제로는 모델학습이 끝난 후에도, feature importance구하기-무의미한 변수 제거하기-(파생변수생성)-모델학습의 반복적 수행\n\n처음에는 간단히 베이스모델을 만들어, SHAP로 1차로 무의미한 변수를 걸러냄\n베스트 모델을 추려낸 후, 오래걸리는 파라미터 튜닝을 퇴근시간 등 활용해 처리\n\n현업케이스 참고\n\n카드사 내부적으로 마케팅을 위해 이탈고객, 휴면예정고객 등에 대한 분석(산출)을 DA에게 의뢰\n\n혜택을 줄여 Mass마케팅 or 예상되는 이탈고객에게 Target마케팅\n위를 위한 모델을 만들어달라는 의뢰를 받아, 데이터마트 생성과 모델링 등을 수행\n중간에 프로모션을 해보며 개선한 뒤, 최종적으로 이탈가능성이 높은 고객을 추려 현업(마케팅)에서 계획 수립\n지속하게되는 경우, ’Batch개발’을 통해 데이터마트(Input데이터)-모델Output의 데이터마트 저장-현업에서 활용을 자동화\nBatch로 운영되는 모델은 주기적으로 고도화되기도 함(모델의 Metric이 아닌, 현업에서 보는 안정성관련 모니터링 지표 등을 확인)\n\n고객센터에서 통화내용의 Speech-to-text로 토픽모델링을 활용해, 고객이 하고픈 말이 어떤 것인지 찾아내는 NLP과제도 있음\n\nCase별 Metric을 선택할 시각 키우는 방법\n\n다양한 대회(공모전)의 케이스를 참고 (주로 쓰이는 것이 몇가지 있다)\n논문 참고하기(성능을 표기한 Metric참고)\n\nSQuAD(LLM을 평가하는 문제지 같은 것)에 대한 성능표기나, SOTA 등 참고\nNLP newsletter(요즘 핫한 NLP모델 논문을 매주 보내주는 글)\n\nCAG(↔︎RAG) : 처리용량이 커졌으므로, 검색이 아니라 데이터를 캐시에 모두 넣고 실행\n\n런타임 오류 감소 등 의외로 성능이 좋았음\n\n\n\nMetric을 정하는 것은 지속하다보면 익숙해질 수 있는 부분\n\nMetric에서 보통 제공하지 않는 Pinball loss등이 필요한 경우\n\n모델학습이 loss를 줄이는 방향이라는 관점에서 맞춰주는 것이 좋음\n없으면 커스텀함수로 만들어서 맞추는 편(다만 크게 차이가 나지는 않는다)\nLoss함수를 일치시켜서 얻는 이득이 큰 편(일부라도 성능올리기 위해 하이퍼파라미터 튜닝 등을 한다는 관점에서 볼 때)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#과제",
    "href": "posts/meta-cm-sql_and_ml_xai-20250119/index.html#과제",
    "title": "[DA스터디/5주차] 평가Metric, XAI, SHAP",
    "section": "과제",
    "text": "과제\n\n지난 주에 모델 학습을 시켜본 뒤 모델에 대해 SHAP을 통해 FI를 구해보세요.\nFeature Selection을 진행 한 뒤 재학습을 진행해 보세요.\n재학습한 모델에 대해 SHAP의 다양한 시각화를 통해 변수들의 설명력을 구해보세요."
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html",
    "href": "posts/dtcontest-ore-20240608/index.html",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(+공공데이터 API활용샘플)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html#개요",
    "href": "posts/dtcontest-ore-20240608/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n공공데이터를 csv로 저장하지 않고 API로 활용해보기로 함"
  },
  {
    "objectID": "posts/dtcontest-ore-20240608/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240608/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)",
    "section": "내용정리",
    "text": "내용정리\n\n과제 요약\n\n과제명 : 핵심 광물별 공급위기 요소 탐지 모델 개발\n과제 개요 : 핵심광물별 가격 변동성, 시세, 생산·수입국 현황 등으로 핵심 광물별로 공급 리스크를 줄일 수 있는 위기 요소 탐지 분석 모델 개발\n제출 서류 : 분석 결과보고서(양식활용), 분석 코드, 근거데이터 파일\n활용 계획 : 전기차 배터리, 반도체 등 첨단산업 민간기업에 다양한 핵심광물 수급현황 위기 요소 탐지에 대한 분석 방법, 결과 등을 제공하여 안정적인 수급리스크 정보지원 강화\n유의사항\n\n핵심광물의 수급현황 등을 기반으로 위기요소 탐지 분석\n\n광물가격 급등 등 변동성, 국제시세, 주요 생산·수입국 현황, 수입량·수입금액·수입의존도, 국내 비축확보율, 재자원화율 및 국내 주요 수요기업 정보 등\n\n인터넷에 공개되어 있는 모든 가용 데이터를 사용하여 자유롭게 작성\n\n단, 사용한 데이터는 출처를 표기하여 근거데이터로 제출(참고 사이트 : 관세청, 외교부, 기재부, 산업부, 무역협회, 수출입은행 등)\n\n\n\n\n\n분석대상 광물선정\n\n공모전 인지 및 시작이 늦어서, 최대한 효율적으로 진행하고자 함(1달이내 남음)\n분석대상 광물을 공모전에 언급된 모든 광물보다는, 중요한 광물 위주로 진행해보고자 함\n\n공모전에 언급된 광물 : 텅스텐, 코발트, 리튬, 망간, 니켈\n\n고려 요소\n\n업종별 소요량 등을 기준으로 가장 공통적인/주요한 광물을 찾아보고자 함\n상기 자료는 찾기가 어려워, 생산량, 소비량, 수출입물량을 기준으로 상대적 비교를 해보고자 함\n\n\n\n\n공공데이터포털 API사용을 위한 파이썬 함수 작성\n\nAPI로 데이터 호출해보니 아래의 내용을 확인할 수 있었음\n\n호출 url의 ’perPage’를 활용해 한번에 가져올 데이터 수(행)을 정할 수 있음\n호출된 json에서 currentCount와 totalCount로 가져온 데이터와 전체 데이터를 확인할 수 있음\n\n확인한 내용을 기반으로, 공공데이터포털에서 json을 지원하는 데이터는 별도 조작없이 가능하게 하자는 목적으로 함수작성함\n\n일단 1개만 호출하여 전체 데이터 수량을 확인하고, perPage값을 바꿔 전체 데이터를 불러옴\n\n\n\nimport requests\nimport json\nimport pandas as pd\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\n\n# API사용을 위한 개인키 입력\nserviceKey = '(개인키)'\n\n\n\n개별분석\n\n광종별 소요량\n\n공공데이터포털 - 한국광해광업공단_광종별 소비현황  https://www.data.go.kr/data/3070245/fileData.do#tab-layer-file\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070245/v1/uddi:d950e6bc-e6a0-407c-baad-dfa87d739ff1_202004091120'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\njson_1 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_1)\n\n{'currentCount': 410,\n 'matchCount': 410,\n 'page': 1,\n 'perPage': 410,\n 'totalCount': 410}\n\n\n\ndf_1 = pd.json_normalize(json_1['data'])\ndf_1\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n0\n293.867\n253.700\n201.000\n225.800\n220.751\n226.106\n247.578\n260.381\n246.674\n190.106\n198.989\n337.973\n272.189\n알루미늄\nAustria\n천톤\nEUROPE\nrefined\n\n\n1\n10.200\n10.200\n10.200\n29.598\n21.898\n30.197\n27.001\n27.052\n31.673\n29.178\n39.481\n3.276\n2.400\n알루미늄\nBelarus\n천톤\nEUROPE\nrefined\n\n\n2\n382.312\n344.280\n236.780\n202.967\n231.648\n184.982\n212.039\n233.065\n181.368\n138.809\n204.808\n257.193\n160.985\n알루미늄\nBelgium\n천톤\nEUROPE\nrefined\n\n\n3\n52.188\n53.527\n60.645\n87.027\n109.824\n110.526\n109.522\n111.927\n119.211\n95.958\n128.057\n121.134\n97.254\n알루미늄\nBulgaria\n천톤\nEUROPE\nrefined\n\n\n4\n77.771\n83.684\n76.755\n66.065\n24.672\n52.033\n96.463\n112.930\n136.757\n143.869\n159.163\n135.867\n129.308\n알루미늄\nCroatia\n천톤\nEUROPE\nrefined\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n405\n939.000\n892.000\n935.000\n962.000\n931.000\n788.980\n829.000\n1800.000\n1800.000\n950.000\n878.000\n973.000\n911.000\n아연\nU.S.A.\n천톤\nAMERICA\nslab\n\n\n406\n0.000\n0.520\n0.514\n0.304\n0.449\n0.316\n0.312\n0.235\n0.235\n0.299\n0.350\n0.360\n0.371\n아연\nUruguay\n천톤\nAMERICA\nslab\n\n\n407\n10.430\n6.656\n9.014\n4.872\n2.246\n1.994\n0.685\n0.596\n0.596\n0.442\n0.360\n0.300\n0.370\n아연\nVenezuela\n천톤\nAMERICA\nslab\n\n\n408\n207.319\n116.293\n180.000\n174.400\n176.000\n178.000\n145.000\n146.700\n146.700\n135.000\n122.000\n125.000\n96.000\n아연\nAustralia\n천톤\nOCEANIA\nslab\n\n\n409\n11.023\n9.275\n8.844\n7.238\n6.876\n8.684\n10.631\n11.550\n11.550\n10.653\n10.332\n9.407\n6.405\n아연\nNew Zealand\n천톤\nOCEANIA\nslab\n\n\n\n\n410 rows × 18 columns\n\n\n\n\n# 광종 및 소비량 확인\nprint(df_1['광종'].unique())\nprint(df_1.columns)\n\n['알루미늄' '카드뮴' '동' '연' '니켈' '주석' '아연']\nIndex(['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량', '광종', '국가', '단위', '대륙', '품목'],\n      dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_1[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']] = df_1[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']].astype(float)\ndf_1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 410 entries, 0 to 409\nData columns (total 18 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   2011 소비량  409 non-null    float64\n 1   2012 소비량  410 non-null    float64\n 2   2013 소비량  410 non-null    float64\n 3   2014 소비량  410 non-null    float64\n 4   2015 소비량  410 non-null    float64\n 5   2016 소비량  410 non-null    float64\n 6   2017 소비량  410 non-null    float64\n 7   2018 소비량  410 non-null    float64\n 8   2019 소비량  410 non-null    float64\n 9   2020 소비량  410 non-null    float64\n 10  2021 소비량  410 non-null    float64\n 11  2022 소비량  410 non-null    float64\n 12  2023 소비량  410 non-null    float64\n 13  광종        410 non-null    object \n 14  국가        410 non-null    object \n 15  단위        410 non-null    object \n 16  대륙        410 non-null    object \n 17  품목        410 non-null    object \ndtypes: float64(13), object(5)\nmemory usage: 57.8+ KB\n\n\n\n# 광종별 소비량\ndf1_consume = df_1.groupby(by=['광종'])[['2011 소비량', '2012 소비량', '2013 소비량', '2014 소비량', '2015 소비량', '2016 소비량',\n       '2017 소비량', '2018 소비량', '2019 소비량', '2020 소비량', '2021 소비량', '2022 소비량',\n       '2023 소비량']].sum()\ndf1_consume\n\n\n\n\n\n\n\n\n2011 소비량\n2012 소비량\n2013 소비량\n2014 소비량\n2015 소비량\n2016 소비량\n2017 소비량\n2018 소비량\n2019 소비량\n2020 소비량\n2021 소비량\n2022 소비량\n2023 소비량\n\n\n광종\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n니켈\n1651.690\n1729.344\n1794.140\n1579.073\n1730.944\n1855.296\n2090.518\n2339.103\n2430.139\n2442.837\n2962.181\n3003.505\n3171.547\n\n\n동\n19488.142\n20281.667\n21085.937\n22704.666\n22716.929\n23112.305\n23236.699\n23825.029\n23937.201\n24764.119\n24777.553\n25897.648\n27632.445\n\n\n아연\n12510.399\n12055.051\n12885.691\n13796.903\n13784.174\n13863.425\n14058.430\n14021.779\n14021.779\n13603.269\n13257.980\n13839.233\n14185.938\n\n\n알루미늄\n44428.294\n47851.313\n50670.176\n54045.963\n56775.631\n59293.053\n59905.431\n62043.652\n62608.661\n63407.663\n68102.645\n67853.241\n68826.117\n\n\n연\n10459.383\n10506.481\n11288.386\n10863.942\n11293.988\n11533.887\n12240.931\n12354.902\n12686.026\n11897.099\n14463.857\n14831.478\n14532.497\n\n\n주석\n372.530\n353.471\n355.130\n389.926\n363.759\n380.857\n375.021\n373.503\n362.765\n380.887\n374.583\n359.100\n344.361\n\n\n카드뮴\n23933.352\n23396.250\n27150.332\n28807.514\n27119.446\n27233.503\n26733.110\n24011.194\n20911.834\n19882.427\n19813.410\n19521.471\n19994.543\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,4))\nsns.heatmap(df1_consume, annot=True, fmt=\".0f\")\n\n\n\n\n\n\n\n\n\n상대적으로 알루미늄의 소비량이 가장 많음\n\n공모전 조원에게 받은 한국지질자원연구서의 보고서에 따르면, 지각 내에 3번째로 많이 들어있음 (2022.02 비철금속 비축 효과성/타당성 평가분석 및 중장기 정부비축방향)\n항공, 건축, 전기, 내화 등 다양한 용도로 사용되고 있으며, 4차 핵심사업의 용도로는 부합하지 않는 것으로 판단됨\n\n\n\nfor ore in df_1['광종'].unique():\n    if ore == '알루미늄' or ore == '카드뮴':\n        continue\n    df1_consume.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n알루미늄을 제외하고 소비량이 상승하고 있는 광물 위주로 추림\n\n앞서 알루미늄에 대한 시각과 같이 4차 핵심사업의 용도로는 니켈만이 유력\n\n\n\ndf1_consume.loc['니켈'].plot()\n\n\n\n\n\n\n\n\n\n\n광종별 생산량\n\n공공데이터포털 - 한국광해광업공단_광종별 국가별 생산량  https://www.data.go.kr/data/3070256/fileData.do#/API%20%EB%AA%A9%EB%A1%9D/getuddi%3A6d31d5bc-5487-4fa1-a335-90f9d9623cc8_202004080953\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070256/v1/uddi:6d31d5bc-5487-4fa1-a335-90f9d9623cc8_202004080953'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\n\njson_2 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_2)\n\n{'currentCount': 695,\n 'matchCount': 695,\n 'page': 1,\n 'perPage': 695,\n 'totalCount': 695}\n\n\n\ndf_2 = pd.json_normalize(json_2['data'])\ndf_2\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n광종\n국가\n단위\n대륙\n품목\n\n\n\n\n0\n561.354\n800.316\n657.100\n605.215\n787.404\n738.612\n740.380\n760.244\n1043.343\n619.748\n675.269\n669.926\n542.114\n알루미늄\nBosnia\n천톤\nEUROPE\nbauxite\n\n\n1\n0.000\n0.000\n0.000\n0.000\n11.900\n9.800\n12.200\n11.800\n14.300\n14.100\n14.500\n13.800\n13.800\n알루미늄\nCroatia\n천톤\nEUROPE\nbauxite\n\n\n2\n80.800\n90.129\n100.000\n71.100\n70.000\n110.000\n110.000\n110.000\n120.760\n123.496\n142.764\n120.000\n120.000\n알루미늄\nFrance\n천톤\nEUROPE\nbauxite\n\n\n3\n2324.000\n1815.328\n1844.000\n1876.000\n1831.270\n1880.000\n1927.145\n1559.360\n1379.123\n1428.639\n1227.000\n1173.000\n869.100\n알루미늄\nGreece\n천톤\nEUROPE\nbauxite\n\n\n4\n277.800\n255.100\n93.700\n14.400\n8.300\n16.700\n4.000\n5.000\n0.000\n0.000\n0.000\n0.000\n0.000\n알루미늄\nHungary\n천톤\nEUROPE\nbauxite\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n690\n662.151\n648.619\n651.637\n649.218\n683.118\n691.400\n598.438\n620.202\n654.971\n685.400\n639.843\n569.613\n502.000\n아연\nCanada\n천톤\nAMERICA\nslab\n\n\n691\n322.116\n323.569\n322.781\n320.924\n326.834\n321.166\n327.000\n335.942\n393.418\n368.200\n357.000\n328.727\n344.151\n아연\nMexico\n천톤\nAMERICA\nslab\n\n\n692\n313.714\n319.280\n346.361\n336.454\n335.422\n341.518\n312.339\n333.677\n356.925\n318.619\n320.000\n349.500\n346.072\n아연\nPeru\n천톤\nAMERICA\nslab\n\n\n693\n248.000\n261.000\n233.000\n180.000\n172.000\n126.000\n132.000\n116.000\n115.000\n180.000\n216.000\n216.000\n220.000\n아연\nU.S.A.\n천톤\nAMERICA\nslab\n\n\n694\n517.000\n508.000\n504.000\n488.000\n489.000\n470.000\n471.000\n491.000\n436.000\n447.300\n432.512\n385.032\n467.044\n아연\nAustralia\n천톤\nOCEANIA\nslab\n\n\n\n\n695 rows × 18 columns\n\n\n\n\n# 광종 및 생산량 확인\nprint(df_2['광종'].unique())\nprint(df_2.columns)\n\n['알루미늄' '안티모니' '카드뮴' '동' '금' '연' '몰리브덴' '니켈' '은' '주석' '아연']\nIndex(['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량', '광종', '국가', '단위', '대륙', '품목'],\n      dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_2[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']] = df_2[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].astype(float)\ndf_2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 695 entries, 0 to 694\nData columns (total 18 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   2011 생산량  691 non-null    float64\n 1   2012 생산량  695 non-null    float64\n 2   2013 생산량  695 non-null    float64\n 3   2014 생산량  695 non-null    float64\n 4   2015 생산량  695 non-null    float64\n 5   2016 생산량  695 non-null    float64\n 6   2017 생산량  695 non-null    float64\n 7   2018 생산량  695 non-null    float64\n 8   2019 생산량  695 non-null    float64\n 9   2020 생산량  695 non-null    float64\n 10  2021 생산량  695 non-null    float64\n 11  2022 생산량  694 non-null    float64\n 12  2023 생산량  694 non-null    float64\n 13  광종        695 non-null    object \n 14  국가        695 non-null    object \n 15  단위        695 non-null    object \n 16  대륙        695 non-null    object \n 17  품목        695 non-null    object \ndtypes: float64(13), object(5)\nmemory usage: 97.9+ KB\n\n\n\n# 광종별 생산량\ndf2_produce = df_2.groupby(by=['광종'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum()\ndf2_produce\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n\n\n광종\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n금\n2570.328\n2644.576\n2886.315\n3017.911\n3083.170\n3180.769\n3.238410e+03\n3.263709e+03\n3.211746e+03\n3.066197e+03\n3.041123e+03\n3.147309e+03\n3.138277e+03\n\n\n니켈\n3467.292\n4140.150\n4538.125\n3925.512\n3970.324\n3839.161\n4.229470e+03\n4.623132e+03\n4.964282e+03\n4.972969e+03\n5.467607e+03\n6.303324e+03\n7.216948e+03\n\n\n동\n35816.170\n37138.574\n39170.057\n41019.128\n42338.447\n43631.801\n4.364408e+04\n4.403226e+04\n4.399504e+04\n4.471063e+04\n4.588754e+04\n4.758409e+04\n5.004668e+04\n\n\n몰리브덴\n268.602\n266.883\n278.853\n303.501\n296.261\n283.849\n2.915410e+02\n2.731790e+02\n2.764260e+02\n2.893510e+02\n2.688010e+02\n2.858460e+02\n2.804480e+02\n\n\n아연\n25676.388\n25887.690\n26613.454\n27099.928\n27286.373\n26155.233\n2.563553e+04\n2.541092e+04\n2.568426e+04\n2.614851e+04\n2.747017e+04\n2.712567e+04\n2.750890e+04\n\n\n안티모니\n156163.000\n174973.000\n192551.000\n158041.000\n155999.000\n165096.000\n1.590890e+05\n1.601380e+05\n1.227800e+05\n1.142970e+05\n8.946900e+04\n8.533900e+04\n8.443900e+04\n\n\n알루미늄\n295511.409\n308026.690\n351054.187\n311065.456\n328534.809\n341423.066\n3.746015e+05\n4.011228e+05\n4.261662e+05\n4.469730e+05\n4.548925e+05\n4.549827e+05\n4.715010e+05\n\n\n연\n15435.226\n15814.869\n16593.862\n16246.173\n15871.816\n16294.015\n1.629023e+04\n1.651490e+04\n1.688362e+04\n1.626823e+04\n1.897150e+04\n2.043378e+04\n1.952917e+04\n\n\n은\n23282.594\n24903.406\n26174.528\n27279.117\n27848.245\n28038.988\n2.643421e+04\n2.651451e+04\n2.625192e+04\n2.425894e+04\n2.563196e+04\n2.477051e+04\n2.527272e+04\n\n\n주석\n670.472\n662.043\n648.298\n694.752\n669.020\n639.671\n7.081480e+02\n7.180450e+02\n7.278870e+02\n7.277420e+02\n7.273120e+02\n6.603090e+02\n6.635190e+02\n\n\n카드뮴\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n2.532580e+07\n2.817260e+07\n2.505329e+07\n2.473016e+07\n2.486415e+07\n2.447090e+07\n2.477489e+07\n\n\n\n\n\n\n\n\n앞서 소비량 분석에서 4차 핵심산업에 부합하지 않을 것으로 배제했던 광물을 제외하고 확인\n\n\nfor ore in df_2['광종'].unique():\n    if ore not in ['동', '연', '주석', '아연', '알루미늄', '카드뮴']:\n        df2_produce.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n생산량이 감소하고 있는 안티모니는 어떤 금속인지에 대해 조사\n\n방염, 페인트 합금, 고무 등에 사용되는 것으로, 안티모니도 대상에서 배제\n\n\n\nfor ore in df_2['광종'].unique():\n    if ore not in ['동', '연', '주석', '아연', '알루미늄', '카드뮴','안티모니']:\n        df2_produce.loc[ore].plot()\nplt.legend()\n\n\n\n\n\n\n\n\n\n# 니켈에 대한 국가별 생산량\ndf2_produce_country = df_2[df_2['광종']=='니켈'].groupby(by=['국가'])[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum()\ndf2_produce_country\n\n\n\n\n\n\n\n\n2011 생산량\n2012 생산량\n2013 생산량\n2014 생산량\n2015 생산량\n2016 생산량\n2017 생산량\n2018 생산량\n2019 생산량\n2020 생산량\n2021 생산량\n2022 생산량\n2023 생산량\n\n\n국가\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlbania\n3.528\n0.728\n2.086\n4.889\n6.309\n3.952\n5.301\n4.204\n2.830\n3.764\n3.615\n1.423\n0.548\n\n\nAustralia\n325.227\n407.700\n433.872\n403.943\n378.205\n323.656\n293.966\n274.539\n265.220\n285.144\n249.845\n251.507\n241.140\n\n\nAustria\n0.500\n0.500\n0.500\n0.600\n0.700\n0.700\n0.700\n0.700\n0.700\n0.700\n0.700\n1.000\n1.000\n\n\nBotswana\n15.675\n17.948\n22.848\n14.952\n16.788\n16.878\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nBrazil\n117.801\n148.344\n130.611\n160.924\n159.906\n155.788\n137.303\n130.454\n109.944\n136.633\n136.788\n142.700\n131.400\n\n\nCanada\n361.470\n358.551\n380.471\n378.353\n384.236\n393.333\n369.061\n323.373\n317.894\n282.033\n235.219\n191.807\n214.341\n\n\nChina\n559.536\n684.172\n803.857\n638.237\n554.597\n537.173\n723.282\n841.119\n957.049\n857.237\n912.392\n974.155\n1044.821\n\n\nColombia\n75.636\n103.188\n98.772\n82.400\n73.400\n74.170\n81.200\n86.200\n81.200\n72.200\n76.600\n83.600\n77.400\n\n\nCuba\n102.532\n94.926\n72.236\n64.839\n64.731\n62.759\n65.041\n63.620\n58.430\n63.144\n64.929\n66.101\n62.213\n\n\nDominican Republic\n26.996\n30.372\n18.800\n0.000\n0.000\n19.826\n31.264\n38.428\n56.900\n42.632\n54.640\n56.800\n38.848\n\n\nFinland\n67.605\n65.473\n63.831\n62.433\n54.122\n75.687\n95.917\n104.337\n100.952\n104.781\n90.887\n129.668\n139.599\n\n\nFrance\n13.916\n13.546\n12.916\n8.812\n6.761\n4.287\n2.417\n3.700\n6.900\n7.300\n8.900\n6.842\n7.125\n\n\nGreece\n39.630\n40.150\n36.177\n39.891\n36.863\n36.463\n35.861\n33.609\n25.689\n13.071\n8.834\n3.124\n0.000\n\n\nGuatemala\n0.000\n2.422\n10.184\n49.899\n67.324\n53.434\n68.147\n53.708\n52.456\n72.537\n84.864\n69.181\n20.177\n\n\nIndia\n0.000\n0.000\n1.010\n2.028\n1.861\n0.329\n0.092\n0.077\n0.055\n0.000\n0.000\n0.000\n0.000\n\n\nIndonesia\n246.597\n640.591\n834.330\n167.151\n176.035\n308.017\n555.580\n907.673\n1250.383\n1411.019\n1915.958\n2745.018\n3589.948\n\n\nIvory Coast\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.576\n6.852\n16.421\n18.988\n27.849\n33.534\n36.279\n\n\nJapan\n156.883\n169.556\n177.810\n177.782\n192.789\n195.565\n187.046\n186.736\n182.652\n171.044\n180.721\n163.401\n153.248\n\n\nKazakhstan\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nKosovo\n15.132\n8.872\n15.214\n13.448\n13.300\n6.825\n15.621\n9.179\n8.548\n11.769\n10.646\n0.339\n0.360\n\n\nMacedonia\n20.892\n20.951\n20.001\n18.054\n17.699\n10.607\n7.175\n10.100\n15.202\n17.747\n17.714\n8.349\n3.812\n\n\nMadagascar\n0.000\n11.390\n50.298\n74.108\n94.542\n84.210\n70.948\n66.366\n67.466\n19.800\n58.704\n71.258\n75.828\n\n\nMorocco\n0.217\n0.288\n0.160\n0.000\n0.203\n0.188\n0.196\n0.126\n0.131\n0.142\n0.144\n0.144\n0.144\n\n\nMyanmar\n0.800\n9.800\n3.340\n39.428\n45.804\n45.200\n43.570\n35.604\n28.890\n49.078\n42.254\n16.868\n11.802\n\n\nNew Caledonia\n168.626\n177.076\n212.777\n237.223\n270.729\n300.193\n319.476\n324.139\n296.106\n272.152\n242.887\n266.171\n302.940\n\n\nNorway\n92.753\n91.940\n91.380\n90.846\n91.465\n92.914\n86.672\n91.054\n92.290\n91.374\n91.485\n82.163\n95.253\n\n\nPapua New Guinea\n0.000\n5.283\n11.370\n20.987\n25.581\n22.269\n34.666\n35.355\n32.722\n33.659\n31.594\n34.304\n33.604\n\n\nPhilippines\n319.353\n317.621\n313.050\n443.909\n466.754\n345.506\n379.377\n389.966\n341.325\n328.911\n393.687\n340.127\n382.568\n\n\nPoland\n1.700\n1.600\n1.200\n1.560\n1.680\n1.540\n1.440\n1.400\n1.440\n1.420\n1.440\n1.600\n1.680\n\n\nRussia\n536.000\n522.700\n506.000\n503.436\n492.916\n413.482\n372.562\n368.667\n379.762\n386.934\n311.033\n367.972\n354.259\n\n\nSouth Africa\n79.320\n78.446\n83.508\n89.656\n98.646\n91.104\n91.451\n82.730\n82.603\n64.030\n69.804\n66.623\n65.699\n\n\nSouth Korea\n16.863\n23.673\n28.130\n24.964\n39.007\n45.616\n47.402\n45.593\n46.269\n45.604\n47.270\n31.959\n40.022\n\n\nSpain\n0.000\n2.398\n7.574\n8.631\n7.213\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nTurkey\n0.848\n3.814\n1.178\n3.035\n8.637\n9.337\n14.767\n13.559\n4.742\n20.201\n11.647\n14.479\n14.400\n\n\nU.S.A.\n0.000\n0.000\n0.000\n4.300\n27.167\n24.114\n22.081\n17.573\n13.494\n16.718\n18.353\n17.475\n16.429\n\n\nUkraine\n22.475\n20.833\n26.751\n25.129\n20.842\n18.920\n15.605\n16.331\n15.133\n15.910\n15.657\n10.093\n0.733\n\n\nUnited Kingdom\n37.400\n39.400\n42.400\n39.100\n39.094\n43.104\n37.090\n38.211\n34.976\n35.177\n30.654\n33.384\n35.851\n\n\nVenezuela\n26.800\n16.200\n6.522\n5.000\n9.702\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nVietnam\n0.000\n0.000\n1.200\n6.932\n8.607\n4.272\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nZambia\n2.888\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.230\n3.780\n3.680\n3.942\n6.550\n\n\nZimbabwe\n11.693\n9.698\n15.761\n18.633\n16.109\n17.743\n16.617\n17.850\n16.278\n16.336\n16.213\n16.213\n16.927\n\n\n\n\n\n\n\n\n# 니켈에 대한 국가별 생산량의 총 합계\ndf2_produce_country_total = df2_produce_country[['2011 생산량', '2012 생산량', '2013 생산량', '2014 생산량', '2015 생산량', '2016 생산량',\n       '2017 생산량', '2018 생산량', '2019 생산량', '2020 생산량', '2021 생산량', '2022 생산량',\n       '2023 생산량']].sum(axis=1)\ndf2_produce_country_total.sort_values(ascending=False)\n\n국가\nIndonesia             14748.300\nChina                 10087.627\nRussia                 5515.723\nPhilippines            4762.154\nCanada                 4190.142\nAustralia              4133.964\nNew Caledonia          3390.495\nJapan                  2295.233\nBrazil                 1798.596\nNorway                 1181.589\nFinland                1155.292\nColombia               1065.966\nSouth Africa           1043.620\nCuba                    905.501\nMadagascar              744.918\nGuatemala               604.333\nUnited Kingdom          485.841\nSouth Korea             482.372\nDominican Republic      415.506\nMyanmar                 372.438\nGreece                  349.362\nPapua New Guinea        321.394\nUkraine                 224.412\nZimbabwe                206.071\nMacedonia               188.303\nU.S.A.                  177.704\nIvory Coast             140.499\nKosovo                  129.253\nTurkey                  120.644\nBotswana                105.089\nFrance                  103.422\nVenezuela                64.224\nAlbania                  43.177\nSpain                    25.816\nZambia                   22.070\nVietnam                  21.011\nPoland                   19.700\nAustria                   9.000\nIndia                     5.452\nMorocco                   2.083\nKazakhstan                0.000\ndtype: float64\n\n\n\n니켈 총생산량 기준 상위 10개국에 대한 시각화진행\n\n인도네시아, 중국, 러시아, 필리핀 순으로 상위 생산국\n\n\n\n# 니켈 총생산량 내림차순 기준 상위 10개국 Pie chart\ntarget_country = df2_produce_country_total.sort_values(ascending=False)[0:10].index.tolist()\ndf2_produce_country_total.loc[target_country].plot(kind='pie',startangle=145, autopct='%.1f%%', pctdistance=0.8)\n\n\n\n\n\n\n\n\n\n니켈에 대한 모델링 등을 할때, 주요 생산국의 관련 지표를 넣으면 좋을 듯 함\n\n\n\n광종별 수출입 현황\n\n공공데이터포털 - 한국광해광업공단_광종별 국내 수출입 현황  https://www.data.go.kr/data/3070177/fileData.do\n\n\nbase_url = 'https://api.odcloud.kr/api'\naddress_get = '/3070177/v1/uddi:09b79733-b804-4ec6-9968-dd0f58638b55_202004090938'\nurl = f'{base_url}{address_get}?page=1&perPage=1&serviceKey={serviceKey}'\n\njson_3 = download_from_data_go_kr_with_json(url)\nchk_json_status_of_data_go_kr(json_3)\n\n{'currentCount': 116,\n 'matchCount': 116,\n 'page': 1,\n 'perPage': 116,\n 'totalCount': 116}\n\n\n\ndf_3 = pd.json_normalize(json_3['data'])\ndf_3\n\n\n\n\n\n\n\n\n광종\n분류\n수입금액(천불)\n수입중량(톤)\n수출금액(천불)\n수출중량(톤)\n연도\n\n\n\n\n0\n금광\n금속광\n67000\n5705.000\n116\n1005.000\n2021\n\n\n1\n은광\n금속광\n302335\n37635.000\n79\n63.000\n2021\n\n\n2\n동광\n금속광\n6017233\n2097948.000\n1185720\n560664.000\n2021\n\n\n3\n연광\n금속광\n2366631\n642414.000\n670\n1102.000\n2021\n\n\n4\n아연광\n금속광\n2044885\n1819759.000\n34679\n23804.000\n2021\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111\n하석\n비금속광\n144\n321.000\n1\n0.000\n2022\n\n\n112\n무연탄\n석탄광\n1661049\n5384000.000\n251\n375.000\n2022\n\n\n113\n유연탄\n석탄광\n26230599\n117752000.000\n0\n0.000\n2022\n\n\n114\n갈탄\n석탄광\n65\n107.000\n0\n0.000\n2022\n\n\n115\n토탄\n석탄광\n25324\n110185.000\n41\n21.000\n2022\n\n\n\n\n116 rows × 7 columns\n\n\n\n\n# 광종 및 소비량 확인\nprint(df_3['광종'].unique())\nprint(df_3.columns)\n\n['금광' '은광' '동광' '연광' '아연광' '철광' '텅스텐광' '몰리브덴광' '망간광' '주석광' '창연' '안티모니광'\n '비소광' '황철석' '니켈' '코발트' '크롬광' '리튬광' '티타늄광' '질코늄광' '알루미늄광' '백금광' '탄탈륨광'\n '바나듐광' '니오븀광' '게르마늄광' '기타 금속' '인상흑연' '토상흑연' '활석' '납석' '장석' '고령토류' '석회석류'\n '규석' '규사' '황' '규조토' '형석' '인광석' '규회석' '운모' '주사' '홍주석' '규선석' '남정석' '중정석'\n '마그네사이트' '석고' '불석' '수정' '붕소광' '금강석' '하석' '기타 비금속' '무연탄' '유연탄' '갈탄' '토탄'\n '기타 석탄']\nIndex(['광종', '분류', '수입금액(천불)', '수입중량(톤)', '수출금액(천불)', '수출중량(톤)', '연도'], dtype='object')\n\n\n\n# str처리된 float값 변환\ndf_3[['수입중량(톤)', '수출중량(톤)',]] = df_3[['수입중량(톤)', '수출중량(톤)']].astype(float)\ndf_3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 116 entries, 0 to 115\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   광종        116 non-null    object \n 1   분류        116 non-null    object \n 2   수입금액(천불)  116 non-null    int64  \n 3   수입중량(톤)   116 non-null    float64\n 4   수출금액(천불)  116 non-null    int64  \n 5   수출중량(톤)   116 non-null    float64\n 6   연도        116 non-null    int64  \ndtypes: float64(2), int64(3), object(2)\nmemory usage: 6.5+ KB\n\n\n\n공모전에 언급된 주요광물에 대해서 추림\n\n\ndf3_sorted = df_3[df_3['광종'].isin(['텅스텐광','망간광','니켈','코발트','리튬광','알루미늄광'])]\ndf3_sorted\n\n\n\n\n\n\n\n\n광종\n분류\n수입금액(천불)\n수입중량(톤)\n수출금액(천불)\n수출중량(톤)\n연도\n\n\n\n\n6\n텅스텐광\n금속광\n60\n1.0\n84\n8.0\n2021\n\n\n8\n망간광\n금속광\n290356\n1325095.0\n0\n0.0\n2021\n\n\n14\n니켈\n금속광\n340685\n3181534.0\n42\n86.0\n2021\n\n\n15\n코발트\n금속광\n1\n0.0\n0\n0.0\n2021\n\n\n17\n리튬광\n금속광\n1047778\n94839.0\n53933\n4980.0\n2021\n\n\n20\n알루미늄광\n금속광\n33126\n424540.0\n45\n156.0\n2021\n\n\n66\n텅스텐광\n금속광\n607\n40.0\n3559\n1241.0\n2022\n\n\n68\n망간광\n금속광\n283632\n1085930.0\n5\n113.0\n2022\n\n\n74\n니켈\n금속광\n382427\n2653287.0\n206\n108.0\n2022\n\n\n75\n코발트\n금속광\n95\n12.0\n0\n0.0\n2022\n\n\n79\n알루미늄광\n금속광\n33150\n417762.0\n46\n478.0\n2022\n\n\n\n\n\n\n\n\nfor ore in ['텅스텐광','망간광','니켈','코발트','리튬광','알루미늄광']:\n    print(df3_sorted[df3_sorted['광종']==ore])\n    print()\n\n      광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n6   텅스텐광  금속광        60      1.0        84      8.0  2021\n66  텅스텐광  금속광       607     40.0      3559   1241.0  2022\n\n     광종   분류  수입금액(천불)    수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n8   망간광  금속광    290356  1325095.0         0      0.0  2021\n68  망간광  금속광    283632  1085930.0         5    113.0  2022\n\n    광종   분류  수입금액(천불)    수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n14  니켈  금속광    340685  3181534.0        42     86.0  2021\n74  니켈  금속광    382427  2653287.0       206    108.0  2022\n\n     광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n15  코발트  금속광         1      0.0         0      0.0  2021\n75  코발트  금속광        95     12.0         0      0.0  2022\n\n     광종   분류  수입금액(천불)  수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n17  리튬광  금속광   1047778  94839.0     53933   4980.0  2021\n\n       광종   분류  수입금액(천불)   수입중량(톤)  수출금액(천불)  수출중량(톤)    연도\n20  알루미늄광  금속광     33126  424540.0        45    156.0  2021\n79  알루미늄광  금속광     33150  417762.0        46    478.0  2022\n\n\n\n[광종별 간단 요약]\n\n텅스텐 : 수입 금액/중량 증가(가격비 비슷), 수출 금액/중량 증가\n망간 : 수입 금액/중량 감소\n니켈 : 수입 금액 증가, 수입 중량 감소 (가격 증가)\n코발트 : 수입 급격한 증가, 수출은 없음\n리튬 : 비교가능 데이터 없음\n\n[수출입 데이터 기준 고려사항]\n\n니켈 : 수입량과 가격 모두 증가\n니켈(수입량 증가와 가격 증가 추세로, 추가분석 필요하여 하단에서 계속 진행)\n코발트(수입 급격히 증가했으나 수출실적은 전혀없는 상황으로 국내 매장량 등 분석하고자 함)\n\n시간 상 관련 수치자료를 찾지 못하여 간단히 몇가지 검색해보니, 소요량에 대한 기사 발견 \n\n[기사]한국기업의 귀속생산량이 전세계 총합 대비 1% 이하 수준 https://www.businesspost.co.kr/BP?command=article_view&num=353613\n[한국경제인협회] 니켈, 리튬, 코발트에 대한 주요 생산국 및 수입량 추이 https://www.fki.or.kr/main/news/statement_detail.do?bbs_id=00035573&category=ST\n\n\n\n\n\n\n결론\n\n추가적인 분석에 따라 달라지겠지만 현재로서는 아래와 같음\n니켈\n\n4차 핵심사업에 소요되는 광물로 2011년 대비 2배이상 소요량 증가\n생산량이 증가하고 있으나 주요 3개국 생산량이 58.3%로 편중으로 인한 리스크 큼(생산국:인도네시아, 중국, 러시아)\n자료 시기가 2021, 2022년으로 많지는 않지만, 수입량의 감소에도 수입금액이 늘었다는 것은 가격 상승이 예상되는 상황\n소요량, 생산량, 수입데이터 기준 가격상승 예상 등 리스크가 있어보여 위험요소 탐지가 필요한 광물로 보임\n\n코발트\n\n수입량은 급격히 증가했으나, 수출량은 전무한 상황으로 생산자체가 불가하거나 국내 사용만으로도 부족한 상황이 아닐지 추측\n간단히 구글링하였을 때, 기사에 따르면 코발트 생산은 콩고 생산량이 68.8%일 정도로 편중이 심함\n코발트 또한 이차 전지에 활용되는 핵심광물로 위험 요소 탐지가 필요해보임\n\n니켈에 대해 분석한 내용을 모델링할 때 감안하거나, 최종보고서의 핵심광물 위험요소에 대한 당위성 부여 등에 활용하고자 함"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "",
    "text": "스터디 진행하며 진행한 과제 기록(MNIST, Neural Network)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html#개요",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "개요",
    "text": "개요\n참여중인 딥러닝 스터디 3주차 기록입니다.\n\nNeural Network로 MNIST다루기\n강사님이 주신 샘플코드 참고해서, 나에게 맞추거나 추가공부 진행"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_2/index.html#과제-작성-neuralnetwork-nonlinear",
    "href": "posts/meta-dl-creditcard-20240615_2/index.html#과제-작성-neuralnetwork-nonlinear",
    "title": "[M_Study_3주차과제2] Neural Network로 MNIST다루기",
    "section": "과제 작성 (NeuralNetwork / NonLinear)",
    "text": "과제 작성 (NeuralNetwork / NonLinear)\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom keras.utils import to_categorical\n\nfrom tensorflow.keras.datasets import mnist\n\n\nMnist Dataset로딩 및 전처리\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nfor i in (x_train, y_train, x_test, y_test):\n    print(i.shape)\n\n(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n\n\n\n# Shape 오류 발생하여 원핫인코딩 수행\n# X는 (28, 28)인데 Y는 그냥 정답(5면 5)여서 그런듯 함\n# ValueError: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 10)\n\nx_train = x_train.reshape((60000, 28, 28, 1))\ny_train_one_hot = to_categorical(y_train, num_classes=10)\n\nx_test = x_test.reshape((10000, 28, 28, 1))\ny_test_one_hot = to_categorical(y_test, num_classes=10)\n\n\n\n모델구성\n\nCodestral에게 MNIST데이터셋에 적합한 파라미터와 레이어로 조정해달라고 하여 맞춤\n수업 때 거의 ReLU에요라고 들었는데 마지막 빼고는 거의 ReLU가 사용되었음\nSoftmax는 주로 마지막 층에 쓰인다고 들은 적이 있는데 여기서도 동일하게 되었음 (추가로 알아볼때도 주로 마지막레이어에 쓰인다는 내용 다수 확인)\n\n\n# 모델 구성\n# Mistral에게 Mnist데이터셋이 적합한 레이어와 파라메터로 구성해달라고 해서 조정\nmodel = Sequential([\n    layers.Input((28,28,1)),\n    layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n    layers.Dropout(0.25),\n    layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2,2)),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\nModel: \"sequential_14\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d_40 (Conv2D)              │ (None, 28, 28, 32)     │           320 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_52          │ (None, 28, 28, 32)     │           128 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_41 (Conv2D)              │ (None, 28, 28, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_53          │ (None, 28, 28, 64)     │           256 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_27 (MaxPooling2D) │ (None, 14, 14, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_39 (Dropout)            │ (None, 14, 14, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_42 (Conv2D)              │ (None, 14, 14, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_54          │ (None, 14, 14, 128)    │           512 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_28 (MaxPooling2D) │ (None, 7, 7, 128)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_40 (Dropout)            │ (None, 7, 7, 128)      │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_14 (Flatten)            │ (None, 6272)           │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (Dense)                │ (None, 256)            │     1,605,888 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_55          │ (None, 256)            │         1,024 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_41 (Dropout)            │ (None, 256)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (Dense)                │ (None, 10)             │         2,570 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 1,703,050 (6.50 MB)\n\n\n\n Trainable params: 1,702,090 (6.49 MB)\n\n\n\n Non-trainable params: 960 (3.75 KB)\n\n\n\n\n\n모델학습 및 학습과정 시각화\n\nhistory = model.fit(x_train, y_train_one_hot, epochs=10, batch_size=64, verbose=0)\nhistory.history\n\n{'accuracy': [0.9526000022888184,\n  0.9849333167076111,\n  0.9872333407402039,\n  0.9904000163078308,\n  0.9906499981880188,\n  0.9917166829109192,\n  0.9925500154495239,\n  0.9933333396911621,\n  0.9935333132743835,\n  0.9948333501815796],\n 'loss': [0.09701579809188843,\n  0.014779138378798962,\n  0.01159658282995224,\n  0.009130637161433697,\n  0.008214634843170643,\n  0.007283014710992575,\n  0.0065074339509010315,\n  0.005834747105836868,\n  0.005641818046569824,\n  0.004907044116407633]}\n\n\n\nplt.plot(history.history['loss'], label='Train loss')\nplt.xlabel('The number of learning')\nplt.ylabel('Cost')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n모델평가 및 모델 활용한 예측\n\n모델평가\n\n\nmodel.evaluate(x_test, y_test_one_hot, verbose=0)\n\n[0.003361478913575411, 0.994700014591217]\n\n\n\n예측\n\n\ny_predict_one_hot = model.predict(x_test)\ny_predict = np.argmax(y_predict_one_hot, axis=1)\ny_predict\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step\n\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=int64)\n\n\n\ny_test\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\n\n\n\n\n[추가] 예측치가 틀린값 추출 후 시각화해보기\n\n예측 틀린 값 확인\n\n\nimport pandas as pd\n\ndf_answersheet = pd.DataFrame(y_test, columns=['Y_test(정답)'])\ndf_answersheet['Y_pred(예측)'] = np.argmax(y_predict_one_hot, axis=1)\ndf_answersheet['비교'] = (df_answersheet['Y_test(정답)'] == df_answersheet['Y_pred(예측)'])\ndf_answersheet\n\n\n\n\n\n\n\n\nY_test(정답)\nY_pred(예측)\n비교\n\n\n\n\n0\n7\n7\nTrue\n\n\n1\n2\n2\nTrue\n\n\n2\n1\n1\nTrue\n\n\n3\n0\n0\nTrue\n\n\n4\n4\n4\nTrue\n\n\n...\n...\n...\n...\n\n\n9995\n2\n2\nTrue\n\n\n9996\n3\n3\nTrue\n\n\n9997\n4\n4\nTrue\n\n\n9998\n5\n5\nTrue\n\n\n9999\n6\n6\nTrue\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_answersheet['비교'].value_counts()\n\n비교\nTrue     9947\nFalse      53\nName: count, dtype: int64\n\n\n\n예측 틀린 값들의 시각화 및 정답/예측치 비교\n\n\nidx_false = df_answersheet[df_answersheet['비교'] == False].index\nidx_false\n\nIndex([ 445,  449,  947, 1014, 1232, 1242, 1247, 1709, 1878, 1901, 2035, 2070,\n       2118, 2130, 2135, 2414, 2454, 2597, 2654, 2896, 2939, 2953, 3422, 3520,\n       3808, 3985, 4027, 4176, 4284, 4571, 4639, 4699, 4740, 4761, 5749, 5955,\n       6571, 6576, 6597, 6625, 8408, 9009, 9015, 9019, 9587, 9620, 9638, 9642,\n       9664, 9679, 9692, 9698, 9729],\n      dtype='int64')\n\n\n\nimport koreanize_matplotlib\n\n# 시각화로 Validation 확인\nfor i in idx_false.tolist():\n    plt.figure(figsize=(2,2))\n\n    # 결과 확인\n    plt.xlabel(f\"정답:{y_test[i]} | 예상:{y_predict[i]}\")\n    plt.imshow(np.reshape(x_test[i], [28, 28]), cmap=plt.cm.binary)\n    plt.show()"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html",
    "href": "posts/meta-dl-creditcard-20240602/index.html",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 2주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240602/index.html#개요",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "",
    "text": "참여중인 딥러닝 스터디 2주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#주차-과제-설명",
    "href": "posts/meta-dl-creditcard-20240602/index.html#주차-과제-설명",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "1주차 과제 설명",
    "text": "1주차 과제 설명\n\n큰 틀에서는 보통 아래의 순서로 진행\n\n데이터생성\nhypothesis\ncost function\noptimizer\ntrain\nPrediction (=Inference)\n\ntf.reduce_mean() : 열(row)끼리의 평균\nNon-Linear Modeling  &gt;\\(y = ax^2 + bx + c\\)\n\n구하고자 하는 값은 \\(a, b, c\\)\n모델 학습 전 임의의 값(\\(a,b,c\\))으로 추세선 긋기  \n내가 가정하는 식을 hypothesis에 넣고 학습\ndef hypothesis(x):\n  return a*(x)**2 + b*x + c\n\ndef cost_fn(pred_y, true_y):\n  return tf.reduce_mean(tf.square(pred_y - true_y))\n\noptimizer = tf.optimizers.Adam(learning_rate = 0.01)\n\ndef train():\n  with tf.GradientTape() as g:\n    pred = hypothesis(X)\n    cost = cost_fn(pred, Y)\n\n  gradients = g.gradient(cost, [a,b,c])                    # 기울기를 계산하는 부분\n  optimizer.apply_gradients(zip(gradients, [a,b,c]))       # 계산된 기울기를 업데이트 해주는 부분\n\nfor step in range(1,1001):\n  train()\n\n  if step % 100 == 0:\n    pred = hypothesis(X)\n    cost = cost_fn(pred, Y)\n    print(f\" step:{step} cost:{cost:.4f} a:{a.numpy()} b:{b.numpy()} c:{c.numpy()} \")\n    line_x = np.arange(min(X), max(X), 0.001)\n    line_y = a*(line_x)**2 + b*(line_x) + c\n모델 학습 후 변경된 추세선 확인하기"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#linear-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#linear-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear Regression의 Motivation\n\n수 많은 데이터를 묘사하는 하나의 선을 긋고, 그 선으로 향후 추이(예를 들어 광고비와 매출의 관계 등)를 판단\n랜덤하게 하나의 선을 긋고(임의의 W, b설정), 실제 데이터(점)과 선(예측값)의 거리[오차]를 줄인다\n\n선형성이 있다는 가정 하에 모델링 수행 &gt; \\(Y = B_0 + B_1X + \\epsilon\\)\n\n기존 수업의 \\(Y = WX+b\\) 식에서 \\(B_0\\)이 \\(b\\), \\(B_1X\\)이 \\(W\\)\n\n\\(B_0\\) : intercept (첫번째 기울기는 \\(B_1X\\)으로 표현)\n\\(B_1X\\) : slope, coefficients\n\\(B_0\\), \\(B_1X\\) 등을 합쳐서 parameter라고 부름 (가장 적절한 parameter를 찾는 것이 AI의 목적)\n\n\nhat (\\(\\hat{B}\\))은 최적화가 되었을 때 씌운다\n\n처음에 임의값을 두었다가 학습을 하며 최적화가 되면 씌움\n\nresidual(잔차)\n\n예측값 \\(\\hat{y}\\)(\\(\\hat{B}_0+\\hat{B}_1x_i\\))에 대해 정의된 오차 \\(e_i\\) (\\(y_i-\\hat{y_i}\\))\n\ni번째 잔차(\\(i\\)th residual)\n\n\nRSS(Residual Sum of Squares) : 잔차를 제곱해서 더한 것 &gt; \\(e_1^2 + e_2^2+...+e_n^2\\)\n\n제곱이 아닌 절대값으로 해도 개념적으로는 오차를 계산 가능\n최소자승법(least square)으로 최적의 \\(B\\)를 구하며 RSS가 줄어듦\n\n최소자승법 : 그래프에서 오차가 가장 낮은, 미분값이 0인 지점 찾기(오차가 0이 되는 지점을 미분으로 찾는 것이며, 이때의 오차는 Train set의 오차임)\n절대값으로 오차를 찾는 경우 미분을 활용할 수 없어, RSS를 사용\n현대에는 최소자승법을 잘 안쓰고 GD를 사용 (GD : 하나의 랜덤한 점을 찍고, 최소점을 향해 근사를 반복해나감[epoch반복])\n\n\n\\(B_0\\), \\(B_1X\\) 등 parameter를 구했을 때, 얼마나 신뢰할 수 있는 숫자인가\n\nStandard error(Variance에 루트를 씌우면 Standard error로, 본질적으로 같음)\n\nVariance는 모델의 안정성\nStandard error, \\(SE(\\hat{B}_1)^2\\) 계산을 통해 얼마나 안정적인지 판단\nSE의 계산식은 데이터(샘플)가 많아질수록 분모(Sum)가 커지므로, 모델의 Variance가 낮아짐\n\n→ 데이터가 많아지면 모델이 좋아진다 (최적의 \\(B\\)를 최소자승법으로 구해 모델을 만들고, 데이터 샘플의 수가 커지면 계산된 최적의 \\(B\\)가 가지는 SE가 줄어들음)\n\n\nConfidence intervals(신뢰구간)\n\n대학원 면접에서 많이 나오는 주제\nVariance(또는 SE)를 기반으로 신뢰구간을 구함 &gt; \\(\\hat{B}_1 +- 2*SE(\\hat{B}_1)\\)\n신뢰구간 95% / 평균 100 / 신뢰구간 80~120의 해석\n\n모집단에서 샘플링을 했을 때 평균이 100\n모집단 평균이 105, 110일수도 있지만 추정한 신뢰구간안에 실제 평균이 존재할 확률이 95%라는 뜻\n\nVariance가 낮아지면 신뢰구간이 좁아지며 좋아짐\n\n\nt-statistic\n\n$ t = _1-0 SE(_1) $\nSE가 낮을수록 좋은 값이라는 점에서, 위 수식(t값)이 클수록 좋다는 직관적 이해 가능\np-value : t값보다 클 확률 (즉 낮을수록 좋다)\n\n결과 표 보며 이해하기  \n\ncoefficient : 구한 parameter값\nStandard error, t-statistic은 단위에 따라 달라질 수 있는 값\np-value는 통일된 값으로, 0.0001보다 작으므로 Variance가 낮고 모델이 안정적이다 (p-value가 낮을수록 결과가 유의하다 라고 표현)\n\n\\(R^2\\)\n\n$ R^2$ = $ TSS - RSS TSS $ = $1 - RSS TSS $\nRSS(오차의 제곱을 모두 더한 잔차)가 클수록 작아지므로, 클수록 좋은 지표임을 이해\nTSS는 RSS보다 무조건 크므로, \\(R^2\\)는 0~1의 값을 갖는다\n\\(R^2\\)가 0.7이면 70%의 설명력을 가진다\n\n주의할 점\n\n상관관계에 대해 분석한 것으로, 인과관계가 아니다 (인과관계라면 상관관계는 있을 수 있다)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#multiple-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#multiple-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\n변수와 계수를 추가하면 Multiple Regression이 됨\n\n수식이 길어지고, * 편미분이 여러개가 나오게 되는 차이\n변수가 2개가 되어 기존의 2차원평면/regression line이 아닌 3차원평면/regression plain이 나옴\nmultiple matrix를 활용해 표현 (row, column)\nRSS도 행렬식으로 표현\n\n역행렬은 엄청난 연산이 필요한데, 이는 least square를 사용하지 않게된 이유임 (정형데이터 위주였던 과거와 달리 이미지 등 데이터와 이에 대한 행렬이 매우 커져, computing성능발전에도 역행렬 계산 어려워짐)\nleast square는 Bias를 최소화하는 방법인데, trade-off로 Variance커질 위험이 커서 사용하지 않게 됨\n\n현대는 모델의 Variance가 중요해짐. Overfitting과 관계된 Variance를 줄이기 위해 약간의 Bias상승은 감내\nLeast square(최소자승법)는 오차가 0이되는 지점을 미분으로 찾는 방법이며, 이때의 오차는 Train Dataset으로 오차가 너무 작아지면 Overfitting\n\n\n\nCorrelation 상관관계\n\nVariable간의 상관관계를 보는 이유 : 중복되는 변수의 과대평가, 타 변수의 과소평가를 방지 (예를 들어 연봉과 자산의 경우 상관관계가 있음)\n\nGD(Gradient Descent, 경사하강법)\n\n역행렬의 계산량문제로 최소자승법(least square)가 아닌 GD방식을 사용\n\\(a\\) (learning rate)로 적절히 작은 숫자를 곱해, 한번에 너무 많이 이동(하강)하지 않게 함\n\n보통 0.01사용\n\n$W = W - a * $ $ c W$\n코드로 이해하기 \n\nCost를 가중치W로 미분 (gradient 함수로 $ c W$구하기)\n\n  # gradient 계산\n  gradients = g.gradient(cost, [W,b])     \n\n가중치W 업데이트 (apply_gradients 함수로 \\(W\\) 업데이트)\n\n  # gradients에 따라 W와 b 업데이트\n  optimizer.apply_gradients(zip(gradients, [W,b])) \n\nSingle Regression에서는 없던 Multiple Regression의 고민\n\n변수의 갯수 (많이 쓴다고 좋은게 아니며, 최적의 갯수 찾기)\n\nforward(↔︎backward) selection\n\n변수의 갯수를 늘려가다가(↔︎줄여가다가), 성능이 낮아질때 직전 갯수로 사용\n\n변수의 갯수가 달라졌을 때 성능의 지표\n\nAIC(Akaike Information Criterion)\nBIC(Bayesian Information Criterion)\nAdjusted \\(R^2\\)\nCV(Cross-validation)\n\n\n요즘은 Linear Regression에서 발전된 알고리즘이 많이 나와서, 위의 것보다 먼저 알아야 하는게 많음\n\nQualitative Predictor(Categorical, 범주형 변수)\n\n숫자가 아닌 상태로 쓰이는 경우 (0남자 1여자 등, 여러개도 Okay) 활용하여 Regression에 반영할 수 있게 됨\n\n예를 들어, 구하고자하는 y가 키(신장)이라면, 성별변수 남자가 0이면 가중치는 음수, 반대면 양수가 될 수 있음\n\n\nInteraction impact(Synergy impact)\n\n변수가 구하고자하는 y가 아니라 다른 변수에도 영향을 미치는 경우\n\n서로 관계있는 두 변수를 곱하여 추가해줌(\\(X^1, X^2\\)가 관계있는 경우 \\(X^1*X^2\\)라는 변수로)\n결과 표로 이해하기  \n\nradio의 p-value가 유의하지 않아 변수제거를 했을 때, 파생변수인 radio*TV는 어떻게 할까?\n\n제거한다 (오리지널 변수를 제거한다면 파생변수도 제거, 오리지널이 있을때만 사용가능)\nHierarchy principle : 파생변수가 존재하려면 오리지널 변수도 있어야 함\n\nCoefficient 값 기준으로 radio가 TV보다 더 중요한 변수인가?\n\nCoefficient는 단위(unit)를 간과하므로, 높다고 해서 반드시 중요한 것은 아니다\n\n\n\n변수(feature)의 관계는 독립적인게 좋음\n\nNon-linearity의 Multiple regrssion 활용한 표현\n\n예를 들어 전반부는 정비례 / 후반부는 반비례 한다면\n\n\\(B_1 *나이 + B_2 * 나이^2\\) 와 같이 표현 가능\n위의 표현식은 변수의 독립성을 저해하는가?(사용해도 되나?)\n\n서로 다른관계를 묘사하는 것이라면 사용 가능\n\n연봉, 자산 모두 대출점수의 양의 상관관계라면 사용 불가\n\\(B_1 *나이\\)는 양의 상관관계, \\(B_2 * 나이^2\\)는 음의 상관관계라면 사용 가능\n\n\n\n다만 변수가 많아지면 한계가 있으므로, 비선형에 적합한 타 모델이 더 좋음\n\n상관관계를 빠르게 파악하는 법 : 모든 산점도(Scatter plot)를 그려보기\n\n미리 파악하여 상관관계가 있는 것을 빼고 모델링하면 더 좋은 결과가 나타남\n중복변수가 들어가거나 하더라도, Robust한 모델을 사용하는 것도 좋은 방법"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#logistic-regression",
    "href": "posts/meta-dl-creditcard-20240602/index.html#logistic-regression",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLinear regression으로 현재까지 예측문제를 풀었다면, Logistic regression으로는 분류문제를 풀 수 있다\n머신러닝으로 푸는 큰 3가지 문제 : 예측 vs 분류 vs 클러스터링\n\n분류 : 메일이 왔을 때 스팸인가 아닌가, 신용카드승인내역이 이상거래인가 정상거래인가\n\nHyphothesis의 반영\n\n기존의 Linear regression(\\(y=Wx+b\\))의 식으로는 놓치는 case가 많이 생김\nLogistic(Sigmoid) function을 활용하여 해결 가능  \n\nz가 아무리 커지거나 작아져도 0~1사이에서 벗어나지 않음\n0.5를 기준으로 판단가능 (Pass/Fail, 스팸/정상 등)\nz부분에 기존에 데이터를 학습했던 Linear regression식(\\(y=Wx+b\\))을 넣으면(plug-in), 분류문제를 푸는 함수로 바꿈\n값이 0.7이 나온다면, Pass확률이 70%인 것으로 해석가능\n\n\nDescision boundary 경계영역\n\n예를 들어 2가지 Case를 분류하는 Linear Line이 있다면, 그것이 Descision boundary (Non Linear한 경우라도 분류문제를 풀 수 있고, 그 Line은 Descision boundary)\n\nCost function에서는 문제가 생김\n\nSigmoid function으로 간편하게 분류문제를 푸는 함수로 바꿨지만, 기존처럼 미분을 하면 문제 발생\n\n값이 조금만 크거나 작아져도 기울기가 0이 됨 → 기존의 Mean Square방식 적용 불가  \nGD 사용시 최적 지점까지 가지 못하는 경우 발생\n\nCross entropy를 사용하게 됨\n  [Cost function으로 사용되기 위한 2가지 조건]\n  1. 클수록 나쁘고 작을수록 좋아야 함\n  2. 미분이 가능해야 함\n    (미분이 안되는 경우 : 평평한 부분이 있거나, 위아래로 변동이 큰 구간이 많은 경우)\n\n  * 두 조건을 충족하는 것은 쉽지 않으며, Accuracy는 1번만 충족\n  * Cross entropy는 위의 2개 조건을 모두 충족\n\n2가지 상황(y=0[pass], y=1[fail])에 대한 상황에 대해 다른 식 사용\n\n필요시 하나의 식으로도 나타낼 수 있음 (y값에 따라 한쪽 식이 0이 되는 형태)\n\n현대에서도 많이 쓰이는 함수인 Cross entropy (gpt4 등)\nGD경사하강법으로도 사용 가능"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240602/index.html#모형예측classification의-성과평가confusion-matrix",
    "href": "posts/meta-dl-creditcard-20240602/index.html#모형예측classification의-성과평가confusion-matrix",
    "title": "[M_Study_2주차] Multiple Regression / Logistic Regression",
    "section": "모형예측(Classification)의 성과평가(Confusion matrix)",
    "text": "모형예측(Classification)의 성과평가(Confusion matrix)\n\nAccuracy(정답률) : 실제값 중 맞춘 비율\n\n분류가 Imbalance한 경우 한계가 있음(신용카드 경우도 대다수가 정상거래) (예를 들어 암환자 비율이 90%이상일 때, 로직없이 그냥 암환자로만 판정해도 정답률 높음)\n\nPrecision(암으로 판정한 사람 중, 실제로 암) vs Recall(실제 암인 사람 중, 암으로 판정된 사람)\n\n암환자를 정상환자로 판단하는 것은 치명적\n암과 같은 케이스는 기본적으로 암으로 ’판단’하는 것이 많아져야하므로 Recall을 사용 (분자가 ’암 판정’인, 분자가 커질수록 점수가 높은 recall을 사용하는게 적절)\n프로젝트 특성에 따라 적합한 모델 뿐 아니라 적합한 지표를 사용하는 것이 중요\n\nPrecision이 더 중요한 케이스\n\n불량제품 하나를 검수하기 위해 1만개의 정상제품을 검사하면 비효율적\n1개의 불량제품을 감수하고 1만개를 살리는게 효율적\n\n분류가 balance한지 imbalance한지를 체크\nF1 score : Precision과 Recall의 조화평균\nConfusion matrix를 그래프로 visualize 해서 보여주면 더 설득에 용이"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240718/index.html",
    "href": "posts/meta-dl-creditcard-20240718/index.html",
    "title": "[MStudy_과제개선3] 신용카드 이상거래 탐지 모델링 with ML",
    "section": "",
    "text": "Kaggle CreditCard Fraud Detection (개선3 : optuna SearchCV, RandomForest)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240718/index.html#데이터셋-구성",
    "href": "posts/meta-dl-creditcard-20240718/index.html#데이터셋-구성",
    "title": "[MStudy_과제개선3] 신용카드 이상거래 탐지 모델링 with ML",
    "section": "데이터셋 구성",
    "text": "데이터셋 구성\n\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('creditcard.db')\ndf = pd.read_sql_query(\"SELECT * FROM creditcard\", conn)\nconn.close()\ndf\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284802\n172786.0\n-11.881118\n10.071785\n-9.834783\n-2.066656\n-5.364473\n-2.606837\n-4.918215\n7.305334\n1.914428\n...\n0.213454\n0.111864\n1.014480\n-0.509348\n1.436807\n0.250034\n0.943651\n0.823731\n0.77\n0\n\n\n284803\n172787.0\n-0.732789\n-0.055080\n2.035030\n-0.738589\n0.868229\n1.058415\n0.024330\n0.294869\n0.584800\n...\n0.214205\n0.924384\n0.012463\n-1.016226\n-0.606624\n-0.395255\n0.068472\n-0.053527\n24.79\n0\n\n\n284804\n172788.0\n1.919565\n-0.301254\n-3.249640\n-0.557828\n2.630515\n3.031260\n-0.296827\n0.708417\n0.432454\n...\n0.232045\n0.578229\n-0.037501\n0.640134\n0.265745\n-0.087371\n0.004455\n-0.026561\n67.88\n0\n\n\n284805\n172788.0\n-0.240440\n0.530483\n0.702510\n0.689799\n-0.377961\n0.623708\n-0.686180\n0.679145\n0.392087\n...\n0.265245\n0.800049\n-0.163298\n0.123205\n-0.569159\n0.546668\n0.108821\n0.104533\n10.00\n0\n\n\n284806\n172792.0\n-0.533413\n-0.189733\n0.703337\n-0.506271\n-0.012546\n-0.649617\n1.577006\n-0.414650\n0.486180\n...\n0.261057\n0.643078\n0.376777\n0.008797\n-0.473649\n-0.818267\n-0.002415\n0.013649\n217.00\n0\n\n\n\n\n284807 rows × 31 columns\n\n\n\n\ndf_x = df.drop(['Time', 'Class'], axis=1).copy()\ndf_y = df['Class'].copy()\n\ndf_x.shape, df_y.shape\n\n((284807, 29), (284807,))\n\n\n\nTrain, Validation, Test 나누기\n\nTrain, Test로만 나누고, optuna의 CV기능을 사용할 예정으로 별도 분할하지 않음\n\n\n# Train, Test 나누기\nfrom sklearn.model_selection import train_test_split\n\n# stratify 적용\nx_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, stratify=df_y)\n\nprint(f\"{x_train.shape}, {x_test.shape}\")\nprint(f\"{y_train.shape}, {y_test.shape}\")\nprint()\nprint(f\"y_train {y_train.value_counts()}\")\nprint(f\"y_test {y_test.value_counts()}\")\n\n(256326, 29), (28481, 29)\n(256326,), (28481,)\n\ny_train Class\n0    255883\n1       443\nName: count, dtype: int64\ny_test Class\n0    28432\n1       49\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240718/index.html#모델-구성-및-학습머신러닝",
    "href": "posts/meta-dl-creditcard-20240718/index.html#모델-구성-및-학습머신러닝",
    "title": "[MStudy_과제개선3] 신용카드 이상거래 탐지 모델링 with ML",
    "section": "모델 구성 및 학습(머신러닝)",
    "text": "모델 구성 및 학습(머신러닝)\n\nRandomForestClassifier with optuna(OptunaSearchCV)\n\noptuna OptunaSearchCV 공식문서\n\nhttps://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.OptunaSearchCV.html\n\noptuna OptunaSearchCV 샘플코드\n\nhttps://github.com/optuna/optuna-examples/blob/main/sklearn/sklearn_optuna_search_cv_simple.py\n\nScikit-learn RandomForestClassifier 공식문서\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#randomforestclassifier\n\nScikit-learn ccp_alpha(Pruning, [과적합방지용]가지치기)\n\nhttps://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n\n\n\nimport optuna\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=10) # n_estimators : number of trees\n\nparam_distributions = {\n    \"n_estimators\":optuna.distributions.IntDistribution(10, 10), # 트리의 수\n    \"max_depth\": optuna.distributions.IntDistribution(2, 32, log=True),\n    \"criterion\": optuna.distributions.CategoricalDistribution(['gini', 'entropy', 'log_loss']),\n    \"class_weight\" : optuna.distributions.CategoricalDistribution(['balanced', 'balanced_subsample']),\n    \"ccp_alpha\" : optuna.distributions.FloatDistribution(0, 0.05, step=0.005)\n}\n\noptuna_search = optuna.integration.OptunaSearchCV(\n    clf, \n    param_distributions, \n    n_jobs=-1, # Number of parallel jobs. -1 means using all processors.\n    cv=5, #  estimator가 classifier & label이 binary or multiclass라면 sklearn.model_selection.StratifiedKFold 적용 (이외는 sklearn.model_selection.KFold)\n    n_trials=100, \n    timeout=600, \n    verbose=2,\n    scoring='f1_weighted',\n    refit=True # Best Parameter로 refit. refitted estimator는 best_estimator_ attribute로 바로 predict가능\n)\n\nX, y = x_train, y_train\noptuna_search.fit(X, y)\n\nprint(\"Best trial:\")\ntrial = optuna_search.study_.best_trial\n\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nC:\\Users\\kibok\\AppData\\Local\\Temp\\ipykernel_20892\\2371441327.py:14: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n  optuna_search = optuna.integration.OptunaSearchCV(\n[I 2024-07-30 22:37:21,982] A new study created in memory with name: no-name-b5929343-b4ec-493f-9805-5e3c4e45dd7c\n[I 2024-07-30 22:37:44,249] Trial 1 finished with value: 0.9952872331714773 and parameters: {'n_estimators': 10, 'max_depth': 2, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.025}. Best is trial 1 with value: 0.9952872331714773.\n[I 2024-07-30 22:37:45,772] Trial 4 finished with value: 0.995843694897976 and parameters: {'n_estimators': 10, 'max_depth': 3, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.05}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:37:59,089] Trial 6 finished with value: 0.9904473457587857 and parameters: {'n_estimators': 10, 'max_depth': 5, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.015}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:01,804] Trial 3 finished with value: 0.9926729729394594 and parameters: {'n_estimators': 10, 'max_depth': 5, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.02}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:18,959] Trial 5 finished with value: 0.990931377564992 and parameters: {'n_estimators': 10, 'max_depth': 6, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.045}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:26,532] Trial 2 finished with value: 0.9916618565971218 and parameters: {'n_estimators': 10, 'max_depth': 24, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.025}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:27,286] Trial 7 finished with value: 0.9957148747000535 and parameters: {'n_estimators': 10, 'max_depth': 24, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.05}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:28,366] Trial 0 finished with value: 0.9923829210347611 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.035}. Best is trial 4 with value: 0.995843694897976.\n[I 2024-07-30 22:38:39,049] Trial 12 finished with value: 0.9970092145586555 and parameters: {'n_estimators': 10, 'max_depth': 2, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.02}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:38:47,101] Trial 10 finished with value: 0.9962438881186829 and parameters: {'n_estimators': 10, 'max_depth': 5, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:38:49,591] Trial 8 finished with value: 0.9934021476853907 and parameters: {'n_estimators': 10, 'max_depth': 25, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.04}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:38:54,134] Trial 9 finished with value: 0.993142523031211 and parameters: {'n_estimators': 10, 'max_depth': 32, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.035}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:06,825] Trial 11 finished with value: 0.9932623645614775 and parameters: {'n_estimators': 10, 'max_depth': 30, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.05}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:21,819] Trial 13 finished with value: 0.99478232737095 and parameters: {'n_estimators': 10, 'max_depth': 6, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.05}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:27,448] Trial 20 finished with value: 0.9952568348608214 and parameters: {'n_estimators': 10, 'max_depth': 2, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:31,517] Trial 15 finished with value: 0.9907314835923732 and parameters: {'n_estimators': 10, 'max_depth': 27, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.02}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:33,576] Trial 14 finished with value: 0.9942530678494605 and parameters: {'n_estimators': 10, 'max_depth': 17, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.015}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:46,802] Trial 16 finished with value: 0.9935641141470762 and parameters: {'n_estimators': 10, 'max_depth': 30, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.035}. Best is trial 12 with value: 0.9970092145586555.\n[I 2024-07-30 22:39:51,642] Trial 17 finished with value: 0.9994244713511794 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'entropy', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 17 with value: 0.9994244713511794.\n[I 2024-07-30 22:39:54,732] Trial 18 finished with value: 0.9994107978877771 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 17 with value: 0.9994244713511794.\n[I 2024-07-30 22:40:00,405] Trial 19 finished with value: 0.9994271656763747 and parameters: {'n_estimators': 10, 'max_depth': 13, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 19 with value: 0.9994271656763747.\n[I 2024-07-30 22:40:25,456] Trial 21 finished with value: 0.999434017386118 and parameters: {'n_estimators': 10, 'max_depth': 12, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 21 with value: 0.999434017386118.\n[I 2024-07-30 22:40:31,434] Trial 23 finished with value: 0.9962951737422998 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 21 with value: 0.999434017386118.\n[I 2024-07-30 22:40:31,957] Trial 22 finished with value: 0.9965979604608748 and parameters: {'n_estimators': 10, 'max_depth': 12, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 21 with value: 0.999434017386118.\n[I 2024-07-30 22:40:34,465] Trial 24 finished with value: 0.9994337632833448 and parameters: {'n_estimators': 10, 'max_depth': 9, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 21 with value: 0.999434017386118.\n[I 2024-07-30 22:40:52,562] Trial 25 finished with value: 0.9994408540197803 and parameters: {'n_estimators': 10, 'max_depth': 12, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 25 with value: 0.9994408540197803.\n[I 2024-07-30 22:40:54,683] Trial 26 finished with value: 0.996698106283177 and parameters: {'n_estimators': 10, 'max_depth': 10, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 25 with value: 0.9994408540197803.\n[I 2024-07-30 22:40:57,905] Trial 27 finished with value: 0.9994515611974206 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:08,439] Trial 28 finished with value: 0.9994120771228445 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:35,264] Trial 29 finished with value: 0.9966293246541232 and parameters: {'n_estimators': 10, 'max_depth': 11, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:37,240] Trial 32 finished with value: 0.9943959629024558 and parameters: {'n_estimators': 10, 'max_depth': 8, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:38,829] Trial 30 finished with value: 0.9970195292623012 and parameters: {'n_estimators': 10, 'max_depth': 10, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:39,059] Trial 31 finished with value: 0.9934055423774755 and parameters: {'n_estimators': 10, 'max_depth': 15, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:56,868] Trial 33 finished with value: 0.99282882500281 and parameters: {'n_estimators': 10, 'max_depth': 8, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:57,826] Trial 34 finished with value: 0.9945005228300087 and parameters: {'n_estimators': 10, 'max_depth': 8, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:41:59,958] Trial 35 finished with value: 0.9933363510923483 and parameters: {'n_estimators': 10, 'max_depth': 8, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:42:17,745] Trial 36 finished with value: 0.993721820632459 and parameters: {'n_estimators': 10, 'max_depth': 18, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:42:41,309] Trial 38 finished with value: 0.9929849780466793 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:42:45,541] Trial 37 finished with value: 0.9929940457138292 and parameters: {'n_estimators': 10, 'max_depth': 19, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:42:46,614] Trial 39 finished with value: 0.9943285703735368 and parameters: {'n_estimators': 10, 'max_depth': 18, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:42:51,170] Trial 40 finished with value: 0.9931415390179374 and parameters: {'n_estimators': 10, 'max_depth': 19, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:43:06,667] Trial 41 finished with value: 0.9933606814925435 and parameters: {'n_estimators': 10, 'max_depth': 19, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.01}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:43:07,777] Trial 42 finished with value: 0.9994489609295651 and parameters: {'n_estimators': 10, 'max_depth': 18, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 27 with value: 0.9994515611974206.\n[I 2024-07-30 22:43:08,669] Trial 43 finished with value: 0.999488225721899 and parameters: {'n_estimators': 10, 'max_depth': 18, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:43:27,259] Trial 44 finished with value: 0.9924828083378433 and parameters: {'n_estimators': 10, 'max_depth': 19, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.03}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:43:48,513] Trial 45 finished with value: 0.992049590337098 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.03}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:43:50,422] Trial 46 finished with value: 0.9932099601175659 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.015}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:43:51,611] Trial 47 finished with value: 0.9993886440078258 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:43:58,958] Trial 48 finished with value: 0.9994401064500785 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:07,137] Trial 50 finished with value: 0.9929105605924612 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.03}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:07,476] Trial 51 finished with value: 0.9919062228404238 and parameters: {'n_estimators': 10, 'max_depth': 13, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.03}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:13,069] Trial 49 finished with value: 0.9993790258572499 and parameters: {'n_estimators': 10, 'max_depth': 13, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:28,940] Trial 52 finished with value: 0.9993922088126771 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:46,614] Trial 53 finished with value: 0.9994186582024277 and parameters: {'n_estimators': 10, 'max_depth': 14, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:52,610] Trial 54 finished with value: 0.9994145698407128 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'gini', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:44:53,677] Trial 55 finished with value: 0.9966165268172439 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:03,419] Trial 56 finished with value: 0.9965818304821432 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:08,608] Trial 58 finished with value: 0.9994701188866102 and parameters: {'n_estimators': 10, 'max_depth': 21, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:11,718] Trial 57 finished with value: 0.9972321227337474 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:16,902] Trial 59 finished with value: 0.9965227246171346 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:35,122] Trial 60 finished with value: 0.9967497598327621 and parameters: {'n_estimators': 10, 'max_depth': 24, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:48,926] Trial 61 finished with value: 0.9965924076680792 and parameters: {'n_estimators': 10, 'max_depth': 22, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:49,771] Trial 63 finished with value: 0.9968081847020078 and parameters: {'n_estimators': 10, 'max_depth': 7, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:55,545] Trial 64 finished with value: 0.9966869244888036 and parameters: {'n_estimators': 10, 'max_depth': 6, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:45:57,437] Trial 62 finished with value: 0.9969938454559518 and parameters: {'n_estimators': 10, 'max_depth': 21, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:00,484] Trial 65 finished with value: 0.9966459887651427 and parameters: {'n_estimators': 10, 'max_depth': 6, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:11,828] Trial 68 finished with value: 0.9974972425443761 and parameters: {'n_estimators': 10, 'max_depth': 4, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:13,138] Trial 66 finished with value: 0.9966811021312786 and parameters: {'n_estimators': 10, 'max_depth': 26, 'criterion': 'log_loss', 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.005}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:17,972] Trial 67 finished with value: 0.9942535796236317 and parameters: {'n_estimators': 10, 'max_depth': 27, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.04}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:50,434] Trial 69 finished with value: 0.999467911342283 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:51,833] Trial 70 finished with value: 0.9994470603825087 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:46:58,637] Trial 71 finished with value: 0.9994393427930474 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:00,805] Trial 72 finished with value: 0.9994521021059237 and parameters: {'n_estimators': 10, 'max_depth': 27, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:01,833] Trial 73 finished with value: 0.9994418358442221 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:10,673] Trial 75 finished with value: 0.9994360984204311 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:13,758] Trial 74 finished with value: 0.999452858045289 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:20,926] Trial 76 finished with value: 0.9994684374402942 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:52,626] Trial 78 finished with value: 0.9994431588645505 and parameters: {'n_estimators': 10, 'max_depth': 17, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:47:52,704] Trial 77 finished with value: 0.9994565806201864 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:01,973] Trial 79 finished with value: 0.9994605390044315 and parameters: {'n_estimators': 10, 'max_depth': 16, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:02,771] Trial 80 finished with value: 0.9994633300944326 and parameters: {'n_estimators': 10, 'max_depth': 29, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:02,865] Trial 81 finished with value: 0.999438774915282 and parameters: {'n_estimators': 10, 'max_depth': 32, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:11,875] Trial 82 finished with value: 0.999426730915667 and parameters: {'n_estimators': 10, 'max_depth': 31, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:14,934] Trial 83 finished with value: 0.999434660837624 and parameters: {'n_estimators': 10, 'max_depth': 32, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n[I 2024-07-30 22:48:19,116] Trial 84 finished with value: 0.9994468408750077 and parameters: {'n_estimators': 10, 'max_depth': 25, 'criterion': 'log_loss', 'class_weight': 'balanced', 'ccp_alpha': 0.0}. Best is trial 43 with value: 0.999488225721899.\n\n\nBest trial:\n  Value:  0.999488225721899\n  Params: \n    n_estimators: 10\n    max_depth: 18\n    criterion: log_loss\n    class_weight: balanced\n    ccp_alpha: 0.0\n\n\n\nAttributes(Best Parameter, Scorer, Best estimator[Fitted])\n\nBest Parameter\n\n\noptuna_search.best_params_\n\n{'n_estimators': 10,\n 'max_depth': 18,\n 'criterion': 'log_loss',\n 'class_weight': 'balanced',\n 'ccp_alpha': 0.0}\n\n\n\nScorer\n\n\noptuna_search.scorer_\n\nmake_scorer(f1_score, response_method='predict', pos_label=None, average=weighted)\n\n\n\nBest estimator[Fitted]\n\n\nbest_model_randomforest = optuna_search.best_estimator_\nbest_model_randomforest\n\nRandomForestClassifier(class_weight='balanced', criterion='log_loss',\n                       max_depth=18, n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(class_weight='balanced', criterion='log_loss',\n                       max_depth=18, n_estimators=10) \n\n\n\n\n모델평가\n\nScikit-learn cross_val_score 공식문서\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score\n\nScikit-learn f1_score 공식문서 (적용한 weighted f1 score에 대한 설명)\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n‘weighted’ : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance it can result in an F-score that is not between precision and recall.\n\n\n\nimport sklearn.model_selection\n\n\nsklearn.model_selection.cross_val_score(best_model_randomforest, x_test, y_test, scoring='f1_weighted', cv=5, \n                                        n_jobs=None, verbose=0)\n\narray([0.99921023, 0.99962696, 0.99945948, 0.99866279, 0.99897608])\n\n\n\nsklearn.model_selection.cross_val_score(best_model_randomforest, x_test, y_test, scoring='f1_macro', cv=5, \n                                        n_jobs=None, verbose=0)\n\narray([0.87482422, 0.9374121 , 0.94991206, 0.81548173, 0.78545062])\n\n\n\nsklearn.model_selection.cross_val_score(best_model_randomforest, x_test, y_test, scoring='accuracy', cv=5, \n                                        n_jobs=None, verbose=0)\n\narray([0.99912235, 0.99982444, 0.99964888, 0.99877107, 0.99929775])\n\n\n\nfrom sklearn.metrics import classification_report \nprint(classification_report(y_test, best_model_randomforest.predict(x_test)))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     28432\n           1       0.97      0.78      0.86        49\n\n    accuracy                           1.00     28481\n   macro avg       0.99      0.89      0.93     28481\nweighted avg       1.00      1.00      1.00     28481"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html",
    "href": "posts/prgms-sql-20240321/index.html",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#개요",
    "href": "posts/prgms-sql-20240321/index.html#개요",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#문제-3월에-태어난-여성-회원-목록-출력하기",
    "href": "posts/prgms-sql-20240321/index.html#문제-3월에-태어난-여성-회원-목록-출력하기",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "문제 : 3월에 태어난 여성 회원 목록 출력하기",
    "text": "문제 : 3월에 태어난 여성 회원 목록 출력하기\n\n\n다음은 식당 리뷰 사이트의 회원 정보를 담은 MEMBER_PROFILE 테이블입니다. MEMBER_PROFILE 테이블은 다음과 같으며 MEMBER_ID, MEMBER_NAME, TLNO, GENDER, DATE_OF_BIRTH는 회원 ID, 회원 이름, 회원 연락처, 성별, 생년월일을 의미합니다.\n\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nMEMBER_ID\n\n\nVARCHAR(100)\n\n\nFALSE\n\n\n\n\nMEMBER_NAME\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nTLNO\n\n\nVARCHAR(50)\n\n\nTRUE\n\n\n\n\nGENDER\n\n\nVARCHAR(1)\n\n\nTRUE\n\n\n\n\nDATE_OF_BIRTH\n\n\nDATE\n\n\nTRUE\n\n\n\n\n\n\n문제\n\n\nMEMBER_PROFILE 테이블에서 생일이 3월인 여성 회원의 ID, 이름, 성별, 생년월일을 조회하는 SQL문을 작성해주세요. 이때 전화번호가 NULL인 경우는 출력대상에서 제외시켜 주시고, 결과는 회원ID를 기준으로 오름차순 정렬해주세요.\n\n\n\n예시\n\n\nMEMBER_PROFILE 테이블이 다음과 같을 때\n\n\n\n\n\nMEMBER_ID\n\n\nMEMBER_NAME\n\n\nTLNO\n\n\nGENDER\n\n\nDATE_OF_BIRTH\n\n\n\n\n\n\njiho92@naver.com\n\n\n이지호\n\n\n01076432111\n\n\nW\n\n\n1992-02-12\n\n\n\n\njiyoon22@hotmail.com\n\n\n김지윤\n\n\n01032324117\n\n\nW\n\n\n1992-02-22\n\n\n\n\njihoon93@hanmail.net\n\n\n김지훈\n\n\n01023258688\n\n\nM\n\n\n1993-02-23\n\n\n\n\nseoyeons@naver.com\n\n\n박서연\n\n\n01076482209\n\n\nW\n\n\n1993-03-16\n\n\n\n\nyoonsy94@gmail.com\n\n\n윤서연\n\n\nNULL\n\n\nW\n\n\n1994-03-19\n\n\n\n\n\nSQL을 실행하면 다음과 같이 출력되어야 합니다.\n\n\n\n\n\nMEMBER_ID\n\n\nMEMBER_NAME\n\n\nGENDER\n\n\nDATE_OF_BIRTH\n\n\n\n\n\n\nseoyeons@naver.com\n\n\n박서연\n\n\nW\n\n\n1993-03-16\n\n\n\n\n\n\n주의사항\n\n\nDATE_OF_BIRTH의 데이트 포맷이 예시와 동일해야 정답처리 됩니다.\n\n\n\n작성답안\n\n\n\nSELECT MEMBER_ID, MEMBER_NAME,  GENDER, TO_CHAR(DATE_OF_BIRTH, 'YYYY-MM-DD')\nFROM MEMBER_PROFILE\nWHERE GENDER = 'W'\n  AND TO_CHAR(DATE_OF_BIRTH, 'MON') = 'MAR'\n  AND TLNO IS NOT NULL\nORDER BY MEMBER_ID ASC\n\n\nFigure 1\n\n\n\n\n\n정리\n\nWHERE 컬럼명 IS NOT NULL\nWHERE NOT 컬럼명 IS NULL"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#작성답안",
    "href": "posts/prgms-sql-20240321/index.html#작성답안",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT MEMBER_ID, MEMBER_NAME,  GENDER, TO_CHAR(DATE_OF_BIRTH, 'YYYY-MM-DD')\nFROM MEMBER_PROFILE\nWHERE GENDER = 'W'\n  AND TO_CHAR(DATE_OF_BIRTH, 'MON') = 'MAR'\n  AND TLNO IS NOT NULL\nORDER BY MEMBER_ID ASC\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240321/index.html#정리",
    "href": "posts/prgms-sql-20240321/index.html#정리",
    "title": "[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기",
    "section": "정리",
    "text": "정리\n\nWHERE 컬럼명 IS NOT NULL\nWHERE NOT 컬럼명 IS NULL"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 6주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#하이퍼-파라미터",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#하이퍼-파라미터",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "하이퍼 파라미터",
    "text": "하이퍼 파라미터\n\n하이퍼 파라미터 튜닝 : 모델의 초기 설정값을 최적의 값으로 구하는 것\n\n\n하이퍼 파라미터 - RF(Random Forest, Bagging)\n\n주요 설정값\n\nn_estimators : 트리의 수 (Default=100)\n\n증가할수록 계산비용 & 성능 증가 (일정수준부터는 크게 상승하지 않음)\n\n↔︎ 감소할수록 계산비용 감소. 그러나 과소적합 발생할 가능성\n\n(경험적으로)천 단위에서 마감하는 것이 좋음(만 단위에서 유의미한 성능향상 X)\n\nmax_depth : 트리의 최대 깊이 (Default=None)\n\n증가할수록 모델의 복잡도 & 과적합가능성 증대(복잡한 패턴을 익힐 수 있음)\n\n↔︎ 감소할수록 과소적합 발생할 수 있음\n\n\nmin_samples_split : 노드를 분할하기 위한 최소 샘플 수 (Default=2)\n\n증가할수록 과적합 방지 & 성능 감소\n\n↔︎ 감소할수록 과적합 위험이 커짐(트리가 얕아짐짐)\n\n\nmin_samples_leaf : 분할이 모두 끝난 노드(리프노드)의 최소 샘플 크기 (Default=1)\n\n증가할수록 과적합 방지 & 성능 감소\n\n↔︎ 감소할수록 과적합 위험이 커짐\n\n\nmax_features : 각 트리를 학습할 때 사용할 feature의 비율 (Default=1.0)\n\n증가할수록 성능/계산비용용 향상 & 과적합 확률 증가\n\n\n\n\n\n하이퍼 파라미터 - XGB(XGBoost, Boosting)\n\n주요 설정값\n\nn_estimators : 부스팅 단계의 수 (Default=100)\n\nRF와 뜻은 다르지만 양상은 비슷함\n\n클수록 계산비용/성능 향상(↔︎과소적합). 천단위 마감\n\n\nmax_depth : 트리의 최대 깊이 (Default=6)\n\n증가할수록 과적합 위험 증대\n\nLearning_rate(eta) : 부스팅 단계에서 학습률을 조절 (Default=0.3)\n\n배깅이 아닌 부스팅이므로, 학습률 개념이 있음\n증가할수록 학습속도가 빨라지고, 초반 데이터에 가중치(+최적해를 놓칠 위험 있음)\n\n↔︎ 낮을수록 느리지만 최적해에 안정적으로 수렴\n\n\nsubsample : 각 단계에서 사용할 데이터 샘플[row]의 비율 (Default=1.0)\n\n증가할수록 과적합방지 & 성능적 단점\n\ncolsample_bytree : 각 트리를 학습할 때 사용할 feature[column]의 비율 (Default=1.0)\n\n증가할수록 성능/비용 증가 & 과적합 위험\n\ngamma : 노드 분할시 필요한 최소 손실 감소량 (Default=0)\n\n0인 경우, Loss감소가 없더라도 성능에 부정적이지 않는다면 분할\n증가할수록 노드 분할을 엄격히 수행(과적합 방지, 성능 하향)\n\n\n\n\n\n하이퍼 파라미터 - LGBM(Light GBM)\n\n주요 설정값\n\n타 모델과 유사\n\nn_estimators[Default=100], learning_rate[Default=0.1], subsample(bagging_fraction, bagging) [Default=1.0],\nFeature_fraction(colsample_bytree)[Default=1.0], max_depth[Default=-1]\n\nNum_leaves : 하나의 트리가 가질 수 있는 최대의 (분할이 끝난)리프 노드 수(Default=31)\n\n값이 증가할수록 트리가 복잡 & 과적합 & 메모리 사용 증가\n\nMin_data_in_leaf(min_child_samples) : 리프 노드의 최소 샘플 수 (Default=20)\n\n\n\n\n하이퍼 파라미터 - Catboost\n\n주요 설정값\n\n타 모델과 유사\n\nn_estimators[Default=100], learning_rate[Default=0.03], subsample[Default=None], Feature_fraction[Default=None]\n\nDepth : 트리의 최대 깂이 (Default=-1)\nBagging_temperature : 샘플링의 무작위성을 제어 (Default=1.0)\n\n높을수록 샘플링이 다양해지며 일반화 성능 향상 (속도 느려짐)\n\nL2_leaf_reg : 리프 노드의 가중치에 부여하는 패널티의 정도 (Default=3)\n\n클수록 과적합 방지"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#optuna",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#optuna",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "optuna",
    "text": "optuna\n\noptuna란\n\n기존의 Tuning Tools\n\nGrid Search : 오래걸리지만 Grid에 적절한 값이 없으면 최적값을 찾지 못함\nRandom Search : 위의 Grid Search 문제를 해결해도 최적값을 꼭 찾지 못함\n\noptuna vs hyperopt\n\n25년 1월 기준, github star 기준으로 optuna 11.3k &gt; hyperopt 7.3k\n\noptuna\n\n정의한 목적함수를 기반으로 최적화를 쉽게 진행\n\n베이지안 최적화와 유사한 TPE알고리즘을 사용한 탐색을 지행(그 외의 다양한 전략도 커스터마이징 가능)\n\n\n\n\n\noptuna 구성요소와 작동방식\n\n구성요소\n\nStudy : 최적화 과정 전체를 관리. 각 하이퍼 파라미터 탐색(Trial)의 결과를 저장\nSearch Space : 하이퍼 파라미터의 범위 지정\nObjective function : 목적함수. 모델을 학습시키고 평가지표를 반환\nTrial : 하이퍼파라미터의 조합을 나타내는 단위 (1번의 Trial로 목적함수를 실행)\n\n작동방식\n\nSearch Space(탐색공간)에서 int/float/categorical 등을 정의\nObjective function(목적함수)를 실행해 모델학습하고 성능지표 반환\n정의된 조합을 효율적으로 선택해, 반복적으로(Trial) 성능 개선\n최적의 하이퍼파라미터와 정보를 Study객체에 저장\n\n참고\n\nSearch Space에서 정의한 범위에서 조합하므로, 그 밖의 범위에서 최적해가 있으면 찾을 수 없음"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#optuna-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#optuna-실습",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "optuna 실습",
    "text": "optuna 실습\n\n데이터 로딩\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, log_loss\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\ndata = pd.read_csv(\"data_preprocessed.csv\")\n\n\n\nLGBM Vanila모델\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\nX = data.drop(columns=[\"TARGET\"])\ny = data[\"TARGET\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Full Model\nfull_model = LGBMClassifier(class_weight=\"balanced\", random_state=42)\nfull_model.fit(X_train, y_train)\n\n# Full Pred\ny_pred_full = full_model.predict(X_test)\ny_proba_full = full_model.predict_proba(X_test)\n\n# Full Results\naccuracy_full = accuracy_score(y_test, y_pred_full)\nauc_full = roc_auc_score(y_test, y_proba_full[:, 1])\ncf_full = confusion_matrix(y_test, y_pred_full)\n\nprint(f'정확도 : {round(accuracy_full,4)}')\nprint(f'AUC : {round(auc_full,4)}')\nprint(f'''Vanilla LGBM의 CF :\n{cf_full}''')\n\n정확도 : 0.703\nAUC : 0.7578\nVanilla LGBM의 CF :\n[[39914 16640]\n [ 1629  3320]]\n\n\n\n\nLGBM with optuna\n\n최적의 하이퍼파라미터 조합 찾기\n\n\nimport optuna\nfrom lightgbm import LGBMClassifier, early_stopping\n\n# 데이터 분할\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n\n\n# 목적 함수 정의\ndef objective(trial):\n    # LGBMClassifier 하이퍼파라미터 설정\n    param = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log = True), # default = 0.1\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100), # default = 31\n        # 'max_depth': trial.suggest_int('max_depth', -1, 50), # default = -1\n        'n_estimators' : trial.suggest_int('n_estimators', 100, 2000), # default = 100\n        'subsample': trial.suggest_float('subsample', 0.8, 1.0), # default = 1.0\n        'min_child_samples' : trial.suggest_int('min_data_in_leaf', 1, 100), # default = 20\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0), # default = 1.0\n    }\n\n    # LGBMClassifier 모델 생성 및 학습\n    model = LGBMClassifier(**param, n_jobs=-1,\n                           class_weight='balanced',\n                           random_state=42,\n                          #  device_type='gpu', colab환경에서 정상적으로 gpu가 활용이 안되네요...이거 해결만 온종일 할 거 같아서 일단 cpu기준으로 학습합니다.\n                          #  https://stackoverflow.com/questions/75981883/can-not-use-lightgbm-gpu-in-colab-lightgbmerror-no-opencl-device-found\n                           )\n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              eval_metric='auc',\n              callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n    )\n\n    # 검증 데이터에 대한 예측\n    proba = model.predict(X_val)\n\n    # 정확도 계산\n    roc_auc = roc_auc_score(y_val, proba)\n\n    return roc_auc\n\n# Optuna 튜닝 실행\nstudy = optuna.create_study(direction='maximize') # 높아질수록 좋은 roc_auc_score에 대해 maximize\nstudy.optimize(objective, n_trials=50)\n\n\n# 최적 파라미터 출력\nbest_params = study.best_trial.params\nprint(best_params)\n\n{'learning_rate': 0.02642230824686883, 'num_leaves': 34, 'n_estimators': 864, 'subsample': 0.8342595493628282, 'min_data_in_leaf': 60, 'colsample_bytree': 0.8360285193709883}\n\n\n\n찾은 조합을 활용한 final model\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfinal_model = LGBMClassifier(**best_params,\n                             n_jobs=-1,\n                             class_weight='balanced',\n                             random_state=42,)\nfinal_model.fit(X_train, y_train)\n\n# optuna Pred\ny_pred_final = final_model.predict(X_test)\ny_proba_final = final_model.predict_proba(X_test)\n\n# optuna Results\naccuracy_final = accuracy_score(y_test, y_pred_final)\nauc_final = roc_auc_score(y_test, y_proba_final[:, 1])\ncf_final = confusion_matrix(y_test, y_pred_final)\n\nprint(f'정확도 : {round(accuracy_final,4)}')\nprint(f'AUC : {round(auc_final,4)}')\nprint(f'''Optuna LGBM의 CF :\n{cf_final}''')\n\n[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n정확도 : 0.7229\nAUC : 0.7602\nOptuna LGBM의 CF :\n[[41245 15309]\n [ 1735  3214]]\n\n\n\n\nLGBM 결과비교 (vanila모델 vs optuna)\n\nLGBM Vanila모델\n\n정확도 : 0.703\nAUC : 0.7578\nVanilla LGBM의 CF : [[39914 16640] [ 1629 3320]]\n\nLGBM with optuna\n\n정확도 : 0.7229\nAUC : 0.7602\nOptuna LGBM의 CF : [[41245 15309] [ 1735 3214]]"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#autogluon",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#autogluon",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "Autogluon",
    "text": "Autogluon\n\nAutogluon이란\n\nAutogluon : optuna와 비슷한 방식으로 최적화하며, 최소한의 코드\n\n\n\nAutogluon구성요소와 설정값\n\n구성요소\n\nTablePredictor : (분류/회귀에 사용) Tabular데이터 처리\nTimeSeriesPredictor : 시계열 데이터 예측\nTextPredictor : (연관성 분석 등) 자연어 처리\nImagePredictor : 이미지 처리\n\n설정값\n\nTime_limit : 학습 제한시간(Default=None)\nPresets : 사전 설정된 학습 전략 (best/high/good/medium quality. medium이 기본값)\nhyperparameters : (dict) 사용할 모델의 하이퍼파라미터들\nAuto_stack : 배깅 및 스택 앙상블링을 자동으로 활용할지 여부(Default=False)\n\nTrue(더 오래/정확히 학습)인 경우, num_bag_fold & num_stack_levels 자동 설정\n\nnum_bag_fold : 배깅에 사용되는 폴드 수. 성능을 높이고 싶다면 5~10 사이를 권장(10이하의 값을 가짐)\nnum_stack_levels : 스택에 사용되는 스태킹 레벨 수. 성능을 높이고 싶다면 2~3 사이를 권장(3이하의 값을 가짐)\n\n\n\nLeader board"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#autogluon실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#autogluon실습",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "Autogluon실습",
    "text": "Autogluon실습\n\n하단 코드의 실행내역 이해하기\n\n실행내역 중 일부만 발췌함\n\n# 0,1로 추정되는 값 bool 변환\nStage 1 Generators:\n    Fitting AsTypeFeatureGenerator...\n        Note: Converting 48 features to boolean dtype as they only contain 2 unique values.\n\n# Null값 처리\nStage 2 Generators:\n    Fitting FillNaFeatureGenerator...\n\n# 그대로 사용할 값 처리(변환X)\nStage 3 Generators:\n    Fitting IdentityFeatureGenerator...\n\n# Unique값 처리\nStage 4 Generators:\n    Fitting DropUniqueFeatureGenerator...\n\n# 중복값 처리\nStage 5 Generators:\n    Fitting DropDuplicatesFeatureGenerator...\n\n# Feature 처리(앞서 전처리한 데이터이기는 하나, autogluon이 판단하여 추가 처리)\nUnused Original Features (Count: 4): ['FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12']\n    These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n    Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n    These features do not need to be present at inference time.\n    ('int', []) : 4 | ['FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12']\n\n# 사용할 모델\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': [{}],\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n    'CAT': [{}],\n    'XGB': [{}],\n    'FASTAI': [{}],\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n\n# sequential하게 trial 진행\nFitting 13 L1 models, fit_strategy=\"sequential\" ...\nFitting model: KNeighborsUnif ... Training model for up to 3595.93s of the 3595.92s of remaining time.\n    0.5271   = Validation score   (roc_auc)\n    1.62s    = Training   runtime\n    50.83s   = Validation runtime\nFitting model: KNeighborsDist ... Training model for up to 3543.23s of the 3543.23s of remaining time.\n    0.5298   = Validation score   (roc_auc)\n    0.63s    = Training   runtime\n    49.39s   = Validation runtime\n...\n\n# 실행시간 및 best model\nAutoGluon training complete, total runtime = 1561.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4907.4 rows/s (24601 batch size)\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\n\npath = 'autogluon_results' # 저장할 경로로\n\n# 데이터 분할 (이전 코드에서 X_train, y_train, X_val, y_val, X_test, y_test를 준비했다고 가정)\n\n# 데이터프레임으로 변환\ntrain_data = X_train\ntrain_data['target'] = y_train\n\nval_data = X_val\nval_data['target'] = y_val\n\n# AutoGluon 학습\npredictor = TabularPredictor(label='target', eval_metric='roc_auc', verbosity=2, path=path)\npredictor.fit(train_data, tuning_data=val_data, time_limit=3600)  # 시간 제한 설정 (1시간)\n\n\n# 예측 및 평가\ntest_data = X_test\ntest_data['target'] = y_test\n\ny_pred_proba_ag = predictor.predict_proba(test_data)\ny_pred_ag = predictor.predict(test_data)\n\n# optuna Results\naccuracy_ag = accuracy_score(y_test, y_pred_ag)\nauc_ag = roc_auc_score(y_test, y_pred_proba_ag.iloc[:, 1])\ncf_ag = confusion_matrix(y_test, y_pred_ag)\n\nprint(f'정확도 : {round(accuracy_ag,4)}')\nprint(f'AUC : {round(auc_ag,4)}')\nprint(f'''autogluon LGBM의 CF :\n{cf_ag}''')\n\n정확도 : 0.9199\nAUC : 0.761\nautogluon LGBM의 CF : \n[[56492    62]\n [ 4864    85]]\n\n\n\n\n결과 비교하기 (optuna vs autogluon)\n\nLGBM with optuna\n\n정확도 : 0.7229\nAUC : 0.7602\nOptuna LGBM의 CF : [[41245 15309] [ 1735 3214]]\n\nAutogluon\n\n정확도 : 0.9199\nAUC : 0.761\nautogluon LGBM의 CF : [[56492 62] [ 4864 85]]\n\n\n\n\nAutogluon leaderboard\n\nLeaderboard : Autogluon이 학습했던 모델들의 Score확인 가능\n\n각 모델의 score나 time 등을 확인 가능\n\n\n\n# 최적 모델 요약 출력\nprint(predictor.leaderboard())\n\n                  model  score_val eval_metric  pred_time_val    fit_time  \\\n0   WeightedEnsemble_L2   0.754022     roc_auc       5.013031  663.924621   \n1              CatBoost   0.753185     roc_auc       0.050006  108.363011   \n2              LightGBM   0.750591     roc_auc       0.188981   20.328846   \n3               XGBoost   0.749953     roc_auc       0.238468   74.874593   \n4            LightGBMXT   0.749184     roc_auc       0.669894   49.791667   \n5         LightGBMLarge   0.748378     roc_auc       0.653803   39.147023   \n6        NeuralNetTorch   0.740312     roc_auc       0.203650  140.524567   \n7       NeuralNetFastAI   0.737314     roc_auc       0.325043  136.662297   \n8      RandomForestEntr   0.731284     roc_auc       1.649255  304.078007   \n9        ExtraTreesEntr   0.722843     roc_auc       1.512995  135.927222   \n10       ExtraTreesGini   0.722248     roc_auc       1.357874  120.171119   \n11     RandomForestGini   0.720812     roc_auc       2.502675  288.912408   \n12       KNeighborsDist   0.529842     roc_auc      49.391568    0.634750   \n13       KNeighborsUnif   0.527099     roc_auc      50.827020    1.620968   \n\n    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                 0.005133           1.482976            2       True   \n1                 0.050006         108.363011            1       True   \n2                 0.188981          20.328846            1       True   \n3                 0.238468          74.874593            1       True   \n4                 0.669894          49.791667            1       True   \n5                 0.653803          39.147023            1       True   \n6                 0.203650         140.524567            1       True   \n7                 0.325043         136.662297            1       True   \n8                 1.649255         304.078007            1       True   \n9                 1.512995         135.927222            1       True   \n10                1.357874         120.171119            1       True   \n11                2.502675         288.912408            1       True   \n12               49.391568           0.634750            1       True   \n13               50.827020           1.620968            1       True   \n\n    fit_order  \n0          14  \n1           7  \n2           4  \n3          11  \n4           3  \n5          13  \n6          12  \n7          10  \n8           6  \n9           9  \n10          8  \n11          5  \n12          2  \n13          1"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#paramater를-직접-지정하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250126/index.html#paramater를-직접-지정하기",
    "title": "[DA스터디/6주차] optuna, Autogluon",
    "section": "paramater를 직접 지정하기",
    "text": "paramater를 직접 지정하기\n\nAutogluon은 이렇게 기본으로 사용해도 훌륭한 성능을 보여줍니다. 자동으로 파라미터를 지정하고, 앙상블까지 해주죠.\n이런 베이스 모델을 생성한 뒤에는, 쓸모 없거나 너무 무거운 모델은 제거하고 다시 학습시킬 수도 있습니다.\nLGBM, XGB, CAT 세 개가 성능도 괜찮고 학습시간도 짧네요. 이거 세 개만 써봅시다.\n위에서 Optuna에서 찾은 파라미터를 여기서 사용하실 수도 있겠죠?\n\n\nAutogluon 하이퍼파라미터 설정\n\n베이스 모델을 생성한 뒤, 쓸모 없거나 너무 무거운 모델을 제거하고 학습 가능(Leaderboard로 확인)\n위의 Leaderboard 결과에서, LGBM/XGB/CAT을 가지고 하이퍼파라미터를 설정\n\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport os\n\nfrom autogluon.tabular import TabularPredictor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\n\npath = 'drive/MyDrive/Metacode/Week6/selected_model'\nos.makedirs(path, exist_ok=True)\n\ntrain_data = X_train\ntrain_data['target'] = y_train\n\nval_data = X_val\nval_data['target'] = y_val\n\nscale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])  # 불균형 데이터 가중치 설정\n\n# 모델별 하이퍼파라미터 지정\ncustom_hyperparameters = {\n    'GBM': {\n        'learning_rate': 0.02642230824686883,\n        'num_leaves': 34,\n        'n_estimators': 864,\n        'subsample': 0.8342595493628282,\n        'min_data_in_leaf': 60,\n        'colsample_bytree': 0.8360285193709883,\n        'n_jobs':-1,\n        'class_weight':'balanced'\n    },\n    'CAT': {\n        'scale_pos_weight': scale_pos_weight\n    },\n    'XGB': {\n        'scale_pos_weight': scale_pos_weight\n    }\n}\n\n# AutoGluon 학습\npredictor = TabularPredictor(label='target', eval_metric='roc_auc', verbosity=2, path=path)\npredictor.fit(\n    train_data,\n    tuning_data=val_data,\n    time_limit=3600,\n    hyperparameters=custom_hyperparameters\n)\n\nWarning: path already exists! This predictor may overwrite an existing predictor! path=\"drive/MyDrive/Metacode/Week6/selected_model\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.2\nPython Version:     3.11.11\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\nCPU Count:          2\nMemory Avail:       8.70 GB / 12.67 GB (68.6%)\nDisk Space Avail:   0.50 GB / 15.00 GB (3.3%)\n    WARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n    We recommend a minimum available disk space of 10 GB, and large datasets may require more.\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n    Recommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n    presets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n    presets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n    presets='high'         : Strong accuracy with fast inference speed.\n    presets='good'         : Good accuracy with very fast inference speed.\n    presets='medium'       : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ... Time limit = 3600s\nAutoGluon will save models to \"/content/drive/MyDrive/Metacode/Week6/selected_model\"\nTrain Data Rows:    246008\nTrain Data Columns: 131\nTuning Data Rows:    24601\nTuning Data Columns: 131\nLabel Column:       target\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    8776.27 MB\n    Train Data (Original)  Memory Usage: 270.46 MB (3.1% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 48 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Unused Original Features (Count: 6): ['FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'CODE_GENDER_XNA']\n        These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n        Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n        These features do not need to be present at inference time.\n        ('float', []) : 1 | ['CODE_GENDER_XNA']\n        ('int', [])   : 5 | ['FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17']\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 88 | ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', ...]\n        ('int', [])   : 37 | ['SK_ID_CURR', 'CNT_CHILDREN', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'FLAG_MOBIL', ...]\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', [])     : 69 | ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', ...]\n        ('int', [])       : 14 | ['SK_ID_CURR', 'CNT_CHILDREN', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'HOUR_APPR_PROCESS_START', ...]\n        ('int', ['bool']) : 42 | ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', ...]\n    4.1s = Fit runtime\n    125 features in original data used to generate 125 features in processed data.\n    Train Data (Processed) Memory Usage: 182.20 MB (2.1% of available memory)\nData preprocessing and feature engineering runtime = 4.62s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n    This metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'GBM': [{'learning_rate': 0.02642230824686883, 'num_leaves': 34, 'n_estimators': 864, 'subsample': 0.8342595493628282, 'min_data_in_leaf': 60, 'colsample_bytree': 0.8360285193709883, 'n_jobs': -1, 'class_weight': 'balanced'}],\n    'CAT': [{'scale_pos_weight': 11.377138257194607}],\n    'XGB': [{'scale_pos_weight': 11.377138257194607}],\n}\nFitting 3 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBM ... Training model for up to 3595.38s of the 3595.38s of remaining time.\n/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n    0.8437   = Validation score   (roc_auc)\n    93.76s   = Training   runtime\n    2.52s    = Validation runtime\nFitting model: CatBoost ... Training model for up to 3498.78s of the 3498.77s of remaining time.\n    0.9877   = Validation score   (roc_auc)\n    1941.71s     = Training   runtime\n    0.38s    = Validation runtime\nFitting model: XGBoost ... Training model for up to 1556.46s of the 1556.46s of remaining time.\n    0.9982   = Validation score   (roc_auc)\n    1556.76s     = Training   runtime\n    11.44s   = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -11.97s of remaining time.\n    Ensemble Weights: {'XGBoost': 1.0}\n    0.9982   = Validation score   (roc_auc)\n    0.35s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 3614.24s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2149.4 rows/s (24601 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/Metacode/Week6/selected_model\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7d2f359dd250&gt;\n\n\n\n# 예측 및 평가\ntest_data = X_test\ntest_data['target'] = y_test\n\ny_pred_proba_ag = predictor.predict_proba(test_data)\ny_pred_ag = predictor.predict(test_data)\n\n# optuna Results\naccuracy_ag = accuracy_score(y_test, y_pred_ag)\nauc_ag = roc_auc_score(y_test, y_pred_proba_ag.iloc[:, 1])\ncf_ag = confusion_matrix(y_test, y_pred_ag)\n\nprint(f'정확도 : {round(accuracy_ag,4)}')\nprint(f'AUC : {round(auc_ag,4)}')\nprint(f'''autogluon CF :\n{cf_ag}''')\n\n정확도 : 0.8675\nAUC : 0.7107\nautogluon CF :\n[[51892  4662]\n [ 3486  1463]]\n\n\n\npredictor.leaderboard()\n\n\n  \n    \n\n\n\n\n\n\nmodel\nscore_val\neval_metric\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nXGBoost\n0.998176\nroc_auc\n11.441149\n1556.763843\n11.441149\n1556.763843\n1\nTrue\n3\n\n\n1\nWeightedEnsemble_L2\n0.998176\nroc_auc\n11.445685\n1557.111241\n0.004536\n0.347398\n2\nTrue\n4\n\n\n2\nCatBoost\n0.987722\nroc_auc\n0.384950\n1941.707015\n0.384950\n1941.707015\n1\nTrue\n2\n\n\n3\nLightGBM\n0.843690\nroc_auc\n2.519189\n93.759019\n2.519189\n93.759019\n1\nTrue\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n결과 비교하기 (optuna vs autogluon)\n\nAutogluon기본\n\n정확도 : 0.9199\nAUC : 0.761\nautogluon LGBM의 CF : [[56492 62] [ 4864 85]]\n\nAutogluon 일부 모델만 추려낸 것 (낮아짐)\n\n정확도 : 0.8675\nAUC : 0.7107\nautogluon CF : [[51892 4662] [ 3486 1463]]\n\n\n\n\n기타\n\n다른 AutoML로 pycaret도 있으나, Autogluon이 더 좋음\n\n코드 한줄정도로 구현이 가능하지만, Autogluon성능이 더 좋았음\n\n실무적으로는, medium quality세팅으로 리더보드를 보면서, feature engineering(파생변수 생성 등)을 진행하게 됨\n\n성능의 영향은 데이터(GIGO). 하이퍼파라미터 튜닝 등 보다는 데이터에 집중\nEDA를 하다보면, 추가 할법한 (파생)변수가 보이기도 함\nSHAP를 확인하다보면 넣으면 좋을 것 같은 변수가 보이기도 함"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240923_1/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240923_1/index.html",
    "title": "[DE스터디/최종과제1-gharchive] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 - 최종과제에 대한 ppt (gharchive 데이터파이프라인 만들기)\n\n개요\n\n참여중인 데이터 엔지니어링 스터디에서 배우는 내용 정리\n\n데이터 수집, 정제 : pyspark, airflow\n저장 : elasticsearch\n시각화 : kibana\n\n최종과제 : 배운 Pyspark, Airflow, Elasticsearch, Kibana로 데이터 파이프라인 만들어보기\n만들다보니 시각화에 대한 부분을 좀 더 해보고 싶어, 과제2를 추가로 진행함\n\n과제 1 : gharchive 데이터파이프라인 + 대시보드 약간\n과제 2 : 4차산업 핵심광물(Un comtrade) 데이터파이프라인 + 대시보드\ngit repo (과제 1, 2 모두 여기에 보관)\n\nhttps://github.com/KR9268/metacode_de-2024\n\n\n\n\n\n최종과제1 : gharchive 데이터파이프라인 + 대시보드\n                \n\n\n\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html",
    "href": "posts/meta-dl-creditcard-20240623/index.html",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "",
    "text": "참여중인 딥러닝 스터디 5주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html#hyper-parameter하이퍼-파라미터",
    "href": "posts/meta-dl-creditcard-20240623/index.html#hyper-parameter하이퍼-파라미터",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "Hyper parameter(하이퍼 파라미터)",
    "text": "Hyper parameter(하이퍼 파라미터)\n\n위 과제에서의 성능향상은, CNN의 적용(계산량의 감소)보다는, 하이퍼 파라미터조정에서 기인함\n하이퍼 파라미터 : 사람이 정해주는 레이어 갯수, 뉴런의 갯수 등 \n\n\n(1) Activation functions : Sigmoid, tanh, ReLU 등\n\n비선형성을 부여하기 위해 사용. 어떤 것을 쓰냐에 따라 Neural Network성능에 큰 영향\nActivation functions의 종류\n\nSigmoid : 0~1 출력. Logistic regression 등 오래부터 쓰였던 함수.\n\n0~1 사이이기 때문에 확률적 해석이 가능\n문제\n\nKilling Gradient : 가장 치명적 문제. 미분값이 0이 되는 구간은 학습이 일어나지 않음\nOutput not Zero-centered : 치명적 문제. 학습은 되지만, 레이어를 통과할 때마다 왜곡 심화(같은 방향으로만 학습)\n\nSGD에서는 Batch-size 반복마다 일종의 자정작용이 있어 약간 완화되기는 함\n\nComputationally expensive : 학습이 안될 정도의 큰 문제는 아님. Exponential계산 등 계산량이 많음\n\n\ntanh : -1~1 출력.\n\nZero-centered이므로 왜곡이 일어나지는 않음\n문제 : 여전히 Killing Gradient문제가 존재하여 Neural Network에서 쓰기는 부적절\n\nReLU(Rectified Linear Unit) : 음수면 0, 양수면 그대로\n\nComputationaliy very efficient(음수면 0반환)\nKilling Gradient문제는 없음. 기울기가 0인 지점이 없으므로(단 0인 부분은 미분이 되지 않음)\n문제\n\nOutput not Zero-centered(Batch-size 적용시 큰 문제가 되진 않음)\n0인 부분은 미분이 되지 않음(0인 경우는 미분값 지정하는 방법으로 해결 가능)\nDead ReLU : 한번 0이되면, Layer가 계속 되어도 기울기가 업데이트 되지 않고 0\n\n\nLeaky ReLU(Rectified Linear Unit)  \n\n음수인 경우 0이 아닌 0.01\\(x\\)를 적용하여Dead ReLU문제 해결 가능\n효율적인 계산이나 Sigmoid/Tanh대비 빠른 장점 등 ReLU의 장점 존재\n문제 : additional paremeter(0.01\\(x\\))를 계산해야해서 계산량이 늘어나기는 함\n\nELU(Exponential Linear Unit) :\n\n기존처럼 인위적인 지정(구간별 미분값 등)없이, 모든 지점에서 자연스럽게 미분이 가능하도록 Exponential 적용\n문제 : 계산량이 늘어남\n\nExponential연산은 nvidia환경에서 지원이 되는 편이긴 함\n\n\n\nActivation function의 적용\n\n기본적으로는 ReLU를 사용. Dead ReLU문제가 있긴하지만 일반적인 프로젝트 수준에서는 무방\n단 Computation자원이 충분하다면 Leaky ReLU나 ELU 사용 가능\nSigmoid나 Tanh는 Hidden layer의 Activation function으로 사용X(Killing Gradient문제로 학습되지 않음)\n참고 : GPT계열 등 대규모 모델은 ReLU계열이나 GELU(ELU를 Gausssian으로 바꾼)\n\n\n\n\n(2) Weight Initialization\n\nGradient Descent에서 임의의 점에서 시작했었던 것과 달리, 좋은 시작지점에서 시작하고자 하는 것\n임의의 크거나 작은 Weight로 시작하는 경우와 문제\n\nSmall Gaussian Random : 작은 Weight로 시작한 경우\n\nLayer를 통과할때마다 한 곳으로 수렴(0으로 수렴)되어 학습이 되지 않음 \n\nLarge  Gaussian Random : 큰 Weight로 시작한 경우\n\nLayer를 통과할때마다 양쪽으로 발산(1,-1로 수렴)되어 학습이 되지 않음 \n\nReLU를 적용하더라도 위와 비슷한 문제는 발생\n\nXavier Initialization : Input dimension에 루트를 씌운 것으로 나눠 준 Weight 사용 (64차원이라면 8로 나눔)\n\nInput variance와 Output variance 맞춰 좋은 시작점(Weight)에서 시작\nLayer를 여러번 통과해도 Robust한 정규분포 모양으로 나오게 됨  \n\nKaiming/MSRA Initialization for ReLU : 2/Input dimension 을 루트 씌운 것으로 나눈 Weight 사용"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html#learning-rate-scheduling",
    "href": "posts/meta-dl-creditcard-20240623/index.html#learning-rate-scheduling",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "Learning rate Scheduling",
    "text": "Learning rate Scheduling\n\n어떤 Learning rate가 가장 좋은가?  \n\nVery high : 이동이 너무 커서 Loss가 급격히 상승\nLow : 이동이 너무 작아서 오래걸림\nHigh : 처음엔 적절한 rate보다도 학습이 빠르지만, 일정 수준부터 Loss감소가 없음\n적절한 rate를 통해, 적절한 속도로 optimum point로 도달하도록 해야함\n\nLearning rate Scheduling(Decay)의 구상\n\nHigh(빠름) → Good(적절) → Low 의 rate를 적용하면 되지 않는지에 대한 의문으로 시작\nEpoch에 따라 learning rate를 조절하여 효율적인 학습속도 구현 (↔︎ constant learning rate) \n모든 프로젝트에 직관을 적용한 Scheduling은 어려우며, 모든 프로젝트에 적용가능한 Scheduling은 없음.\n\n정해진 Scheduling은방식(Cosine, Linear, Inverse Sqrt 등)이 존재\n\n\n\n\n(1) Learning rate Scheduling의 종류\n\n크게 3가지 종류가 있음\n\nLinear Learning rate : [세심하게 학습] 가장 기본적. Epoch을 거듭할수록 rate 감소\nCosine Learning rate : [빠른 학습] High rate를 좀 더 오래 유지\nInverse Sqrt rate : [무난하게 학습] rate를 빠르게 감소시킴. Optimal Point를 세밀하게 찾고 싶을 때 사용\n\n분야마다 많이 사용되는 방법은 다름\n\n자연어처리는 큰 모델을 다루고 학습데이터가 많은 편이므로 Cosine Learning rate Scheduler 사용\n작은 문제나 모델이라면 Linear/Inverse Sqrt earning rate Scheduler 고려 가능\n\nLearning rate Warm-up?\n\n눈으로 Loss 등을 보며 감을 잡는 구간\n\nLoss가 엄청 높게 튀는 경우 Learning rate가 높으므로 낮춤\n\n\n\n\n\n(2) 기타 참고사항\n\nHyper parameter가 적용되는 부분\n\nActivation function은 레이어마다 적용\nLearning rate epoch마다 적용\nWeight Initialization은 처음에만 적용\n\n대략적인 가이드라인\n\n레이어나 뉴런의 수는 가이드라인이 없는 편 (실험해보며 조정)\n\n레이어의 수가 많을수록 복잡한/비선형적인 관계를 표현 가능\n레이어의 수가 30개가 좋은지 50개가 좋은진 알 수 없음\n\nLearning rate는 특정 값이 높거나 낮다고 볼 수 없음 (실험해보며 조정)\n\nLoss값이 너무 튀면 learning rate를 먼저 조정 (대부분 말도안되게 튀는 것은 learning rate문제)\n\n\n우선순위 : (loss가 튐)Learning rate 조정 → Optimizer"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html#regularization-for-overfitting",
    "href": "posts/meta-dl-creditcard-20240623/index.html#regularization-for-overfitting",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "Regularization for Overfitting",
    "text": "Regularization for Overfitting\n\n(1) Overfitting\n\n새로운 데이터에 대해서는 잘 동작하지 않음 (↔︎Underfitting. 학습한 데이터도 처리하지 못함. 과거의 전통적 머신러닝[모델의 한계 등])\n파라미터의 갯수가 늘어나고 복잡해지는 경우 많음\n\n\n\n(2) Regularization\n\n파라미터가 커질수록 오차도 커짐 → 파라미터가 커져 Overfitting이 일어나는 것을 방지\n람다(\\(\\lambda\\))의 크기로 패널티의 크기를 설정. 커질수록 하단 Boundary(L2는 원형, L1은 마름모)는 작아짐\n기존의 최적화 지점(Parameter제약 없을 때)에서 그래프의 화살표 방향(Boundary안)으로 이동\n\n\nRidge Regression(L2 Norm)\n\n기존의 오차정의(MSE 등)에, Penalty term으로 파라미터를 제곱해서 더함  \n\n\n\nLasso Regression(L1 Norm)\n\n기존의 오차정의(MSE 등)에, Penalty term으로 파라미터를 절대값으로 더함 (Ridge와의 차이점 : 필요없는 파라미터를 삭제해줌)  \n\n\n\n\n(3) Regularization for Deep Nueral Networks\n\nWeight decay\n\nRidge(L1, 제곱, 원형마름모Boundary), Lasso(L2, 절대값, 마름모Boundary)의 적용\nMultiple regression 등 과거엔 많이 쓰였지만 딥러닝에선 잘 안쓰임\n\n\n\nEarly stopping\n\nTrain set에서 Validation set을 분리해두고, Validation점수가 높아질때까지(Loss저하) 학습\n\nTest loss가 Overfitting지점부터 발생하는 점을 참고\n\nAcuraccy와 같은 Metric으로 적용하는 것이 트렌드\nLoss는 미분가능 & 작을수록 좋은 지표 등을 이유로 채택했지만 실제 알고싶은 것은 Loss가 아닌 Accuracy임\nF1-score 등은 미분 불가로 적용 불가했지만, 측정 목적이라면 Accuracy를 채택\n단점 : 측정을 위한 Validation set을 만들기 위해 데이터가 많이 필요(데이터가 적은 바이오분야 등은 적용 어려움)\n\n다만 딥러닝은 보통 데이터가 많은 경우에 사용하므로 Early stopping 적용이 가능함\n\n주의점 : Train/Validation/Test로 나눌 때, Test Set은 정한 뒤엔 건드리지 않는다 (Train set에서 나눈 Train set와 Validation set의 비율 조절은 가능하다)\nFinal metric으로 사용된다\n\n\n\nDrop out\n\nRandom한 뉴런을 끈다\n다소 무식하지만 효과가 좋아 잘 사용됨(GPT 등에서도 활용)\n\n예시) 고양이의 눈/귀/코를 특징으로 잡을 때, 호랑이의 눈이 들어온다면 오류가 발생할 수 있는데, 눈에 대한 특징(뉴런)이 꺼져있다면 오류를 방지할 수도 있다\n\nCut out(Drop out의 응용) : 사진에서 특정 부분을 Random하게 삭제하여 입력하여 판단시키는 방법으로 Overfitting극복\nLarge fully-connected layers에 사용"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html#optimization",
    "href": "posts/meta-dl-creditcard-20240623/index.html#optimization",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "Optimization",
    "text": "Optimization\n\nSGD : 전체 데이터를 다 넣기 어려워 Batch를 적용해 극복\n\n\n(1) SGD의 문제\n\nJittering문제 : 원하는 것은 전체 파라미터의 최적화지만, 특정 파라미터만 학습이 진행됨\n\n예를 들어 자산&월급이 있을 때, 값이 큰 자산 위주로 최적화가 진행됨\nSingular value가 큰 쪽으로 학습이 이루어짐\n\nLocal optimum문제 : 전체의 최적이 아닌, 부분의 최적인 Local Optimum에 빠지는 문제\n\n다만, 딥러닝에서는 거의 일어나지 않는 문제임. 모든 파라미터 대부분이 양수거나 음수인 특수한 상황에서 발생\n\nSaddle points문제 : 말의 안장같이 생겨, 극소인 동시에 극대인 지점이 발생. 학습이 이루어지지 않는 문제\n\n오히려 Saddle point문제가 딥러닝에서는 더 많이 발생 (미분값이 여러개의 양수/음수가 섞임)\n\nInaccurate Gradient Estimation문제\n\n어느정도까지는(~50% 정도) Batch size가 클수록 성능향상이 있음\n\n장비(메모리)가 유효하게 많을때만 가능한 방법으로, 잘 갖춰진 환경에서만 적용 가능함(Google의 연구논문이었음)\n\n즉 현실적으로는 Batch size 확대는 적용이 어려운 점이 있음\n\n82.76~83% 구간의 변화로, Practical하게 유의미한 결과로 보긴 어려움(Academic한 측면에서 유의미)\n\n적용Insignt : 나에게 1만개의 데이터가 있을 때, 5천 건 정도의 Size(2의 배수인 4096으로) 적용\n\n\n\n\n(2) SGD의 문제들에 대한 해결방법\n\nSGD + Momentum (Saddle Points문제 해결)\n\n현실과 달리 GD의 적용시에는 관성이 없으므로 Saddle point에서 멈추게 되므로, 관성을 주어 해결\n\nLocal minimum에서는 적용 불가. saddle point형태이기에 가능한 방법 \n\n\nAdaGrad[AdaptiveGrad] (Jittering문제 해결)\n\nScale이 달라 발생하는 문제이므로, Scaling을 해주어 맞춰줌\n\nRMSProp [Leaky AdaGrad] (AdaGrad문제 해결)\n\nAdaGrad가 나누기를 반복하다보니 분모가 커져 0에 수렴하는 문제를 해결\n현재의 비율과 과거의 비율을 조절해 분모가 너무 커지는 것을 방지\nAdaGrad의 Scaling개념을 이해하며 사용하는 것이 중요\n\nAdam\n\nRMSProp + SGD with Momentum\n\nFirst vs Second order Optimization : 실전적으로 잘 쓰이지 않음\n\n역행렬이 있다는 점에서 계산량이 많아 쓰기 어렵다는 직관적 이해 정도 갖기"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240623/index.html#batch-normalization",
    "href": "posts/meta-dl-creditcard-20240623/index.html#batch-normalization",
    "title": "[M_Study_5주차] Overfitting Control & Hyper-Parameter",
    "section": "Batch Normalization",
    "text": "Batch Normalization\n\nData Preprocessing (Nomalization)\n\n통상적으로 Zero-centering, Scaling 두가지를 해 줌\nLayer를 통과할 때마다 왜곡이 커지는 문제를 방지 \n\nZero-centering : 중심에서 점점 멀어지는 문제 해결 (평균을 뺀다)\nScaling : 분포가 점점 길어지는 문제 해결 (표준편차 등 특정 숫자로 나눠 준다)\n\n일반적인 ML은 처음에 전처리를 하면 되지만, DL은 레이어를 통과하며 다시 왜곡이 심해질 수 있음\n\n왜곡을 막기 위해 레이어 통과마다 반복하는 Batch Normalization을 진행\n\n\n\n\nBatch Normalization\n\n레이어를 통과할 대마다 Zero-centering, Scaling을 해줌\nDrop-out과 함께 많이 사용됨\n적용은\n\nActivation function 적용하기 전에\nFully-connected layer 통과한 다음에\n\n\n\n\nBatch Normalization의 종류\n\nBatch / Layer / Instance / Group Normalization\n기본적으로는 대부분은 Batch Normalization을 많이 사용(특히 이미지)\n자연어 처리는 Layer Normalization을 사용"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240924/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240924/index.html",
    "title": "[DE스터디/최종과제_피드백정리] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 - 최종과제에 대한 피드백 정리\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240924/index.html#과제1-피드백",
    "href": "posts/meta-de-spark_and_airflow-20240924/index.html#과제1-피드백",
    "title": "[DE스터디/최종과제_피드백정리] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "과제1 피드백",
    "text": "과제1 피드백\n\nRetry 주기는 길지 않게 설정하는 것이 좋다\n\n데이터가 1시간 단위로 업데이트되니 부재일 것을 우려하여 2시간 단위로 Retry한 것으로 보임\n하지만 발표에서도 그랬듯이 실제로는 데이터가 없어서 문제가 발생하는게 많지 않음\n문제해결을 빠르게 하는 것이 중요하므로, 문제사유가 어떤 것이든 1~2시간은 길고 5~10분이 좋을 듯 하다"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240924/index.html#과제2-피드백",
    "href": "posts/meta-de-spark_and_airflow-20240924/index.html#과제2-피드백",
    "title": "[DE스터디/최종과제_피드백정리] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "과제2 피드백",
    "text": "과제2 피드백\n\n데이터가 수시로 업데이트된다면, 이처럼 업데이트된 것만 현황을 확인하고 가져오는 것이 필수적이고 좋은 전략\n\nAPI형태가 아니라 Kafka 등도 업데이트 된 것만 가져오는 기능을 제공\n\n적절한 제한량 문제\n\n제한이 있으므로 분산처리를 안한다했으나, 제한량이 있을 때 유용한 것이 분산처리이기도 함\nIP기준으로 사용량을 체크한다면, 여러 노드의 IP대역 등을 분리하는 방안도 있음\n\n현재는 Local환경에서 실습하므로 쓸 수는 없는 방법\n크롤링 등은 IP대역으로 체크하므로, 여러 노드에 대역을 할당해 하는 것이 가장 빠름\n\nAPI Key라면 Key를 여러개 받아 관리하는 방법도 있음\n\nAPI로 데이터를 호출했다면, Raw데이터를 보존\n\n복구가 불가능한 상황에 대한 대비\n문제 상황에 대한 원인 파악 등 사용(API호출시기에 따라 달라져있으므로 확인이 불가할 수 있음)\n\n제공중이던 학습 데이터가 갑자기 튀어 AI학습에 문제생겼을 때, 확인 등을 위해 사용\n\n\n생성한 Unique ID의 규칙\n\n과제 진행한 것처럼, 기간이나 카테고리 등으로 들어가는게 맞긴 함(기간+HSCODE+수출국+수입국+수출/수입 여부)\n성능개선이 필요하다면, 데이터 쏠림을 막을 수 있는 방법을 고민해야함\n\n과제의 Index로는 특정 수출국이 많다면 데이터 쏠림(Skew)이 발생할 수 있음\nID를 한번 해싱(앞에 랜덤해싱을 붙인다던가)하면 분산이 잘 될 수 있음\n이러한 일정한 규칙인 Sort값을 앞에 붙여 해싱하면 더 다양하게 해시값이 생성되고 고르게 분산됨\n기간, hscode 등 다 알고있는 값을 일방향해싱하면 되므로 만들기 쉽고 분산이 잘될 것임"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240924/index.html#기타-과제-중-궁금했던-점-질의응답",
    "href": "posts/meta-de-spark_and_airflow-20240924/index.html#기타-과제-중-궁금했던-점-질의응답",
    "title": "[DE스터디/최종과제_피드백정리] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "기타 과제 중 궁금했던 점 질의응답",
    "text": "기타 과제 중 궁금했던 점 질의응답\n\nLocal환경 실습을 위해 며칠 컴퓨터를 켜두었는데 느려지기도 하고 메모리(Ram)점유율이 올라갔는데, 가비지콜렉션(GC)를 따로 적용하면 되는 문제인지?\n\nSpark도 GC를 못해서 죽는 경우가 있고(연산량이 크다던가 이유로), 강제로 GC를 호출하는 방법도 있음\n\nmap이나 For each 등 너무 큰 연산이 있을 때 루프마다 GC를 호출하는 등 방법\n\n문제의 원인은 Airflow일 확률이 높아보임\n\nAirflow는 Log rotate를 직접 작성해주어야 함\n\nLog rotate는 간단히 말해 주기적으로 로그 등을 삭제해주는 것(가장 최근 것을 링크걸고 나머지 삭제 등 진행)\n\n(Airflow가 아직 덜 성숙한 플랫폼이라는 얘기를 했었는데)Dag추가나 Crontab으로 Log rotate구현이 필요해 보임\n중간 값 확인 등을 위해 dataframe의 show 등이 많아 로그가 많이 쌓인 것으로 추정"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blake Park",
    "section": "",
    "text": "I’m Blake Park. Welcome to my blog. I’m not an expert in my favorites yet, but I’m working on becoming one. I hope you to have nice time with my blog.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "about.html#hello",
    "href": "about.html#hello",
    "title": "Blake Park",
    "section": "",
    "text": "I’m Blake Park. Welcome to my blog. I’m not an expert in my favorites yet, but I’m working on becoming one. I hope you to have nice time with my blog."
  },
  {
    "objectID": "about.html#topics",
    "href": "about.html#topics",
    "title": "Blake Park",
    "section": "Topics",
    "text": "Topics\nPython, MachineLearning, DeepLearning"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "",
    "text": "스터디 진행하며 진행한 과제 기록(MNIST, Softmax)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html#개요",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "개요",
    "text": "개요\n참여중인 딥러닝 스터디 3주차 기록입니다.\n\nSoftmax로 MNIST다루기\n강사님이 주신 샘플코드 참고해서, 나에게 맞추거나 추가공부 진행"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240615_1/index.html#과제-작성-softmax-linear",
    "href": "posts/meta-dl-creditcard-20240615_1/index.html#과제-작성-softmax-linear",
    "title": "[M_Study_3주차과제1] Softmax로 MNIST다루기",
    "section": "과제 작성 (Softmax / Linear)",
    "text": "과제 작성 (Softmax / Linear)\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n\n\nMnist Dataset로딩 및 전처리\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nfor i in (x_train, y_train, x_test, y_test):\n    print(i.shape)\n\n(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n\n\n\nfloat변환\n\nx_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n\n\n\nFlatten\n\n# Flatten (num_features=784)\nnum_features = 784 # 28*28 (Data의 Shape)\n\nprint('Flatten전 : ', x_train.shape, x_test.shape)\nx_train, x_test = x_train.reshape(-1, num_features), x_test.reshape(-1, num_features)\nprint('Flatten후 : ', x_train.shape, x_test.shape)\n\nFlatten전 :  (60000, 28, 28) (10000, 28, 28)\nFlatten후 :  (60000, 784) (10000, 784)\n\n\n\n\nNormalize\n\n# Normalize (0~255사이의 값을 0~1 사이의 값으로)\nx_train, x_test = x_train / 255., x_test / 255.\n\n\n\n\n함수 및 파라메터 설정\n\n# Parameters\nlearning_rate = 0.01\ntraining_steps = 1000\nbatch_size = 256\n\nnum_classes = 10 # MNIST의 0~9 숫자 10개\nnum_features = 784 # 28*28 (Data의 Shape)\n\n# Variables\nW = tf.Variable(tf.ones([num_features, num_classes]), name='weight')\nb = tf.Variable(tf.zeros([num_classes]), name='bias')\n\n# Functions\ndef softmax(x):\n    z = tf.matmul(x, W) + b\n    sm = tf.nn.softmax(z)\n    return sm\n\ndef cross_entropy(y_pred, y_true):\n    y_true = tf.one_hot(y_true, depth=num_classes)\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.) # clip_by_value에서 1e-9 최소값지정사유 : \n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1))\n\n\ntf.nn.softmax : 0~1 사이로 출력\ntf.one_hot(y_true, depth=num_classes) : y_true인 대상을 depth에 맞춰 원핫인코딩\n  # 샘플\n  tf.one_hot([2], depth=5).numpy()\n  &gt;&gt;&gt; array([[0., 0., 1., 0., 0.]], dtype=float32)\ntf.clip_by_value(y_pred, 1e-9, 1.) : y_pred인 대상을 제시한 min, max에 맞춰 변환\n  # 샘플\n  # 변환 전\n  t = tf.constant([[-1, 0, 1], [2, 3, 4]], dtype=tf.float32)\n  t.numpy()\n  &gt;&gt;&gt; array([[-1.,  0.,  1.],\n     [ 2.,  3.,  4.]], dtype=float32)\n  # 변환 후\n  tf.clip_by_value(t, clip_value_min=0, clip_value_max=2).numpy()\n  &gt;&gt;&gt; array([[0., 0., 1.],\n     [2., 2., 2.]], dtype=float32)\n\n\n# Optimization\ndef accuracy(y_pred, y_true):\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = 0)\n\ndef batch_maker(text, data_row, batch_size):\n    random_idx = np.random.randint(data_row, size = batch_size)\n    return x_train[random_idx], y_train[random_idx]\n\ndef run_optimization(x, y):\n    with tf.GradientTape() as g:\n        pred = softmax(x)\n        cost = cross_entropy(pred, y)\n\n    gradients = g.gradient(cost, [W, b])\n\n    optimizer.apply_gradients(zip(gradients, [W, b]))\n\n# Optimizer (Stochastic Gradient Descent)\noptimizer = tf.optimizers.SGD(learning_rate)\n\nfor step in range(training_steps): # training_steps = 1000로 위에서 지정해둠\n    batch_x, batch_y = batch_maker('training', 60000, batch_size) # batch_size = 256로 위에서 지정해둠\n\n    run_optimization(batch_x, batch_y)\n\n    if step % 100 == 0:\n        pred = softmax(batch_x)\n        cost = cross_entropy(pred, batch_y)\n        acc = accuracy(pred, batch_y)\n        print(f\"Step : {step} | loss : {cost} / accuracy : {acc}\")\n\nStep : 0 | loss : 2.287696123123169 / accuracy : 0.6875\nStep : 100 | loss : 1.5578452348709106 / accuracy : 0.7578125\nStep : 200 | loss : 1.188745379447937 / accuracy : 0.8359375\nStep : 300 | loss : 1.010088324546814 / accuracy : 0.796875\nStep : 400 | loss : 0.8610934019088745 / accuracy : 0.859375\nStep : 500 | loss : 0.807215690612793 / accuracy : 0.8359375\nStep : 600 | loss : 0.7144550085067749 / accuracy : 0.8671875\nStep : 700 | loss : 0.7559677362442017 / accuracy : 0.81640625\nStep : 800 | loss : 0.6656553745269775 / accuracy : 0.8515625\nStep : 900 | loss : 0.59657222032547 / accuracy : 0.8828125\n\n\n\n\n학습 후 Validation(Test)\n\n# W와 b가 학습된 model로 Validation(Test Dataset사용)\nprediction = softmax(x_test)\nprint(f\"Test accuracy : {accuracy(prediction, y_test)}\")\n\nTest accuracy : 0.8716999888420105\n\n\n\n\n추가적으로 검증기능 구현해보기\n\n추가적으로, 아래의 기능을 구현해보았음\n\nnumber_to_look을 입력하여 원하는 횟수만큼 모델검증\nrandint를 활용하여 랜덤추출, tested_list로 추출내역 관리하여 중복회피\npyplot으로 형태/예측/정답을 시각화\n\n\n\nimport koreanize_matplotlib\n\n# 시각화로 Validation 확인\nnumber_to_look = 4\ntested_list = []\n\nfor i in range(number_to_look):\n    plt.figure(figsize=(2,2))\n    # 테스트할 데이터 랜덤추출\n    idx = -1\n    while idx not in tested_list:\n        idx = np.random.randint(0, y_test.shape[0])\n        tested_list.append(idx)\n\n    # 결과 확인\n    plt.xlabel(f\"예상:{np.argmax(prediction[idx])} | 정답:{y_test[idx]}\")\n    plt.imshow(np.reshape(x_test[idx], [28, 28]), cmap=plt.cm.binary)\n    plt.show()"
  },
  {
    "objectID": "posts/pycon-20230813/index.html",
    "href": "posts/pycon-20230813/index.html",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "",
    "text": "파이콘에서 들었던 ’짠내나는 데이터 다루기’세션 내용정리입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/pycon-20230813/index.html#메모리-확보하기",
    "href": "posts/pycon-20230813/index.html#메모리-확보하기",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "메모리 확보하기",
    "text": "메모리 확보하기\n\n가상메모리 설정, 그래픽 설정 낮추기, 백그라운드 비활성화, 캐시 제거, 재부팅"
  },
  {
    "objectID": "posts/pycon-20230813/index.html#메모리-사용량-줄이기",
    "href": "posts/pycon-20230813/index.html#메모리-사용량-줄이기",
    "title": "[Pycon2023] 짠내나는 데이터 다루기 세션 정리",
    "section": "메모리 사용량 줄이기",
    "text": "메모리 사용량 줄이기\n\n데이터 샘플링 (행/열 줄이기)\n\ndf.sample(n), df.sample(frac=0.1) * frac:비율\n도메인에 따라 샘플링 기준 달라짐(특정 상품군/기간/고객군 등)\n필요없는 컬럼 제거\n\n\n\n청크(Chunked Processing)\n\n메모리를 작은 청크로 나누어 처리(메모리에 청크 단위로 로드)\npd.read_csv(chunksize=100) * chunksize만큼의 row를 가져옴\n아래와 같은 형식으로 사용\n# Chunksize만큼 나누어서 리스트에 저장\nchunk_list = []\nfor chunk in pd.read_scv('sample.csv', chunksize=100):\n  # pd.to_numeric(, downcast='float)와 같은 옵션을 같이 사용하면 좋음\n  chunk_list.apppend(chunk)\n\n# 리스트를 concat을 활용하여 하나로 결합\npd.concat(chunk_list)\n추가검색해보니 Chunk size지정에 대한 용량별 가이드가 있어 참고해봄 (공식문서아님, 블로그글 참고함) 출처 : https://acepor.github.io/2017/08/03/using-chunksize/ \n\n\n\nParquet형식 사용(데이터 압축)\n\nParquet : 효율적 데이터 저장/검색을 위한 오픈소스, 열 지향 형식, Java/Python/C++ 등 지원\n샘플코드(Codestral에게 유사하게 만들어달라고 함)\n\nimport pyarrow.parquet as pq\nimport glob\n\nparquet_files = glob.glob('yourdirectory/*.parquet')\n\nfor file in parquet_files:\n    metadata = pq.read_metadata(file)\n    print('Schema:', metadata.schema)\n    print('Other Metadata:', metadata.metadata)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 31.37GB → 4.95GB로 감소\n\n\n\n데이터 타입을 지정해서 불러오기\n\n아래와 같이 dtype을 지정해서 불러오는 경우 메모리 사용량이 절약될 수 있다\n\n  dtype_dict = {'기준년도':'uint16', '가입자일련번호':'uint32'}\n  pd.read_csv('sample.csv', dtype=dtype_dict)\n\nKaggle의 Hotel booking demand로 실험한 것 보여주셨는데, 29MB → 5.6MB로 감소\n\n\n\n분산처리 프레임워크(Dask, Vaex, PySpark 등 사용)\n\nDask : 병렬처리를 위한 분산 컴퓨팅 프레임워크, 큰 데이터를 처리할 수 있음, pandas와 유사한 API\nVaex : 디스크 기반의 컬럼 지향방식을 활용하여 대용량 데이터 처리 \nPySpark : Apache spark의 Python API (대규모 데이터 처리를 위한 분산컴퓨팅프레임워크 Spark를 파이썬에서 사용)"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html",
    "href": "posts/coach-ds-20240821/index.html",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "",
    "text": "서울 생활물류 데이터로 품목별로 어떤 구에서 가장 많이 받았을지 분석하기\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#데이터-로딩",
    "href": "posts/coach-ds-20240821/index.html#데이터-로딩",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "데이터 로딩",
    "text": "데이터 로딩\n\n컬럼 정보\n\n배송년월일 : 배송일자 YYYYMMDD\n송하인_시명, 송하인_시코드 : 출발지 명칭/코드(시 기준)\n수하인_시명, 수하인_시코드 : 도착지 명칭/코드(서울특별시)\n수하인_구명, 수하인_구코드 : 도착지 명칭/코드(구 기준)\n\n품목별로 서울특별시의 ’구’별로 물동량 추이를 볼 예정으로, 수하인_구코드를 기준값으로 사용할 예정\n\n수하인_구코드로 GIS Developer에서 제공한 대한민국 최신 행정구역정보에 매칭하여 시각화\n\n\n‘대분류_착지물동량’ 으로 시작하는 모든 컬럼 : 패션의류 등 품목별 물동량\n\n\n\nimport pandas as pd\nimport folium\n\ndf_kosis = pd.read_csv('logistsics_seoul.csv', encoding=\"cp949\")\n\n\ndf_kosis.head()\n\n\n\n\n\n\n\n\n배송년월일\n송하인_시명\n송하인_시코드\n수하인_시명\n수하인_시코드\n수하인_구명\n수하인_구코드\n대분류_착지물동량 가구/인테리어\n대분류_착지물동량 기타\n대분류_착지물동량 도서/음반\n대분류_착지물동량 디지털/가전\n대분류_착지물동량 생활/건강\n대분류_착지물동량 스포츠/레저\n대분류_착지물동량 식품\n대분류_착지물동량 출산/육아\n대분류_착지물동량 패션의류\n대분류_착지물동량 패션잡화\n대분류_착지물동량 화장품/미용\n\n\n\n\n0\n20231231\n강원특별자치도\n51\n서울특별시\n11\n은평구\n11380\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20231231\n강원특별자치도\n51\n서울특별시\n11\n성동구\n11200\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n20231231\n제주특별자치도\n50\n서울특별시\n11\n강동구\n11740\n0\n0\n0\n0\n0\n0\n16\n0\n0\n0\n0\n\n\n3\n20231231\n제주특별자치도\n50\n서울특별시\n11\n송파구\n11710\n0\n0\n0\n0\n0\n0\n25\n0\n0\n0\n0\n\n\n4\n20231231\n제주특별자치도\n50\n서울특별시\n11\n강남구\n11680\n0\n0\n0\n0\n0\n0\n43\n0\n0\n0\n0"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#데이터-가공",
    "href": "posts/coach-ds-20240821/index.html#데이터-가공",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "데이터 가공",
    "text": "데이터 가공\n\n메모리 사용 등 고려하여 사용하지 않을 데이터는 Drop : ‘송하인_시코드’, ‘수하인_시코드’,‘배송년월일’\nnumeric_only로 숫자만 계산 + 숫자가 아닌 경우 자동제외되도록 설정\nChoropleth에 사용할 예정으로, reset_index()로 형태 변경\n[중요] 구코드가 integer로 되어있는데, 향후 매칭할 GIS의 구 코드는 string이므로 변환 진행\n\n\n# 데이터 사전 확인 후, 지역별(수하인_구코드) 합산할 예정이므로 불필요한 컬럼 제외하고 계산\ndf_new = df_kosis.drop(['송하인_시코드', '수하인_시코드','배송년월일'], axis=1).groupby(['수하인_구코드','수하인_구명']).sum(numeric_only=True).reset_index()\n\n# 수하인_구코드가 int형태여서 str로 변환 (GeoJson과 데이터형이 일치해야 Choropleth 사용가능)\ndf_new['수하인_구코드'] = df_new['수하인_구코드'].astype(str)\ndf_new\n\n\n\n\n\n\n\n\n수하인_구코드\n수하인_구명\n대분류_착지물동량 가구/인테리어\n대분류_착지물동량 기타\n대분류_착지물동량 도서/음반\n대분류_착지물동량 디지털/가전\n대분류_착지물동량 생활/건강\n대분류_착지물동량 스포츠/레저\n대분류_착지물동량 식품\n대분류_착지물동량 출산/육아\n대분류_착지물동량 패션의류\n대분류_착지물동량 패션잡화\n대분류_착지물동량 화장품/미용\n\n\n\n\n0\n11110\n종로구\n1061135\n5077032\n2324409\n2498637\n6100861\n693559\n9061759\n694657\n5873420\n2551117\n2816472\n\n\n1\n11140\n중구\n1179088\n7046471\n2524885\n3042749\n7188036\n849276\n9600012\n797910\n8157026\n3466631\n3661852\n\n\n2\n11170\n용산구\n1460335\n6646174\n2376545\n5857864\n8100292\n993358\n12180374\n1090316\n7710275\n3259013\n4316009\n\n\n3\n11200\n성동구\n1674244\n8696553\n3251087\n3864834\n9451570\n1260283\n13672448\n1624729\n13020677\n5989994\n5154328\n\n\n4\n11215\n광진구\n1898465\n7741003\n2749241\n4207995\n9999510\n1263335\n14690460\n1433213\n11270848\n4460511\n5927127\n\n\n5\n11230\n동대문구\n1746528\n7771321\n2822380\n3822811\n9658396\n1168293\n14246334\n1502772\n11960318\n4396142\n5547731\n\n\n6\n11260\n중랑구\n1670196\n6751178\n2037567\n3545981\n9357911\n1139490\n13609236\n1522388\n10815541\n3997109\n5333394\n\n\n7\n11290\n성북구\n2086327\n9657395\n3732671\n4572046\n11668035\n1394075\n17676984\n1949670\n16559818\n5793794\n6640316\n\n\n8\n11305\n강북구\n1225245\n5005354\n1713368\n2690915\n7052214\n830690\n10986959\n999207\n8230578\n3020069\n3973925\n\n\n9\n11320\n도봉구\n1310173\n5179854\n1858639\n2941378\n7531143\n895648\n11604328\n1165943\n8444644\n3118088\n4249067\n\n\n10\n11350\n노원구\n2202491\n8914209\n3625840\n4890103\n12736505\n1543047\n19369361\n2085943\n14178854\n5215147\n7055362\n\n\n11\n11380\n은평구\n2076030\n7907376\n3070666\n4350726\n11718331\n1441215\n17980771\n1844681\n12662793\n4846272\n6551438\n\n\n12\n11410\n서대문구\n1632033\n6786437\n3114862\n3585855\n8914395\n1061254\n13659061\n1394924\n9440717\n3873421\n4963137\n\n\n13\n11440\n마포구\n2572807\n10660981\n4606235\n5564233\n13520836\n1618041\n18953737\n1917625\n13319364\n5671800\n7147512\n\n\n14\n11470\n양천구\n1910418\n8471154\n3746852\n4331039\n11242673\n1400966\n17072440\n1820116\n11828626\n4671370\n6419446\n\n\n15\n11500\n강서구\n3077854\n12785828\n4830026\n6929255\n17334255\n2119475\n24977927\n2917884\n19264445\n7115482\n10197434\n\n\n16\n11530\n구로구\n1965116\n8349861\n3218337\n4922505\n11617812\n1397904\n16929005\n1952948\n11780328\n4559226\n6272729\n\n\n17\n11545\n금천구\n1313317\n6555648\n1878737\n4283528\n7635677\n1027585\n10247593\n1058856\n9678370\n3979399\n4111394\n\n\n18\n11560\n영등포구\n2293404\n10019834\n4077897\n5744885\n12803490\n1603446\n18626803\n2019618\n12145725\n5160243\n7305597\n\n\n19\n11590\n동작구\n1974949\n7779806\n3480433\n4230506\n10614089\n1294371\n16801158\n1702084\n11688094\n4607169\n6354113\n\n\n20\n11620\n관악구\n2734290\n9812981\n3675152\n5682047\n13510190\n1619002\n21111519\n1729914\n15487924\n6038209\n8337576\n\n\n21\n11650\n서초구\n2671719\n12400796\n5709278\n6407755\n15132959\n1949417\n22810717\n2199684\n14122732\n5860011\n8144566\n\n\n22\n11680\n강남구\n4029234\n20015950\n8176952\n9429821\n22992763\n2807890\n32750262\n2711474\n22037953\n8998652\n12580793\n\n\n23\n11710\n송파구\n3574418\n15265180\n6162689\n8039974\n20155951\n2588209\n29971000\n3342858\n20817933\n8374716\n11494303\n\n\n24\n11740\n강동구\n2124558\n8269870\n3330031\n4558387\n11788178\n1477715\n17718222\n2127315\n12302300\n5069046\n6606198"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#geojson-적용-준비-choropleth",
    "href": "posts/coach-ds-20240821/index.html#geojson-적용-준비-choropleth",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "geojson 적용 준비 (Choropleth)",
    "text": "geojson 적용 준비 (Choropleth)\n\nGIS Developer의 정보로 geojson을 활용하여 Choropleth 시각화를 진행하고자 함\n\n필요한 진행방법 (geojson파일 준비)\n\nGIS Developer에서 대한민국 최신 행정구역 SHP파일 다운로드\n\nhttp://www.gisdeveloper.co.kr/?p=2332\n구를 기준으로 매칭할 것이므로 시군구 데이터를 활용\n\nQGIS를 받아 SHP파일을 geojson파일로 변환\n\nhttps://qgis.org/download/\n\n변환된 파일을 용량이 작은 파일로 축소 (용량이 커서 Colab이 다운되는 문제가 있었음)\n\nhttps://mapshaper.org/\n위 사이트에 업로드 후 Simplify옵션으로 용량을 축소시킬 수 있다\n\n\n필요한 진행방법2 (EPSG규격변환)\n\nGIS Developer와 folium의 사용규격이 다르므로 맞춰주어야 함\n\nGIS Developer는 5159, Folium은 4326을 사용함\n지도만 뜨고 Choropleth는 뜨지않는 문제가 발생하며, EPSG규격번호를 잘못 입력하는 경우 엉뚱한 나라에 적용될 수 있음\n이 부분은 ChatGPT에 요청해서 변환 코드를 받음\n\n\n\nEPSG규격변환 코드\n\n\nfrom pyproj import Transformer\nimport json\n\n# geojson로딩 (앞서 SHP파일로 변환/축소한 파일을 사용한다)\ngeo_path = 'sigungoo.json'\ngeo_str = json.load(open(geo_path, encoding='utf-8'))\n\n# EPSG 규격변환 코드 by ChatGPT\n## 좌표계 변환 (GIS Developer는 5159, Folium은 4326을 사용해 일치시키지 않으면 Choropleth 뜨지않음)\ntransformer = Transformer.from_crs(\"EPSG:5179\", \"EPSG:4326\", always_xy=True)\n\ndef convert_coords(coords):\n    return [transformer.transform(x, y) for x, y in coords]\n\ndef convert_geojson(geojson):\n    for feature in geojson['features']:\n        geometry = feature['geometry']\n        if geometry['type'] == 'Polygon':\n            geometry['coordinates'] = [convert_coords(ring) for ring in geometry['coordinates']]\n        elif geometry['type'] == 'MultiPolygon':\n            geometry['coordinates'] = [[convert_coords(ring) for ring in poly] for poly in geometry['coordinates']]\n    return geojson\n\n# GeoJSON 데이터를 WGS84로 변환\ngeo_str_wgs84 = convert_geojson(geo_str)\ngeo_str_wgs84 = json.dumps(geo_str_wgs84)"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#folium-함수생성",
    "href": "posts/coach-ds-20240821/index.html#folium-함수생성",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "folium 함수생성",
    "text": "folium 함수생성\n\n컬럼(품목)별로 시각화할 예정으로, Map을 생성하는 함수를 만들고 변수로 컬럼(11종) 등을 넣을 예정\n\n‘대분류_착지물동량 가구/인테리어’, ‘대분류_착지물동량 기타’, ‘대분류_착지물동량 도서/음반’\n‘대분류_착지물동량 디지털/가전’, ‘대분류_착지물동량 생활/건강’, ‘대분류_착지물동량 스포츠/레저’\n‘대분류_착지물동량 식품’, ‘대분류_착지물동량 출산/육아’, ‘대분류_착지물동량 패션의류’\n‘대분류_착지물동량 패션잡화’, ‘대분류_착지물동량 화장품/미용’\n\nChoropleth로 색만 칠해진 경우 구분이 어려워, 커서를 올리면 지역명을 볼 수 있게 Tooltip기능을 추가 \n[중요] Choropleth의 key_on은 사용할 geojson파일의 구조 확인 후 실제 사용할 코드와 매칭되도록 정해야함\n\n코드예시\n\nfolium.Choropleth(\n      geo_data=geojson,\n      name=df_new,\n      data=df_new,\n      columns=['수하인_구코드', category],\n      key_on='feature.properties.SIG_CD',\n      )\n\ngeojson 구조예시\n\n{\"type\":\"FeatureCollection\", \n\"features\": [{\"type\":\"Feature\",\n              \"geometry\":{\"type\":\"Polygon\",\n              \"coordinates\":[[[956893.7850491797,1953687.9806976495],[956893.7850491797,1953687.9806976495]]]},\n              \"properties\":{\"SIG_CD\":\"11110\",\n                            \"SIG_ENG_NM\":\"Jongno-gu\",\n                            \"SIG_KOR_NM\":\"종로구\"}},\n  ]}\n\n\n# folium 함수생성\n\ndef create_folium_choropleth(width:int, height:int, zoom_start:int, geojson:json, category:str):\n  # 기본 맵 생성(서울중심 위/경도를 가운데로 세팅)\n  m = folium.Map(\n    location=[37.55, 126.95],\n    zoom_start=zoom_start,\n    width=width,\n    height=height\n  )\n\n  # GeoJson 적용\n  gjson = folium.GeoJson(\n      geojson,\n      name='수하인_구코드',\n      # 툴팁추가\n       tooltip=folium.features.GeoJsonTooltip(fields=['SIG_ENG_NM', 'SIG_CD'],\n            aliases=['EngName','SIG_CD'],\n            style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\"),\n            sticky=True)\n  ).add_to(m)\n\n  # Choropleth 그리기\n  folium.Choropleth(\n      geo_data=geojson,\n      name=df_new,\n      data=df_new,\n      columns=['수하인_구코드', category],\n      key_on='feature.properties.SIG_CD',\n      fill_color='YlOrRd',\n      fill_opacity=0.7,\n      line_opacity=0.5,\n      legend_name=category,\n      nan_fill_color='white',\n      nan_fill_opacity=0,\n\n  ).add_to(m)\n\n  m.keep_in_front(gjson)\n\n  return m"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#folium-choropleth-시각화-진행",
    "href": "posts/coach-ds-20240821/index.html#folium-choropleth-시각화-진행",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "folium Choropleth 시각화 진행",
    "text": "folium Choropleth 시각화 진행\n\n품목별로 차이가 클 것으로 예상했지만, 대체로 비슷한 추이를 보임\n\n모든 품목이\n\n전반적으로 강서/강남/송파구 물량이 많음\n전반적으로 강남이 송파보다 물량이 많음\n\n스포츠/레저, 패션의류, 화장품은 차이가 거의 없었음\n육아용품은 오히려 송파구가 강남보다 물량이 많음\n\n\n\n# Choropleth 시각화 코드\n\n## 반복문으로 각 컬럼을 지정해 각각 folium Choropleth Map을 그림\nfor column_name in df_new.columns:\n  if column_name in ['수하인_구코드', '수하인_구명']:\n    continue\n  else:\n    print(column_name)\n    display(create_folium_choropleth(600, 400, 10, geo_str_wgs84, column_name))\n\n대분류_착지물동량 가구/인테리어\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 기타\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 도서/음반\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 디지털/가전\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 생활/건강\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 스포츠/레저\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 식품\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 출산/육아\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 패션의류\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 패션잡화\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n대분류_착지물동량 화장품/미용\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#stacked-bar-graph-시각화",
    "href": "posts/coach-ds-20240821/index.html#stacked-bar-graph-시각화",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "Stacked Bar Graph 시각화",
    "text": "Stacked Bar Graph 시각화\n\n세부적으로는 다르지만 전반적으로 분포는 비슷하게 보임\n구별로 어떤 품목 순위가 높은지는 눈에 들어오지 않아, 순위가 잘 보이게 별도로 확인 예정\n\n\n# GPT를 활용해 구별 품목을 시각화\nimport koreanize_matplotlib\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming df_new is your DataFrame# Step 1: Filter the columns that contain '대분류_' in their names\nmain_category_columns = [col for col in df_new.columns if '대분류_' in col]\n\n# Step 2: Melt the DataFrame to long format\ndf_melted = pd.melt(df_new, id_vars=['수하인_구명'], value_vars=main_category_columns,\n                    var_name='Category', value_name='Amount')\n\n# Step 3: Group by '수하인_구명' and sum the amounts\ndf_grouped = df_melted.groupby(['수하인_구명', 'Category']).sum().reset_index()\n\n# Step 4: Calculate the total sum per '수하인_구명' for proportion calculation\ndf_grouped['Total'] = df_grouped.groupby('수하인_구명')['Amount'].transform('sum')\ndf_grouped['Proportion'] = df_grouped['Amount'] / df_grouped['Total']\n\n# Step 5: Pivot the DataFrame for stacking\ndf_pivot = df_grouped.pivot_table(index='수하인_구명', columns='Category', values='Proportion', fill_value=0)\n\n# Step 6: Plot the stacked bar chart using Matplotlib\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Create a stacked bar plot\nbottoms = pd.Series([0] * len(df_pivot), index=df_pivot.index)\ncolors = sns.color_palette(\"tab20\", len(df_pivot.columns))\n\nfor i, category in enumerate(df_pivot.columns):\n    p = ax.bar(df_pivot.index, df_pivot[category], bottom=bottoms, color=colors[i], label=category)\n    bottoms += df_pivot[category]\n\n    ax.bar_label(p, labels=[f'{h:.2f}' for h in p.datavalues], label_type='center')\n# Adding labels and title\nax.set_xlabel('수하인_구명')\nax.set_ylabel('Proportion')\nax.set_title('Proportion of 수하인_구명 by Main Category')\nax.legend(title='Main Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Rotate x-axis labels if needed\nplt.xticks(rotation=45, ha='right')\n\n# Show plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/coach-ds-20240821/index.html#데이터프레임으로-확인-구별-상위-5개-품목",
    "href": "posts/coach-ds-20240821/index.html#데이터프레임으로-확인-구별-상위-5개-품목",
    "title": "[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?",
    "section": "데이터프레임으로 확인 (구별 상위 5개 품목)",
    "text": "데이터프레임으로 확인 (구별 상위 5개 품목)\n\n전반적으로 비슷한 추이를 보임\n\n식품이 1위, 생활/건강 or 패션의류가 2~3위, 기타 4위\n\n다만 5위는 대체로 비슷한 품목이지만 용산구, 금천구는 디지털/가전이 많았음\n\n용산은 추측이 되었으나 금천구는 추측되는 부분이 없어서 별도로 검색\n금천구는 가산디지털단지가 있으며, 용산처럼 도소매 전자상가까진 아니더라도 관련 유통사/제조사 등이 분포함을 확인함\n\n\n\n# 상위 5개 품목만 추려서 확인\ntemp_dict = {}\nfor each_index in df_pivot.index:\n  temp_dict[each_index] = df_pivot.loc[each_index].sort_values(ascending=False)[0:5].index.tolist()\n  temp_dict[each_index] = [item.replace('대분류_착지물동량 ', '') for item in temp_dict[each_index]]\n\n# 데이터 프레임 출력 (컬럼생략되지 않도록 set_option사용)\npd.set_option('display.max_columns', None)\nrank_df = pd.DataFrame(temp_dict)\nrank_df\n\n\n  \n    \n\n\n\n\n\n\n강남구\n강동구\n강북구\n강서구\n관악구\n광진구\n구로구\n금천구\n노원구\n도봉구\n동대문구\n동작구\n마포구\n서대문구\n서초구\n성동구\n성북구\n송파구\n양천구\n영등포구\n용산구\n은평구\n종로구\n중구\n중랑구\n\n\n\n\n0\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n식품\n\n\n1\n생활/건강\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n패션의류\n생활/건강\n패션의류\n생활/건강\n패션의류\n패션의류\n패션의류\n패션의류\n생활/건강\n생활/건강\n패션의류\n생활/건강\n패션의류\n패션의류\n\n\n2\n패션의류\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n패션의류\n생활/건강\n패션의류\n생활/건강\n생활/건강\n생활/건강\n생활/건강\n패션의류\n패션의류\n생활/건강\n패션의류\n생활/건강\n생활/건강\n\n\n3\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n기타\n\n\n4\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n디지털/가전\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n패션잡화\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용\n디지털/가전\n화장품/미용\n화장품/미용\n화장품/미용\n화장품/미용"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240711/index.html",
    "href": "posts/meta-dl-creditcard-20240711/index.html",
    "title": "[M_Study_최종과제] 신용카드 이상거래 탐지 모델링",
    "section": "",
    "text": "Kaggle CreditCard Fraud Detection (개선예정)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240711/index.html#데이터-확인kaggle-설명",
    "href": "posts/meta-dl-creditcard-20240711/index.html#데이터-확인kaggle-설명",
    "title": "[M_Study_최종과제] 신용카드 이상거래 탐지 모델링",
    "section": "데이터 확인(Kaggle 설명)",
    "text": "데이터 확인(Kaggle 설명)\n\nKaggle 링크 : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n컬럼별 정보\n\nV1~V28 : may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28)\nAmount : Transaction amount\nClass : 1 for fraudulent transactions, 0 otherwise"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240711/index.html#데이터-확인-및-전처리",
    "href": "posts/meta-dl-creditcard-20240711/index.html#데이터-확인-및-전처리",
    "title": "[M_Study_최종과제] 신용카드 이상거래 탐지 모델링",
    "section": "데이터 확인 및 전처리",
    "text": "데이터 확인 및 전처리\n\nimport pandas as pd\nimport sqlite3\n\n# Create a connection to the SQLite database\nconn = sqlite3.connect('creditcard.db')\n\n# Read the data from the database into a pandas DataFrame\ndf = pd.read_sql_query(\"SELECT * FROM creditcard\", conn)\n\n# Close the connection\nconn.close()\ndf\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284802\n172786.0\n-11.881118\n10.071785\n-9.834783\n-2.066656\n-5.364473\n-2.606837\n-4.918215\n7.305334\n1.914428\n...\n0.213454\n0.111864\n1.014480\n-0.509348\n1.436807\n0.250034\n0.943651\n0.823731\n0.77\n0\n\n\n284803\n172787.0\n-0.732789\n-0.055080\n2.035030\n-0.738589\n0.868229\n1.058415\n0.024330\n0.294869\n0.584800\n...\n0.214205\n0.924384\n0.012463\n-1.016226\n-0.606624\n-0.395255\n0.068472\n-0.053527\n24.79\n0\n\n\n284804\n172788.0\n1.919565\n-0.301254\n-3.249640\n-0.557828\n2.630515\n3.031260\n-0.296827\n0.708417\n0.432454\n...\n0.232045\n0.578229\n-0.037501\n0.640134\n0.265745\n-0.087371\n0.004455\n-0.026561\n67.88\n0\n\n\n284805\n172788.0\n-0.240440\n0.530483\n0.702510\n0.689799\n-0.377961\n0.623708\n-0.686180\n0.679145\n0.392087\n...\n0.265245\n0.800049\n-0.163298\n0.123205\n-0.569159\n0.546668\n0.108821\n0.104533\n10.00\n0\n\n\n284806\n172792.0\n-0.533413\n-0.189733\n0.703337\n-0.506271\n-0.012546\n-0.649617\n1.577006\n-0.414650\n0.486180\n...\n0.261057\n0.643078\n0.376777\n0.008797\n-0.473649\n-0.818267\n-0.002415\n0.013649\n217.00\n0\n\n\n\n\n284807 rows × 31 columns\n\n\n\n\nNull값 확인\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n\n\n\nAmount가 0인 값 확인\n\n결제에 대한 Validation 등에 대한 기록으로 추정\n이상거래(Class 1)인 데이터도 있긴 하지만, 실질적인 돈의 이동이 없는 것을 이상거래로 잡아야할지에 대한 의문\nAmount 0인 값은 제외하는 것으로 결정\n\n\n\ndf[df['Amount']==0]\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n383\n282.0\n-0.356466\n0.725418\n1.971749\n0.831343\n0.369681\n-0.107776\n0.751610\n-0.120166\n-0.420675\n...\n0.020804\n0.424312\n-0.015989\n0.466754\n-0.809962\n0.657334\n-0.043150\n-0.046401\n0.0\n0\n\n\n514\n380.0\n-1.299837\n0.881817\n1.452842\n-1.293698\n-0.025105\n-1.170103\n0.861610\n-0.193934\n0.592001\n...\n-0.272563\n-0.360853\n0.223911\n0.598930\n-0.397705\n0.637141\n0.234872\n0.021379\n0.0\n0\n\n\n534\n403.0\n1.237413\n0.512365\n0.687746\n1.693872\n-0.236323\n-0.650232\n0.118066\n-0.230545\n-0.808523\n...\n-0.077543\n-0.178220\n0.038722\n0.471218\n0.289249\n0.871803\n-0.066884\n0.012986\n0.0\n0\n\n\n541\n406.0\n-2.312227\n1.951992\n-1.609851\n3.997906\n-0.522188\n-1.426545\n-2.537387\n1.391657\n-2.770089\n...\n0.517232\n-0.035049\n-0.465211\n0.320198\n0.044519\n0.177840\n0.261145\n-0.143276\n0.0\n1\n\n\n575\n430.0\n-1.860258\n-0.629859\n0.966570\n0.844632\n0.759983\n-1.481173\n-0.509681\n0.540722\n-0.733623\n...\n0.268028\n0.125515\n-0.225029\n0.586664\n-0.031598\n0.570168\n-0.043007\n-0.223739\n0.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283719\n171817.0\n-0.750414\n0.904175\n0.996461\n0.427284\n1.720336\n0.929256\n0.794272\n0.176719\n-1.836261\n...\n0.050750\n0.115532\n-0.623995\n-0.186896\n0.733759\n2.558151\n-0.188835\n0.001654\n0.0\n0\n\n\n283782\n171870.0\n2.083677\n-0.065811\n-1.442870\n0.135416\n0.043035\n-1.306975\n0.335835\n-0.371635\n0.730560\n...\n-0.147536\n-0.246599\n0.194758\n-0.082277\n0.012887\n-0.069278\n-0.048995\n-0.065482\n0.0\n0\n\n\n283949\n172027.0\n2.132569\n-0.057836\n-1.724522\n-0.030326\n0.412146\n-0.903088\n0.345843\n-0.348132\n0.722638\n...\n-0.188739\n-0.343876\n0.105024\n-0.763831\n0.117381\n-0.027682\n-0.047514\n-0.071700\n0.0\n0\n\n\n284085\n172140.0\n-2.210521\n-1.039425\n0.189704\n-1.291932\n3.742120\n-1.665061\n3.120388\n-2.324089\n0.364926\n...\n-0.286359\n1.326003\n-0.361764\n-0.268117\n1.051309\n0.334629\n-1.930149\n-0.899888\n0.0\n0\n\n\n284770\n172759.0\n-0.822731\n1.270140\n-0.138566\n0.479620\n1.242101\n0.795218\n0.454284\n0.556038\n-1.550610\n...\n0.138766\n0.450908\n-0.192146\n-0.196218\n-0.261664\n2.372675\n-0.042743\n0.109613\n0.0\n0\n\n\n\n\n1825 rows × 31 columns\n\n\n\n\ndf_filtered1 = df[df['Amount'] != 0].copy()\ndf_filtered1\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284802\n172786.0\n-11.881118\n10.071785\n-9.834783\n-2.066656\n-5.364473\n-2.606837\n-4.918215\n7.305334\n1.914428\n...\n0.213454\n0.111864\n1.014480\n-0.509348\n1.436807\n0.250034\n0.943651\n0.823731\n0.77\n0\n\n\n284803\n172787.0\n-0.732789\n-0.055080\n2.035030\n-0.738589\n0.868229\n1.058415\n0.024330\n0.294869\n0.584800\n...\n0.214205\n0.924384\n0.012463\n-1.016226\n-0.606624\n-0.395255\n0.068472\n-0.053527\n24.79\n0\n\n\n284804\n172788.0\n1.919565\n-0.301254\n-3.249640\n-0.557828\n2.630515\n3.031260\n-0.296827\n0.708417\n0.432454\n...\n0.232045\n0.578229\n-0.037501\n0.640134\n0.265745\n-0.087371\n0.004455\n-0.026561\n67.88\n0\n\n\n284805\n172788.0\n-0.240440\n0.530483\n0.702510\n0.689799\n-0.377961\n0.623708\n-0.686180\n0.679145\n0.392087\n...\n0.265245\n0.800049\n-0.163298\n0.123205\n-0.569159\n0.546668\n0.108821\n0.104533\n10.00\n0\n\n\n284806\n172792.0\n-0.533413\n-0.189733\n0.703337\n-0.506271\n-0.012546\n-0.649617\n1.577006\n-0.414650\n0.486180\n...\n0.261057\n0.643078\n0.376777\n0.008797\n-0.473649\n-0.818267\n-0.002415\n0.013649\n217.00\n0\n\n\n\n\n282982 rows × 31 columns\n\n\n\n\ndf_filtered1[df_filtered1['Amount']==0]\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n\n\n0 rows × 31 columns\n\n\n\n\nLabel값(이상거래 비중) 확인\n\nImbalance한 경우, Accuracy는 성능측정에 한계가 있으므로, 다른 지표를 사용\nF1-Score 사용 예정\n\n\n\ndf_filtered1[\"Class\"].value_counts(normalize=True)\n\nClass\n0    0.998357\n1    0.001643\nName: proportion, dtype: float64\n\n\n\nX와 Y로 나누고, Scaler 적용(StandardScaler, MinMaxScaler)\n\n\ndf_x = df_filtered1.drop(['Time', 'Class'], axis=1).copy()\ndf_y = df_filtered1['Class'].copy()\n\n\ndf_x.shape, df_y.shape\n\n((282982, 29), (282982,))\n\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler_minmax = MinMaxScaler()\ndf_x_scaled = scaler_minmax.fit_transform(df_x)\n\nscaler_std = StandardScaler()\ndf_x_scaled = scaler_std.fit_transform(df_x_scaled)\n\n\ndf_x_scaled\n\narray([[-0.6947547 , -0.04287463,  1.67720049, ...,  0.33079262,\n        -0.06431265,  0.24200481],\n       [ 0.60928   ,  0.16247803,  0.11211972, ..., -0.02292114,\n         0.0439881 , -0.34378459],\n       [-0.6940121 , -0.81075824,  1.17321784, ..., -0.13798659,\n        -0.18145722,  1.15515532],\n       ...,\n       [ 0.98117703, -0.18130206, -2.14391593, ...,  0.01042466,\n        -0.08098516, -0.08388116],\n       [-0.12269922,  0.32263185,  0.46611873, ...,  0.26940666,\n         0.31584664, -0.31464064],\n       [-0.27242357, -0.11373381,  0.46666497, ..., -0.0066233 ,\n         0.04073321,  0.51063947]])\n\n\n\ntrain, test데이터 나누기\n\n\nfrom sklearn.model_selection import train_test_split\n\n# train + test\nx_train, x_test = train_test_split(df_x_scaled, test_size=0.3)\ny_train, y_test = train_test_split(df_y, test_size=0.3)\n\nprint('Train과 Test로 나누기')\nprint(x_train.shape, x_test.shape)\nprint(x_train.shape, y_test.shape)\n\n# train + validation\nx_train, x_validate = train_test_split(x_train, test_size=0.3)\ny_train, y_validate = train_test_split(y_train, test_size=0.3)\n\nprint()\nprint('Train과 Validation으로 나누기')\nprint(x_train.shape, x_validate.shape, x_test.shape)\nprint(y_train.shape, y_validate.shape, y_test.shape)\n\nTrain과 Test로 나누기\n(198087, 29) (84895, 29)\n(198087, 29) (84895,)\n\nTrain과 Validation으로 나누기\n(138660, 29) (59427, 29) (84895, 29)\n(138660,) (59427,) (84895,)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240711/index.html#모델링",
    "href": "posts/meta-dl-creditcard-20240711/index.html#모델링",
    "title": "[M_Study_최종과제] 신용카드 이상거래 탐지 모델링",
    "section": "모델링",
    "text": "모델링\n\n모델링 (기초 딥러닝)\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# 모델링\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Input((29,1)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid') # 이진분류이므로 Sigmoid사용\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy', # 0과 1의 이진분류이므로 binary_crossentropy 사용\n              metrics=['F1Score'])\n\n# 모델 학습\nhistory = model.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 4s 763us/step - F1Score: 0.0037 - loss: 0.0360\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 776us/step - F1Score: 0.0036 - loss: 0.0140\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 761us/step - F1Score: 0.0035 - loss: 0.0129\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 730us/step - F1Score: 0.0033 - loss: 0.0123\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 718us/step - F1Score: 0.0036 - loss: 0.0131\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 728us/step - F1Score: 0.0033 - loss: 0.0119\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 740us/step - F1Score: 0.0036 - loss: 0.0128\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 730us/step - F1Score: 0.0033 - loss: 0.0118\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 726us/step - F1Score: 0.0033 - loss: 0.0114\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 3s 729us/step - F1Score: 0.0035 - loss: 0.0122\n* 모델평가\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n\n\n\n모델링 (Keras tuner[Hyper parameter세팅])\n\nHidden Layer의 수\nNeuron의 수\nActivation fuction : ReLU, ELU 중 택1\n\nReaky ReLU도 고려대상에 넣고싶었지만, String이 아닌 별도 함수로 적용해야해서 제외\n\nOptimizer : 같은 조건으로 2개의 Optimizer(Adam, Nadam)로 먼저 돌렸다가 Tuner에게 추천받은 Nadam으로 설정\n\n\nimport keras_tuner as kt\n\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # Input & Flatten\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n\n    # Hidden Layers\n    for i in range(hp.Int('num_layers',min_value=1,max_value=20)):\n\n        # For Dense\n        units = hp.Int('units',min_value=5,max_value=150,step=5) # For Neurons\n        activation = hp.Choice('activation'+str(i),values=['relu','elu']) # For Activation\n\n        model.add(tf.keras.layers.Dense(units, activation=activation))\n\n        # For Dropout\n        dropout_rate = hp.Choice('dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n        \n    \n    optimizer=hp.Choice('optimizer',values=['Nadam'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\nwith tf.device('/device:GPU:0'):\n    tuner=kt.RandomSearch(build_model,\n                        objective=kt.Objective('val_F1Score', direction='max'),# accuracy 미사용\n                        overwrite=True,\n                        max_trials=9,\n                        project_name='randomsearch_model')\n\n\n    tuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 01m 10s]\nval_F1Score: 0.0028702165000140667\n\nBest val_F1Score So Far: 0.0028861388564109802\nTotal elapsed time: 00h 18m 28s\n\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'num_layers': 12,\n 'units': 90,\n 'activation0': 'relu',\n 'dropout0': 0.9,\n 'optimizer': 'Nadam',\n 'activation1': 'relu',\n 'dropout1': 0.1,\n 'activation2': 'relu',\n 'dropout2': 0.1,\n 'activation3': 'relu',\n 'dropout3': 0.1,\n 'activation4': 'relu',\n 'dropout4': 0.1,\n 'activation5': 'relu',\n 'dropout5': 0.1,\n 'activation6': 'relu',\n 'dropout6': 0.1,\n 'activation7': 'relu',\n 'dropout7': 0.1,\n 'activation8': 'relu',\n 'dropout8': 0.1,\n 'activation9': 'relu',\n 'dropout9': 0.1,\n 'activation10': 'relu',\n 'dropout10': 0.1,\n 'activation11': 'relu',\n 'dropout11': 0.1}\n\n\n\nmodel_2= tuner.get_best_models(num_models=1)[0]\nmodel_2.summary()\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 55 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 29)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 90)             │         2,700 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (Dropout)            │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (Dropout)            │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (Dense)                │ (None, 1)              │            91 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 92,881 (362.82 KB)\n\n\n\n Trainable params: 92,881 (362.82 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# 모델 학습\nhistory = model_2.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_2.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_2.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 15s 2ms/step - F1Score: 0.0031 - loss: 0.0463\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0222\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0031 - loss: 0.0144\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0032 - loss: 0.0202\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0200\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0166\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0036 - loss: 0.0167\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 12s 3ms/step - F1Score: 0.0032 - loss: 0.0131\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 11s 3ms/step - F1Score: 0.0033 - loss: 0.0229\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 11s 3ms/step - F1Score: 0.0032 - loss: 0.0216\n* 모델평가\n4334/4334 - 4s - 1ms/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 787us/step - F1Score: 0.0029 - loss: 0.0113\n\n\n\nF1Score 및 Loss 개선 비교\n\n모델평가(기본) \n\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n모델평가(Tuner) \n\n4334/4334 - 4s - 860us/step - F1Score: 0.0034 - loss: 0.0175\n2653/2653 - 2s - 862us/step - F1Score: 0.0029 - loss: 0.0164\n\n\n\n\n\n모델링 (Keras tuner + Initialization 추가)\n\nHeNormal (Kaiming) 적용\n\n수업시간에 배운 Random/Xavier/ Kaiming/MSRA Initialization for ReLU에 대해 검색해 봄\n\n미설정시 기본값은 Xavier(GlorotNormal)로 적용된다고 함\nPytorch와 Tensorflow에서의 Initialization 명칭이 다름\n\nHeNormal(Tensorflow) / Kaiming (Pytorch)\nGlorotNormal(Tensorflow) / XavierNormal (Pytorch)\nRandom Normall(Tensorflow) / Random(Pytorch)\n\nInnitialization은 kernel_initializer(Weight)뿐 아니라 bias_initializer(Bias)도 있음\n\n\n\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # Input & Flatten\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n\n    # Hidden Layers\n    for i in range(hp.Int('num_layers',min_value=1,max_value=20)):\n\n        # For Dense\n        units = hp.Int('units',min_value=5,max_value=150,step=5) # For Neurons\n        activation = hp.Choice('activation'+str(i),values=['relu','elu']) # For Activation\n\n        model.add(tf.keras.layers.Dense(units, activation=activation,\n                                        # 기본값은 glorot_uniform(Xavier), He(Kaiming)적용\n                                        kernel_initializer=tf.keras.initializers.HeNormal())) \n\n        # For Dropout\n        dropout_rate = hp.Choice('dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n        \n    \n    optimizer=hp.Choice('optimizer',values=['Nadam'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\nwith tf.device('/device:GPU:0'):\n    tuner=kt.RandomSearch(build_model,\n                        objective=kt.Objective('val_F1Score', direction='min'),# accuracy 미사용\n                        overwrite=True,\n                        max_trials=9,\n                        project_name='randomsearch_model_+initialize')\n\n\n    tuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 01m 32s]\nval_F1Score: 0.002870013704523444\n\nBest val_F1Score So Far: 0.0\nTotal elapsed time: 00h 21m 15s\n\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'num_layers': 17,\n 'units': 145,\n 'activation0': 'elu',\n 'dropout0': 0.9,\n 'optimizer': 'Nadam',\n 'activation1': 'relu',\n 'dropout1': 0.9,\n 'activation2': 'relu',\n 'dropout2': 0.8,\n 'activation3': 'elu',\n 'dropout3': 0.9,\n 'activation4': 'elu',\n 'dropout4': 0.6,\n 'activation5': 'elu',\n 'dropout5': 0.9,\n 'activation6': 'relu',\n 'dropout6': 0.4,\n 'activation7': 'relu',\n 'dropout7': 0.2,\n 'activation8': 'elu',\n 'dropout8': 0.7,\n 'activation9': 'relu',\n 'dropout9': 0.3,\n 'activation10': 'elu',\n 'dropout10': 0.4,\n 'activation11': 'relu',\n 'dropout11': 0.8,\n 'activation12': 'elu',\n 'dropout12': 0.2,\n 'activation13': 'relu',\n 'dropout13': 0.1,\n 'activation14': 'relu',\n 'dropout14': 0.1,\n 'activation15': 'relu',\n 'dropout15': 0.1,\n 'activation16': 'relu',\n 'dropout16': 0.1}\n\n\n\nmodel_3= tuner.get_best_models(num_models=1)[0]\nmodel_3.summary()\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 75 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 29)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 145)            │         4,350 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (Dropout)             │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_13 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_13 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_14 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_15 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (Dense)                │ (None, 145)            │        21,170 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (Dropout)            │ (None, 145)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (Dense)                │ (None, 1)              │           146 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 343,216 (1.31 MB)\n\n\n\n Trainable params: 343,216 (1.31 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# 모델 학습\nhistory = model_3.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_3.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_3.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 26s 5ms/step - F1Score: 0.0031 - loss: 0.3784\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 21s 5ms/step - F1Score: 0.0033 - loss: 0.0700\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 19s 4ms/step - F1Score: 0.0031 - loss: 0.1938\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 20s 5ms/step - F1Score: 0.0032 - loss: 0.2460\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 18s 4ms/step - F1Score: 0.0035 - loss: 0.2332\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 19s 4ms/step - F1Score: 0.0038 - loss: 0.0661\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 18s 4ms/step - F1Score: 0.0033 - loss: 0.0123\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 20s 5ms/step - F1Score: 0.0033 - loss: 0.0124\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 20s 5ms/step - F1Score: 0.0031 - loss: 0.0583\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 20s 4ms/step - F1Score: 0.0033 - loss: 0.4909\n* 모델평가\n4334/4334 - 6s - 1ms/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 4s - 1ms/step - F1Score: 0.0029 - loss: 0.0109\n\n\n\nF1Score 및 Loss 개선 비교\n\n모델평가(기본) \n\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n모델평가(Tuner) \n\n4334/4334 - 4s - 860us/step - F1Score: 0.0034 - loss: 0.0175\n2653/2653 - 2s - 862us/step - F1Score: 0.0029 - loss: 0.0164\n\n모델평가(Tuner + Kaiming Initialization) \n\n4334/4334 - 6s - 1ms/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 4s - 1ms/step - F1Score: 0.0029 - loss: 0.0109\n\n\n\n\n\n모델링 (Keras tuner + Batch Normalization 추가)\n\nActivation layer 전에 Batch normalization 적용\n\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # Input & Flatten\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n\n    # Hidden Layers\n    for i in range(hp.Int('num_layers',min_value=1,max_value=20)):\n\n        # For Dense\n        units = hp.Int('units',min_value=5,max_value=150,step=5) # For Neurons\n        activation = hp.Choice('activation'+str(i),values=['relu','elu']) # For Activation\n\n        model.add(tf.keras.layers.Dense(units, activation=activation,\n                                        # 기본값은 glorot_uniform(Xavier), He는 Kaiming\n                                        kernel_initializer=tf.keras.initializers.HeNormal())) \n        \n        # Add Batch Normalization\n        model.add(tf.keras.layers.BatchNormalization()) # Layer통과후 & Activation 전\n        \n        # For Dropout\n        dropout_rate = hp.Choice('dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n        \n    \n    optimizer=hp.Choice('optimizer',values=['Nadam'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\nwith tf.device('/device:GPU:0'):\n    tuner=kt.RandomSearch(build_model,\n                        objective=kt.Objective('val_F1Score', direction='min'),# accuracy 미사용\n                        overwrite=True,\n                        max_trials=9,\n                        project_name='randomsearch_model_+initialize+batchnormalize')\n\n\n    tuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 05m 00s]\nval_F1Score: 0.003198833204805851\n\nBest val_F1Score So Far: 0.003198833204805851\nTotal elapsed time: 00h 25m 05s\n\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'num_layers': 3,\n 'units': 5,\n 'activation0': 'relu',\n 'dropout0': 0.2,\n 'optimizer': 'Nadam',\n 'activation1': 'relu',\n 'dropout1': 0.1,\n 'activation2': 'relu',\n 'dropout2': 0.1}\n\n\n\nmodel_4= tuner.get_best_models(num_models=1)[0]\nmodel_4.summary()\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 31 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 29)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 5)              │           150 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (None, 5)              │            20 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 5)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 5)              │            30 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_1           │ (None, 5)              │            20 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 5)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 5)              │            30 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ (None, 5)              │            20 │\n│ (BatchNormalization)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 5)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │             6 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 276 (1.08 KB)\n\n\n\n Trainable params: 246 (984.00 B)\n\n\n\n Non-trainable params: 30 (120.00 B)\n\n\n\n\n# 모델 학습\nhistory = model_4.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 13s 2ms/step - F1Score: 0.0038 - loss: 0.0143\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0030 - loss: 0.0116\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0033 - loss: 0.0125\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 9s 2ms/step - F1Score: 0.0034 - loss: 0.0130\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0126\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0035 - loss: 0.0132\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0130\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0036 - loss: 0.0133\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0029 - loss: 0.0111\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0035 - loss: 0.0131\n* 모델평가\n4334/4334 - 3s - 788us/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 757us/step - F1Score: 0.0029 - loss: 0.0110\n\n\n\nF1Score 및 Loss 개선 비교\n\n모델평가(기본) \n\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n모델평가(Tuner) \n\n4334/4334 - 4s - 860us/step - F1Score: 0.0034 - loss: 0.0175\n2653/2653 - 2s - 862us/step - F1Score: 0.0029 - loss: 0.0164\n\n모델평가(Tuner + Kaiming Initialization) \n\n4334/4334 - 6s - 1ms/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 4s - 1ms/step - F1Score: 0.0029 - loss: 0.0109\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization) \n\n4334/4334 - 3s - 788us/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 757us/step - F1Score: 0.0029 - loss: 0.0110\n\n\n\n\n\n모델 중간 저장\n\n이후 부터는 Learning rate 등의 변경만 있을 예정으로, Tuner를 사용하지 않고 모델을 저장했다가 사용\n\n\nmodel_4 = tuner.get_best_models(num_models=1)[0]\nmodel_4.save('model_4.keras')\n\n\n\n모델링 (현재 모델에서 Learning rate별 비교 : 0.1, 0.01, 0.05)\n\nLearning rate : 0.1\n\nLoss값이 튀는 상황으로 줄여야 할 것으로 보임\n\n\n# 새로운 학습률 설정\nnew_learning_rate = 0.1\n\nmodel_4 = keras.models.load_model('model_4.keras')\nnew_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\nmodel_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\nhistory = model_4.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 9s 2ms/step - F1Score: 0.0038 - loss: 0.0155\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0036 - loss: 0.0142\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0035 - loss: 0.0136\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0034 - loss: 0.0137\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0143\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0036 - loss: 0.0144\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0031 - loss: 0.0120\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0037 - loss: 0.0145\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0036 - loss: 0.0141\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0034 - loss: 0.0131\n* 모델평가\n4334/4334 - 3s - 639us/step - F1Score: 0.0034 - loss: 0.0130\n2653/2653 - 2s - 629us/step - F1Score: 0.0032 - loss: 0.0127\n\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLearning rate : 0.01\n\nLoss값이 원만하게 하락\n\n\n# 새로운 학습률 설정\nnew_learning_rate = 0.01\n\nmodel_4 = keras.models.load_model('model_4.keras')\nnew_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\nmodel_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\nhistory = model_4.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step - F1Score: 0.0034 - loss: 0.0137\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0031 - loss: 0.0122\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0037 - loss: 0.0137\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0029 - loss: 0.0114\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0128\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0127\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0126\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0033 - loss: 0.0123\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0037 - loss: 0.0134\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0033 - loss: 0.0122\n* 모델평가\n4334/4334 - 3s - 649us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 658us/step - F1Score: 0.0032 - loss: 0.0121\n\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLearning rate : 0.05\n\nLoss값이 조금 위아래로 움직임.\n향후 현재의 10회가 아닌 1000회 epoch을 돌려볼 것이므로 학습시간을 고려새 0.01이 아닌 0.05로 적용하는 것을 고려\n\n\n# 새로운 학습률 설정\nnew_learning_rate = 0.05\n\nmodel_4 = keras.models.load_model('model_4.keras')\nnew_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\nmodel_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\nhistory = model_4.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step - F1Score: 0.0032 - loss: 0.0137\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0120\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0119\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0130\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0131\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0033 - loss: 0.0124\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0033 - loss: 0.0126\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0128\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0038 - loss: 0.0141\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0032 - loss: 0.0119\n* 모델평가\n4334/4334 - 3s - 666us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 648us/step - F1Score: 0.0032 - loss: 0.0119\n\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n추가적용 : Learning rate scheduler\n\nLearning rate 0.05를 적용하기로 했으므로, 동일한 기준인 10 epoch중 loss가 증가했던 구간부터 rate 변경 적용 #### 기본 Learning rate scheduler\n적용 전과 비교했을 때, Loss가 전반적으로 우하향 하는 추세를 보여줌\n\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport keras\nimport matplotlib.pyplot as plt\n\n\ndef scheduler(epoch, lr):\n    if epoch &lt; 5:\n        return float(lr)\n    else:\n        return float(lr * tf.exp(-0.1))\n\nlr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n\n# 새로운 학습률 설정\nnew_learning_rate = 0.05\n\nmodel_4 = keras.models.load_model('model_4.keras')\nnew_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\nmodel_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\n# 모델 학습\nhistory = model_4.fit(x_train, y_train, epochs=10, \n                      callbacks=[lr_scheduler])\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 1ms/step - F1Score: 0.0032 - loss: 0.0134 - learning_rate: 0.0500\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0134 - learning_rate: 0.0500\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0031 - loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 5s 1ms/step - F1Score: 0.0033 - loss: 0.0125 - learning_rate: 0.0500\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0031 - loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.04524187371134758.\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0129 - learning_rate: 0.0452\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.04093654081225395.\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0031 - loss: 0.0119 - learning_rate: 0.0409\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.037040915340185165.\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0037 - loss: 0.0137 - learning_rate: 0.0370\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.03351600840687752.\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0127 - learning_rate: 0.0335\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.030326539650559425.\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0127 - learning_rate: 0.0303\n* 모델평가\n4334/4334 - 3s - 643us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 611us/step - F1Score: 0.0032 - loss: 0.0120\n\n\n\n적용 전과 비교했을 때, Loss가 전반적으로 우하향 하는 추세를 보여줌\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nCosine Learning rate scheduler\n\nChatGPT의 도움을 받아 기존의 scheduler코드를 수업 때 배웠던 Cosine decay로 변경\nCosine decay 적용 전보다 더 부드럽게 우하향하는 추세를 보여줌\n\n\nimport numpy as np\n\ntotal_epoch_value = 10\n\ndef scheduler(epoch, lr):\n    # Compute the cosine decay factor\n    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / total_epoch_value))\n    # Update learning rate\n    new_lr = lr * cosine_decay\n    return float(new_lr)\n\nlr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n\n# 새로운 학습률 설정\nnew_learning_rate = 0.05\n\nmodel_4 = keras.models.load_model('model_4.keras')\nnew_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\nmodel_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\n# 모델 학습\nhistory = model_4.fit(x_train, y_train, epochs=total_epoch_value, \n                      callbacks=[lr_scheduler])\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - F1Score: 0.0036 - loss: 0.0149 - learning_rate: 0.0500\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.048776413634204034.\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0130 - learning_rate: 0.0488\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.04411868114727063.\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0033 - loss: 0.0123 - learning_rate: 0.0441\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.03502549477486741.\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0132 - learning_rate: 0.0350\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.022924484773646628.\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0035 - loss: 0.0129 - learning_rate: 0.0229\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.011462242342531681.\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0030 - loss: 0.0114 - learning_rate: 0.0115\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.003960107332522642.\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 1ms/step - F1Score: 0.0032 - loss: 0.0119 - learning_rate: 0.0040\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.0008162073473510564.\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0032 - loss: 0.0119 - learning_rate: 8.1621e-04\n\nEpoch 9: LearningRateScheduler setting learning rate to 7.794086367885386e-05.\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0032 - loss: 0.0120 - learning_rate: 7.7941e-05\n\nEpoch 10: LearningRateScheduler setting learning rate to 1.9073487425812585e-06.\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 6s 1ms/step - F1Score: 0.0034 - loss: 0.0126 - learning_rate: 1.9073e-06\n* 모델평가\n4334/4334 - 3s - 640us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 670us/step - F1Score: 0.0032 - loss: 0.0119\n\n\n\nCosine decay 적용 전보다 더 부드럽게 우하향하는 추세를 보여줌\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nF1Score 및 Loss 개선 비교\n\n모델평가(기본) \n\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n모델평가(Tuner) \n\n4334/4334 - 4s - 860us/step - F1Score: 0.0034 - loss: 0.0175\n2653/2653 - 2s - 862us/step - F1Score: 0.0029 - loss: 0.0164\n\n모델평가(Tuner + Kaiming Initialization) \n\n4334/4334 - 6s - 1ms/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 4s - 1ms/step - F1Score: 0.0029 - loss: 0.0109\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization) \n\n4334/4334 - 3s - 788us/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 757us/step - F1Score: 0.0029 - loss: 0.0110\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization + Learning rate Scheduling) \n\n4334/4334 - 3s - 640us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 670us/step - F1Score: 0.0032 - loss: 0.0119\n\n\n\n\n\n추가적용 : EarlyStopping with patience\n\nepoch 1000회로 세팅\npatience 200으로 세팅\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\ntotal_epoch_value = 1000\n\ndef scheduler(epoch, lr):\n    # Compute the cosine decay factor\n    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / total_epoch_value))\n    # Update learning rate\n    new_lr = lr * cosine_decay\n    return float(new_lr)\n\n\nwith tf.device('/device:GPU:0'):\n    lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n    es = EarlyStopping(monitor='F1Score', mode='max', verbose=1, patience=200)\n\n    # 새로운 학습률 설정\n    new_learning_rate = 0.05\n\n    model_4 = keras.models.load_model('model_4.keras')\n    new_optimizer = tf.keras.optimizers.Nadam(learning_rate=new_learning_rate)\n    model_4.compile(optimizer=new_optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n\n    # 모델 학습\n    history = model_4.fit(x_train, y_train, epochs=total_epoch_value, \n                        callbacks=[lr_scheduler, es],\n                        validation_data=(x_test,y_test))\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_4.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_4.evaluate(x_test, y_test, verbose=2)\n\n\nEpoch 1: LearningRateScheduler setting learning rate to 0.05000000074505806.\nEpoch 1/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0031 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0500\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.049999877375102676.\nEpoch 2/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0500\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.04999938433308759.\nEpoch 3/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0500\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.04999827576351572.\nEpoch 4/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0038 - loss: 0.0142 - val_F1Score: 0.0032 - val_loss: 0.0126 - learning_rate: 0.0500\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.04999630210880771.\nEpoch 5/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0138 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0500\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.049993217571968625.\nEpoch 6/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.04998877640343745.\nEpoch 7/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.04998273291321625.\nEpoch 8/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0500\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.04997484148299723.\nEpoch 9/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.04996485285374156.\nEpoch 10/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0029 - loss: 0.0111 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0500\n\nEpoch 11: LearningRateScheduler setting learning rate to 0.049952525311787795.\nEpoch 11/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0500\n\nEpoch 12: LearningRateScheduler setting learning rate to 0.04993761352668951.\nEpoch 12/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0499\n\nEpoch 13: LearningRateScheduler setting learning rate to 0.04991987228817006.\nEpoch 13/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0499\n\nEpoch 14: LearningRateScheduler setting learning rate to 0.04989906024197024.\nEpoch 14/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0499\n\nEpoch 15: LearningRateScheduler setting learning rate to 0.04987493245373588.\nEpoch 15/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0499\n\nEpoch 16: LearningRateScheduler setting learning rate to 0.04984724786871271.\nEpoch 16/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0114 - val_F1Score: 0.0032 - val_loss: 0.0123 - learning_rate: 0.0498\n\nEpoch 17: LearningRateScheduler setting learning rate to 0.04981576932241795.\nEpoch 17/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0498\n\nEpoch 18: LearningRateScheduler setting learning rate to 0.049780256105761714.\nEpoch 18/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0137 - val_F1Score: 0.0032 - val_loss: 0.0123 - learning_rate: 0.0498\n\nEpoch 19: LearningRateScheduler setting learning rate to 0.049740471422997326.\nEpoch 19/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0497\n\nEpoch 20: LearningRateScheduler setting learning rate to 0.049696178679942346.\nEpoch 20/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0497\n\nEpoch 21: LearningRateScheduler setting learning rate to 0.049647145217296636.\nEpoch 21/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0496\n\nEpoch 22: LearningRateScheduler setting learning rate to 0.04959314232045517.\nEpoch 22/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0133 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0496\n\nEpoch 23: LearningRateScheduler setting learning rate to 0.04953394150819476.\nEpoch 23/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0495\n\nEpoch 24: LearningRateScheduler setting learning rate to 0.049469314543987404.\nEpoch 24/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0495\n\nEpoch 25: LearningRateScheduler setting learning rate to 0.04939904088730174.\nEpoch 25/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0494\n\nEpoch 26: LearningRateScheduler setting learning rate to 0.049322900261303024.\nEpoch 26/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0493\n\nEpoch 27: LearningRateScheduler setting learning rate to 0.0492406763828536.\nEpoch 27/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0039 - loss: 0.0144 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0492\n\nEpoch 28: LearningRateScheduler setting learning rate to 0.04915215697099108.\nEpoch 28/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0492\n\nEpoch 29: LearningRateScheduler setting learning rate to 0.04905713747321047.\nEpoch 29/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0135 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0491\n\nEpoch 30: LearningRateScheduler setting learning rate to 0.048955409918060896.\nEpoch 30/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0490\n\nEpoch 31: LearningRateScheduler setting learning rate to 0.04884677779609719.\nEpoch 31/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0137 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0488\n\nEpoch 32: LearningRateScheduler setting learning rate to 0.04873104491241159.\nEpoch 32/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0138 - val_F1Score: 0.0032 - val_loss: 0.0126 - learning_rate: 0.0487\n\nEpoch 33: LearningRateScheduler setting learning rate to 0.04860802282816687.\nEpoch 33/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0137 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0486\n\nEpoch 34: LearningRateScheduler setting learning rate to 0.048477530864382556.\nEpoch 34/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0038 - loss: 0.0141 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0485\n\nEpoch 35: LearningRateScheduler setting learning rate to 0.04833938867581208.\nEpoch 35/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0131 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0483\n\nEpoch 36: LearningRateScheduler setting learning rate to 0.048193427402064476.\nEpoch 36/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0125 - learning_rate: 0.0482\n\nEpoch 37: LearningRateScheduler setting learning rate to 0.04803948224003987.\nEpoch 37/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0480\n\nEpoch 38: LearningRateScheduler setting learning rate to 0.04787739244900734.\nEpoch 38/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0131 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0479\n\nEpoch 39: LearningRateScheduler setting learning rate to 0.04770701249149665.\nEpoch 39/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0029 - loss: 0.0111 - val_F1Score: 0.0032 - val_loss: 0.0133 - learning_rate: 0.0477\n\nEpoch 40: LearningRateScheduler setting learning rate to 0.04752819718190602.\nEpoch 40/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0124 - learning_rate: 0.0475\n\nEpoch 41: LearningRateScheduler setting learning rate to 0.04734080911500294.\nEpoch 41/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0473\n\nEpoch 42: LearningRateScheduler setting learning rate to 0.047144726085502936.\nEpoch 42/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0471\n\nEpoch 43: LearningRateScheduler setting learning rate to 0.04693982624336337.\nEpoch 43/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0469\n\nEpoch 44: LearningRateScheduler setting learning rate to 0.046726002933231724.\nEpoch 44/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0467\n\nEpoch 45: LearningRateScheduler setting learning rate to 0.04650315356208633.\nEpoch 45/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0465\n\nEpoch 46: LearningRateScheduler setting learning rate to 0.04627118701398982.\nEpoch 46/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0463\n\nEpoch 47: LearningRateScheduler setting learning rate to 0.04603002364280635.\nEpoch 47/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0460\n\nEpoch 48: LearningRateScheduler setting learning rate to 0.045779591559066454.\nEpoch 48/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0129 - learning_rate: 0.0458\n\nEpoch 49: LearningRateScheduler setting learning rate to 0.04551983403368002.\nEpoch 49/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0141 - learning_rate: 0.0455\n\nEpoch 50: LearningRateScheduler setting learning rate to 0.04525069467084936.\nEpoch 50/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0453\n\nEpoch 51: LearningRateScheduler setting learning rate to 0.04497213962521406.\nEpoch 51/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0450\n\nEpoch 52: LearningRateScheduler setting learning rate to 0.044684139069935486.\nEpoch 52/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0447\n\nEpoch 53: LearningRateScheduler setting learning rate to 0.04438667459485334.\nEpoch 53/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0113 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0444\n\nEpoch 54: LearningRateScheduler setting learning rate to 0.044079742893301965.\nEpoch 54/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0131 - val_F1Score: 0.0032 - val_loss: 0.0144 - learning_rate: 0.0441\n\nEpoch 55: LearningRateScheduler setting learning rate to 0.04376335204512589.\nEpoch 55/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0114 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0438\n\nEpoch 56: LearningRateScheduler setting learning rate to 0.04343751780458211.\nEpoch 56/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0434\n\nEpoch 57: LearningRateScheduler setting learning rate to 0.04310227467966714.\nEpoch 57/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0431\n\nEpoch 58: LearningRateScheduler setting learning rate to 0.04275766481926046.\nEpoch 58/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0029 - loss: 0.0108 - val_F1Score: 0.0032 - val_loss: 0.0137 - learning_rate: 0.0428\n\nEpoch 59: LearningRateScheduler setting learning rate to 0.04240374169564687.\nEpoch 59/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0424\n\nEpoch 60: LearningRateScheduler setting learning rate to 0.042040577473459635.\nEpoch 60/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0135 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0420\n\nEpoch 61: LearningRateScheduler setting learning rate to 0.04166825190326737.\nEpoch 61/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0029 - loss: 0.0112 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0417\n\nEpoch 62: LearningRateScheduler setting learning rate to 0.041286855998470276.\nEpoch 62/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0413\n\nEpoch 63: LearningRateScheduler setting learning rate to 0.040896499394979254.\nEpoch 63/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0409\n\nEpoch 64: LearningRateScheduler setting learning rate to 0.04049729925174605.\nEpoch 64/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0405\n\nEpoch 65: LearningRateScheduler setting learning rate to 0.04008939129741767.\nEpoch 65/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0401\n\nEpoch 66: LearningRateScheduler setting learning rate to 0.03967291873571355.\nEpoch 66/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0397\n\nEpoch 67: LearningRateScheduler setting learning rate to 0.03924803959794496.\nEpoch 67/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0039 - loss: 0.0142 - val_F1Score: 0.0032 - val_loss: 0.0223 - learning_rate: 0.0392\n\nEpoch 68: LearningRateScheduler setting learning rate to 0.03881492302789103.\nEpoch 68/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0139 - val_F1Score: 0.0032 - val_loss: 0.0125 - learning_rate: 0.0388\n\nEpoch 69: LearningRateScheduler setting learning rate to 0.038373756621834496.\nEpoch 69/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0384\n\nEpoch 70: LearningRateScheduler setting learning rate to 0.03792473166260165.\nEpoch 70/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0379\n\nEpoch 71: LearningRateScheduler setting learning rate to 0.03746805782613492.\nEpoch 71/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0135 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0375\n\nEpoch 72: LearningRateScheduler setting learning rate to 0.03700395210292682.\nEpoch 72/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0370\n\nEpoch 73: LearningRateScheduler setting learning rate to 0.036532649808634195.\nEpoch 73/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0365\n\nEpoch 74: LearningRateScheduler setting learning rate to 0.036054393511249797.\nEpoch 74/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0361\n\nEpoch 75: LearningRateScheduler setting learning rate to 0.035569436681775514.\nEpoch 75/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0133 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0356\n\nEpoch 76: LearningRateScheduler setting learning rate to 0.03507804366240132.\nEpoch 76/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0031 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0351\n\nEpoch 77: LearningRateScheduler setting learning rate to 0.034580493306341685.\nEpoch 77/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0346\n\nEpoch 78: LearningRateScheduler setting learning rate to 0.03407706792429393.\nEpoch 78/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0341\n\nEpoch 79: LearningRateScheduler setting learning rate to 0.03356806794275915.\nEpoch 79/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0028 - loss: 0.0108 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0336\n\nEpoch 80: LearningRateScheduler setting learning rate to 0.03305380085029894.\nEpoch 80/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0114 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0331\n\nEpoch 81: LearningRateScheduler setting learning rate to 0.0325345775020496.\nEpoch 81/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0325\n\nEpoch 82: LearningRateScheduler setting learning rate to 0.03201072309373603.\nEpoch 82/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0320\n\nEpoch 83: LearningRateScheduler setting learning rate to 0.03148256978963389.\nEpoch 83/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0315\n\nEpoch 84: LearningRateScheduler setting learning rate to 0.030950456692133768.\nEpoch 84/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0137 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0310\n\nEpoch 85: LearningRateScheduler setting learning rate to 0.030414729810794083.\nEpoch 85/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0133 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0304\n\nEpoch 86: LearningRateScheduler setting learning rate to 0.029875742030884136.\nEpoch 86/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0299\n\nEpoch 87: LearningRateScheduler setting learning rate to 0.029333851252558933.\nEpoch 87/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0115 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0293\n\nEpoch 88: LearningRateScheduler setting learning rate to 0.028789420362314282.\nEpoch 88/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0127 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0288\n\nEpoch 89: LearningRateScheduler setting learning rate to 0.028242819031350504.\nEpoch 89/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0282\n\nEpoch 90: LearningRateScheduler setting learning rate to 0.027694420029314302.\nEpoch 90/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0277\n\nEpoch 91: LearningRateScheduler setting learning rate to 0.027144599198612736.\nEpoch 91/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0271\n\nEpoch 92: LearningRateScheduler setting learning rate to 0.026593733603646846.\nEpoch 92/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0266\n\nEpoch 93: LearningRateScheduler setting learning rate to 0.026042203332819286.\nEpoch 93/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0260\n\nEpoch 94: LearningRateScheduler setting learning rate to 0.025490389649056783.\nEpoch 94/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0255\n\nEpoch 95: LearningRateScheduler setting learning rate to 0.02493867678984359.\nEpoch 95/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0249\n\nEpoch 96: LearningRateScheduler setting learning rate to 0.0243874446546497.\nEpoch 96/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0028 - loss: 0.0105 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0244\n\nEpoch 97: LearningRateScheduler setting learning rate to 0.02383707607775812.\nEpoch 97/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0238\n\nEpoch 98: LearningRateScheduler setting learning rate to 0.0232879495225387.\nEpoch 98/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0233\n\nEpoch 99: LearningRateScheduler setting learning rate to 0.022740442709790577.\nEpoch 99/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0227\n\nEpoch 100: LearningRateScheduler setting learning rate to 0.022194928963840378.\nEpoch 100/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0222\n\nEpoch 101: LearningRateScheduler setting learning rate to 0.021651780837725836.\nEpoch 101/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0217\n\nEpoch 102: LearningRateScheduler setting learning rate to 0.021111362830849738.\nEpoch 102/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0211\n\nEpoch 103: LearningRateScheduler setting learning rate to 0.020574038650582698.\nEpoch 103/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0206\n\nEpoch 104: LearningRateScheduler setting learning rate to 0.020040162123402632.\nEpoch 104/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0115 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0200\n\nEpoch 105: LearningRateScheduler setting learning rate to 0.019510082641125054.\nEpoch 105/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0036 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0195\n\nEpoch 106: LearningRateScheduler setting learning rate to 0.01898414334111084.\nEpoch 106/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0135 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0190\n\nEpoch 107: LearningRateScheduler setting learning rate to 0.018462677481004153.\nEpoch 107/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0185\n\nEpoch 108: LearningRateScheduler setting learning rate to 0.017946012067510217.\nEpoch 108/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0179\n\nEpoch 109: LearningRateScheduler setting learning rate to 0.01743446604557586.\nEpoch 109/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0174\n\nEpoch 110: LearningRateScheduler setting learning rate to 0.0169283466852938.\nEpoch 110/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0169\n\nEpoch 111: LearningRateScheduler setting learning rate to 0.016427951403963508.\nEpoch 111/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0164\n\nEpoch 112: LearningRateScheduler setting learning rate to 0.015933567776404835.\nEpoch 112/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0159\n\nEpoch 113: LearningRateScheduler setting learning rate to 0.015445473545956377.\nEpoch 113/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0154\n\nEpoch 114: LearningRateScheduler setting learning rate to 0.014963933929296461.\nEpoch 114/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0150\n\nEpoch 115: LearningRateScheduler setting learning rate to 0.014489202538243347.\nEpoch 115/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0145\n\nEpoch 116: LearningRateScheduler setting learning rate to 0.014021520496817624.\nEpoch 116/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0140\n\nEpoch 117: LearningRateScheduler setting learning rate to 0.013561116462970584.\nEpoch 117/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0127 - learning_rate: 0.0136\n\nEpoch 118: LearningRateScheduler setting learning rate to 0.013108206651183544.\nEpoch 118/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0131\n\nEpoch 119: LearningRateScheduler setting learning rate to 0.012662993956245575.\nEpoch 119/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0127\n\nEpoch 120: LearningRateScheduler setting learning rate to 0.012225667081091017.\nEpoch 120/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0123 - learning_rate: 0.0122\n\nEpoch 121: LearningRateScheduler setting learning rate to 0.01179640236465126.\nEpoch 121/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0118\n\nEpoch 122: LearningRateScheduler setting learning rate to 0.011375362011883477.\nEpoch 122/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0114\n\nEpoch 123: LearningRateScheduler setting learning rate to 0.010962693228735833.\nEpoch 123/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0110\n\nEpoch 124: LearningRateScheduler setting learning rate to 0.01055853005235644.\nEpoch 124/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0106\n\nEpoch 125: LearningRateScheduler setting learning rate to 0.010162992486406664.\nEpoch 125/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0030 - loss: 0.0112 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0102\n\nEpoch 126: LearningRateScheduler setting learning rate to 0.009776186536514266.\nEpoch 126/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0098\n\nEpoch 127: LearningRateScheduler setting learning rate to 0.00939820335139082.\nEpoch 127/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0094\n\nEpoch 128: LearningRateScheduler setting learning rate to 0.009029121052566404.\nEpoch 128/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0090\n\nEpoch 129: LearningRateScheduler setting learning rate to 0.008669002981605245.\nEpoch 129/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0087\n\nEpoch 130: LearningRateScheduler setting learning rate to 0.00831789863593302.\nEpoch 130/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0083\n\nEpoch 131: LearningRateScheduler setting learning rate to 0.007975844602221773.\nEpoch 131/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0080\n\nEpoch 132: LearningRateScheduler setting learning rate to 0.00764286280991781.\nEpoch 132/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0076\n\nEpoch 133: LearningRateScheduler setting learning rate to 0.007318962806057378.\nEpoch 133/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0073\n\nEpoch 134: LearningRateScheduler setting learning rate to 0.007004140902813916.\nEpoch 134/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0070\n\nEpoch 135: LearningRateScheduler setting learning rate to 0.006698380665480482.\nEpoch 135/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0067\n\nEpoch 136: LearningRateScheduler setting learning rate to 0.006401653399573791.\nEpoch 136/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0115 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0064\n\nEpoch 137: LearningRateScheduler setting learning rate to 0.006113918192281751.\nEpoch 137/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0061\n\nEpoch 138: LearningRateScheduler setting learning rate to 0.00583512239919073.\nEpoch 138/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0058\n\nEpoch 139: LearningRateScheduler setting learning rate to 0.005565202574146112.\nEpoch 139/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0056\n\nEpoch 140: LearningRateScheduler setting learning rate to 0.005304084064791188.\nEpoch 140/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0053\n\nEpoch 141: LearningRateScheduler setting learning rate to 0.005051681497723035.\nEpoch 141/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0051\n\nEpoch 142: LearningRateScheduler setting learning rate to 0.004807899705742073.\nEpoch 142/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0127 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0048\n\nEpoch 143: LearningRateScheduler setting learning rate to 0.004572634652071941.\nEpoch 143/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0046\n\nEpoch 144: LearningRateScheduler setting learning rate to 0.004345772581226123.\nEpoch 144/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0131 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0043\n\nEpoch 145: LearningRateScheduler setting learning rate to 0.0041271918277528885.\nEpoch 145/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0041\n\nEpoch 146: LearningRateScheduler setting learning rate to 0.003916761966443078.\nEpoch 146/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0039\n\nEpoch 147: LearningRateScheduler setting learning rate to 0.0037143456165374526.\nEpoch 147/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0037\n\nEpoch 148: LearningRateScheduler setting learning rate to 0.003519798694295597.\nEpoch 148/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0035\n\nEpoch 149: LearningRateScheduler setting learning rate to 0.0033329702241221442.\nEpoch 149/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0119 - learning_rate: 0.0033\n\nEpoch 150: LearningRateScheduler setting learning rate to 0.003153703693298654.\nEpoch 150/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0032\n\nEpoch 151: LearningRateScheduler setting learning rate to 0.002981837080382247.\nEpoch 151/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0122 - learning_rate: 0.0030\n\nEpoch 152: LearningRateScheduler setting learning rate to 0.002817203763890849.\nEpoch 152/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0028\n\nEpoch 153: LearningRateScheduler setting learning rate to 0.002659632548117222.\nEpoch 153/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0027\n\nEpoch 154: LearningRateScheduler setting learning rate to 0.002508948787459685.\nEpoch 154/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0025\n\nEpoch 155: LearningRateScheduler setting learning rate to 0.0023649744086335148.\nEpoch 155/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0024\n\nEpoch 156: LearningRateScheduler setting learning rate to 0.002227528810319298.\nEpoch 156/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0022\n\nEpoch 157: LearningRateScheduler setting learning rate to 0.002096428882420658.\nEpoch 157/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0021\n\nEpoch 158: LearningRateScheduler setting learning rate to 0.0019714901202732477.\nEpoch 158/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0020\n\nEpoch 159: LearningRateScheduler setting learning rate to 0.0018525268586946407.\nEpoch 159/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0038 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0019\n\nEpoch 160: LearningRateScheduler setting learning rate to 0.001739352177175322.\nEpoch 160/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0017\n\nEpoch 161: LearningRateScheduler setting learning rate to 0.0016317790070494893.\nEpoch 161/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0016\n\nEpoch 162: LearningRateScheduler setting learning rate to 0.0015296205786924965.\nEpoch 162/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0015\n\nEpoch 163: LearningRateScheduler setting learning rate to 0.0014326904304467975.\nEpoch 163/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0014\n\nEpoch 164: LearningRateScheduler setting learning rate to 0.0013408029622755352.\nEpoch 164/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0013\n\nEpoch 165: LearningRateScheduler setting learning rate to 0.0012537739867035033.\nEpoch 165/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0113 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0013\n\nEpoch 166: LearningRateScheduler setting learning rate to 0.0011714208419180048.\nEpoch 166/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0012\n\nEpoch 167: LearningRateScheduler setting learning rate to 0.0010935627215877873.\nEpoch 167/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 0.0011\n\nEpoch 168: LearningRateScheduler setting learning rate to 0.0010200213286971748.\nEpoch 168/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 0.0010\n\nEpoch 169: LearningRateScheduler setting learning rate to 0.0009506207664891017.\nEpoch 169/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0037 - loss: 0.0133 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 9.5062e-04\n\nEpoch 170: LearningRateScheduler setting learning rate to 0.0008851880802090808.\nEpoch 170/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 8.8519e-04\n\nEpoch 171: LearningRateScheduler setting learning rate to 0.0008235533085124968.\nEpoch 171/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 8.2355e-04\n\nEpoch 172: LearningRateScheduler setting learning rate to 0.0007655498590444079.\nEpoch 172/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 7.6555e-04\n\nEpoch 173: LearningRateScheduler setting learning rate to 0.000711014611544236.\nEpoch 173/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 7.1101e-04\n\nEpoch 174: LearningRateScheduler setting learning rate to 0.0006597882362160549.\nEpoch 174/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0029 - loss: 0.0109 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 6.5979e-04\n\nEpoch 175: LearningRateScheduler setting learning rate to 0.0006117152403282328.\nEpoch 175/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 6.1172e-04\n\nEpoch 176: LearningRateScheduler setting learning rate to 0.0005666441221124275.\nEpoch 176/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 9s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 5.6664e-04\n\nEpoch 177: LearningRateScheduler setting learning rate to 0.0005244275774159951.\nEpoch 177/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 5.2443e-04\n\nEpoch 178: LearningRateScheduler setting learning rate to 0.0004849225434646702.\nEpoch 178/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0036 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 4.8492e-04\n\nEpoch 179: LearningRateScheduler setting learning rate to 0.0004479903496044815.\nEpoch 179/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 4.4799e-04\n\nEpoch 180: LearningRateScheduler setting learning rate to 0.00041349667884561114.\nEpoch 180/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0030 - loss: 0.0111 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 4.1350e-04\n\nEpoch 181: LearningRateScheduler setting learning rate to 0.00038131174396874263.\nEpoch 181/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.8131e-04\n\nEpoch 182: LearningRateScheduler setting learning rate to 0.00035131022098520786.\nEpoch 182/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.5131e-04\n\nEpoch 183: LearningRateScheduler setting learning rate to 0.000323371316538639.\nEpoch 183/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.2337e-04\n\nEpoch 184: LearningRateScheduler setting learning rate to 0.0002973788077995383.\nEpoch 184/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.9738e-04\n\nEpoch 185: LearningRateScheduler setting learning rate to 0.00027322094806250124.\nEpoch 185/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 2.7322e-04\n\nEpoch 186: LearningRateScheduler setting learning rate to 0.0002507905060636847.\nEpoch 186/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 2.5079e-04\n\nEpoch 187: LearningRateScheduler setting learning rate to 0.00022998472463185788.\nEpoch 187/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0038 - loss: 0.0136 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.2998e-04\n\nEpoch 188: LearningRateScheduler setting learning rate to 0.00021070525252817312.\nEpoch 188/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0127 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 2.1071e-04\n\nEpoch 189: LearningRateScheduler setting learning rate to 0.00019285808961416642.\nEpoch 189/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0133 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.9286e-04\n\nEpoch 190: LearningRateScheduler setting learning rate to 0.00017635351865172405.\nEpoch 190/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.7635e-04\n\nEpoch 191: LearningRateScheduler setting learning rate to 0.00016110605041721006.\nEpoch 191/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0030 - loss: 0.0111 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.6111e-04\n\nEpoch 192: LearningRateScheduler setting learning rate to 0.00014703428907961074.\nEpoch 192/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0127 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.4703e-04\n\nEpoch 193: LearningRateScheduler setting learning rate to 0.00013406090414221943.\nEpoch 193/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.3406e-04\n\nEpoch 194: LearningRateScheduler setting learning rate to 0.00012211248289829325.\nEpoch 194/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.2211e-04\n\nEpoch 195: LearningRateScheduler setting learning rate to 0.00011111943637018841.\nEpoch 195/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.1112e-04\n\nEpoch 196: LearningRateScheduler setting learning rate to 0.00010101588562002027.\nEpoch 196/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.0102e-04\n\nEpoch 197: LearningRateScheduler setting learning rate to 9.173956161771835e-05.\nEpoch 197/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 9.1740e-05\n\nEpoch 198: LearningRateScheduler setting learning rate to 8.323166577470412e-05.\nEpoch 198/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 8.3232e-05\n\nEpoch 199: LearningRateScheduler setting learning rate to 7.543677716041947e-05.\nEpoch 199/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 7.5437e-05\n\nEpoch 200: LearningRateScheduler setting learning rate to 6.830270726935913e-05.\nEpoch 200/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 6.8303e-05\n\nEpoch 201: LearningRateScheduler setting learning rate to 6.178038169635723e-05.\nEpoch 201/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 6.1780e-05\n\nEpoch 202: LearningRateScheduler setting learning rate to 5.5823715672683846e-05.\nEpoch 202/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0031 - loss: 0.0115 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 5.5824e-05\n\nEpoch 203: LearningRateScheduler setting learning rate to 5.038949665635076e-05.\nEpoch 203/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 5.0389e-05\n\nEpoch 204: LearningRateScheduler setting learning rate to 4.543725097729416e-05.\nEpoch 204/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0031 - loss: 0.0114 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 4.5437e-05\n\nEpoch 205: LearningRateScheduler setting learning rate to 4.0929124160673104e-05.\nEpoch 205/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 4.0929e-05\n\nEpoch 206: LearningRateScheduler setting learning rate to 3.682975847646033e-05.\nEpoch 206/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0131 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.6830e-05\n\nEpoch 207: LearningRateScheduler setting learning rate to 3.31061677502092e-05.\nEpoch 207/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.3106e-05\n\nEpoch 208: LearningRateScheduler setting learning rate to 2.9727622537092672e-05.\nEpoch 208/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0117 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.9728e-05\n\nEpoch 209: LearningRateScheduler setting learning rate to 2.6665534176340642e-05.\nEpoch 209/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.6666e-05\n\nEpoch 210: LearningRateScheduler setting learning rate to 2.3893339378491008e-05.\nEpoch 210/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0115 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 2.3893e-05\n\nEpoch 211: LearningRateScheduler setting learning rate to 2.138639024030536e-05.\nEpoch 211/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.1386e-05\n\nEpoch 212: LearningRateScheduler setting learning rate to 1.9121844781739403e-05.\nEpoch 212/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0119 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.9122e-05\n\nEpoch 213: LearningRateScheduler setting learning rate to 1.70785645133303e-05.\nEpoch 213/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.7079e-05\n\nEpoch 214: LearningRateScheduler setting learning rate to 1.5237012502896017e-05.\nEpoch 214/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0031 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.5237e-05\n\nEpoch 215: LearningRateScheduler setting learning rate to 1.357915519287874e-05.\nEpoch 215/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.3579e-05\n\nEpoch 216: LearningRateScheduler setting learning rate to 1.2088370381438874e-05.\nEpoch 216/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0035 - loss: 0.0127 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.2088e-05\n\nEpoch 217: LearningRateScheduler setting learning rate to 1.0749355670971902e-05.\nEpoch 217/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.0749e-05\n\nEpoch 218: LearningRateScheduler setting learning rate to 9.548043047235348e-06.\nEpoch 218/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0126 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 9.5480e-06\n\nEpoch 219: LearningRateScheduler setting learning rate to 8.4715155183082e-06.\nEpoch 219/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 8.4715e-06\n\nEpoch 220: LearningRateScheduler setting learning rate to 7.507931450634886e-06.\nEpoch 220/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0035 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 7.5079e-06\n\nEpoch 221: LearningRateScheduler setting learning rate to 6.646446085543275e-06.\nEpoch 221/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0032 - loss: 0.0116 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 6.6464e-06\n\nEpoch 222: LearningRateScheduler setting learning rate to 5.877143136087781e-06.\nEpoch 222/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0134 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 5.8771e-06\n\nEpoch 223: LearningRateScheduler setting learning rate to 5.190965951257081e-06.\nEpoch 223/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0031 - loss: 0.0113 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 5.1910e-06\n\nEpoch 224: LearningRateScheduler setting learning rate to 4.579655882737102e-06.\nEpoch 224/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 4.5797e-06\n\nEpoch 225: LearningRateScheduler setting learning rate to 4.035690191498989e-06.\nEpoch 225/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 4.0357e-06\n\nEpoch 226: LearningRateScheduler setting learning rate to 3.5522267110029194e-06.\nEpoch 226/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.5522e-06\n\nEpoch 227: LearningRateScheduler setting learning rate to 3.123050022982594e-06.\nEpoch 227/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.1230e-06\n\nEpoch 228: LearningRateScheduler setting learning rate to 2.7425225352732546e-06.\nEpoch 228/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0037 - loss: 0.0132 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 2.7425e-06\n\nEpoch 229: LearningRateScheduler setting learning rate to 2.4055370383374177e-06.\nEpoch 229/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0129 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.4055e-06\n\nEpoch 230: LearningRateScheduler setting learning rate to 2.1074729261271663e-06.\nEpoch 230/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0039 - loss: 0.0139 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 2.1075e-06\n\nEpoch 231: LearningRateScheduler setting learning rate to 1.8441558584291099e-06.\nEpoch 231/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0123 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.8442e-06\n\nEpoch 232: LearningRateScheduler setting learning rate to 1.611819649663071e-06.\nEpoch 232/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0036 - loss: 0.0130 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.6118e-06\n\nEpoch 233: LearningRateScheduler setting learning rate to 1.4070711650071392e-06.\nEpoch 233/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0121 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 1.4071e-06\n\nEpoch 234: LearningRateScheduler setting learning rate to 1.2268571134422569e-06.\nEpoch 234/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0033 - loss: 0.0120 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.2269e-06\n\nEpoch 235: LearningRateScheduler setting learning rate to 1.0684341126453187e-06.\nEpoch 235/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0124 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 1.0684e-06\n\nEpoch 236: LearningRateScheduler setting learning rate to 9.293406181791023e-07.\nEpoch 236/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0034 - loss: 0.0125 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 9.2934e-07\n\nEpoch 237: LearningRateScheduler setting learning rate to 8.073707546410956e-07.\nEpoch 237/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0028 - loss: 0.0103 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 8.0737e-07\n\nEpoch 238: LearningRateScheduler setting learning rate to 7.005507269301096e-07.\nEpoch 238/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 8s 2ms/step - F1Score: 0.0040 - loss: 0.0143 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 7.0055e-07\n\nEpoch 239: LearningRateScheduler setting learning rate to 6.071166600852295e-07.\nEpoch 239/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0035 - loss: 0.0128 - val_F1Score: 0.0032 - val_loss: 0.0120 - learning_rate: 6.0712e-07\n\nEpoch 240: LearningRateScheduler setting learning rate to 5.254945474903442e-07.\nEpoch 240/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 5.2549e-07\n\nEpoch 241: LearningRateScheduler setting learning rate to 4.542818008736369e-07.\nEpoch 241/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0033 - loss: 0.0122 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 4.5428e-07\n\nEpoch 242: LearningRateScheduler setting learning rate to 3.9223019466086514e-07.\nEpoch 242/1000\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 7s 2ms/step - F1Score: 0.0032 - loss: 0.0118 - val_F1Score: 0.0032 - val_loss: 0.0121 - learning_rate: 3.9223e-07\nEpoch 242: early stopping\n* 모델평가\n4334/4334 - 3s - 594us/step - F1Score: 0.0034 - loss: 0.0121\n2653/2653 - 2s - 584us/step - F1Score: 0.0032 - loss: 0.0121\n\n\n\n# Loss 및 F1-Score 시각화\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nax1.plot(history.history['loss'], label='Loss')\nax2.plot(history.history['F1Score'], label='F1-score')\nax1.legend(), ax2.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nF1Score 및 Loss 개선 비교\n\n모델평가(기본) \n\n4334/4334 - 2s - 531us/step - F1Score: 0.0034 - loss: 0.0114\n2653/2653 - 1s - 511us/step - F1Score: 0.0029 - loss: 0.0123\n\n모델평가(Tuner) \n\n4334/4334 - 4s - 860us/step - F1Score: 0.0034 - loss: 0.0175\n2653/2653 - 2s - 862us/step - F1Score: 0.0029 - loss: 0.0164\n\n모델평가(Tuner + Kaiming Initialization) \n\n4334/4334 - 6s - 1ms/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 4s - 1ms/step - F1Score: 0.0029 - loss: 0.0109\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization) \n\n4334/4334 - 3s - 788us/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 757us/step - F1Score: 0.0029 - loss: 0.0110\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization + Learning rate Scheduling) \n\n4334/4334 - 3s - 640us/step - F1Score: 0.0034 - loss: 0.0124\n2653/2653 - 2s - 670us/step - F1Score: 0.0032 - loss: 0.0119\n\n모델평가(Tuner + Kaiming Initialization + Batch Normalization + Learning rate Scheduling + Early Stopping) \n\n4334/4334 - 3s - 594us/step - F1Score: 0.0034 - loss: 0.0121\n2653/2653 - 2s - 584us/step - F1Score: 0.0032 - loss: 0.0121\n\n\n\n\n\n결론\n\nLoss는 지속적인 개선이 되고 있으나 F1score는 계속 비슷한 수치를 보임\n\nLearning rate Scheduling 적용시에만 약간의 개선이 있음\n\n과제 발표 후 관련 내용 질의 및 개선 예정\n과제 진행 중 생긴 궁금한 점들에 대해서도 추가 질의 예정\n\n이런 모델은 결국 사용하고자 만드는 것인데, PCA로 만든 모델이면 새로운 거래가 생성된 경우 그대로 넣어도 모델이 판별 가능한지\nepoch 수 대비 적절한 Early stopping patience값이 있는지"
  },
  {
    "objectID": "posts/dtcontest-ore-20240615/index.html",
    "href": "posts/dtcontest-ore-20240615/index.html",
    "title": "[공모전] 공공데이터 공모전-4(모델에 대한 Feature개발 및 평가지표)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(모델에 대한 Feature개발 및 평가지표)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240615/index.html#개요",
    "href": "posts/dtcontest-ore-20240615/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-4(모델에 대한 Feature개발 및 평가지표)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n분석 대상 광물에 대한 HSCODE 등 기준 고민\n모델 평가지표에 대한 고민 : Recall\n모델에 사용할 지표 개발에 대한 부분 (UN Comtrade 활용)\n\n생산국 수출량\n생산국 판매가격\n수입국 수입량"
  },
  {
    "objectID": "posts/dtcontest-ore-20240615/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240615/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-4(모델에 대한 Feature개발 및 평가지표)",
    "section": "내용정리",
    "text": "내용정리\n\n지난 회의정리\n\n모델링에 사용할 지표 중 시장위험지수 제거에 대한 동의\n\n수급안정화지수에 공통항목이 많아 추이가 비슷하여 제외\n\nFeature 중 하나로 BDI지수 추가\nFeature로 사용할 데이터를 추가로 발굴\n\nUN Comtrade database에서도 활용할 데이터 있는지 검토(타 팀원 제보)\n\n잘 만들어진 기존 모델들을 잘 사용하는 방안 고려\n\n이상요소 탐지에 R AnomalyDetection패키지 사용 (타 팀원이 적용예정)\n\n\n\n\n회의내용에 대한 Self고찰 및 아이디어 Develope\n\n분석할 광물에 대해 어떤 기준을 적용할지 고민\n\nUN Comtrade를 활용할 때 어떤 HSCODE를 적용할지에 대한 고민\n\n한국무역통계진흥원에서 제공하는 HSCODE분류서비스를 활용해, 가장 신고율이 높은 HSCODE 채택\n\n‘이차전지용’ 등 여러 용도가 있음에도 가장 높은 신고율은 ’기타’인 것으로 확인해 이 로직은 Drop\n\n현대경제연구원의 2차전지 보고서 중 품목별 HSCODE 내용이 있어 활용 검토\n\n핵심사업에 사용될 광물에 대한 분석이라는 점에서, 2차전지 컨셉 차용이 나쁘지 않은 것으로 보여 활용제안 예정\n\n산화/수산화리튬(282520), 산화/수산화코발트(282200), 황산코발트(283329)\n탄산리튬(283691), 이산화망간(850610), 산화/수산화니켈(282540), 황산니켈(283324)\n\n\n\n\n모델링에 사용할 평가기준에 대한 분석\n\n위험요소 탐지에 대한 모델이므로 Recall 사용으로 제안 예정\n\n위기로 ’판정’하는게 많아져야 위기를 놓치지 않을 확률이 크므로 Recall이 맞다고 생각했음\n\n\n모델링에 사용할 Feature에 대한 생각 (아래의 3개 지표를 개발하여 활용하도록 의견제안 예정)\n\nUN Comtrade의 요소를 바탕으로 아래의 지표를 생성하는 것을 고려\n\n생산국 수출량 : 각 광물의 생산량 상위 국가를 from기준으로 하여 수출(공급)량 판단\n생산국 판매가격 : 각 광물의 판매가격을 수출량으로 나누어 kg당 단가로 판단\n\n가격은 CIF, FOB중 기타비용을 제외하는 FOB를 우선고려, 데이터를 보고 추가결정\n\n수입국 수입량 : 특정 국가에서 수입량 변동이 급격히 변동(상승)하는 것 탐지"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Kibana\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html#airflow",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html#airflow",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Airflow",
    "text": "Airflow\n\nDag작성 샘플\n\nspark-submit.sh 스크립트 파일을 Bash operator로 사용\ngharchive의 변동 등으로 인한 사항을 적용 후 직접돌리기 위해 catchup=False\n\n  from airflow import DAG\n  from airflow.operators.bash_operator import BashOperator\n\n  from datetime import datetime, timedelta\n\n\n  default_args = {\n      \"owner\": \"airflow\",\n      \"depends_on_past\": False,\n      \"start_date\": datetime(2024, 6, 26),\n      \"retries\": 1,\n      \"retry_delay\": timedelta(minutes=2),\n      # \"on_failure_callback\": ,\n  }\n\n  dag = DAG(\"github-archive-pipeline\", \n          default_args=default_args, \n          max_active_runs=1, \n          schedule_interval=\"30 0 * * *\", \n          catchup=False, \n          tags=['data'])\n\n  dt = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n  download_data = BashOperator(\n      task_id='download-data',\n      bash_command=f\"/opt/airflow/jobs/download-data.sh {dt} \",\n      dag=dag\n  )\n\n  filename = '/opt/airflow/jobs/main.py'\n  filter_data = BashOperator(\n      task_id='filter-data',\n      bash_command=f'/opt/airflow/jobs/spark-submit.sh {filename} ',\n      dag=dag\n  )\n\n  download_data &gt;&gt; filter_data"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Kibana",
    "text": "Kibana\n\nData Visualization\n\n데이터에 따라 적합한 시각화가 다름   \n\n다양한 시각화 툴\n\nTableau\n\n유료 툴. ai 등 여러기능이 있으며 무거움\n교육 등이 활성화되어 있음\n\nHighCharts\n\n가볍게 1회성으로 작성시 활용가능\n\nPower BI\n\nTableau와 사용성이 비슷한 Windows툴\n\nGoogle Chart\n\n온라인으로 사용가능한 기본차트 제공\n\nD3.js, Chart.js\n\n자유도가 높은 라이브러리 (Javascript library)\nJavascript이므로 웹에서 데이터 렌더링할 때 사용\n실시간반영가능, Interactivity \n\nGrafana\n\n보통 클라우드의 모니터링을 위해 많이 사용\n서버 모니터링에 특화(Conetivity 등)\n\n\nKibana\n\nES에 데이터를 전송하면, Kibana에 자동반영 \n\n\nKibana 장단점\n\n장점\n\nElasticsearch 시스템 모니터링, 검색 성능 평가에 탁월\n\nES와 세트로 만들어져있으며, 별도의 import등이 필요 없음\n\n다양한 visualization 생성, 인사이트 발굴 가능\n데이터 검색/탐색 + (데이터 쌓이는 것)모니터링 + 분석을 하나의 UI 에서 진행\nvisualization의 다양성과 interactivity 가 떨어짐\n\n단점\n\nElasticsearch 특화\nplugin 설치가 번거로움\n잦은 Elasticsearch 버젼업에 따라가기 버거움\n\n항상 ES와 Kibana의 버전을 같이 써야함 \n\n\n\nKibana 기타\n\nObservability\nSecurity\nDiscover\nManagement - Index Patterns, Saved Object생성, Dev tool 등\n\nKibana 세팅\n\nservices:\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.4.3 # ES와 버전 맞추기\n    networks:\n      - default-network\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=http://es:9200\n      - ELASTICSEARCH_USERNAME=kibana\n      - ELASTICSEARCH_PASSWORD=password\n    ports:\n      - 5601:5601  # 로컬포트:컨테이너포트\n    depends_on:\n      - es\n\n\nKibana Manual install\n\nhttps://www.elastic.co/guide/en/kibana/current/install.html"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana-사용실습discover-visualize-library-dashboard",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana-사용실습discover-visualize-library-dashboard",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Kibana 사용실습(Discover, Visualize Library, Dashboard)",
    "text": "Kibana 사용실습(Discover, Visualize Library, Dashboard)\n\nDiscover\n\n기본화면\n\n좌측상단 Create data view\n\nName : 원하는 이름 지정\nIndex pattern : 사용할 index. daily-stats-*과 같이 와일드카드 가능\nTimestamp : @timestamp를 기본으로 쓰나 다른 필드로 변경할 수 있다\n\n좌측상단 Add filter (+버튼)\n\n+버튼을 눌러 UI로 필터링 가능\n\n좌측 Selected/Available fields\n\nSelected로 필드추가하여 테이블 형태로 볼 수 있다\n\n상단 Visualization type\n\n그래프 타입 선택 가능 \n\n상단 Documents\n\n그래프 막대를 클릭하여 해당일자만 조회 가능 \n\n\nCreate field\n\n타 필드에서 산출가능하다면 데이터를 최소로 저장하고 필드추가(Script Field생성)하는 방법이 있음\n좌측상단 Create field\n\nName지정 및 아래와 같이 script로 계산가능\n\n  if (doc['fork_count'].size() != 0){\n      return doc['fork_count'].value + 1;\n  }\n\n\n\nclass_w6-5.jpg\n\n\n\nDiscover에서 값을 확인하다가, 필드클릭 후 Visualize로 이동하여 보는 경우 많음 \n\nVisualize Library\n\n종류\n\nLens : 그래프 등 기본 시각화\nMaps : 지도 위에 시각화\nTSVB : TSVB로 들어간 series data를 위한 기능\nCustom visualization : Script로 시각화\n\nLens 실습\n\nCreate - Lens로 진입한 후 좌측상단에서 view를 선택.\n추천하는 시각화로 표현되어 있음\n하단의 Suggestions의 시각화이며, 교체도 가능\n\n우측에서 대상 필드 및 Median, Average등 가능  \n\nKQL 샘플\n\nFilter ratio example\ncount(kql='response.status_code &gt; 400') / count()\nWeek over week example (previous week : shift='1w')\n\n서로 다른 단위는 연산 불가\n\npercentile(system.network.in.bytes, percentile=99) /\npercentile(system.network.in.bytes, percentile=99, shift='1w')\nPercent of total example\nsum(products.base_price) / overall_sum(sum(products.base_price))\n\n\nDashboard\n\n만들어 둔 Visualization 또는 새로 생성하여 구성 가능"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana-기타",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html#kibana-기타",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Kibana 기타",
    "text": "Kibana 기타\n\nPainless lab\n\nManagement - Devtools - Painless lab\n\n필드 추가 등을 직접하다가 오류나는 경우, Elastic search가 멈출 수 있고 실제 서비스라면 문제될 수 있음\n\nScript field : * ES field에 없는 값을 kibana에서 사용하고 싶을 때 생성할 수 있는 필드\n\nPainless lab에서 테스트하면 위와 같은 경우를 방지할 수 있음\n\n샘플코드 (필드추가)\nGET /_search\n  {\n      \"query\" : {\n          \"match_all\": {}\n      },\n      \"script_fields\" : {\n          \"필드명1\" : {\n              \"script\" : {\n                  \"lang\": \"painless\",\n                  \"source\": \"doc['price'].value * 2\"             }\n          },\n                  }    }"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240909/index.html#기타-참고자료",
    "href": "posts/meta-de-spark_and_airflow-20240909/index.html#기타-참고자료",
    "title": "[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "기타 참고자료",
    "text": "기타 참고자료\n\nKibana Demo (샘플데이터로 시각화해보기)\n\nhttps://demo.elastic.co/cookie/index.html?goto=%2F\n\nD3 Best practice\n\nhttps://observablehq.com/collection/@observablehq/featured-creators\n\n상황별 시각화\n\nhttps://medium.com/@abdallahashraf90x/how-to-choose-the-right-visualization-for-your-data-data-analysis-c49b1469a583\n\nKibana 스크립트 필드에서 Painless 사용\n\nhttps://www.elastic.co/kr/blog/using-painless-kibana-scripted-fields"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240906/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240906/index.html",
    "title": "[DE스터디/5주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Airflow\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240906/index.html#elasticsearch",
    "href": "posts/meta-de-spark_and_airflow-20240906/index.html#elasticsearch",
    "title": "[DE스터디/5주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "ElasticSearch",
    "text": "ElasticSearch\n\nES에 기록하기\n\ndf.write.format(\"org.elasticsearch.spark.sql\") \\\n      .mode(\"append\") \\\n      .option(\"es.nodes\", &lt;host주소&gt;) \\\n      .option(\"es.index.auto.create\",  \"yes\") \\\n      .option(\"es.resource\", &lt;대상 index&gt;) \\\n      .save()\n\nkibana\n\nElastic Stack을 탐색할 수 있게 하는 시각화 및 관리 서비스\n\n데이터 검색/모니터링/보안관리/분석/시각화\n\nElastic search의 데이터가 들어오는 것도 쉽게 볼 수 있다 (일일이 쿼리 필요없이)"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240906/index.html#airflow",
    "href": "posts/meta-de-spark_and_airflow-20240906/index.html#airflow",
    "title": "[DE스터디/5주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Airflow",
    "text": "Airflow\n\nWorkflow Platform\n\nBatch Task 처리/스케줄링/모니터링을 위한 플랫폼\n한 눈에 볼 수 있는 UI + Task병렬처리 + 모니터링/스케줄링 필요함\ncrontab은 스케줄링은 가능하지만, 한 눈에 볼 수 있는 UI가 없고 병렬처리 기능 없음 \n\nAirflow\n\nBatch Task 처리/스케줄링/모니터링을 위한 Workflow Platform 중 하나\nopen-source platform forbatch-oriented workflows\n여러 플러그인을 제공해 connectivity 좋음 \n\nAirflow UI(Dashboard)\n\n실패한 경우 처음부터 실행할 필요 없이, 실패한 log만 보거나 실행 가능\n다양한 Status : deferred, failed, running 등\nCalender탭 : 요일별 성공/실행 등\nTask Duration탭 : 작업별 소요시간 등\nCode탭 : 코드실행 결과 등을 확인. 수정은 불가하나 수정사항은 바로 확인 가능  \n\nAirflow UI(DAGs)\n\n작업별 on/off, 태그 설정, Last Run 등 확인 가능  \n\nAirflow UI(Calender)\n\n큰 장애가 있던 날 등을 확인 가능  \n\nAirflow 구조\n\n플러그인을 Webserver와 Scheduler에 설치\n작업을 설정하면 Metadata DB에 들어가고 Scheduler가 읽어들여 작업 스헹\n유저가 작성한 DAG로 작업수행 후 결과를 다시 DB에 저장\n작업결과를 Webserver가 읽어 UI로 보여줌\n위와 같은 구조이므로 DB가 먼저 떠야함\n\n실습환경은 postgres로 되어있음(depends_on: postgres)   \n\n\nAirflow 구조 (실무적 시나리오)\n\nDAG작성자,Workflow플랫폼 관리자, Operation user(작업여부 등 확인, DAG작성자일 수 있음)\n여러 개의 Execution node가 작성된 DAG job 실행\nOperation user가 실행결과나 로그 등을 확인하거나 긴급/필요한 경우 작업 중단 등 수행  \n\nComponents(구성요소)\n\nDatabase : Storing metadata\nScheduler : Schedule & execute DAG\nWorker : Execute tasks\n\nScheduler와 같은 역할. Scheduler가 여러개 뜬 것과 비슷하게 이해\n\nWeb server : UI \n\nREST API\n\nAirflow설정이나 강제실행 등을 외부에서 원격으로 REST API호출해서 가능\n운영 편의성의 한 방법으로 제공(리소스 조정이나 권한부여 등)\njson input & response \n\nAirflow Pros & Cons(장단점)\n\n장점\n\n범용성 높음(타 워크플로우 대비 플러그인 많음)\n활발한 커뮤니티(유저 많음)\nPython으로 쉽고, 정교한(xml보다) DAG 구성가능\nRetry기능\n\n단점\n\n기술적으로 성숙 중(여러 이슈가 있음)\n\n실무에서 사용하기에는 무리 없는 정도 \n\n\n\n기타 유사 플랫폼\n\nJenkins (Devops구성 등에 많이 사용)\n\nHudson에서 Jenkins로 이름변경. 처음에는 build 실패시 로그 등 확인하고자 만들어짐(build자동화)\n\n실패가 누적되면 좌측 상단 사람아이콘이 점점 화난 얼굴로 바뀌어 확인 가능\n\n플러그인을 활용해서 자동화 작업 처리\n\n업데이트가 안되거나 불안정한 플러그인이 좀 있음\n\nTask파이프라인을 만들어 CI/CD를 구축하는데 사용\n\nAzkaban\n\nHadoop Batch job을 위해 LinkedIn 이 개발\nUI담당 웹서버 + Auth + 스케줄링 + 모니터링이 각각 1개만 존재\n\n위와 함께 별도의 executor가 다수 존재(executor만 다수 존재)\n\n다양한 기능 부족(airflow는 좀 더 범용적인 타겟)\nmultiple executor mode 설정해야 병렬처리 가능\nbusy-waiting 등의 작업대기 패턴 지정 불가\nHadoop기반은 좀 더 유리한 점이 있고, Airflow와는 서로 있거나 없는 기능이 존재\n\nOozie, Luigi 등 \n\nAirflow관련 실습 참고사항(세팅방법)\n\nDockerfile\n\n실습상 Pyspark로 실행되므로 Airflow에 파이썬 패키지를 깔 일은 거의 없음\n\nAirflow에서 실행하기 위한 패키지 등이 필요하다면 Dockerfile을 수정\n하단 코드의 pip install 부분을 수정\n\nairflow가 뜨지 않는다면 arm64가 맞는지 확인 후 수정\n\n하단 코드의 ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-arm64 수정\n\nDockerfile수정 후 단순히 down & up하면 반영되지 않음\n\n기존 이미지가 있는지 확인 후 이미지가 있으면 Docker는 별도 작업을 하지 않음\ndocker stop → rm → rmi 하여 기존 이미지 제거 후, up하여 build 진행\nFROM apache/airflow:2.7.1-python3.11\n\nUSER root\nRUN apt-get update\nRUN apt-get install -y gcc python3-dev openjdk-11-jdk wget\nRUN apt-get clean\n\n# Set JAVA_HOME environment variable\nENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-arm64\n\nUSER airflow\n\nRUN pip install apache-airflow apache-airflow-providers-apache-spark pyspark elasticsearch\n\n\nDocker-compose.yml\nwebserver:\n  &lt;&lt;: *airflow-common\n  command: webserver   # application 중 webserver\n  ports:\n    - \"8081:8080\"\n  depends_on:\n    - scheduler # scheduler가 먼저떠야해서 depends_on 설정\n\nscheduler:\n  &lt;&lt;: *airflow-common # 오류가 있다면 하단 command에 'airflow db init &&' 를 추가\n  command: bash -c \"airflow db migrate && airflow users create --username airflow --firstname airflow --lastname airflow --role Admin --email airflow@gmail.com --password airflow && airflow scheduler\"\n\n\nAirflow 설치방법\n\nInstallation\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\n\nQuick Start\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/start.html"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240906/index.html#dag-작성하는법",
    "href": "posts/meta-de-spark_and_airflow-20240906/index.html#dag-작성하는법",
    "title": "[DE스터디/5주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "DAG 작성하는법",
    "text": "DAG 작성하는법\n\nDAG(Directed Acyclic Graph)\n\n작업의 dependencies과 relationships를 정의\n다양한 Operator가 있음 (Operator = Task의 단위) \n\nDAG 선언\n\nwith문 사용\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),\n    schedule=\"@daily\", # 리소스가 비는 시간에 실행해줌\n):\n\nEmptyOperator(task_id=\"task\")\n변수에 할당 후 operator에 넣기\nmy_dag = DAG(\n    dag_id=\"my_dag_name\",\n    start_date=datetime.datetime(2021, 1, 1),      \n    schedule=\"@daily\",\n)\nEmptyOperator(task_id=\"task\", dag=my_dag)\n데코레이터 사용\nfrom airflow.decorators import dag\n\n@dag(start_date=datetime.datetime(2021, 1, 1), schedule=\"@daily\") \ndef generate_dag():\n    EmptyOperator(task_id=\"task\")\n\ngenerate_dag()\n데코레이터로 task지정 (+순서지정)\nfrom airflow.decorators import task\nfrom airflow.operators.bash import BashOperator\n\n# A DAG represents a workflow, a collection of tasks\nwith DAG(dag_id=\"demo\", start_date=datetime(2022, 1, 1), schedule=\"0 0 * * *\") as dag: # Tasks are represented as operators\n    hello = BashOperator(task_id=\"hello\", bash_command=\"echo hello\")\n\n    @task()\n    def airflow():\n        print(\"airflow\")\n\n    # Set dependencies between tasks\n    hello &gt;&gt; airflow()\n\n\nOperators(플러그인 역할)\n\nHttpOperator\nMySqlOperator\nPostgresOperator\nMsSqlOperator\nOracleOperator\nJdbcOperator\nDockerOperator\nHiveOperator\nS3FileTransformOperator\nPrestoToMySqlOperator\nSlackAPIOperator\n\nPython operator예시\n\n샘플코드에서 xcom_pull()은 DAG 내의 task 사이에서 데이터를 전달할 때 사용\ndag = DAG(\n    dag_id=\"example_template_as_python_object\",\n    schedule=None,              # @daily, 0 0 * * * 등\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,              # start_date와 현재 사이의 실행이 안된 분량을 catchup=True하여 실행\n    render_template_as_native_obj=True,\n    )\n\n# task 데코레이터 사용\n@task(task_id=\"extract\")\ndef extract():\n    data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}' \n    return json.loads(data_string)\n\nextract_task = extract()\n\n# Python operator 사용\ndef transform(order_data):\n    print(type(order_data))\n    total_order_value = 0\n    for value in order_data.values():\n    total_order_value += value\n    return {\"total_order_value\": total_order_value}\n\ntransform_task = PythonOperator(\n    task_id=\"transform\",\n    # transform함수의 argument넣기 & xcom_pull로 extract task데이터 가져오기\n    op_kwargs={\"order_data\": \"{{ti.xcom_pull('extract')}}\"}, \n    python_callable=transform,\n    )\n\nextract_task &gt;&gt; transform_task\n\n\nDependency 정의(예시)\n\n순서가 중요하지 않다면 리스트[]에 넣기\n\nfirst_task &gt;&gt; [second_task, third_task]\n\n방향(&lt;&lt;)에 따라 역순으로 실행\n\nthird_task &lt;&lt; fourth_task\n\nfirst_task 후 [second_task, third_task]\n\nfirst_task.set_downstream([second_task, third_task])\n\nfourth_task 후 third_task\n\nthird_task.set_upstream(fourth_task)\n\ncross_downstream\n# Before\n# [op1, op2] &gt;&gt; op3\n# [op1, op2] &gt;&gt; op4\n\n# After (Before의 2줄 코드를 아래와 같이 1줄로 표현 가능)\nfrom airflow.models.baseoperator import cross_downstream\n\ncross_downstream([op1, op2], [op3, op4])\nchain\n# Before\n# op1 &gt;&gt; op2 &gt;&gt; op3 &gt;&gt; op4\n\n# After1 (&gt;&gt; 대신 chain 사용)\nfrom airflow.models.baseoperator import chain\n\nchain(op1, op2, op3, op4)\n# After2 (task많은 경우 comprehension으로 dynamic하게 사용)\nchain(*[EmptyOperator(task_id='op' + i) for i in range(1, 6)])\n\n\nRetry\n\ndefault_args에 넣어서 설정\n후반부로 갈수록 느리게하는 exponential 전략 등 있음\n\n  from airflow.models.dag import DAG\n  from airflow.operators.bash import BashOperator\n  import datetime\n  import pendulum\n\n  dag = DAG(\n      \"tutorial\",\n      default_args={\n          \"depends_on_past\": True, # 과거 실행에 의존\n          \"retries\": 1,            # retry 횟수\n          \"retry_delay\": datetime.timedelta(minutes=3), # retry주기\n          },\n      start_date=pendulum.datetime(2015, 12, 1, tz=\"UTC\"), \n      description=\"A simple tutorial DAG\", # UI에서 보임\n      schedule=\"@daily\", \n      catchup=False,\n  )\n\nBranch\n\n샘플코드\n    @task.branch(task_id=\"branch_task\")\n\n    def branch_func(ti=None):\n        xcom_value = int(ti.xcom_pull(task_ids=\"start_task\")) \n        if xcom_value &gt;= 5:\n            return \"continue_task\"\n        elif xcom_value &gt;= 3:\n            return \"stop_task\"\n        else:\n            return None\n\n    start_op = BashOperator(\n        task_id=\"start_task\",\n        bash_command=\"echo 5\",\n        do_xcom_push=True, # 터미널에 찍힌 위의 5값이 xcom으로 전달\n        dag=dag,\n        )\n\n    branch_op = branch_func()\n\n    # branch_func에 필요한 \"continue_task\", \"stop_task\"\n    continue_op = EmptyOperator(task_id=\"continue_task\", dag=dag) \n    stop_op = EmptyOperator(task_id=\"stop_task\", dag=dag) \n\n    start_op &gt;&gt; branch_op &gt;&gt; [continue_op, stop_op]\n샘플코드 with 이미지\n\n\n\n\nclass_w5_6.jpg\n\n\n  dag = DAG(\n      dag_id=\"branch_without_trigger\",\n      schedule=\"@once\",\n      start_date=pendulum.datetime(2019, 2, 28, tz=\"UTC\"),\n  )\n\n  run_this_first = EmptyOperator(task_id=\"run_this_first\", dag=dag)\n\n  @task.branch(task_id=\"branching\")\n  def do_branching():\n      return \"branch_a\"\n\n  branching = do_branching()\n  branch_a = EmptyOperator(task_id=\"branch_a\", dag=dag)\n\n  follow_branch_a = EmptyOperator(task_id=\"follow_branch_a\", dag=dag) \n  branch_false = EmptyOperator(task_id=\"branch_false\", dag=dag) \n  join = EmptyOperator(task_id=\"join\", dag=dag)\n\n  run_this_first &gt;&gt; branching\n\n  branching &gt;&gt; branch_a &gt;&gt; follow_branch_a &gt;&gt; join\n  branching &gt;&gt; branch_false &gt;&gt; join\n\n\n\nTrigger rule\n\n종류\n\nall_success (default): All upstream tasks have succeeded\nall_failed: All upstream tasks are in a failed or upstream_failed state\nall_done: All upstream tasks are done with their execution\nall_skipped: All upstream tasks are in a skipped state\none_failed: At least one upstream task has failed (does not wait for all upstream tasks to be done)\none_success: At least one upstream task has succeeded (does not wait for all upstream tasks to be done)\none_done: At least one upstream task succeeded or failed\nnone_failed: All upstream tasks have not failed or upstream_failed - that is, all upstream tasks have succeeded or been skipped\nnone_failed_min_one_success: All upstream tasks have not failed or upstream_failed, and at least one upstream task has succeeded.\nnone_skipped: No upstream task is in a skipped state - that is, all upstream tasks are in a success, failed, or upstream_failed state\nalways: No dependencies at all, run this task at any time\n\n샘플코드\nwith DAG(\n    dag_id=\"latest_only_with_trigger\",\n    schedule=datetime.timedelta(hours=4),\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"example3\"], # UI에서 태그로 확인가능\n) as dag:\n\n    latest_only = LatestOnlyOperator(task_id=\"latest_only\")\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n    task3 = EmptyOperator(task_id=\"task3\")\n    # trigger_rule 입력\n    ## TriggerRule.ALL_DONE 이므로 upstream인 latest_only/task1/task2 실행되면 실행\n    task4 = EmptyOperator(task_id=\"task4\", trigger_rule=TriggerRule.ALL_DONE) \n\n    latest_only &gt;&gt; task1 &gt;&gt; [task3, task4]\n    task2 &gt;&gt; [task3, task4]\n\n\nTask Group\n\nTask를 묶고 싶을 때 사용(+설정을 일괄로 부여하고 싶을 때)\n\n샘플코드1 (Task묶기)\nfrom airflow.decorators import task_group\n\n# task를 그룹으로 묶음\n@task_group()\ndef group1():\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = EmptyOperator(task_id=\"task2\")\n\ntask3 = EmptyOperator(task_id=\"task3\")\n\ngroup1() &gt;&gt; task3\n샘플코드2 (Task를 묶고 설정을 일괄로 부여)\n\n우선순위는 task에 개별로 부여한 설정을 적용한 후, group의 설정을 적용한다\n# task를 그룹으로 묶고, retries 등 일괄로 설정할 때\n@task_group(default_args={\"retries\": 3})\ndef group1():\n    \"\"\"This docstring will become the tooltip for the TaskGroup.\"\"\"\n    task1 = EmptyOperator(task_id=\"task1\")\n    task2 = BashOperator(task_id=\"task2\", bash_command=\"echo Hello World!\", retries=2) \n    print(task1.retries)  # print값 : 3\n    print(task2.retries)  # print값 : 1\n\n\n\n\nSub Dags\n\nSub Dags끼리 Args(설정) 등을 적용할 때 사용\n샘플코드(Dag를 만든 후, SubDagOperator를 사용)\nwith DAG(\n    dag_id=DAG_NAME,\n    default_args={\"retries\": 2},\n    start_date=datetime.datetime(2022, 1, 1),\n    schedule=\"@once\",\n    tags=[\"example\"],\n) as dag:\n\n    start = EmptyOperator(task_id=\"start\",)\n\n    section_1 = SubDagOperator(task_id=\"section-1\",\n        subdag=subdag(DAG_NAME, \"section-1\", dag.default_args), )\n\n    some_other_task = EmptyOperator(task_id=\"some-other-task\",)\n\n    section_2 = SubDagOperator(task_id=\"section-2\",\n        subdag=subdag(DAG_NAME, \"section-2\", dag.default_args), )\n\n    end = EmptyOperator(task_id=\"end\",)\n\n    start &gt;&gt; section_1 &gt;&gt; some_other_task &gt;&gt; section_2 &gt;&gt; end\n\n\nSub Dag와 Task group 차이\n\nConfiguration 적용\n\nSub dags는 여러 Dag Conf, Task group은 하나의 Dag Conf\n\nView and statistics\n\nSub dags는 여러개의 현황이, Task group은 하나로 보임\n\nJob의 병렬 처리\n\nSub dags는 여러개의 job으로, Task group은 하나의 job으로 executor가 실행\n\n선언(declaration)과 규칙(naming restirctions)\n\nSub dags가 좀 더 어려움"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240830/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240830/index.html",
    "title": "[DE스터디/3주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240830/index.html#spark-submit.sh-스크립트-참고해서-option-바꾸어보기",
    "href": "posts/meta-de-spark_and_airflow-20240830/index.html#spark-submit.sh-스크립트-참고해서-option-바꾸어보기",
    "title": "[DE스터디/3주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "1. spark-submit.sh 스크립트 참고해서 option 바꾸어보기",
    "text": "1. spark-submit.sh 스크립트 참고해서 option 바꾸어보기\n\nmemory옵션들, num-executors, executor-cores 바꿔보기\n\n&lt;path-to-spark-submit&gt;/spark-submit \\\n    --class &lt;project.class.path&gt; \\\n    --name HelloWorld \\    # job name지정\n    --master spark://spark-master:7077 \\                     # master(현재 실습기준으로는 local)\n    --driver-cores 2 \\                                       # optimization옵션에 해당\n    --driver-memory 2g \\                                     # optimization옵션에 해당\n    --num-executors 4 \\                                      # optimization옵션에 해당\n    --executor-cores 2 \\                                     # optimization옵션에 해당 (병렬처리를 얼마나 많이 할지)\n    --executor-memory 2g \\                                   # optimization옵션에 해당\n    --conf spark.driver.memoryOverhead=1g                     \n    --conf spark.executor.memoryOverhead=1g                  \n    --conf spark.dynamicAllocation.enabled=true \\            \n    --conf spark.dynamicAllocation.executorIdleTimeout=2m \\   \n    --conf spark.dynamicAllocation.minExecutors=1 \\          \n    --conf spark.dynamicAllocation.maxExecutors=9 \\          \n    --conf spark.dynamicAllocation.initialExecutors=1 \\      \n    --conf spark.memory.offHeap.enabled=true \\               \n    --conf spark.memory.offHeap.size=2G \\                    \n    --conf spark.shuffle.service.enabled=true \\              \n    --conf spark.driver.maxResultSize=0 \\                    \n    --conf spark.logConf=true \\                              \n    --jars /opt/bitnami/spark/resources/elasticsearch-spark-302.12-8.4.3.jar\n\n미사용\n\n–class &lt;project.class.path&gt; # 실행될 기본 클래스 지정\n–deploy-mode : client/cluster # yarn이 아닌 local환경으로 실습중"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240830/index.html#원하는-최종-스키마를-만들기-위한-정제-코드-filter.py-작성하기",
    "href": "posts/meta-de-spark_and_airflow-20240830/index.html#원하는-최종-스키마를-만들기-위한-정제-코드-filter.py-작성하기",
    "title": "[DE스터디/3주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "2. 원하는 최종 스키마를 만들기 위한 정제 코드 filter.py 작성하기",
    "text": "2. 원하는 최종 스키마를 만들기 위한 정제 코드 filter.py 작성하기\n\n# filter.py (main.py에서 import하여, 로딩한 dataframe을 넣고 사용)\nclass PytorchTopIssuerFilter(BaseFilter):\n    def filter(self, df):\n        # Filter : repo_name = pytorch\n        base_df = df.filter(F.col('userid_and_repo_name') == 'pytorch/pytorch')\n\n        issues_event_exists = base_df.filter(base_df[\"type\"] == \"IssuesEvent\").count() &gt; 0\n        if issues_event_exists:\n            filtered_df = base_df.filter(F.col('type') == 'IssuesEvent')\n        else:\n            return None # 이후 None이 아닌 경우에만 저장 등의 작업을 하도록 main.py 설정\n\n        # groupby\n        result_df = filtered_df.groupBy('user_name').pivot('type').count()\n        result_df = result_df.cache()\n        result_df.where((~F.col('user_name').contains('[bot]'))) \\\n                    .orderBy(F.desc('IssuesEvent')) \\\n                    .limit(10)\n        return result_df"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240830/index.html#작성한-정제-코드-spark-submit-해보기",
    "href": "posts/meta-de-spark_and_airflow-20240830/index.html#작성한-정제-코드-spark-submit-해보기",
    "title": "[DE스터디/3주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "3. 작성한 정제 코드 spark-submit 해보기",
    "text": "3. 작성한 정제 코드 spark-submit 해보기\n\n제출할 내용: spark-submit.sh, filter.py (파일명은 바꾸셔도 무방합니다)\n\n\ndocker exec -it metacode_de-2024-spark-master-1 spark-submit \\\n  --master spark://spark-master:7077 \\\n  --jars /opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar \\\n  jobs/main.py\n\n\n\n\nwork_w3_sparksubmit.jpg"
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html",
    "href": "posts/hanbitn-copilot-20230914/index.html",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "",
    "text": "한빛앤 Copilot세미나 내용 정리(+Codeium, Codestral’)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html#개요",
    "href": "posts/hanbitn-copilot-20230914/index.html#개요",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "개요",
    "text": "개요\n\n한빛앤 코파일럿 세미나 다녀와서 정리"
  },
  {
    "objectID": "posts/hanbitn-copilot-20230914/index.html#내용-정리",
    "href": "posts/hanbitn-copilot-20230914/index.html#내용-정리",
    "title": "[한빛앤] GitHub Copilot 세미나 정리",
    "section": "내용 정리",
    "text": "내용 정리\n\nAuto complete : 코드를 먼저 쓰면 해주는 제안을 활용가능\n비사용자 대비 코딩시간 최대 55% 단축\n\n두 그룹에 자바스크립트 작성을 시킨 후 비교  https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/\n\n시간을 단축하고 개발자의 정신적에너지를 절약하여 고부가가치에 집중하게 하기\n보안이슈\n\n코파일럿은 제품 개선을 위해 데이터를 수집함(암호화 및 일부만 열람하게 한다고는 함)\n내 코드가 학습에 사용되는게 싫으면 세팅에서 수집하지 않도록 할 수 있음\n\n코파일럿 X\n\nChatGPT처럼 질문할 수 있고 답변받을 수 있음\n대화한 내용에 대해 물어볼만한 질문도 선택지로 제시\n각 프로젝트의 언어(Python 등)에 맞춰 답변\n\n대체제\n\nCodeium : 개인사용 무료, Autocomplete 가능, Chat 가능\nTabnine : 무료버전 있음, Local Machine mode(보안에 좋을 듯)\n\n사용 후기\n\nCopilot : 한달 무료 사용. Autocomplete기능 자체를 처음 써봐서 좋긴 했음 xml과 beautifulsoup관련 어려움이 있었는데 도움이 꽤 되었음\nCodeium : 개인플랜 무료 사용. Chat기능을 별도로 쓸 일이 없었음 설치형이라 회사에 도입은 어려울 것 같았음(설치는 늘 승인받으라해서)\n직업 개발자가 아니어서인지 고급기능을 쓸 일이 없어서, 내 경우는 Autocomplete정도로 충족이 되어 Codeium을 사용하기로 함\n(2024.06추가) 요즘은 코드특화된 모델이 많이 나와서 채팅 쪽은 오히려 코드스트랄 등이 체감상 더 나은 것 같음\n\nCodestral(Mistral Codemodel) : Mistral Lechat 접속 후 아래와 같이 모델을 변경하여 사용 가능"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "",
    "text": "한빛앤 데이터웨어하우스 세미나 내용 정리\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#서론",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#서론",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "서론",
    "text": "서론\n\n데이터 분석의 8할은 데이터 가공이다\nData-driven을 위해 빅데이터 시스템이 꼭 필요한가?\n\n데이터의 품질이 중요함 (많더라도 쓸모가 없으면 의미없음)\n\n‘데이터(비즈니스) 목적 정의’ ~ ’정의한 사항에 대한 일치여부 지속확인’까지 필요"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dwdata-warehouse의-개념과-특징",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dwdata-warehouse의-개념과-특징",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DW(Data Warehouse)의 개념과 특징",
    "text": "DW(Data Warehouse)의 개념과 특징\n\nDW는 사용자 관점에서 주제 별로 통합해 별도의 장소에 보관한 DB\n\nOLTP의 단점을 극복하기 위해 출현\n\nOLTP(트랜잭션 단위로 동시에 처리) vs OLAP(다양한 관점에서 데이터를 보게 해줌)\n\n장점 : 정제 및 검증을 통한 양질의 데이터(의미있는 데이터만 남도록 해줌)\n\n데이터 구조와 스키마는 빠른 SQL쿼리에 최적화되도록 정의\n\nschema-on-write. DW구현 이전에 스키마 설계함\n신뢰할 수 있는 선별된 데이터를 저장"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dldata-lake의-개념과-특징",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dldata-lake의-개념과-특징",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DL(Data Lake)의 개념과 특징",
    "text": "DL(Data Lake)의 개념과 특징\n\nDL은 영상 등 (비정형데이터를 포함한) 모든 데이터를 일단 보관\nDW와 달리 스키마구조가 정의되어있지 않음 (DW는 RDB관점이어서 정의되어있음)\n\nschema-on-read. 사용자가 필요할 때 스키마를 정의함\n선별되거나 선별되지 않은 모든 데이터 저장(비용부담이 될 수 있음)\n\n이러한 문제로 DL을 표방한 DW를 사용함\n\n\n데이터를 읽을 때 잘못되면 스키마가 공백으로 나올 수 있고, 사용자는 데이터가 없는 것으로 착각할 수 있다"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dw의-구축",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dw의-구축",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DW의 구축",
    "text": "DW의 구축\n\n사용중인 RDB솔루션 사용가능\n디스크 용량은 큰 것이 좋음(속도가 높으면 좋지만 무조건 빠른게 필요하진 않음)\nCPU코어가 많은 것이 유리(집계 연산이 많기 때문, 클럭보다 코어 많은게 유리)\n메모리는 많을수록 좋음(대량의 데이터를 조회/가공하므로)"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#etlextracttransformload-추출변환로드",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#etlextracttransformload-추출변환로드",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "ETL(Extract/Transform/Load, 추출/변환/로드)",
    "text": "ETL(Extract/Transform/Load, 추출/변환/로드)\n\n추출 : 전체/부분 추출\n변환 : 필터링, 정렬, 조인, 중복제거, 유효성검사 등 (+스테이징[Staging]환경 업로드)\n로드 : 변환된 데이터를 스테이징 영역에서 보강 후 변환\nETL Tool : Airflow(오픈소스), AWS, Azure, InfoSphere, Oracle 등"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#초기로딩과-주기적갱신",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#초기로딩과-주기적갱신",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "초기로딩과 주기적갱신",
    "text": "초기로딩과 주기적갱신\n\n초기로딩 : 필요한 데이터의 최초 로딩\n주기적갱신 : CDC(Change Data Capture), DML트리거 등 솔루션으로 변경사항이 있을때마다 별도 테이블에 기록하여 로딩 (Insert된 데이터 뿐 아니라 Update된 데이터도 가져와야 함)\n\n데이터에 따른 예시\n\n날짜컬럼이 있다면, 날짜컬럼이 Update된 경우 가져옴\n날짜컬럼이 없다면, Outer join으로 백업데이터와 비교(이 방법은 데이터가 커지면 쉽지 않음)\n\nCDC나 DML트리거를 활용하는 방안으로 적용"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#스타-스키마",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#스타-스키마",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "스타 스키마",
    "text": "스타 스키마\n\n다차원의 데이터를 효과적으로 저장\n하나의 팩트 테이블 + 다수의 디멘젼 테이블로 구성\n\n팩트(사실) : 분석하고자 하는 대상\n디멘젼(차원, Dim) : 팩트를 보는 관섬\n\n하나의 모델에는 하나의 팩트테이블이 존재"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#스노우플레이크-스키마",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#스노우플레이크-스키마",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "스노우플레이크 스키마",
    "text": "스노우플레이크 스키마\n\n스타스키마의 팩트테이블 구조를 동일하게 유지하며, 차원테이블은 정규화한 구조\n팩트테이블과 조인되는 디멘젼테이블이 있으며, 디멘젼테이블은 또 다른 테이블의 키를 가짐"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dmdata-mart",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dmdata-mart",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DM(Data Mart)",
    "text": "DM(Data Mart)\n\nDW가 1차적인 가공이 되어있다면, DM은 목적에 맞게 재집계되어있음\n\nCEO가 원하는 월별 매출, 지점담당자의 지역매출 등\n\nDW가 데이터가 너무 많고 복잡(사용자에게 불친절)해 DM을 사용"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#데이터의-추출-및-가공dq",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#데이터의-추출-및-가공dq",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "데이터의 추출 및 가공(DQ)",
    "text": "데이터의 추출 및 가공(DQ)\n\n데이터 포맷 정형화 (0/1, 남/여 등을 M/F로 변환)\n메타데이터 관리\n\n예) 주소입력시 ’서울’을 치면 ’서울특별시’로 입력되게 하는 것"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dm의-구축",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dm의-구축",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DM의 구축",
    "text": "DM의 구축\n\n하향식 접근법 : 일반적으로 많이 사용. DW로 DM 만들기\n\n한번 쿼리한 결과를 테이블로 저장해두는 경우 등\n\n상향식 접근법 : 거의 없는 케이스"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#차원의-이해",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#차원의-이해",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "차원의 이해",
    "text": "차원의 이해\n\n차원 : 분석하고자 하는 관점, 큐브를 구성하는 축\n차원항목 : 각 축의 좌표\n큐브와 셀 : 각 차원을 구성하는 항목의 조합에 대한 데이터를 저장하는 공간\n희박성(Sparsify) : 데이터가 존재하지 않는 셀은 물리적으로 존재하지 않는 것\n\n차원 때문에 매칭을 하다보니 희박성(없는 부분)이 발생하게 됨\n\n하이퍼 큐브 : ‘입력된 데이터’ 상태에서, ‘계산된 데이터’ 추가되면 큐브가 팽창하여 시스템다운\n희박성 증가와 팽창계수 : 차원이 증가하고 희박성이 커지면 DB의 팽창계수는 급속히 증가"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#dw-실무운영-팁",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#dw-실무운영-팁",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "DW 실무운영 팁",
    "text": "DW 실무운영 팁\n\n데이터 재처리를 위해 항상 RAW데이터도 함께 보관\n집계 중 오류가 발생하면 부분재처리보단 전체재처리를 권장\n자주 사용되는 패턴은 미리 집계하여 저장\n대량 데이터를 집계할 때는 부분집합으로 나누어 처리한 다음, 상위 처리과정으로 집계하여 리소스를 효율적으로\n\n시스템 하드웨어가 약해도 큰 데이터 집계가 가능해짐\n\n집계된 결과는 백업하여 유사시 대비\n배치 작업의 분리 및 순서에 대한 전략 고민\n불필요하게 많은 차원은 리소스 낭비"
  },
  {
    "objectID": "posts/hanbitn-datawarehouse-20240725/index.html#요약",
    "href": "posts/hanbitn-datawarehouse-20240725/index.html#요약",
    "title": "[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리",
    "section": "요약",
    "text": "요약\n\n대부분의 데이터는 RDB에 저장되어 있음\n빅데이터 시스템이 있어도, 결국 집계된 데이터는 RDB에서 관리\nDW만 잘 만들어도 대부분의 분석이 비용효율적으로 가능하다\nGIGO를 잊지 말자(Garbage in, Garbage out. 데이터의 품질 중요)\n중요한 것은 무엇을 분석할지에 대한 목표, 평가\n관점에 따라 같은 데이터도 다양한 인사이트를 만들어 냄"
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html",
    "href": "posts/coach-ds-20221113/index.html",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "",
    "text": "KOSIS 지역별 / 상품군별 온라인쇼핑 해외직접판매액 데이터를 활용한 간단한 분석 KOSIS 상품군별 온라인쇼핑 해외직접판매액\n실습 기록용으로 남깁니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#개요",
    "href": "posts/coach-ds-20221113/index.html#개요",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "",
    "text": "KOSIS 지역별 / 상품군별 온라인쇼핑 해외직접판매액 데이터를 활용한 간단한 분석 KOSIS 상품군별 온라인쇼핑 해외직접판매액\n실습 기록용으로 남깁니다."
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#분석",
    "href": "posts/coach-ds-20221113/index.html#분석",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "분석",
    "text": "분석\n\n데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('online_export.csv')#, encoding=\"cp949\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\n국가(대륙)별\n상품군별\n판매유형별\n시점\n데이터\n\n\n\n\n0\n0\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.1/4\n1054\n\n\n1\n1\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.2/4\n946\n\n\n2\n2\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.3/4\n791\n\n\n3\n3\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.4/4\n854\n\n\n4\n4\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2018.1/4\n2033\n\n\n\n\n\n\n\n\n데이터 확인 중 시점 컬럼에 p가 붙어있어서 의미를 확인해봄\n\nKOSIS주석 : e: 추정치, p: 잠정치, -: 자료없음, …: 미상자료, x: 비밀보호, ▽: 시계열 불연속\n\n\n\ndf['시점'].unique()\n\narray(['2017.1/4', '2017.2/4', '2017.3/4', '2017.4/4', '2018.1/4',\n       '2018.2/4', '2018.3/4', '2018.4/4', '2019.1/4', '2019.2/4',\n       '2019.3/4', '2019.4/4', '2020.1/4', '2020.2/4', '2020.3/4',\n       '2020.4/4', '2021.1/4', '2021.2/4', '2021.3/4', '2021.4/4',\n       '2022.1/4', '2022.2/4 p)'], dtype=object)\n\n\n\n\n시점 데이터의 분류 (연도, 분기의 구분), 단위표기\n\ndf['연도'] = df['시점'].map(lambda x : x.split('.')[0])\ndf['분기'] = df['시점'].map(lambda x : x.split('.')[1].split('/')[0])\ndf[['연도','분기']] = df[['연도','분기']].astype(int)\n\ndf = df.rename(columns={'데이터':'백만'})\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\n국가(대륙)별\n상품군별\n판매유형별\n시점\n백만\n연도\n분기\n\n\n\n\n0\n0\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.1/4\n1054\n2017\n1\n\n\n1\n1\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.2/4\n946\n2017\n2\n\n\n2\n2\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.3/4\n791\n2017\n3\n\n\n3\n3\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2017.4/4\n854\n2017\n4\n\n\n4\n4\n미국\n컴퓨터 및 주변기기\n면세점 이외\n2018.1/4\n2033\n2018\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2767\n2767\n기타\n기 타\n면세점 이외\n2021.2/4\n1278\n2021\n2\n\n\n2768\n2768\n기타\n기 타\n면세점 이외\n2021.3/4\n1154\n2021\n3\n\n\n2769\n2769\n기타\n기 타\n면세점 이외\n2021.4/4\n1076\n2021\n4\n\n\n2770\n2770\n기타\n기 타\n면세점 이외\n2022.1/4\n2325\n2022\n1\n\n\n2771\n2771\n기타\n기 타\n면세점 이외\n2022.2/4 p)\n725\n2022\n2\n\n\n\n\n2772 rows × 8 columns\n\n\n\n\n\n데이터 자체에 대한 분석 (describe)\n\ndf.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\n백만\n연도\n분기\n\n\n\n\ncount\n2772.000000\n2772.000000\n2772.000000\n2772.000000\n\n\nmean\n1385.500000\n1613.116522\n2019.272727\n2.409091\n\n\nstd\n800.351798\n4273.426107\n1.600909\n1.114533\n\n\nmin\n0.000000\n-10003.000000\n2017.000000\n1.000000\n\n\n25%\n692.750000\n41.750000\n2018.000000\n1.000000\n\n\n50%\n1385.500000\n291.500000\n2019.000000\n2.000000\n\n\n75%\n2078.250000\n1181.000000\n2021.000000\n3.000000\n\n\nmax\n2771.000000\n47689.000000\n2022.000000\n4.000000\n\n\n\n\n\n\n\n\n특이하게도 최소값에 음수(-10003)이 있음\n\n해외역직구와 같은 경향인가 추측해보았지만, 전체 국가/연도에서 단 1개값만 그렇다고 보기는 어려움\n이상치로 보여 제거하고 분석을 진행하면 어떨까 싶음\n\n\n\n\n국가별 판매액에 대한 heatmap을 통한 파악\n\nsales_country = df.pivot_table(values='백만', index='국가(대륙)별', columns='연도', aggfunc='sum')\nsns.heatmap(data=sales_country, annot=True, fmt='.0f', cmap='Blues')\n\n\n\n\n\n\n\n\n\n미국, 일본, 중국의 판매액이 두드러짐\n\n\n\n판매액 상위 3개국에 대한 판매액 추세 시각화\n\nsns.lineplot(data=df[df['국가(대륙)별'].isin(['미국','중국','일본'])], \n             x='연도',y='백만',hue='국가(대륙)별', errorbar=None, estimator='sum'\n            )\nplt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0) #bbox_to_anchor(그래프와의 관격, 위/아래 위치)\n\n\n\n\n\n\n\n\n\n2020, 2021년도 급격한 하락이 보임\n\n당시의 큰 이벤트로는 코로나가 있으며, 해당 시기의 국가봉쇄/여객기 항편취소/화물기 감편 등이 원인일 것으로 보임\n\n\n\n\n2020년 국가별 주요 상품군 분석\n\n판매액 비중이 높은 주요 국가에 대해 분석 진행\n\n\ndf_2020 = df[df['연도']==2020].groupby(['국가(대륙)별','분기'])['백만'].sum().unstack().copy()\nsns.heatmap(data=df_2020, annot=True, fmt='.0f', cmap='Blues')\n\n\n\n\n\n\n\n\n\n주요 3개국(미국, 일본, 중국)에 대한 제품군별 판매액 분석\n\n\nfilter_rule = (df['연도']==2020) & (df['국가(대륙)별'].isin(['미국','중국','일본']))\ndf_2020_top3 = df[filter_rule][['국가(대륙)별','상품군별','백만']]\n\n# 잘 팔리는 상품군을 딕셔너리에 저장\nbest_category = {}\nbest2_category = {}\nfor i, country in enumerate(df_2020_top3['국가(대륙)별'].unique()):\n    filter_country = df_2020_top3['국가(대륙)별'] == country\n    globals()[country] = df_2020_top3[filter_country].groupby(['상품군별'])['백만'].sum().copy()\n    best_category[i] = globals()[country].sort_values(ascending=False).index[0]\n    best2_category[i]= globals()[country].sort_values(ascending=False).index[1]\n\n#그래프 기본 틀\nfig, axs = plt.subplots(ncols=3, figsize=(15,8), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.7},) #gridspec으로 그래프 사이 여백 설정\n\n#그래프 그리기 (반복문으로 그래프를 그리고, 범례설정과 국가/1위카테고리를 입력한 제목 생성)\nsub_title=[]\nfor i, country  in enumerate(df_2020_top3['국가(대륙)별'].unique()):\n    globals()[country].plot(kind='pie',startangle=145, autopct='%.1f%%', ax=axs[i], pctdistance=0.8, #rotatelabels=True,\n                           )\n    sub_title.append(\"[\"+country+\"]\")  \n    axs[i].set_title(sub_title[i]+ chr(10) +best_category[i])\n    axs[i].set_ylabel('')\n    axs[i].labels=None\n\n\n\n\n\n\n\n\n\n국가별 판매액 상위\n\n미국 : 의류 및 패션관련상품\n중국 : 음반 비디오 악기\n일본 : 의류 및 패션관련 상품\n\n4분기 판매가 두드러지는데, 특별히 비중이 많은 상품이 있는지 확인\n\n\nfilter_rule = (df['연도']==2020) & (df['국가(대륙)별'].isin(['미국','중국','일본'])) & (df['분기']==4)\ndf_2020_top3_quarter4 = df[filter_rule][['국가(대륙)별','상품군별','백만']]\n\n# 잘 팔리는 상품군을 딕셔너리에 저장\nbest_category = {}\nbest2_category = {}\nfor i, country in enumerate(df_2020_top3_quarter4['국가(대륙)별'].unique()):\n    filter_country = df_2020_top3_quarter4['국가(대륙)별'] == country\n    globals()[country] = df_2020_top3_quarter4[filter_country].groupby(['상품군별'])['백만'].sum().copy()\n    best_category[i] = globals()[country].sort_values(ascending=False).index[0]\n    best2_category[i]= globals()[country].sort_values(ascending=False).index[1]\n\n#그래프 기본 틀\nfig, axs = plt.subplots(ncols=3, figsize=(15,8), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.7},) #gridspec으로 그래프 사이 여백 설정\n\n#그래프 그리기 (반복문으로 그래프를 그리고, 범례설정과 국가/1위카테고리를 입력한 제목 생성)\nsub_title=[]\nfor i, country  in enumerate(df_2020_top3_quarter4['국가(대륙)별'].unique()):\n    globals()[country].plot(kind='pie',startangle=145, autopct='%.1f%%', ax=axs[i], pctdistance=0.8, #rotatelabels=True,\n                           )\n    sub_title.append(\"[\"+country+\"]\")  \n    axs[i].set_title(sub_title[i]+ chr(10) +best_category[i])\n    axs[i].set_ylabel('')\n    axs[i].labels=None\n\n\n\n\n\n\n\n\n\n4분기 국가별 판매액 상위 (변동없음)\n\n미국 : 의류 및 패션관련상품\n중국 : 음반 비디오 악기\n일본 : 의류 및 패션관련 상품\n\n비중이 아닌 상품군별 규모를 확인해봄\n\n음반 비디오 악기가 가장 규모가 컸고, 다음으로 의류 및 패션관련 상품의 규모가 컸음\n\n\n\n# 상위 3개국가에 대해 상품군별 시각화해서 분석함\nplt.figure(figsize=(20,5))\nsns.barplot(data=df_2020_top3, x='상품군별', y='백만', hue='국가(대륙)별', errorbar=None, dodge=False)\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=6, borderaxespad=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n미국, 일본은 ‘의류 및 패션상품’ 판매가 두드러지므로 해외직구활성화 방안 등을 강구한다면 해당 업종을 눈여겨 봐야할 듯 함\n두번째로는 ‘화장품’ 판매실적이 좋은데, 분류가 되어있지 않으니 기초/색조 여부 등을 보긴 해야겠지만\n\n기초화장품 쪽 주력인 업체가 있다면 진출을 고민하는 등의 방향을 고민해보면 좋을 듯 하고\n색조화장품 쪽 주력인 업체라면 해당 국가들의 미의 기준(선호색 등)을 파악하며 시작하면 좋을 듯 함\n\n중국은 ‘음반 비디오 악기’ 판매 실적이 좋은데, 2020년이라면 한류가 원인 중 하나가 아닐까 싶음\n\n간단히 구글링을 했을 때 ’한국구제문화교류진흥원’이란 곳의 보고서에 따르면 아래와 같은 상태 부분이 있음\n\n20년 1월~7월의 한국 3대 엔터테인먼트들의 주가는 상승했고, 원인 중 하나가 글로벌 팬덤의 확대와 빌보드 차트 진입과 중국 현지화 전략의 순항이라 함 (물론 위에서 말한 원인은 구체적 자료를 제시한 내용은 아님)\n중국판 미생의 방영 등 한류 자체는 긍정적인 상황으로 보임(물론 중국판 미생이 `18년 제작을 마쳤으니 위의 수치엔 영향 없을듯 함)\n\n\n중국의 한류에 대한 내용검색을 위한 보고서였지만, 제조업(화장품)관련 보고서 내용도 있었는데,\n\n화장품수출 강세는 색조화장품 뿐 아니라 기초화장품도 늘었다는 내용으로 보아 두 품목 모두 상승을 견인한 것으로 보임\n의류 강세관련하여, ’팬데믹으로 의류산업은 고전’중이지만 ’마스크, 방호복 등 관련 수요 증가’라는 내용이 있어 의류품목 관련 판단은 좀 더 해보아야겠음\n\n참고한 자료 : https://kofice.or.kr/b20industry/b20_industry_00_view.asp?seq=1134&tblID=gongji&clsID=0\n\n\n\n판매액 상위국가에 대한 주요 상품 시각화\n\nfilter_rule2 = (df['국가(대륙)별'].isin(['미국','일본','중국']))&(df['상품군별'].isin([\"의류 및 패션 관련상품\", \"화장품\", \"음반·비디오·악기\"]))\ndf_top3_categorical = df[filter_rule2].copy()\n\nfig, axs = plt.subplots(ncols=3, figsize=(15,4), # ncols(그래프 수), figsize(공간크기)\n                       gridspec_kw={'wspace':0.3},) #gridspec으로 그래프 사이 여백 설정\n\nylabel_text = {0:'백만',1:'',2:''}\nfor i, country  in enumerate(df_top3_categorical['국가(대륙)별'].unique()):\n    sns.lineplot(data=df_top3_categorical[df_top3_categorical['국가(대륙)별'] == country], \n                 x='연도', y='백만', hue='상품군별', errorbar=None, marker='o', palette=[\"b\", \"r\", 'g'],  style='상품군별',\n                 ax = axs[i])\n    axs[i].set_title('국가(대륙별) - ' + country)\n    axs[i].spines[['top','right']].set_visible(False) # 그래프 테두리 왼쪽,위,오른쪽 안보이게(false)\n    axs[i].legend().set_visible(False)\n    axs[i].set_ylabel(ylabel_text[i])\naxs[2].legend(bbox_to_anchor=(2,1), loc=0, borderaxespad=4)\n\n\n\n\n\n\n\n\n\n의류 분야는 세 국가 모두 `21년을 기점으로 하락세\n화장품은 미국/중국 하락세이나 일본이 크게 성장하여, `22년의 화장품 판매액은 일본의 영향이 크지 않을까 추측\n음반은 `21~22년도에 중국에서 판매액 증가가 뚜렷\n실제로도 확인해보니, `22년도 판매액은 일본의 비중이 컸음(하단 그래프)\n\n\nplt.figure(figsize=(10,4))\nsns.barplot(data=df[(df['연도'] == 2022) & (df['상품군별'] == '화장품')],\n             x='국가(대륙)별', y='백만', hue='상품군별', dodge=False)\nplt.legend(bbox_to_anchor=(1.1,0.5), loc=6, borderaxespad=0) #bbox_to_anchor(그래프와의 관격, 위/아래 위치)\n             #loc(좌우), borderaxespad(클수록 아래로)"
  },
  {
    "objectID": "posts/coach-ds-20221113/index.html#추가과제",
    "href": "posts/coach-ds-20221113/index.html#추가과제",
    "title": "[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)",
    "section": "추가과제",
    "text": "추가과제\n\n현재 분석과는 관계없지만, 함께 진행한 과제가 있어 기록만 해둠\n\n\n# Choropleth 시각화\n\n# 데이터 불러오기 & 가공(월 합계 등)\nraw_data = pd.read_csv(\n    '국가별_수출입현황_20221119232639.csv', \n    encoding=\"cp949\")\n\niso_table = pd.read_excel('iso_alpha.xlsx', engine='openpyxl')\niso_table = iso_table[['나라 이름','alpha-3']]\n\nraw_data_saved = raw_data.copy()\nraw_data_saved['2022년합계'] = raw_data_saved[raw_data_saved.columns[2:]].sum(axis=1)\n\n# Pivot table(국가별 합계) - 합계 0은 제외 (시각화되어있지않으면 0으로 간주, 메모리도 더 절약될 것이라 생각)\npivoted = raw_data_saved.groupby(['국가별(1)'])[['2022년합계']].sum()\npivoted = pivoted[pivoted['2022년합계']!=0]\n\n# replace로 국가명↔코드로 변환 (영문으로 변환되지 않은 국가['키리바티', '타지크' 등 생소한 국가]는 제외했습니다)\npivoted['iso_table']=pivoted.index\npivoted['국가명']=pivoted.index\npivoted['iso_table'] = pivoted['iso_table'].replace(iso_table['나라 이름'].tolist(),iso_table['alpha-3'].tolist())\n\npivoted['iso_table'] = pivoted['iso_table'][((pivoted['iso_table'].str.upper()) != (pivoted['iso_table'].str.lower()))]\npivoted\n\n\n\n\n\n\n\n\n2022년합계\niso_table\n국가명\n\n\n국가별(1)\n\n\n\n\n\n\n\n가나\n146268\nGHA\n가나\n\n\n가봉\n-709661\nGAB\n가봉\n\n\n가이아나\n20358\nGUY\n가이아나\n\n\n감비아\n-1354\nGMB\n감비아\n\n\n건지\n7\nNaN\n건지\n\n\n...\n...\n...\n...\n\n\n필리핀\n6247769\nPHL\n필리핀\n\n\n허드 앤 맥도날드 군도\n-27\nNaN\n허드 앤 맥도날드 군도\n\n\n헝가리\n3696148\nHUN\n헝가리\n\n\n호주\n-19871753\nAUS\n호주\n\n\n홍콩\n20619857\nHKG\n홍콩\n\n\n\n\n244 rows × 3 columns\n\n\n\n\n# plotly의 choropleth를 활용\n# 지도위에 마우스를 올리면 국가명/iso code / 무역수지(백만단위)를 표현\nimport plotly.express as px\n\nfig = px.choropleth(pivoted, locations=\"iso_table\",\n                    color=\"2022년합계\",\n                    hover_name=\"국가명\",\n                    color_continuous_scale=px.colors.sequential.Plasma,\n                   title='2022년 국가별 무역수지 현황')\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nQuarto rendring 문제로 출력 예시 이미지 첨부"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 2주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "자료형",
    "text": "자료형\n\nSQL문법마다 조금씩 다르지만, 일반적으로 아래와 같음\n\n정보가 없는 경우 : NULL\n문자형 : TEXT (STRING / VARCHAR)\n숫자형 : INT (INTEGER), REAL (FLOAT / DOUBLE)\n날짜/시간형 : TIMESTAMP (DATE / TIME / DATETIME / TIMESTAMP)\n\n\n\n문자 자료형\n\nTEXT : 길이 제한이 없는 문자열 변수. 로그, 리뷰 등 긴 텍스트 저장\nCHAR : 길이가 고정된 문자열 변수. 전화/우편번호 등 길이가 일정한 문자열 저장\n\n길이가 부족하면 공백으로 채움\n\nVARCHAR : 길이가 가변인 문자열 변수. 가장 일반적으로 사용\n\n최대길이를 지정하여 사용\n\n\n\n\n숫자 자료형\n\n정수형 변수\n\nINT: (소숫점이 없는)정수형 변수. 판매수량, 차량 대수 등 저장\n\nBIGINT / SMALLINT : 자릿수에 따라 지정. 매출 등 큰 숫자를 BIGINT로 저장\n\n큰 숫자일 필요가 없다면 SMALLINT지정하여 메모리를 절약\n\nINT간 연산은 INT를 반환하므로, 결과가 FLOAT 등 소수가 될 경우 유의\n\n\n부동소수점 변수\n\nREAL: 부동소수점인 실수형 변수.\n\n부동소수점 : 소수점의 위치가 바뀌며, 유효숫자 X 10^지수 형태로 저장\n부동소수점 예시 (지수의 값에 따라 소숫점이 이동한다)\n\n1234.5 → 유효숫자 1.2345 & 지수 \\(10^3\\)\n1.2345 → 유효숫자 1.2345 & 지수 \\(10^0\\)\n1.2345e3 와 같은 형태로 나타내기도 함\n\n일반적으로 REAL은 7자리 정도의 유효숫자를 가짐\n\nDOUBLE : REAL보다 약 2배인 15자리 정도의 유효숫자를 가지는 실수형 변수\nFLOAT(p) : (REAL, DOUBLE과 달리)정밀도를 지정할 수 있는 실수형 변수\n\nFLOAT(24) : REAL과 동일 / FLOAT(53) : DOUBLE과 동일\n\n추가검색해본 결과, p는 유효숫자수가 아니며, FLOAT(24)=REAL은 바이트 수 기준.\n\n여기까지 REAL, DOUBLE, FLOAT를 부동소수점 변수라 부릅니다.\n\n\n고정소수점 변수\n\nDECIMAL(m, d): 고정소수점인 실수형 변수.\n\n고정소수점 : 정밀도가 아니라 소수점의 위치를 고정(DECIMAL 형태의 자료형)\nm은 유효숫자의 개수, d는 소수점 이하의 자릿수\nDECIMAL 지정예시 : 1234.5 → DECIMAL(5,1)\n\n\n부동소수점 vs 고정소수점 비교\n\n\n\n\n\n\n\n\n특징\n부동소수점\n고정소수점\n\n\n\n\n소수점 위치\n가변적\n고정적\n\n\n저장 방식\n유효숫자와 지수를 저장\n정수와 소수점 위치를 기반으로 저장\n\n\n표현 가능 범위\n매우 크거나 작은 값 표현 가능\n표현 가능 범위 제한\n\n\n정밀도\n제한적, 오차 발생 가능\n정밀도 100% 보장\n\n\n연산 속도\n상대적으로 느림\n상대적으로 빠름\n\n\n주요 사용 사례\n과학적 계산, 통계\n금융, 회계, 정밀한 금액 계산\n\n\nSQL 자료형\nREAL, FLOAT, DOUBLE\nDECIMAL, NUMERIC\n\n\n\n\n\n\n날짜/시간 자료형\n\nDATE / TIME : 날짜/시간 형태 데이터 (YYYY-MM-DD 또는 HH:MM:SS 형태로 저장)\nDATETIME / TIMESTAMP : 날짜+시간 형태의 자료 (YYYY-MM-DD HH:MM:SS 형태로 저장)\n\nDATETIME VS TIMESTAMP\n\nDATETIME은 입력값을 그대로 저장 / TIMESTAMP는 UTC(협정세계시)로 변환\n글로벌서비스라면 TIMESTAMP, 값을 유지하고 싶으면 DATETIME"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형-변환cast함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형-변환cast함수",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "자료형 변환(CAST함수)",
    "text": "자료형 변환(CAST함수)\n\n일반적으로 CAST함수로 변환. CAST(컬럼명 AS 변수형) 형태로 사용"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형-변환-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#자료형-변환-실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "자료형 변환 실습",
    "text": "자료형 변환 실습\n\nimport sqlite3\nimport pandas as pd\n\n# SQLite 데이터베이스 파일 연결 (db가 없으면 자동 생성)\nconn = sqlite3.connect('my_database.db', timeout=30)\ncursor = conn.cursor()\n\n\n# INT간 연산의 잘못된 예시 : c_camp_rt = campaign / (campaign + previous)\n## 5행(idx 41187)의 값이 0.75가 아닌 0으로 잘못되었음\npd.read_sql('SELECT campaign / (campaign + previous) AS c_camp_rt, campaign, previous FROM tb_camp', conn).tail(5)\n\n\n\n\n\n\n\n\nc_camp_rt\ncampaign\nprevious\n\n\n\n\n41183\n1\n1\n0\n\n\n41184\n1\n1\n0\n\n\n41185\n1\n2\n0\n\n\n41186\n1\n1\n0\n\n\n41187\n0\n3\n1\n\n\n\n\n\n\n\n\n# CAST를 활용해 연산 정확하게 하기\n## 메모리 효율을 위해, (FLOAT가 아닌)REAL 사용\n## 메모리 효율을 위해, CAST 횟수를 최소화 (분모에서 합한 후 사용)\npd.read_sql('SELECT CAST(campaign AS REAL) / CAST((campaign + previous) AS REAL) AS c_camp_rt, campaign, previous FROM tb_camp', conn).tail(5)\n\n\n\n\n\n\n\n\nc_camp_rt\ncampaign\nprevious\n\n\n\n\n41183\n1.00\n1\n0\n\n\n41184\n1.00\n1\n0\n\n\n41185\n1.00\n2\n0\n\n\n41186\n1.00\n1\n0\n\n\n41187\n0.75\n3\n1"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#문자열-함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#문자열-함수",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "문자열 함수",
    "text": "문자열 함수\n\n사용하는 SQL에 따라 상이하나 일반적으로 아래와 같음\n\nUPPER(col) / LOWER(col) -- 문자열을 대 / 소문자로 변환하는 함수입니다.\nLENGTH(col) -- 문자열의 길이를 반환하는 함수입니다.\nSUBSTRING(col, start, n) -- 문자열의 start 부터 n개의 문자를 반환하는 함수입니다.\nREPLACE(문자열, 문자1, 문자2) -- 문자1을 문자2로 통째로 변환합니다.\n\nUPPER(대문자 변환), LOWER(소문자 변환), LENGTH(문자열 길이)\n\n# 초기 데이터\nq = \"\"\"SELECT\n    job\nFROM tb_cst\n\"\"\"\n\npd.read_sql_query(q, conn).head(5)\n\n\n\n\n\n\n\n\njob\n\n\n\n\n0\nhousemaid\n\n\n1\nservices\n\n\n2\nservices\n\n\n3\nadmin.\n\n\n4\nservices\n\n\n\n\n\n\n\n\n# UPPER를 통한 대문자 변환 / LENGTH를 통한 문자길이 계산\nq = \"\"\"SELECT\n    UPPER(job) as upper_job,\n    LENGTH(job) as len_job\nFROM tb_cst\n\"\"\"\n\npd.read_sql_query(q, conn).head(5)\n\n\n\n\n\n\n\n\nupper_job\nlen_job\n\n\n\n\n0\nHOUSEMAID\n9\n\n\n1\nSERVICES\n8\n\n\n2\nSERVICES\n8\n\n\n3\nADMIN.\n6\n\n\n4\nSERVICES\n8\n\n\n\n\n\n\n\n\n\nSUBSTR(지정한 위치부터 N개 글자 가져오기)\n\n아래는 데이터의 첫 글자만 떼어내어, CHAR(1)로 저장해 메모리를 절약하는 예제\n\n\n# 초기 데이터(marital 컬럼의 모든 데이터의 unique값 첫 글자가 일치하지 않음)\nq = \"\"\"SELECT\n    marital\nFROM tb_cst\n\"\"\"\n\npd.read_sql_query(q, conn)['marital'].unique()\n\narray(['married', 'single', 'divorced', 'unknown'], dtype=object)\n\n\n\nq = \"\"\"SELECT\n    CAST(SUBSTR(marital,1,1) AS CHAR(1)) as mr_c\nFROM tb_cst\n\"\"\"\n\npd.read_sql_query(q, conn).head(10)\n\n\n\n\n\n\n\n\nmr_c\n\n\n\n\n0\nm\n\n\n1\nm\n\n\n2\nm\n\n\n3\nm\n\n\n4\nm\n\n\n5\nm\n\n\n6\nm\n\n\n7\nm\n\n\n8\ns\n\n\n9\ns\n\n\n\n\n\n\n\n\n아래는 SUBSTR로 뒤의 2글자를 자르는 예제\n\n\nq = \"\"\"SELECT\n    marital,\n    SUBSTR(marital, LENGTH(marital)-1, 2) as last2\nFROM tb_cst\n\"\"\"\npd.read_sql_query(q, conn).head(10)\n\n\n\n\n\n\n\n\nmarital\nlast2\n\n\n\n\n0\nmarried\ned\n\n\n1\nmarried\ned\n\n\n2\nmarried\ned\n\n\n3\nmarried\ned\n\n\n4\nmarried\ned\n\n\n5\nmarried\ned\n\n\n6\nmarried\ned\n\n\n7\nmarried\ned\n\n\n8\nsingle\nle\n\n\n9\nsingle\nle\n\n\n\n\n\n\n\n\n\nREPLACE(단어 찾아 바꾸기)\n\n아래는 다양한 곳에서 예약어로 사용되는 .을, 데이터에서 제거하는 예제\n\n\n# 초기 데이터(education 컬럼의 unique값)\npd.read_sql('select * from tb_cst', conn).job.unique()\n\narray(['housemaid', 'services', 'admin.', 'blue-collar', 'technician',\n       'retired', 'management', 'unemployed', 'self-employed', 'unknown',\n       'entrepreneur', 'student'], dtype=object)\n\n\n\n# Replace적용된 데이터 (. 찾아 삭제)\nq = \"\"\"SELECT job, REPLACE(job, 'admin.', 'admin') AS new_job\nFROM tb_cst\n\"\"\"\npd.read_sql(q, conn).new_job.unique()\n\narray(['housemaid', 'services', 'admin', 'blue-collar', 'technician',\n       'retired', 'management', 'unemployed', 'self-employed', 'unknown',\n       'entrepreneur', 'student'], dtype=object)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#문자열-함수-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#문자열-함수-실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "문자열 함수 실습",
    "text": "문자열 함수 실습\n\ntb_cst에서 job이 ’blue*collar’인 사람에 대해\n하이픈 (-)을 언더바(_*로 바꾼뒤, 직업을 대문자로 바꿔서 new_job을 반환하기\n\n\n# 실습 답안\nq = \"\"\"SELECT job, UPPER(REPLACE(job, 'blue-collar', 'blue_collar')) AS new_job\nFROM tb_cst\n\"\"\"\npd.read_sql(q, conn).head(10)\n\n\n\n\n\n\n\n\njob\nnew_job\n\n\n\n\n0\nhousemaid\nHOUSEMAID\n\n\n1\nservices\nSERVICES\n\n\n2\nservices\nSERVICES\n\n\n3\nadmin.\nADMIN.\n\n\n4\nservices\nSERVICES\n\n\n5\nservices\nSERVICES\n\n\n6\nadmin.\nADMIN.\n\n\n7\nblue-collar\nBLUE_COLLAR\n\n\n8\ntechnician\nTECHNICIAN\n\n\n9\nservices\nSERVICES\n\n\n\n\n\n\n\n\n# 메모리 절약하기 : 처음 Replace부터 대문자를 적용하여 UPPER함수 미사용\nq = \"\"\"SELECT job, REPLACE(job, 'blue-collar', 'BLUE_COLLAR') AS new_job\nFROM tb_cst\n\"\"\"\npd.read_sql(q, conn).head(10)\n\n\n\n\n\n\n\n\njob\nnew_job\n\n\n\n\n0\nhousemaid\nhousemaid\n\n\n1\nservices\nservices\n\n\n2\nservices\nservices\n\n\n3\nadmin.\nadmin.\n\n\n4\nservices\nservices\n\n\n5\nservices\nservices\n\n\n6\nadmin.\nadmin.\n\n\n7\nblue-collar\nBLUE_COLLAR\n\n\n8\ntechnician\ntechnician\n\n\n9\nservices\nservices"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#숫자-함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#숫자-함수",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "숫자 함수",
    "text": "숫자 함수\n\n집계함수(SUM, AVG, VAR, COUNT 등)는 계산결과를 보여주지만, 이번에는 단일 숫자를 변환\nFLOOR(버림), CEIL(올림), ROUND(반올림)\nABS(절대값), MOD(나눈 후 나머지), LOG, LN(자연로그), EXP, SQRT 등\n\nLOG(숫자, 컬럼명)의 경우 숫자는 밑이며, 생략시 10이거나 SQL에 따라 달라짐\n\n사용하는 SQL에 따라 문법은 다를 수 있음\n\n\n# 버림/올림/반올림\nq = \"\"\"SELECT\n    cons_price_idx,\n    FLOOR(cons_price_idx) AS fl_idx,\n    CEIL(cons_price_idx) AS cl_idx,\n    ROUND(cons_price_idx, 1) AS rd_idx1,\n    ROUND(cons_price_idx, 2) AS rd_idx2\nFROM tb_out\n\"\"\"\npd.read_sql(q, conn)\n\n\n\n\n\n\n\n\ncons_price_idx\nfl_idx\ncl_idx\nrd_idx1\nrd_idx2\n\n\n\n\n0\n93.994\n93.0\n94.0\n94.0\n93.99\n\n\n1\n93.994\n93.0\n94.0\n94.0\n93.99\n\n\n2\n93.994\n93.0\n94.0\n94.0\n93.99\n\n\n3\n93.994\n93.0\n94.0\n94.0\n93.99\n\n\n4\n93.994\n93.0\n94.0\n94.0\n93.99\n\n\n...\n...\n...\n...\n...\n...\n\n\n370\n94.767\n94.0\n95.0\n94.8\n94.77\n\n\n371\n94.767\n94.0\n95.0\n94.8\n94.77\n\n\n372\n94.767\n94.0\n95.0\n94.8\n94.77\n\n\n373\n94.767\n94.0\n95.0\n94.8\n94.77\n\n\n374\n94.767\n94.0\n95.0\n94.8\n94.77\n\n\n\n\n375 rows × 5 columns\n\n\n\n\n# 로그, 로그2읫, e의 4승, 4의 제곱근, e의 4승의 자연로그, 4를 2로 나눈 나머지, 4의 절대값\nq = \"\"\"SELECT\n    4,\n    LOG(4),\n    LOG(2,4),\n    EXP(4),\n    SQRT(4),\n    LN(EXP(4)),\n    MOD(4,2),\n    ABS(4)\n\"\"\"\npd.read_sql(q, conn)\n\n\n\n\n\n\n\n\n4\nLOG(4)\nLOG(2,4)\nEXP(4)\nSQRT(4)\nLN(EXP(4))\nMOD(4,2)\nABS(4)\n\n\n\n\n0\n4\n0.60206\n2.0\n54.59815\n2.0\n4.0\n0.0\n4"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#숫자-함수-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#숫자-함수-실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "숫자 함수 실습",
    "text": "숫자 함수 실습\n\ntb_out 테이블의\nnr_employed 열에 대해서\n자연로그를 씌워서 ln_nr_employed열에,\n제곱근을 구해서 sqrt_nr_employed열에 할당해 봅시다\n\n\n# write your code\nq = \"\"\"SELECT\n    LN(nr_employed) as ln_nr_employed,\n    SQRT(nr_employed) as sqrt_nr_employed\nFROM tb_out\n\"\"\"\npd.read_sql(q, conn)\n\n\n\n\n\n\n\n\nln_nr_employed\nsqrt_nr_employed\n\n\n\n\n0\n8.554682\n72.048595\n\n\n1\n8.554682\n72.048595\n\n\n2\n8.554682\n72.048595\n\n\n3\n8.554682\n72.048595\n\n\n4\n8.554682\n72.048595\n\n\n...\n...\n...\n\n\n370\n8.509887\n70.452821\n\n\n371\n8.509887\n70.452821\n\n\n372\n8.509887\n70.452821\n\n\n373\n8.509887\n70.452821\n\n\n374\n8.509887\n70.452821\n\n\n\n\n375 rows × 2 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#날짜-함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#날짜-함수",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "날짜 함수",
    "text": "날짜 함수\n\nSQL별 문법차이가 커서, 자세한 부분은 사용하기 전 공식문서를 확인\nSTFRTIME 함수는 공통적으로 있는 편\nSTRFTIME('format', col) -- 날짜 데이터를 원하는 지정한 format의 문자열로 변환해서 반환합니다.\n\n\n\n\n\n\n\n\n코드\n설명\n예시\n\n\n\n\n%Y\n연도 (4자리)\n2024\n\n\n%y\n연도 (2자리)\n24\n\n\n%m\n월 (2자리)\n01 (1월), 12 (12월)\n\n\n%d\n일 (2자리)\n01 (1일), 31 (31일)\n\n\n%H\n시간 (24시간제, 2자리)\n00 (자정), 23 (23시)\n\n\n%M\n분 (2자리)\n00, 59\n\n\n%S\n초 (2자리)\n00, 59\n\n\n%w\n요일 (0: 일요일, 6: 토요일)\n0 (일요일), 6 (토요일)\n\n\n%W\n연중 주 번호 (0부터 시작)\n00, 52\n\n\n%j\n연중 일 번호 (1부터 시작)\n001 (1월 1일), 365\n\n\n%f\n소수점 이하 초 (마이크로초 단위, 6자리)\n123456\n\n\n%s\nUnix Timestamp (1970년 1월 1일부터의 초)\n1708953600\n\n\n%z\nUTC 오프셋 (시간대 정보)\n+0000, -0800\n\n\n%Z\n시간대 이름 (예: UTC)\nUTC\n\n\n%%\n% 문자 그대로\n%\n\n\n\n\n\n# 실습용 테이블 생성\n\n## 테이블 생성\nq = \"\"\"CREATE TABLE tb_dates (\n    order_id INTEGER PRIMARY KEY,\n    order_date TEXT\n);\"\"\"\nconn.execute(q)\n\n## 샘플데이터 입력\nq = \"\"\"INSERT INTO tb_dates (order_id, order_date) VALUES\n(1, '2024-12-26 10:30:00'),\n(2, '2024-12-25 14:20:00'),\n(3, '2024-12-24 08:00:00');\n\"\"\"\nconn.execute(q)\n\n\n# STFRTIME 실습\nq =\"\"\"SELECT order_id, order_date, STRFTIME('%Y', order_date) AS order_year FROM tb_dates;\"\"\"\npd.read_sql(q, conn)\n\n\n\n\n\n\n\n\norder_id\norder_date\norder_year\n\n\n\n\n0\n1\n2024-12-26 10:30:00\n2024\n\n\n1\n2\n2024-12-25 14:20:00\n2024\n\n\n2\n3\n2024-12-24 08:00:00\n2024\n\n\n\n\n\n\n\n\nDB별 날짜 간의 차이구하는 샘플\n\n\n\n\n\n\n\n\n\nDatabase\nSQL Query\n설명\n\n\n\n\nMySQL\nSELECT DATEDIFF('2024-12-31', '2024-12-25') AS days_difference;\n두 날짜 간의 차이를 일 단위로 반환\n\n\nOracle\nSELECT TO_DATE('2024-12-31', 'YYYY-MM-DD') - TO_DATE('2024-12-25', 'YYYY-MM-DD') AS days_difference FROM dual;\n날짜 간 차이를 일 단위로 반환\n\n\nSQLite3\nSELECT JULIANDAY('2024-12-31') - JULIANDAY('2024-12-25') AS days_difference;\n두 날짜의 율리우스 날짜 차이를 계산\n\n\nHive\nSELECT DATEDIFF('2024-12-31', '2024-12-25') AS days_difference;\n날짜 차이를 일 단위로 반환\n\n\nPostgreSQL\nSELECT '2024-12-31'::DATE - '2024-12-25'::DATE AS days_difference;\n두 날짜 간의 차이를 일 단위로 반환"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#case-when-조건문",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#case-when-조건문",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "CASE WHEN 조건문",
    "text": "CASE WHEN 조건문\n\nCASE WHEN\n\nCASE WHEN 조건1 THEN 결과1\n    WHEN 조건2 THEN 결과2\n    ...\n    ELSE 모든 조건을 만족하지 않을 때의 결과 END AS 변수명\n\n아래 표의 education(학력)은, 순위가 있는 Ordinal변수\n\n한 자리 숫자코드로 지정해 모델이 이해하기 쉽게 / 메모리효율이 높아지게 함\n\n\n\npd.read_sql('SELECT education FROM tb_cst', conn).education.unique()\n\narray(['basic.4y', 'high.school', 'basic.6y', 'basic.9y',\n       'professional.course', 'unknown', 'university.degree',\n       'illiterate'], dtype=object)\n\n\n\nq = \"\"\"SELECT\n    education,\n    CASE WHEN education = 'illiterate' THEN 0\n         WHEN education = 'basic.4y' THEN 1\n         WHEN education = 'basic.6y' THEN 2\n         WHEN education = 'basic.9y' THEN 3\n         WHEN education = 'high.school' THEN 4\n         WHEN education = 'university.degree' THEN 5\n         WHEN education = 'professional.course' THEN 6\n         WHEN education = 'unknown' THEN 99\n         ELSE 99 END AS education_c\nFROM tb_cst\n\"\"\"\npd.read_sql(q, conn).head(10)\n\n\n\n\n\n\n\n\neducation\neducation_c\n\n\n\n\n0\nbasic.4y\n1\n\n\n1\nhigh.school\n4\n\n\n2\nhigh.school\n4\n\n\n3\nbasic.6y\n2\n\n\n4\nhigh.school\n4\n\n\n5\nbasic.9y\n3\n\n\n6\nprofessional.course\n6\n\n\n7\nunknown\n99\n\n\n8\nprofessional.course\n6\n\n\n9\nhigh.school\n4\n\n\n\n\n\n\n\n\n학력별 고객 수를 1K 이상/이하 등으로 지정하는 예시\n\n\nq = \"\"\"SELECT\n    education,\n    COUNT(*) AS n_cst,\n    CASE WHEN COUNT(*) &gt; 1000 THEN 'Above 1K'\n         ELSE 'Below 1K'\n         END AS n_cst_over_1k\nFROM tb_cst\nGROUP BY education\n\"\"\"\npd.read_sql(q, conn)\n\n\n\n\n\n\n\n\neducation\nn_cst\nn_cst_over_1k\n\n\n\n\n0\nbasic.4y\n4176\nAbove 1K\n\n\n1\nbasic.6y\n2292\nAbove 1K\n\n\n2\nbasic.9y\n6045\nAbove 1K\n\n\n3\nhigh.school\n9515\nAbove 1K\n\n\n4\nilliterate\n18\nBelow 1K\n\n\n5\nprofessional.course\n5243\nAbove 1K\n\n\n6\nuniversity.degree\n12168\nAbove 1K\n\n\n7\nunknown\n1731\nAbove 1K"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#sql실행순서-case-when-사용시-유의할-점",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#sql실행순서-case-when-사용시-유의할-점",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "SQL실행순서 (+CASE WHEN 사용시 유의할 점)",
    "text": "SQL실행순서 (+CASE WHEN 사용시 유의할 점)\n\nSQL실행순서\n\nFROM & JOIN\nWHERE\nGROUP BY\nHAVING\nSELECT (window function, case when)\nORDER BY\nLIMIT\n\nCASE WHEN은 SELECT 안에서 실행되므로, CASE WHEN으로 생성한 변수를 쓰려면 별칭이 아닌 구문 전체를 사용\n\nAS는 오류를 일으키는 경우가 많아 END까지만 작성"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#case-when-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#case-when-실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "CASE WHEN 실습",
    "text": "CASE WHEN 실습\n\n고객의 직업별로\n확실히 파산한 고객의 수를 n_default_cst에 할당한 뒤 (Unique한 수를 세려면 DISTINCT를 열 앞에 붙여주세요. COUNT(DISTINCT COL) 이렇게요.)\n1명 이상 있다면 1 없으면 0을 is_default_job에 할당해주세요.\n\n\nq = \"\"\"SELECT\njob, \nSUM(CASE WHEN is_default = 'yes' THEN 1\n        ELSE 0\n        END) AS n_default_cst,\nCASE WHEN SUM(CASE WHEN is_default = 'yes' then 1\n        ELSE 0\n        END) &gt;= 1 THEN 1\n        ELSE 0\n        END AS is_default_job\nFROM tb_cst\nGROUP BY job\n\"\"\"\npd.read_sql(q, conn)\n\n# 위 사항은 자주 사용되는 케이스임\n\n\n\n\n\n\n\n\njob\nn_default_cst\nis_default_job\n\n\n\n\n0\nadmin.\n0\n0\n\n\n1\nblue-collar\n0\n0\n\n\n2\nentrepreneur\n0\n0\n\n\n3\nhousemaid\n0\n0\n\n\n4\nmanagement\n0\n0\n\n\n5\nretired\n0\n0\n\n\n6\nself-employed\n0\n0\n\n\n7\nservices\n0\n0\n\n\n8\nstudent\n0\n0\n\n\n9\ntechnician\n2\n1\n\n\n10\nunemployed\n1\n1\n\n\n11\nunknown\n0\n0"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#join과-union",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#join과-union",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "JOIN과 UNION",
    "text": "JOIN과 UNION\n\n서로 다른 두 테이블을 합치고 싶을 때 JOIN 사용(Python pandas의 merge와 동일)\nFROM절 다음에 사용\n\n    SELECT\n    tb1.col1_from_tb1,\n    tb1.col2_from_tb1,\n    tb1.join_key,\n    tb2.col1_from_tb2\n    FROM table1 AS tb1\n    'JOIN 방법' JOIN tabl2 AS tb2\n    ON tb1.join_key = tb2.join_key\n        AND ...\n\n\n\nNote_week2_1.jpg\n\n\n\n# JOIN대상 테이블1\ntb_cst = pd.read_sql('SELECT * FROM tb_cst', conn)\ntb_cst.head(5)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\n\n\n4\n2023-01-01\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\n\n\n\n\n\n\n\n\n# JOIN대상 테이블2\ntb_camp = pd.read_sql('SELECT * FROM tb_camp', conn)\ntb_camp.head(5)\n\n\n\n\n\n\n\n\ndate\nidx\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\n\n\n\n\n0\n2023-01-01\n0\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\n\n\n1\n2023-01-01\n1\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\n\n\n2\n2023-01-01\n2\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\n\n\n3\n2023-01-01\n3\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\n\n\n4\n2023-01-01\n4\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\n\n\n\n\n\n\n\n\n# JOIN대상 테이블3\ntb_out = pd.read_sql('SELECT * FROM tb_out', conn)\ntb_out.head(5)\n\n\n\n\n\n\n\n\ndate\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\n\n\n\n\n0\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n1\n2023-01-02\n1.1\n93.994\n-36.4\n4.856\n5191.0\n\n\n2\n2023-01-03\n1.1\n93.994\n-36.4\n4.855\n5191.0\n\n\n3\n2023-01-04\n1.1\n93.994\n-36.4\n4.859\n5191.0\n\n\n4\n2023-01-05\n1.1\n93.994\n-36.4\n4.860\n5191.0\n\n\n\n\n\n\n\n\nLEFT JOIN 예시\n\n\n# LEFT JOIN 예시\nq = \"\"\"SELECT\n    c1.date, -- from절에서 가져올 열들을 별칭을 활용해 지정해 줍니다.\n    c1.idx,\n    c1.age,\n    c1.job,\n    c1.marital,\n    c1.education,\n    c1.is_default,\n    c1.housing,\n    c1.loan,\n    o1.emp_var_rate, -- 마찬가지로 join절에서 가져올 열 역시 지정이 가능합니다.\n    o1.cons_price_idx,\n    o1.cons_conf_idx,\n    o1.euribor3m,\n    o1.nr_employed\nFROM tb_cst AS c1\nLEFT JOIN tb_out AS o1\n  ON c1.date = o1.date\n\"\"\"\npd.read_sql(q, conn).head(5)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n4\n2023-01-01\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\n1.1\n93.994\n-36.4\n4.857\n5191.0\n\n\n\n\n\n\n\n\nINNER JOIN 예시\n\n\n# JOIN대상 테이블4\ntb_y = pd.read_sql('SELECT * FROM tb_y', conn)\ntb_y.head(5)\n\n\n\n\n\n\n\n\ndate\nidx\ny\n\n\n\n\n0\n2023-01-01\n0\nno\n\n\n1\n2023-01-01\n1\nno\n\n\n2\n2023-01-01\n2\nno\n\n\n3\n2023-01-01\n3\nno\n\n\n4\n2023-01-01\n5\nno\n\n\n\n\n\n\n\n\n# INNER JOIN 예시\nq = \"\"\"SELECT\n    c1.date, -- from절에서 가져올 열들을 별칭을 활용해 지정해 줍니다.\n    c1.idx,\n    c1.age,\n    c1.job,\n    c1.marital,\n    c1.education,\n    c1.is_default,\n    c1.housing,\n    c1.loan,\n    y1.y\nFROM tb_cst AS c1\nINNER JOIN tb_y AS y1\n  ON c1.date = y1.date\n    AND c1.idx = y1.idx\n\"\"\"\nco = pd.read_sql(q, conn)\nco.head(5)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ny\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\nno\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\nno\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\nno\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\nno\n\n\n4\n2023-01-01\n5\n45\nservices\nmarried\nbasic.9y\nunknown\nno\nno\nno\n\n\n\n\n\n\n\n\nprint(f\"\"\"INNER JOIN의 결과 행의 개수 : {co.shape[0]}\n기존 tb_cst의 행의 개수 : {tb_cst.shape[0]}\"\"\")\n\nINNER JOIN의 결과 행의 개수 : 37534\n기존 tb_cst의 행의 개수 : 41188"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#coalesce또는-nvl",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#coalesce또는-nvl",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "COALESCE(또는 NVL)",
    "text": "COALESCE(또는 NVL)\n\nNULL이 들어갈 자리에 지정한 값을 넣음\nCOALESCE(tb1.col1, 'no') as col1\n위에서 INNER JOIN사용시 값이 줄었는데, Y값이 없다는 것을 YES가 아닌 NO나 UNKNOWN으로 구분해 이진/다중 분류로 사용하고 싶을 때에도 활용 가능\n\n\nq = \"\"\"SELECT\n    c1.date, -- from절에서 가져올 열들을 별칭을 활용해 지정해 줍니다.\n    c1.idx,\n    c1.age,\n    c1.job,\n    c1.marital,\n    c1.education,\n    c1.is_default,\n    c1.housing,\n    c1.loan,\n    COALESCE(y1.y, 'no') as y\nFROM tb_cst AS c1\nLEFT JOIN tb_y AS y1\n  ON c1.date = y1.date\n    AND c1.idx = y1.idx\n\"\"\"\nco2 = pd.read_sql(q, conn)\nco2.head(5)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ny\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\nno\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\nno\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\nno\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\nno\n\n\n4\n2023-01-01\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\nno\n\n\n\n\n\n\n\n\n# NULL로 채워넣어 행의 수가 줄지 않음\nprint(f\"\"\"INNER JOIN의 결과 행의 개수 : {co2.shape[0]}\n기존 tb_cst의 행의 개수 : {tb_cst.shape[0]}\"\"\")\n\nINNER JOIN의 결과 행의 개수 : 41188\n기존 tb_cst의 행의 개수 : 41188"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#join과-coalese-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#join과-coalese-실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "JOIN과 COALESE 실습",
    "text": "JOIN과 COALESE 실습\n\ntb_cst, tb_camp, tb_out, tb_y를 전부 JOIN해 tb_master table을 만들어 저장하세요.\ntb_cst, tb_camp, tb_y는 date, idx를 key로, tb_out은 date를 key로 갖습니다.\ntb_y가 null인 경우, ’unknown’으로 값을 채워 넣으세요.\n\n\nconn.execute(\"DROP TABLE IF EXISTS tb_master\")\n\n&lt;sqlite3.Cursor at 0x150ffa8a5c0&gt;\n\n\n\nq = \"\"\"\nCREATE TABLE tb_master AS\n    SELECT\n        *\n        ,COALESCE(c3.y, 'unknown')\n    FROM tb_cst as c1\n    LEFT JOIN tb_camp as c2\n        ON c1.date = c2.date AND c1.idx = c2.idx\n    LEFT JOIN tb_y as c3\n        ON c1.date = c3.date AND c1.idx = c3.idx\n    LEFT JOIN tb_out as d1\n        ON c1.date = d1.date\n\"\"\"\nconn.execute(q)\n\npd.read_sql('SELECT * FROM tb_master', conn).head(10)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ndate:1\n...\ndate:2\nidx:2\ny\ndate:3\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\nCOALESCE(c3.y, 'unknown')\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\n2023-01-01\n...\n2023-01-01\n0.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\n2023-01-01\n...\n2023-01-01\n1.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\n2023-01-01\n...\n2023-01-01\n2.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\n2023-01-01\n...\n2023-01-01\n3.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n2023-01-01\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\n2023-01-01\n...\nNone\nNaN\nNone\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nunknown\n\n\n5\n2023-01-01\n5\n45\nservices\nmarried\nbasic.9y\nunknown\nno\nno\n2023-01-01\n...\n2023-01-01\n5.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n6\n2023-01-01\n6\n59\nadmin.\nmarried\nprofessional.course\nno\nno\nno\n2023-01-01\n...\n2023-01-01\n6.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n7\n2023-01-01\n7\n41\nblue-collar\nmarried\nunknown\nunknown\nno\nno\n2023-01-01\n...\n2023-01-01\n7.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n8\n2023-01-01\n8\n24\ntechnician\nsingle\nprofessional.course\nno\nyes\nno\n2023-01-01\n...\n2023-01-01\n8.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n9\n2023-01-01\n9\n25\nservices\nsingle\nhigh.school\nno\nyes\nno\n2023-01-01\n...\n2023-01-01\n9.0\nno\n2023-01-01\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n10 rows × 29 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#서브쿼리와-cte",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#서브쿼리와-cte",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "서브쿼리와 CTE",
    "text": "서브쿼리와 CTE\n\n실무에서는 쿼리가 길고 많은 테이블을 가져다가 쓰게 됨\n매번 위처럼 CREATE TABLE로 업무하면 전산팀 등에서 용량 문제로 연락올 수 있음\n이런 문제가 발생치 않도록 할 수 있는 것이 서브쿼리와 CTE\n서브쿼리는 사용이 편하나 가독성/재활용성이 떨어지고, CTE는 이해하기 용이함\n\n본인의 업무에 맞게 활용\n\n\n\n서브쿼리\n\nFROM, WHERE, HAVING 등 다양한 곳에 사용할 수 있음\n\n보통 FROM, JOIN, WHERE에 많이 사용\n\n장점 : 별도 테이블로 저장/삭제할 필요없이 쿼리를 적어 사용가능\n단점 : 가독성, 재활용\n서브쿼리 샘플 (LEFT JOIN부분)\n\nSELECT\n    c1.*,\n    c2.marital_rate_per_job\nFROM tb_cst AS c1\nLEFT JOIN (\nSELECT\n    job,\n    SUM(CASE WHEN marital = 'married' THEN 1.0 ELSE 0.0 END) / COUNT(*) AS marital_rate_per_job\nFROM tb_cst\nGROUP BY job\n) AS c2 ON c1.job = c2.job\n\n\nCTE(Common Table Expressions)\n\nWITH + 임시테이블명을 지정 후, 호출하여 사용가능\n\nWITH 임시테이블명 AS (\n원래 서브쿼리에 들어가던 쿼리\n)...\n\n여러개를 지정할 경우, WITH는 한번만 쓰고 ,(쉼표)로 구분\n적절한 테이블네이밍과 주석을 통해, 장기적 관점에서 관리/활용이 용이함\nCTE 샘플\n\nWITH tb_marital_rate_per_job AS (\n    SELECT\n        job,\n        SUM(CASE WHEN marital = 'married' THEN 1.0 ELSE 0.0 END) / COUNT(*) AS marital_rate_per_job\n    FROM tb_cst\n    GROUP BY job\n    )\n    SELECT\n        c1.*,\n        c2.marital_rate_per_job\n    FROM tb_cst AS c1\n    LEFT JOIN tb_marital_rate_per_job AS c2 ON c1.job = c2.job"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#서브쿼리와-cte실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#서브쿼리와-cte실습",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "서브쿼리와 CTE실습",
    "text": "서브쿼리와 CTE실습\n\n켐페인 정보 테이블에 대해서\ncontact별 평균 pdays를 구해 avg_pdays_per_contact에 할당하고 (단 999는 제외할 것)\nday_of_week별 평균 duration을 구해 avg_duration_per_dow에 할당하고\n해당 두 컬럼을 켐페인 정보 테이블에 LEFT JOIN한 결과를 서브쿼리와 CTE 각각을 사용해 구하세요.\n\n\n# 서브쿼리\nq = \"\"\"\nSELECT\n    *\nFROM tb_camp AS main\nLEFT JOIN (SELECT contact, AVG(pdays)\n      FROM tb_camp\n      WHERE pdays != 999\n      GROUP BY contact) AS avg_pdays_per_contact\n    ON main.contact = avg_pdays_per_contact.contact\nLEFT JOIN (SELECT day_of_week, AVG(duration)\n            FROM tb_camp\n            GROUP BY day_of_week) AS avg_duration_per_dow\n    ON main.day_of_week = avg_duration_per_dow.day_of_week\n\"\"\"\n\npd.read_sql_query(q, conn).head(10)\n\n\n\n\n\n\n\n\ndate\nidx\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\ncontact\nAVG(pdays)\nday_of_week\nAVG(duration)\n\n\n\n\n0\n2023-01-01\n0\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n1\n2023-01-01\n1\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n2\n2023-01-01\n2\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n3\n2023-01-01\n3\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n4\n2023-01-01\n4\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n5\n2023-01-01\n5\ntelephone\nmay\nmon\n198\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n6\n2023-01-01\n6\ntelephone\nmay\nmon\n139\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n7\n2023-01-01\n7\ntelephone\nmay\nmon\n217\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n8\n2023-01-01\n8\ntelephone\nmay\nmon\n380\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n9\n2023-01-01\n9\ntelephone\nmay\nmon\n50\n1\n999\n0\nnonexistent\ntelephone\n6.212389\nmon\n246.568358\n\n\n\n\n\n\n\n\n# CTE\nq = \"\"\"WITH avg_pdays_per_contact AS (\n        SELECT contact, AVG(pdays) as average_pdays\n      FROM tb_camp\n      WHERE pdays != 999\n      GROUP BY contact\n      ),\navg_duration_per_dow AS (\n        SELECT day_of_week, AVG(duration) as average_duration\n        FROM tb_camp\n        GROUP BY day_of_week\n      )\nSELECT\n    main.*\n    ,c1.average_pdays\n    ,c2.average_duration\nFROM tb_camp AS main\nLEFT JOIN avg_pdays_per_contact AS c1\n    ON main.contact = c1.contact\nLEFT JOIN avg_duration_per_dow AS c2\n    ON main.day_of_week = c2.day_of_week\n\"\"\"\n\npd.read_sql_query(q, conn).head(10)\n\n\n\n\n\n\n\n\ndate\nidx\ncontact\nmonth\nday_of_week\nduration\ncampaign\npdays\nprevious\npoutcome\naverage_pdays\naverage_duration\n\n\n\n\n0\n2023-01-01\n0\ntelephone\nmay\nmon\n261\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n1\n2023-01-01\n1\ntelephone\nmay\nmon\n149\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n2\n2023-01-01\n2\ntelephone\nmay\nmon\n226\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n3\n2023-01-01\n3\ntelephone\nmay\nmon\n151\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n4\n2023-01-01\n4\ntelephone\nmay\nmon\n307\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n5\n2023-01-01\n5\ntelephone\nmay\nmon\n198\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n6\n2023-01-01\n6\ntelephone\nmay\nmon\n139\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n7\n2023-01-01\n7\ntelephone\nmay\nmon\n217\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n8\n2023-01-01\n8\ntelephone\nmay\nmon\n380\n1\n999\n0\nnonexistent\n6.212389\n246.568358\n\n\n9\n2023-01-01\n9\ntelephone\nmay\nmon\n50\n1\n999\n0\nnonexistent\n6.212389\n246.568358"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#심화-window함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#심화-window함수",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "심화 : Window함수",
    "text": "심화 : Window함수\n\nWindow함수는 데이터의 현재 구조를 유지하며 추가 정보를 제공하기 위해 사용\n\n앞서 서브쿼리로 했던, 집계함수 계산 후 Join했던 부분을 대체가능함\n\n(Sorting영향을 많이 받으므로)데이터가 엉망일수록 오히려 성능이 안좋을 수 있으나, 가독성 측면에서 이점\nWindow함수의 문법\n집계 함수 OVER ([PARTITION BY 파티션 기준 열] [ORDER BY 정렬 기준 열])\n\n집계 함수\n\nRANK, SUM, AVG, COUNT, MIN, MAX\nROW_NUMBER : RANK와 유사하나, 값이 같아도 행 번호는 다름\n\nOVER\n\n모든 WINDOW함수와 함께 사용되는 기본 문법\n\nPARTITION BY\n\n해당 열을 기준으로 데이터를 쪼개어(GROUP BY와 유사) 계산\n서브쿼리와 CTE에서 GROUP BY에 해당하는 열이 들어가는 자리\n\nORDER BY\n\nRANK처럼 순서가 유의미한 함수를 사용할 때 사용\nSQLITE3은 미지정시 첫 열을 기준으로 정렬\n\n\n\n\n# Window함수 적용 전 샘플\nq = \"\"\"WITH tb_marital_rate_per_job AS (\n  SELECT\n    job,\n    SUM(CASE WHEN marital = 'married' THEN 1.0 ELSE 0.0 END) / COUNT(*) AS marital_rate_per_job\n  FROM tb_cst\n  GROUP BY job\n)\nSELECT\n    c1.*,\n    c2.marital_rate_per_job\nFROM tb_cst AS c1\nLEFT JOIN tb_marital_rate_per_job AS c2 ON c1.job = c2.job\n\"\"\"\n\ntb_cte = pd.read_sql_query(q, conn)\ntb_cte.head(10)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\nmarital_rate_per_job\n\n\n\n\n0\n2023-01-01\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\n0.733019\n\n\n1\n2023-01-01\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\n0.577979\n\n\n2\n2023-01-01\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\n0.577979\n\n\n3\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\n0.504030\n\n\n4\n2023-01-01\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\n0.577979\n\n\n5\n2023-01-01\n5\n45\nservices\nmarried\nbasic.9y\nunknown\nno\nno\n0.577979\n\n\n6\n2023-01-01\n6\n59\nadmin.\nmarried\nprofessional.course\nno\nno\nno\n0.504030\n\n\n7\n2023-01-01\n7\n41\nblue-collar\nmarried\nunknown\nunknown\nno\nno\n0.722606\n\n\n8\n2023-01-01\n8\n24\ntechnician\nsingle\nprofessional.course\nno\nyes\nno\n0.544268\n\n\n9\n2023-01-01\n9\n25\nservices\nsingle\nhigh.school\nno\nyes\nno\n0.577979\n\n\n\n\n\n\n\n\n# Window함수 적용 후 샘플\nq = \"\"\"SELECT\n    *,\n    SUM(CASE WHEN marital = 'married' THEN 1.0 ELSE 0.0 END) OVER(PARTITION BY job) / COUNT(*) OVER(PARTITION BY job) AS marital_rate_per_job\nFROM tb_cst\n\"\"\"\n\ntb_window = pd.read_sql_query(q, conn)\ntb_window.head(10)\n\n\n\n\n\n\n\n\ndate\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\nmarital_rate_per_job\n\n\n\n\n0\n2023-01-01\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\n0.50403\n\n\n1\n2023-01-01\n6\n59\nadmin.\nmarried\nprofessional.course\nno\nno\nno\n0.50403\n\n\n2\n2023-01-01\n24\n37\nadmin.\nmarried\nhigh.school\nno\nyes\nno\n0.50403\n\n\n3\n2023-01-01\n30\n46\nadmin.\nmarried\nunknown\nno\nno\nno\n0.50403\n\n\n4\n2023-01-01\n38\n41\nadmin.\nmarried\nuniversity.degree\nno\nyes\nno\n0.50403\n\n\n5\n2023-01-01\n42\n38\nadmin.\nsingle\nprofessional.course\nno\nno\nno\n0.50403\n\n\n6\n2023-01-01\n43\n57\nadmin.\nmarried\nuniversity.degree\nno\nno\nyes\n0.50403\n\n\n7\n2023-01-01\n44\n44\nadmin.\nmarried\nuniversity.degree\nunknown\nyes\nno\n0.50403\n\n\n8\n2023-01-01\n46\n57\nadmin.\nmarried\nuniversity.degree\nno\nyes\nyes\n0.50403\n\n\n9\n2023-01-01\n48\n35\nadmin.\nmarried\nuniversity.degree\nno\nyes\nno\n0.50403"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#기타",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#기타",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "기타",
    "text": "기타\n\nGROUPBY와 PARTITION BY의 차이 : PARTITION BY는 원래 데이터 형태를 해치치 않음\n\nGROUPBY는 지정한 컬럼에 대해 값만 남겨놓고 계산(그룹핑할 데이터를 가져 옴옴)\n\n예를 들어 위의 데이터는 41188 row를 가지고있었는데, 이 row를 모두 쓰지 않음\n\nPARTITION BY는 원래 데이터 형태를 해치치않고, 계산을 위한 임시Window를 활용\n\n파이썬은 느린편이어서, 가능한 SQL 등으로 전처리를 하고 넘어오는 것이 좋다\n\n(강사님 기준 대용량 처리) Polar(가 duckdb보다 좀 더 편했음), ductdb &gt; 파이썬"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#과제",
    "href": "posts/meta-cm-sql_and_ml_xai-20241229/index.html#과제",
    "title": "[DA스터디/2주차] SQL심화",
    "section": "과제",
    "text": "과제\n\n프로그래머스 과제 풀기\n\n저작권 관련 프로그래머스 링크\n\n  코딩 테스트 연습 문제\n  코딩테스트 연습에 공개된 문제는 (주)그렙이 저작권을 가지고 있습니다.(지문 하단에 별도 저작권 표시 문제 제외)코딩테스트 연습 문제의 지문, 테스트케이스, 풀이 등과 같은 정보는 비상업적, 비영리적 용도로 게시할 수 있습니다.다만 문제의 지문, 풀이 등과 같은 정보를 단순히 게시하는 것을 넘어, 이를 바탕으로 문제를 풀고 채점이 가능하도록 하는 등의 방식으로 활용하는 것은 제한됩니다.\n\n  ※ 2020 KAKAO BLIND RECRUITMENT, Summer/Winter Coding 등의 문제는 기업 코딩 테스트에 나온 문제이나, 코딩테스트 연습에 공개된 문제이기 때문에 마찬가지로 비상업적, 비영리적 용도로 게시할 수 있습니다.\n\n  (2021. 01. 08 업데이트)\n\n문제 이미지가 너무 길어서 풀이했던 것만 남김\nhttps://school.programmers.co.kr/learn/courses/30/lessons/299307\n\nSELECT ID, \n    CASE \n    WHEN SIZE_OF_COLONY &lt;= 100 THEN 'LOW'\n    WHEN SIZE_OF_COLONY &gt; 100 AND SIZE_OF_COLONY &lt;= 1000 THEN 'MEDIUM'\n    WHEN SIZE_OF_COLONY &gt; 1000 THEN 'HIGH'\n    END AS SIZE\nFROM ECOLI_DATA\nORDER BY ID ASC;\n\nhttps://school.programmers.co.kr/learn/courses/30/lessons/151139\n\nSELECT C1.CAR_ID\n    ,C1.CAR_TYPE\n    ,ROUND(C1.DAILY_FEE * 30 * (1-C3.DISCOUNT_RATE/100)) AS FEE\nFROM CAR_RENTAL_COMPANY_CAR AS C1\n    LEFT JOIN CAR_RENTAL_COMPANY_RENTAL_HISTORY AS C2\n        ON C1.CAR_ID = C2.CAR_ID\n            AND C2.END_DATE &gt;= '2022-11-01'\n            AND C2.START_DATE &lt;= '2022-11-30'        \n    JOIN (SELECT *\n                FROM CAR_RENTAL_COMPANY_DISCOUNT_PLAN\n                WHERE DURATION_TYPE = '30일 이상') AS C3\n        ON C1.CAR_TYPE = C3.CAR_TYPE\nWHERE 1=1\n    AND C2.CAR_ID IS NULL\n    AND C1.CAR_TYPE IN ('세단', 'SUV')\nHAVING 500000 &lt;= FEE AND FEE &lt; 2000000\nORDER BY FEE DESC, CAR_TYPE ASC, CAR_ID DESC\n\nhttps://school.programmers.co.kr/learn/courses/30/lessons/59044\n\nSELECT\n    AI.NAME,\n    AI.DATETIME\nFROM ANIMAL_INS AS AI\nLEFT JOIN ANIMAL_OUTS AS AO\nON AI.ANIMAL_ID = AO.ANIMAL_ID\nWHERE 1=1\n    AND AO.ANIMAL_ID IS NULL\nORDER BY DATETIME\nLIMIT 3\n\nhttps://school.programmers.co.kr/learn/courses/30/lessons/157339\n\nSELECT MONTH(START_DATE) AS MONTH\n    , CAR_ID\n    , COUNT(*) AS RECORDS\nFROM CAR_RENTAL_COMPANY_RENTAL_HISTORY\nWHERE 1=1\n    AND START_DATE BETWEEN '2022-08-01' AND '2022-11-01'\n    AND CAR_ID IN (SELECT\n                CAR_ID\n                FROM CAR_RENTAL_COMPANY_RENTAL_HISTORY\n                WHERE 1=1\n                AND START_DATE BETWEEN '2022-08-01' AND '2022-11-01'\n                GROUP BY CAR_ID\n                HAVING COUNT(*)&gt;=5)\nGROUP BY MONTH, CAR_ID\nORDER BY MONTH ASC, CAR_ID DESC"
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html",
    "href": "posts/dtcontest-ore-20240610/index.html",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "",
    "text": "2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가 기록.(+github활용한 온라인db구축)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html#개요",
    "href": "posts/dtcontest-ore-20240610/index.html#개요",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "개요",
    "text": "개요\n\n2024년 12회 산업통상자원부 공공데이터 활용 아이디어 공모전 참가해보기로 함\n\nhttps://datacontest.kr/apply/applyAdd/3\n\n단순 API로딩이 아니라, 각자 바로 데이터를 로딩할 수 있는 매체에 대한 고민 후 구현\n\n코딩이 익숙하지 않은 팀원이 쉽게 이용할 수 있도록하고, readme에 바로 사용할 수 있게 샘플코드 제공\ngithub를 활용해 pandas에서 바로 로딩할 수 있도록 구현"
  },
  {
    "objectID": "posts/dtcontest-ore-20240610/index.html#내용정리",
    "href": "posts/dtcontest-ore-20240610/index.html#내용정리",
    "title": "[공모전] 공공데이터 공모전-2(github온라인db)",
    "section": "내용정리",
    "text": "내용정리\n\n도입목적\n\n본격적으로 모델 학습을 하기 전, 데이터 이용 편의를 증진하고자 함\n팀원들이 R이나 통계분석에는 익숙하나 파이썬 코딩에는 익숙하지않아, 최대한 모델링에 집중하도록 지원\n\n하나의 repository에서 원하는 데이터를 한번에 확인 가능\n업데이트 일자를 표기하여 얼마나 최신 데이터인지 확인 가능\n\n데이터를 하나의 페이지에서 통합관리(공공데이터 홈페이지 접속 등 불필요)\n개인서버(NAS)에서 매일 특정시간 구동하여 별도의 수작업없이 자동으로 최신화\n\n\n\n구동방식\n\n공공데이터 리스트와 API키가 저장된 json파일 로딩\n지정된 공공데이터를 다운로드하고 csv파일로 저장\n바로 로딩하기위한 파일 주소생성, 업데이트 날짜 저장\nREADME 파일에 파일주소와 업데이트 날짜 등 업데이트\ngit_push함수로 github repository에 자동업로드\n\n\n\ngithub reposiroty주소\nhttps://github.com/KR9268/db_datagokr\n\n\n샘플코드(패키지 및 함수)\n\nimport requests\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport subprocess\nimport os\nimport time\n\ndef json_load(json_path, encoding='utf-8'):\n    with open(json_path, 'r', encoding=encoding) as file:\n        json_data = json.load(file)\n    return json_data\n\ndef request_and_to_json(url):\n    response = requests.get(url)\n    json_ob = json.loads(response.text)\n    return json_ob\n\ndef chk_json_status_of_data_go_kr(json_obj):\n    other_data = ['currentCount', 'matchCount', 'page', 'perPage', 'totalCount']\n    result_dict = dict()\n    \n    for each_column in other_data:\n        result_dict[each_column] = json_obj[each_column]  \n    return result_dict \n\ndef download_from_data_go_kr_with_json(url):\n    json_ob = request_and_to_json(url)\n\n    json_status = chk_json_status_of_data_go_kr(json_ob)\n    if json_status['currentCount'] &lt; json_status['totalCount']:\n        url = url.replace('perPage=1',f'perPage={json_status[\"totalCount\"]}')\n        json_ob = request_and_to_json(url)\n\n    return json_ob\n\ndef update_readme(new_content_list):\n    # Open the README.md file in read mode\n    with open('README.md', 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    # Find the index of the line that starts with '* 데이터 현황'\n    index = next((i for i, line in enumerate(lines) if line.startswith('* 데이터 현황')), None)\n\n    # If the line is found, remove the following lines and insert new content\n    if index is not None:\n        lines = lines[:index+1] # Remove the following lines\n        #lines.extend(new_content) # Insert new content\n        lines.extend(new_content_list) # Insert new content\n\n    # Open the README.md file in write mode and write the updated content\n    with open('README.md', 'w', encoding='utf-8') as file:\n        file.writelines(lines)\n\ndef git_push():\n    # Get a list of all .csv files in the current directory\n    csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n\n    # Stage all .csv files\n    for file in csv_files:\n        subprocess.call(['git', 'add', file])\n\n    subprocess.call(['git', 'add', 'README.md'])\n    # Commit the changes with a message\n    subprocess.call(['git', 'commit', '-m', 'Automatic commit'])\n\n    # Push the changes to the remote repository\n    subprocess.call(['git', 'push'])\n\n\n\n샘플코드(메인코드)\n\n# json load\nserviceKey = json_load('option.json')['serviceKey']\ndb_list = json_load('db_list.json', 'cp949')\n\n\n# main\n\n# 작업하기\ntxt_for_readme = ['\\n']\nfor each in db_list:\n    # 다운로드\n    url = f\"{each['base_url']}{each['address_get']}?page=1&perPage=1&serviceKey={serviceKey}\"\n    json_data = download_from_data_go_kr_with_json(url)\n    result_data = chk_json_status_of_data_go_kr(json_data)\n    \n    # 저장\n    if result_data['currentCount'] == result_data['totalCount']:\n        pd.json_normalize(json_data['data']).to_csv(f\"{each['file_name_to']}.csv\",encoding='cp949', index=False)\n\n    # 파일주소 및 이름, 업데이트시간 저장\n    owner = 'KR9268'\n    repo = 'db_datagokr'\n    branch = 'main'\n    file_path = f\"{each['file_name_to']}.csv\"\n\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}\"\n\n    txt_for_readme.append(f\"  *  {datetime.strftime(datetime.now(),'%Y-%m-%d')}업데이트 : {each['name']}\\n{url}\\n\")\n    time.sleep(1)\n\n# 업데이트 내역과 파일 git push\nupdate_readme(txt_for_readme)\ngit_push()"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240923_2/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240923_2/index.html",
    "title": "[DE스터디/최종과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 - 최종과제에 대한 ppt (4차산업 핵심광물 데이터파이프라인+대시보드 with UNcomtrade API)\n\n개요\n\n참여중인 데이터 엔지니어링 스터디에서 배우는 내용 정리\n\n데이터 수집, 정제 : pyspark, airflow\n저장 : elasticsearch\n시각화 : kibana\n\n최종과제 : 배운 Pyspark, Airflow, Elasticsearch, Kibana로 데이터 파이프라인 만들어보기\n만들다보니 시각화에 대한 부분을 좀 더 해보고 싶어, 과제2를 추가로 진행함\n\n과제 1 : gharchive 데이터파이프라인 + 대시보드 약간\n과제 2 : 4차산업 핵심광물(Un comtrade) 데이터파이프라인 + 대시보드\ngit repo (과제 1, 2 모두 여기에 보관)\n\nhttps://github.com/KR9268/metacode_de-2024\n\n\n\n\n\n최종과제2 : 4차산업 핵심광물 데이터파이프라인 + 시각화\n                      \n\n\n\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html",
    "href": "posts/prgms-sql-20240320/index.html",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#개요",
    "href": "posts/prgms-sql-20240320/index.html#개요",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#문제-조건에-부합하는-중고거래-댓글-조회하기",
    "href": "posts/prgms-sql-20240320/index.html#문제-조건에-부합하는-중고거래-댓글-조회하기",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "문제 : 조건에 부합하는 중고거래 댓글 조회하기",
    "text": "문제 : 조건에 부합하는 중고거래 댓글 조회하기\n\n    &lt;h6 class=\"guide-section-title\"&gt;문제 설명&lt;/h6&gt;\n    &lt;div class=\"markdown solarized-dark\"&gt;&lt;p&gt;다음은 중고거래 게시판 정보를 담은 &lt;code&gt;USED_GOODS_BOARD&lt;/code&gt; 테이블과 중고거래 게시판 첨부파일 정보를 담은 &lt;code&gt;USED_GOODS_REPLY&lt;/code&gt; 테이블입니다. &lt;code&gt;USED_GOODS_BOARD&lt;/code&gt; 테이블은 다음과 같으며 &lt;code&gt;BOARD_ID&lt;/code&gt;, &lt;code&gt;WRITER_ID&lt;/code&gt;, &lt;code&gt;TITLE&lt;/code&gt;, &lt;code&gt;CONTENTS&lt;/code&gt;, &lt;code&gt;PRICE&lt;/code&gt;, &lt;code&gt;CREATED_DATE&lt;/code&gt;, &lt;code&gt;STATUS&lt;/code&gt;, &lt;code&gt;VIEWS&lt;/code&gt;은 게시글 ID, 작성자 ID, 게시글 제목, 게시글 내용, 가격, 작성일, 거래상태, 조회수를 의미합니다.&lt;/p&gt;\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nBOARD_ID\n\n\nVARCHAR(5)\n\n\nFALSE\n\n\n\n\nWRITER_ID\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nTITLE\n\n\nVARCHAR(100)\n\n\nFALSE\n\n\n\n\nCONTENTS\n\n\nVARCHAR(1000)\n\n\nFALSE\n\n\n\n\nPRICE\n\n\nNUMBER\n\n\nFALSE\n\n\n\n\nCREATED_DATE\n\n\nDATE\n\n\nFALSE\n\n\n\n\nSTATUS\n\n\nVARCHAR(10)\n\n\nFALSE\n\n\n\n\nVIEWS\n\n\nNUMBER\n\n\nFALSE\n\n\n\n\n\nUSED_GOODS_REPLY 테이블은 다음과 같으며 REPLY_ID, BOARD_ID, WRITER_ID, CONTENTS, CREATED_DATE는 각각 댓글 ID, 게시글 ID, 작성자 ID, 댓글 내용, 작성일을 의미합니다.\n\n\n\n\n\nColumn name\n\n\nType\n\n\nNullable\n\n\n\n\n\n\nREPLY_ID\n\n\nVARCHAR(10)\n\n\nFALSE\n\n\n\n\nBOARD_ID\n\n\nVARCHAR(5)\n\n\nFALSE\n\n\n\n\nWRITER_ID\n\n\nVARCHAR(50)\n\n\nFALSE\n\n\n\n\nCONTENTS\n\n\nVARCHAR(1000)\n\n\nTRUE\n\n\n\n\nCREATED_DATE\n\n\nDATE\n\n\nFALSE\n\n\n\n\n\n\n문제\n\n\nUSED_GOODS_BOARD와 USED_GOODS_REPLY 테이블에서 2022년 10월에 작성된 게시글 제목, 게시글 ID, 댓글 ID, 댓글 작성자 ID, 댓글 내용, 댓글 작성일을 조회하는 SQL문을 작성해주세요. 결과는 댓글 작성일을 기준으로 오름차순 정렬해주시고, 댓글 작성일이 같다면 게시글 제목을 기준으로 오름차순 정렬해주세요.\n\n\n\n예시\n\n\nUSED_GOODS_BOARD 테이블이 다음과 같고\n\n\n\n\n\nBOARD_ID\n\n\nWRITER_ID\n\n\nTITLE\n\n\nCONTENTS\n\n\nPRICE\n\n\nCREATED_DATE\n\n\nSTATUS\n\n\nVIEWS\n\n\n\n\n\n\nB0001\n\n\nkwag98\n\n\n반려견 배변패드 팝니다\n\n\n정말 저렴히 판매합니다. 전부 미개봉 새상품입니다.\n\n\n12000\n\n\n2022-10-01\n\n\nDONE\n\n\n250\n\n\n\n\nB0002\n\n\nlee871201\n\n\n국내산 볶음참깨\n\n\n직접 농사지은 참깨입니다.\n\n\n3000\n\n\n2022-10-02\n\n\nDONE\n\n\n121\n\n\n\n\nB0003\n\n\ngoung12\n\n\n배드민턴 라켓\n\n\n사놓고 방치만 해서 팝니다.\n\n\n9000\n\n\n2022-10-02\n\n\nSALE\n\n\n212\n\n\n\n\nB0004\n\n\nkeel1990\n\n\n디올 귀걸이\n\n\n신세계강남점에서 구입. 정품 아닐시 백퍼센트 환불\n\n\n130000\n\n\n2022-10-02\n\n\nSALE\n\n\n199\n\n\n\n\nB0005\n\n\nhaphli01\n\n\n스팸클래식 팔아요\n\n\n유통기한 2025년까지에요\n\n\n10000\n\n\n2022-10-02\n\n\nSALE\n\n\n121\n\n\n\n\n\nUSED_GOODS_REPLY 테이블이 다음과 같을 때\n\n\n\n\n\nREPLY_ID\n\n\nBOARD_ID\n\n\nWRITER_ID\n\n\nCONTENTS\n\n\nCREATED_DATE\n\n\n\n\n\n\nR000000001\n\n\nB0001\n\n\ns2s2123\n\n\n구매하겠습니다. 쪽지 드립니다.\n\n\n2022-10-02\n\n\n\n\nR000000002\n\n\nB0002\n\n\nhoho1112\n\n\n쪽지 주세요.\n\n\n2022-10-03\n\n\n\n\nR000000003\n\n\nB0006\n\n\nhwahwa2\n\n\n삽니다. 연락주세요.\n\n\n2022-10-03\n\n\n\n\nR000000004\n\n\nB0007\n\n\nhong02\n\n\n예약중\n\n\n2022-10-06\n\n\n\n\nR000000005\n\n\nB0009\n\n\nhanju23\n\n\n구매완료\n\n\n2022-10-07\n\n\n\n\n\nSQL을 실행하면 다음과 같이 출력되어야 합니다.\n\n\n\n\n\nTITLE\n\n\nBOARD_ID\n\n\nREPLY_ID\n\n\nWRITER_ID\n\n\nCONTENTS\n\n\nCREATED_DATE\n\n\n\n\n\n\n반려견 배변패드 팝니다\n\n\nB0001\n\n\nR000000001\n\n\ns2s2123\n\n\n구매하겠습니다. 쪽지 드립니다.\n\n\n2022-10-02\n\n\n\n\n국내산 볶음참깨\n\n\nB0002\n\n\nR000000002\n\n\nhoho1112\n\n\n쪽지 주세요.\n\n\n2022-10-03\n\n\n\n\n\n\n주의사항\n\n\nCREATED_DATE의 포맷이 예시의 포맷과 일치해야 정답처리 됩니다.\n\n\n  &lt;/div&gt;\n\n작성답안\n\n\n\nSELECT board.TITLE, board.BOARD_ID,\n       reply.REPLY_ID, reply.WRITER_ID, reply.CONTENTS, TO_CHAR(reply.CREATED_DATE, 'YYYY-MM-DD')\nFROM USED_GOODS_BOARD board, USED_GOODS_REPLY reply\nWHERE TO_CHAR(board.CREATED_DATE, 'YYYYMM') = '202210'\n  AND board.BOARD_ID = reply.BOARD_ID\nORDER BY reply.CREATED_DATE ASC, board.TITLE ASC;\n\n\nFigure 1\n\n\n\n\n\n정리\n\n각 테이블의 BOARD_ID 일치시키는 것을 실수하였음. 향후 동일케이스에 대해서는 고려하여 풀기\n다중정렬 &gt; ORDER BY reply.CREATED_DATE ASC, board.TITLE ASC; 앞의 컬럼일수록 정렬 우선순위를 가짐"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#작성답안",
    "href": "posts/prgms-sql-20240320/index.html#작성답안",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT board.TITLE, board.BOARD_ID,\n       reply.REPLY_ID, reply.WRITER_ID, reply.CONTENTS, TO_CHAR(reply.CREATED_DATE, 'YYYY-MM-DD')\nFROM USED_GOODS_BOARD board, USED_GOODS_REPLY reply\nWHERE TO_CHAR(board.CREATED_DATE, 'YYYYMM') = '202210'\n  AND board.BOARD_ID = reply.BOARD_ID\nORDER BY reply.CREATED_DATE ASC, board.TITLE ASC;\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240320/index.html#정리",
    "href": "posts/prgms-sql-20240320/index.html#정리",
    "title": "[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기",
    "section": "정리",
    "text": "정리\n\n각 테이블의 BOARD_ID 일치시키는 것을 실수하였음. 향후 동일케이스에 대해서는 고려하여 풀기\n다중정렬 &gt; ORDER BY reply.CREATED_DATE ASC, board.TITLE ASC; 앞의 컬럼일수록 정렬 우선순위를 가짐"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240819/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240819/index.html",
    "title": "[DE스터디/2주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark, Docker\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240819/index.html#rdd",
    "href": "posts/meta-de-spark_and_airflow-20240819/index.html#rdd",
    "title": "[DE스터디/2주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "RDD",
    "text": "RDD\n\nRDD (Resilient Distributed Dataset, 분산 데이터 모델)\n\nRDD를 사용할 때는 How를 기술\nDefault는 여러 클러스터의 메모리 상에 존재하는 데이터 구조\nMapReduce와 비슷한 역할\n데이터 포맷이 텍스트가 아니어도 전부 텍스트로 읽어오는데, 이럴 때 RDD를 사용\n파이썬에서 RDD를 생성하는 예제\n\n보통 대용량 처리에 쓰기 때문에, 이처럼 Local에서 처리하는 일은 실제로 많지 않음을 참고\n\n#  local dataset\ndata = [\"one\", \"two\", \"three\"] \nsc.parallelize(data)\n# range\nsc.range(1, 6) \n# read file\nsc.textFile(\"data/data1.txt\")\n\n\n(RDD데이터 정제시에 사용하는)RDD API에 들어있는 Function은 2가지가 있음\n\nRDD Transformation & RDD Actions \n\nRDD Transformation\n\nRDD를 또 다른 RDD로 변환하는 것(return value가 RDD인 Operation)\n\n기존의 변수는 바뀌지 않고, 새로운 RDD를 리턴한다\n\n예를 들어 sc.range(1, 6)로 만든 RDD의 모든 숫자에 1을 더한다면, 기존 변수는 건드리지 않고 새로운 RDD생성\n기존 변수를 건드리지 않고 새로운 값을 리턴받아 사용해야한다는 점은 함수형 프로그래밍의 기본사항임을 참고\n\n\n기존 데이터는 메모리에 남아있으며, Spark가 메모리 삭제 해주기 전까지는 남아있음\nRDD Transformation예시\n\n문장을 단어로 split하거나 문단을 문장으로 split해서 리턴하는 경우\n\nrdd.flatMap(lambda line: line.split(‘ ‘))\n\n모든 숫자 중 3보다 큰 것만 가져오는 filtering (3보다 큰 것만 추출한 새로운 데이터 구조 리턴)\n\nrdd.filter(lambda x: x &gt; 3)\n\n\nRDD Transformation with shuffle\n\n(대용량 데이터가 분산 처리되고 있고, 여러 노드에 데이터가 저장되어 있는 상황에서) 각 Node에서 데이터 이동이 이루어짐\n\n예를 들어 10개 Cluster가 10%씩 데이터가 있을 때, Node에서 데이터 이동이 이루어져야 Count되는 경우가 Shuffle\nDisk I/O와 Network Traffic(I/O)을 동반하므로, 느리고 부하를 준다\n\nShuffle이 들어간 Transformation은 정해져있으므로, 느릴 수 있다는 것을 미리 생각하고 돌려야함\n\nreduceByKey : key가 동일한 value를 가져와서 작업하는데, 같은 Cluster에 있다는 보장은 없음\n\n실습환경인 Local에서는 체감이 없겠지만, 대용량/분산 환경에서는 부하와 함께 메모리가 터지기도 함\n기본적으로 계산에 사용하는 것만 메모리에 올려있으므로, 작업을 위해 올리는 과정에서부터 문제될 수 있음 \n\n\n\nRDD Actions\n\nRDD를 리턴하는 동작 자체를 의미\n\nprint로 화면에 보여주거나, write와 같은 동작(사용자에게 반환)\nSpark은 lazy하게 동작하게 되어있어, action이 있어야 실질적인 데이터 처리 이루어짐\n\nprint나 write없이 transformation만 있는 경우, 동작이 필요없다고 판단\n\n뒤의 다른 동작이 정의될 때까지 기다렸다가 한번에 최적화하게 되어있음\n예를 들어 print 등이 없는 경우, 굉장히 빠르게 작업이 끝났다면 계산을 아예 안한 것\n\n\n\nRDD의 크기가 커서 Driver client로 리턴할 수 없는 경우, 파일로 쓰는 처리만 가능\n\n데이터의 print/write 등은 Driver가 처리하도록 되어있음\n\nExecutor가 작업 후 넘기면 Driver가 print하는 방식\nprint할 RDD자체가 너무 크면 Driver로 리턴불가하고, Disk I/O만 일어남\n\n\nAction의 종류\n\nrdd.saveAsTextFile : 텍스트파일로 저장\nrdd.collect() : 파이썬 로컬 데이터셋으로 변경 \n\n\nRDD API 공식문서 (pyspark.RDD)\n\nTransformation이나 Action 등 확인 가능\nhttps://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html\nRDD 관련 기본적인 기능들\n\naggregate, dictinct, filter, foreach, groupby, join\n\njoin 등은 데이터를 모으는 기능이 있으니 shuffle이 발생함을 유의\n\n\n분산처리에 특화된 몇가지 API\n\naggregate, aggregateByKey, barrier, cache, cartesian \n\n\nPair RDD\n\nPair데이터를 다루는 RDD (하나의 Row에 key-value 쌍으로 구성된 경우 등)\n\nPairRDD : key-value 쌍으로 구성된 RDD\n\nPair RDD API(InputRDD가 Pair가 아닌 경우 오류)\n\ncountByKey 키 별로 개수 세기\nlookup 단일 키로 값 찾기\nmapValues 키를 변경하지 않고 pairRDD값만 변경가능\n\nmap과 유사하지만, 키를 변경하지 않음에서 차이\n\nflatMapValues 각 키 값을 0~1개이상 값으로 매핑해 RDD에 포함한 요소 개수 변경\n\nflatmap 예시\n\n원본 : [“Hello world”, “Hello, World”]\n.split(” “) : [[“Hello”, “world”], [“Hello,”, “World”]]\nflatmap lambda x:x.split : [“Hello”, “world”, “Hello,”, “World”]\n\n\nreduceByKey 각 키의 모든 값을, 동일한 타입의 단일 값으로 병합\n\n두 값을 하나로 병합하는 merge 함수 필요 : lambda a, b:a+b\n하나의 값이 남을 때까지 merge 함수 호출\n\nfoldByKey : reduceByKey와 같은 기능 (+항등원인 zeroValue를 추가로 전달)\n\na, b:a+b,0 과 같이 입력해, 첫번째 value를 0으로 함\n\naggregateByKey : 타입을 바꾸고 싶은 경우 사용 (zeroValue를 받아 RDD값을 병합한다)\n\n타입 변환 함수 : (U, V) → U\n병합 함수 : (U, U) → U\n함수를 2개 전달하고, 항등원도 전달해 3개의 파라미터를 전달하는 복잡한 API임 \n\n\n\nGlobal Variance\n\nSpark는 Job을 Clustermanager가 각 노드(클러스터)에 나누어 보내줌\n파이썬에서 Global변수를 선언하더라도 노드 안에서만 작용함\n모든 클러스터에 대해서 작용할 수 있도록, BroadCast변수를 사용해야함\nbroadcast()로 한번 감싸주어야 클러스터 내에서 Global변수로 사용 가능 \n\nAccumulator\n\n변수 = sc.accumulator('변수이름')과 같은 형태로 사용\n예를 들어 여러 개의 노드가 있을 때, 성공하거나 실패한 횟수를 계산하고자 할때 노드 별로 출력되어 확인 어려움\n\naccumulator를 통해 각 노드별 상황을 취합해 하나의 값으로 확인할 수 있음\n\n코드 예시\n\n이해를 위한 예시이며, 실제로는 SparkUI에서 아래 예제와 같은 값은 확인가능하므로 print불필요\n\nac1 = sc.accumulator(\"ac1\")\n\n# executor (.add로 값을 더함)\nsc.range(0, 10).foreach(lambda v: ac1.add(v))\n\n# driver에서 print로 값 출력\nprint(\"Accumulator\", ac1.value)\n\n\nRDD 장점\n\nIn memory연산과 분산처리(parallel)로 효율적임\nImmutable(기존 변수를 바꾸지 않고 새로운 변수 리턴)하여 기존데이터를 바꾸지 않으므로 Consistency가 깨지지 않음\n\n함수형 코딩의 특징에 해당함\n\nmissing or damaged partition 등으로 인한 node fail 발생시 알아서 recompute하는 기능이 있음 \n\nRDD 사용할 때(어떤 경우에 사용)\n\n(low-level transformation와 action을 제공하므로)low-level 연산을 하고싶을 때\nUnstructured data를 사용할 때 (media streams, text stream, 실시간 데이터셋, image, sound 등)\n함수형 프로그래밍에 익숙한 경우(return value가 immutable)\n스키마가 불필요한 경우\n(거의 없지만) Optimization & perfomance benefit이 불필요한 경우\n\nDataFrame이나 Dataset은 훨씬 더 많은 최적화가 적용되어 있고 빠름 (table data는 DataFrame 등이 더 좋음) \n\n\n참고\n\n앞서 실습해본 printSchema()는 스키마와 컬럼을 가졌다는 것이므로, RDD는 대상이 아님\n회사에서는 이미지를 처리할 때 RDD를 많이 사용하기도 함(Unstructured data)\nfeature data도 RDD로 읽는게 더 편할 때도 있음 (실제 기업의 모든 데이터가 Strucctured data는 아님)"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240819/index.html#dataframe",
    "href": "posts/meta-de-spark_and_airflow-20240819/index.html#dataframe",
    "title": "[DE스터디/2주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "DataFrame",
    "text": "DataFrame\n\nDataFrame : 컬럼으로 organized된 데이터셋 (Spark이므로 distributed)\n\npandas보다 더 다양한 데이터 소스, 분산처리 가능\n\njson, parquet, hive,mysql 등\n\n공식문서(pyspark.sql.DataFrame, RDD와 분리된/SQL처럼 처리할 수 있는)\n\n링크 : https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n\nRDD에서 할 수 있는 것을 DataFrame에서도 대부분 처리 가능 (RDD는 컬럼기능은 불가함은 참고) \n\nRDD와 DataFrame의 Transform 방식의 차이 (RDD → DataFrame)\n\nmap(flatMap) → select\nObject → Row(columns)\nFunction (lambda base) → Column (SQL) \n\nDataFrame의 장점\n\nType-safety보장 : 스키마를 읽었기 때문에 컬럼별 타입 보장\nEasy to use : 컬럼이 많은 경우 유리 (high-level function, SQL처럼 쿼리하듯이 사용가능)\nFast and optimized : Spark의 optimization엔진인 catalyst엔진이 많은 역할을 함\n\n성능비교(Aggregateing 10 million int pairs [Seconds])\n\n[RDD인 경우] Python vs Scala : Python 9초 이상 / Scala 4초 이상으로 큰 차이\n[DataFrame인 경우] Python vs Scala : 모두 2초 이상으로 유의미한 차이가 보이지 않음\n\nDataframe에 optimization이 많이 되어있기 때문 \n\n\n\n\nDataFrame이 적합한 경우\n\nSchema가 있는 Structured data\nSQL쿼리처럼 High-level transformation이 필요한 경우\nType safety를 원하는 경우"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240819/index.html#dataset",
    "href": "posts/meta-de-spark_and_airflow-20240819/index.html#dataset",
    "title": "[DE스터디/2주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "Dataset",
    "text": "Dataset\n\nDataset : DataFrame과 유사하나 타입을 넣어줄 수 있는 Typed API (Dataset[T])\n\nDataFrame은 Row타입만 들어갈 수 있는 Dataset과 유사함 (Dataset[Row])\n\nDataset 장점 및 특징\n\nDataFrame대비 Optimization이 좀 더 반영되어있고, 조금 더 빠름\nSerialization과 Garbage collection에서 유리\nJava와 Scala만 지원\n\nTyped vs Untyped\n\nTyped (Java & Scala)\nUntyped (Python and R) : 대부분의 Dataset API의 이점이 DataFrame API에서도 가능함\n\nSpark SQL\n\n기존 SQL문처럼 사용가능\n\nps.sql(\"SELECT * FROM range(10) where id &gt; 7\")"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html",
    "href": "posts/meta-dl-creditcard-20240609/index.html",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 3주차 기록입니다.\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240609/index.html#개요",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "",
    "text": "참여중인 딥러닝 스터디 3주차 기록입니다."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-설명",
    "href": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-설명",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "2주차 과제 설명",
    "text": "2주차 과제 설명\n\ntf.random.set_seed(2024)\n\n서로 다른 모델을 비교하는 경우, 시작점이 다른 것 때문에 성능우위가 다르게 측정되는 경우 발생 (같은 성능임에도 다르게 나오거나, 좋은 모델이 더 나쁜 모델로 오인되는 경우)\n이러한 경우를 방지하고자, set_seed로 같은 지점에서 시작하게 할 수 있음\n딥러닝은 복잡한 다차원의 함수이므로, 좋은 시작점에 따라 달라질 수 있음 (좋은 Optimizer를 사용하고 좋은 데이터를 쓴다면 차이는 줄어들 수 있음)\n\nMatrix Multiplication\n\nA(1,2), B(2,1)와 같은 Matrix에서, A의 열(,2)과 B의 행(2,)의 숫자가 같아야 가능\n\nCost function에서 마이너스(-)를 붙이는 경우\n\n높을수록 안좋은 척도여야 할 때, 계산식이 높을수록 좋은 값인 경우 붙여서 변환\n\nLearning rate\n\nGD에서 안정적으로 최적점에 가게하기 위해 학습률을 조정\n사람이 지정하는 hyper parameter, 경험에 의해 넣는 경우가 많다(정답은 없음)\n\nConfusion Matrix\n\nRecall, Precision 으로 표현하는 것은 경영진 등에는 와닿지 않을 수 있으므로 시각화하여 보여주면 좋음"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-logistic-regression-코드-작성한-것",
    "href": "posts/meta-dl-creditcard-20240609/index.html#주차-과제-logistic-regression-코드-작성한-것",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "2주차 과제 Logistic regression 코드 작성한 것",
    "text": "2주차 과제 Logistic regression 코드 작성한 것\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# Train data\nX_train = np.array([[1., 1.],\n                   [1., 2.],\n                   [2., 1.],\n                   [3., 2.],\n                   [3., 3.],\n                   [2., 3.]],\n                   dtype=np.float32)\nY_train = np.array([[0.],\n                   [0.],\n                   [0.],\n                   [1.],\n                   [1.],\n                   [1.],],\n                   dtype=np.float32)\n\n# 회귀선 작성 전 분포확인\ncolors=['red' if L&gt;0.5 else 'blue' for L in Y_train]\nplt.scatter(X_train[:,0], X_train[:, 1], label='Logistics regression', color=colors)\n\n\n\nimage.png\n\n\n# 모델 학습\ntf.random.set_seed(2020)\nW = tf.Variable(tf.random.normal([2,1], mean=0.0))\nb = tf.Variable(tf.random.normal([1], mean=0.0))\n\ndef hypothesis(X):\n    z = tf.matmul(X, W) + b\n    sigmoid = 1 / (1 + tf.exp(-z))\n    return sigmoid\n\ndef cost_fn(H, Y):\n    cost = -tf.reduce_mean(Y*tf.math.log(H) + (1-Y)*tf.math.log(1-H))\n    return cost\n\nlearning_rate = 0.01\noptimizer = tf.optimizers.SGD(learning_rate)\n\nfor step in range(5001):\n    with tf.GradientTape() as g:\n        pred = hypothesis(X_train)\n        cost = cost_fn(pred, Y_train)\n\n        gradients = g.gradient(cost, [W,b])\n    \n    optimizer.apply_gradients(zip(gradients, [W, b]))\n\n    if step % 1000 == 0:\n        print(f'Step={step+1}, Cost = {cost}, W={W.numpy()}, b = {b.numpy()}')\n\nw_hat = W.numpy()\nb_hat = b.numpy()\n\nStep=1, Cost = 0.7932398319244385, W=[[-0.10415223] [0.68125504]], b = [0.3810195]  Step=1001, Cost = 0.5122759938240051, W=[[0.1809378] [0.55177015]], b = [-0.97815347]  Step=2001, Cost = 0.39883172512054443, W=[[0.5135696] [0.6884617]], b = [-1.9777462]  Step=3001, Cost = 0.32507583498954773, W=[[0.7515713] [0.8368167]], b = [-2.7877953]  Step=4001, Cost = 0.27400580048561096, W=[[0.9350327] [0.97824335]], b = [-3.4628296]  Step=5001, Cost = 0.2367737740278244, W=[[1.0848083] [1.1075894]], b = [-4.039375]\n\n# Slope(Coefficient) 확인\nslope = w_hat[0]/w_hat[1]\nxx = np.linspace(np.min(X_train[:,0]),np.max(X_train[:,0])) # min과 max 사이 구간의 숫자를 생성. x값\nyy = -slope*xx - b_hat/w_hat[1]                             # xx(x값)입력하여 y값 생성\n\n# train data분포\nplt.scatter(X_train[:, 0], X_train[:, 1], label='Logistics regression', color=colors)\n\n# 분류선(Decision Boundary)확인\nplt.plot(xx, yy, label='Decision Boundary')\nplt.legend()\n\n\n\nimage-2.png\n\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nX_test = np.array([[1., 0.],\n                   [0., 2.],\n                   [1., 1.],\n                   [3., 2.],\n                   [3., 3.],\n                   [2., 3.]],\n                   dtype=np.float32)\nY_test = np.array([[0.],\n                   [0.],\n                   [1.],\n                   [0.],\n                   [1.],\n                   [0.],],\n                   dtype=np.float32)\n\nY_actual = Y_test\nY_predicted = hypothesis(X_test)\nY_predicted_binary = np.where(Y_predicted &gt;= 0.5, 1, 0)\n\nCM_array = confusion_matrix(Y_actual, Y_predicted_binary, labels=[0, 1]) \nCM_array\n\narray([[2, 2], [1, 1]], dtype=int64)\n\n# Confusion matrix 시각화(seaborn)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass_labels = ['Negative', 'Positive']\nplt.figure(figsize=(8, 6))\nsns.heatmap(CM_array, annot=True, cmap='Blues',\n            xticklabels=class_labels, yticklabels=class_labels)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n\n\nimage-3.png\n\n\n# Confusion matrix 시각화(Scikit-learn)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=CM_array, display_labels=[0, 1])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\nimage-4.png"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240609/index.html#multi-class-classfication-regression",
    "href": "posts/meta-dl-creditcard-20240609/index.html#multi-class-classfication-regression",
    "title": "[M_Study_3주차] Multi-class Classification / Artificial Neural Network",
    "section": "Multi-class Classfication regression",
    "text": "Multi-class Classfication regression\n\n2가지 분류가 아닌 3가지 이상의 분류(A,B,C로 나누는 신용등급 등)\nBinary classification과 달리 하나의 Decision boundary로는 해결 불가\nOne vs All(Rest)\n\n하나의 대상과, 아닌 것’들’로 Binary Classification을 여러번 수행\n예를 들어 a, b, c 3가지를 분류하는 경우\n\n아래와 같은 같은 3개의 식으로 표현할 수 있고, \n아래와 같이 하나의 행렬로 한번에 표현할 수 있다  \n\nSoftmax : 각 결과값(\\(H_a, H_b, H_c\\))의 비율(확률)이 나오게 됨(총합이 1)\n\nCross entropy cost function\n\n정보량은 확률에 반비례한다고 정의 (정보량= $ 1 p$)\n\n특정 성씨의 사람을 뽑는다고 할 때, 한국의 주요 성씨인 김씨\\(1 \\over 10\\) vs 소수 성씨인 남궁씨\\(1 \\over 100\\)\n\n로그를 취하여 전개하면 Cross entropy 식이 된다\nEntropy : Measure for uncertainty (불확실성의 측정)  \nCase별 Cross entropy(cost function)\n\n출력값(결과값 softmax)과 실제값이 비슷한 경우 : 특정 값 산출\n출력값(결과값 softmax)과 실제값이 완전히 다른 경우 : 무한대\n출력값(결과값 softmax)과 실제값이 완전히 동일한 경우 : 0\n\nCross entropy를 개인이 직접구현한다면 놓칠 수 있는 부분(cross entropy의 무한대)의 문제\n\n파이토치 등 많은 사람이 참여한 패키지를 사용하면 방지할 수 있음\n컴퓨터에서의 Zero division error 등의 경우, 텐서플로우 등 패키지에선 분모에 0.00001등을 더하여 실제값엔 영향이 작게하며 오류 제거"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240622/index.html",
    "href": "posts/meta-dl-creditcard-20240622/index.html",
    "title": "[M_Study_4주차과제] CNN으로 MNIST다루기",
    "section": "",
    "text": "스터디 진행하며 진행한 과제 기록(MNIST, CNN)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240622/index.html#개요",
    "href": "posts/meta-dl-creditcard-20240622/index.html#개요",
    "title": "[M_Study_4주차과제] CNN으로 MNIST다루기",
    "section": "개요",
    "text": "개요\n참여중인 딥러닝 스터디 3주차 기록입니다.\n\nCNN로 MNIST다루기\n강사님이 주신 샘플코드 참고해서, 나에게 맞추거나 추가공부 진행"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240622/index.html#과제-작성-mnist-cnn",
    "href": "posts/meta-dl-creditcard-20240622/index.html#과제-작성-mnist-cnn",
    "title": "[M_Study_4주차과제] CNN으로 MNIST다루기",
    "section": "과제 작성 (MNIST CNN)",
    "text": "과제 작성 (MNIST CNN)\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport koreanize_matplotlib\n\n\n데이터 로드 및 Shape 확인\n\n(X_train, Y_train), (X_test, Y_test) = datasets.mnist.load_data()\nprint(X_train.shape, Y_train.shape, Y_train[0])\nprint(X_test.shape, Y_test.shape, Y_test[0])\n\n(60000, 28, 28) (60000,) 5\n(10000, 28, 28) (10000,) 7\n\n\n\nprint(f\"Min : {X_train[0].min()}, Max : {X_train[0].max()}\")\nplt.figure(figsize=(1,1))\nplt.imshow(X_train[0])\n\nMin : 0, Max : 255\n\n\n\n\n\n\n\n\n\n\n\n3D tensor로 변경 및 float32변환, Normalize\n\n# 3D Tensor\nX_train.reshape(60000,28,28,1)\nX_test.reshape(10000,28,28,1)\n\n# Float32변환\nX_train, X_test = np.array(X_train, np.float32), np.array(X_test, np.float32)\n\n# Normalize (0~255값을 0~1로)\nX_train, X_test = X_train / 255., X_test / 255.\n\n\n\nFeature Learning + Fully-connected layer\n\n### 특성 추출 (Feature Learning)\n\nmodel = Sequential()\nmodel.add(Input(shape=(28, 28, 1)))\n\n# Output계산식(Convolution layer) : (Input - Filter + 2 * Padding) / stride + 1\n\n# 3*3인 Filter(Kernel) 32개, 2번째줄 옵션은 미입력시 기본값 \n#  (Valid Padding은 Padding 미적용을 의미)\n# → Output(activation map=(26,26,32)) : [(28-3+2*0)/1 + 1] * 32(Filter 수)\n# → Weights : (3*3)size*32filter / biases : 32filter\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n                 strides=(1,1), padding='valid', kernel_initializer='glorot_uniform'))\n\n# 2*2인 최대값만 남기는 Pool, 2번째줄 옵션은 미입력시 기본값\n# → Output(activation map=(13,13,32)) : [26/2]\n# → Weights and biases : Pooling은 학습이 일어나지 않는다\nmodel.add(MaxPooling2D(pool_size=(2, 2),\n                       strides=None, padding='valid'))\n\n# 3*3인 Filter(Kernel) 64개\n# → Output(activation map=(11,11,64)) : [(13-3+2*0)/1 + 1] * 64(Filter 수)\n# → Weights : (3*3)size*64filter*32activation map / biases : 64filter\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n\n# 2*2인 최대값만 남기는 Pool\n# → Output(activation map=(5,5,64)) : [11/2]\n# → Weights and biases : Pooling은 학습이 일어나지 않는다\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3*3인 Filter(Kernel) 64개\n# → Output(activation map=(3,3,64)) : [(5-3+2*0)/1 + 1] * 64(Filter 수)\n# → Weights : (3*3)size*64filter*64activation map / biases : 64filter\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n\n\n### Fully connected Layer\n\n# flatten shape(576) : [3*3*64]\nmodel.add(Flatten())\n\n# 이전 Layer와 결합. Node=64\n# parameters(36928) : [576*64(weights)+64(biases)]\nmodel.add(Dense(64, activation='relu'))\n\n# 이전 Layer와 결합. Output = 10\n# parameters(650) : [64*10(weights)+10(biases)]\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                 │ (None, 26, 26, 32)     │           320 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 13, 13, 32)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 11, 11, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 5, 5, 64)       │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)               │ (None, 3, 3, 64)       │        36,928 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 576)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 64)             │        36,928 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 10)             │           650 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 93,322 (364.54 KB)\n\n\n\n Trainable params: 93,322 (364.54 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n\n모델 컴파일(Loss function, optimizer, metrics 설정)\n\n# Loss function은\n#  Label = number 인 경우 → sparse_categorical_crossentropy (one-hot이 아니므로 메모리소요가 적다)\n#  Label = one-hot 인 경우 → categorical_crossentropy\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n모델 학습\n\nhistory = model.fit(X_train, Y_train, epochs=5)\n\nEpoch 1/5\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 9s 4ms/step - accuracy: 0.9056 - loss: 0.3245\nEpoch 2/5\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9861 - loss: 0.0442\nEpoch 3/5\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 10s 5ms/step - accuracy: 0.9901 - loss: 0.0313\nEpoch 4/5\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 8s 4ms/step - accuracy: 0.9926 - loss: 0.0245\nEpoch 5/5\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 7s 4ms/step - accuracy: 0.9953 - loss: 0.0163\n\n\n\n\n학습횟수에 따른 Loss(Cost)시각화\n\n# 저장된 값 확인\nhistory.history\n\n{'accuracy': [0.9584500193595886,\n  0.9862833619117737,\n  0.9898499846458435,\n  0.9920166730880737,\n  0.9940166473388672],\n 'loss': [0.13941822946071625,\n  0.04484217241406441,\n  0.032598868012428284,\n  0.025325138121843338,\n  0.01914270594716072]}\n\n\n\n# Plot 시각화\nplt.plot(history.history['loss'], label='Train')\nplt.xlabel('The number of Learning')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n모델평가\n\ntest_result = model.evaluate(X_test, Y_test, verbose=2)\nprint()\nprint(test_result)\nprint(f\"Test Loss = {test_result[0]} / Test Accuracy = {test_result[1]}\")\n\n313/313 - 1s - 3ms/step - accuracy: 0.9909 - loss: 0.0343\n\n[0.03433739021420479, 0.9908999800682068]\nTest Loss = 0.03433739021420479 / Test Accuracy = 0.9908999800682068\n\n\n\n\n예측\n\npredictions = model.predict(X_test)\n\n313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n\n\n\nimport matplotlib as mpl\nmpl.rcParams['figure.max_open_warning'] = 10000\n\nfor i in range(len(predictions)):\n    if np.argmax(predictions[i]) != Y_test[i]:\n        plt.figure(figsize=(2,2))\n        plt.imshow(X_test[i])\n        plt.title(f\"예측{np.argmax(predictions[i])} / 정답{Y_test[i]}\")"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 3주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#금융데이터",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#금융데이터",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "금융데이터",
    "text": "금융데이터\n\n금융데이터의 특징\n\n정형성 : tabular데이터 형태로 적재, 최근 비정형 데이터 증가 추세\n\n별도의 스키마에 저장되어 있음\n\n대규모 : 일 단위 적재데이터의 단위가 페타 바이트에 육박\n\nTrino나 Presto등 대규모 처리 특화언어를 사용하게 됨\n\n민감성 : 금융 당국의 규제, 보안에 대한 높은 수준요구\n\n데이터별 접근권한, 모든 조회기록 로그 등\n\n폐쇄성 : 폐쇠망에서 처리되는 경우가 많음. AWS 등을 사용해도 Grey zone형태로 구축\n\n외부와 내부 폐쇄망 사이의 grey zone을 정보보호 및 IT인력이 관리\n\n불균형성 : Target(Y)의 정보가 불균형\n\n\n\n금융데이터 분야의 과제\n\n최근 채용공고 기반의 금융데이터 관련 과제\n\n고객 행동 분석 및 예측\n\n카드추천알고리즘, 이탈예측, 고객 클러스터링 등\n\n리스크 관리 및 신용평가\n\n대출 상환 가능성 예측, 이상거래 탐지 등\n\n데이터 기반 마케팅 최적화\n비정형 데이터 분석\n\n콜센터상담 피드백분석, SNS데이터 감성분석 등\n\n\n실무케이스 예시(회사별로 다름)\n\n협업Case : PM급 메인리더와 팀원의 협업\n\n리더가 데이터마트 설계, 팀원이 데이터마트 구축/완성\n모델링 작업, 앙상블 등의 베스트모델이나 기법을 각자 다른 방향으로 진행해봄\n\n개인작업Case\n\n퀀트같은 경우는 개인별로 시장 데이터 등을 분석하여 각자 전략구축"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#eda-이론",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#eda-이론",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "EDA 이론",
    "text": "EDA 이론\n\nEDA : 데이터가 어떻게 생겼는지, 통계적 수치의 확인 및 시각화 진행\n\n평균/중앙값\n최대/최소값\n사분위수\n왜도/첨도\n이상/결측치\n불균형도\n상관계수\n최빈값\n카테고리 개수, 카테고리별 개수\n\n데이터에 따른 시각화 기법\n\nBar plot : 범주형 변수\nHistogram : 연속형 변수 분포, 불균형도/왜도 분포, Y값의 분포\nLine plot : 시계열 데이터의 분포, 시계열성 확인\nBox plot : 이상치 확인, 일반적인 변수의 분포\nHeatmap : Y와의 상관계수, 다중공선성 확인\n\n다중공선성은, 회귀분석에서의 기본가정인 독립 변수는 서로 독립적이어야 한다는 가정을 지킬 수 있는지 등을 확인하여 분석결과의 신뢰도를 높이고자 확인"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#전처리-이론",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#전처리-이론",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "전처리 이론",
    "text": "전처리 이론\n\n전처리 종류\n\n결측/이상값 처리\n표준화(Standardization)\n정규화(Normalization)\n변환(Transformation)\n인코딩\n구간화(Binning)\n\n\n\n결측/이상값 처리\n\n결측이 많지않다면 삭제할 수도 있으나, 일반적으로는 대체를 활용\n\nMean/Median Imputation: 수치형 변수 대체시 주로 사용\n\n평균/중앙값 중 어느 것을 쓸지는 데이터의 분포를 보고 결정\n\nMode Imputation: 범주형 변수 대체시 주로 사용\nZero Imputation : 0으로 대체(또는 -999처럼 특정값 사용)\nInterpolation(보간법) : 시계열 데이터에서 전후 데이터를 활용\n\nKNN이나 ML활용도 가능하지만 잘 사용되지 않았으나, AutoML의 발전으로 활용성이 높아짐\n\ny를 예측하기 위해, x를 또 예측한다는 점에서 비효율적이었으나, AutoML 등의 발전으로 효율성이 높아짐\n\n\n\n이상값은 수가 적거나 예측에 해가 되는 경우가 많아 종종 삭제함\n\n유효성 검증이 되지 않아 생년월일 1800년대, 200살로 나오는 경우 등\n이상값이 유의미한 인사이트를 제공하는 경우도 있음\n\n이상값을 결측값으로 대체한 뒤, is_outlier같은 추가변수 만들어줌\n변환을 통해 값의 분산을 줄이기도 함\n\n\n\n\n\n표준화/정규화\n\n표준화 vs 정규화\n\n표준화 : 데이터를 평균 0, 표준편차 1의 정규분포로 수정\n정규화 : 0~1 또는 -1~1의 분포로 수정\n\n특성의 크기(Scale)의 차이로 인한 편향을 막기위해 사용\n\nAI학습에서 Loss를 줄일 때, 변수 간의 Scale이 다르면 loss 및 학습에 영향이 생겨 성능저하가 일어날 수 있음\n\n트리 기반의 알고리즘을 제외한 거의 대부분은 표준화/정규화 필요\n전통적으로 ML분야에서는 표준화를 선호하는 경향이 강함\n\nWhen in doubt, just standardize the data, it shouldn’t hurt\n\n\n\n\n변환\n\n로그 변환, 로트 변환, Box-cox변환 등이 있음\n\nBox-cox변환은, λ로 로그/제곱근 변환 등을 하나의 수식으로 표현가능한 ’일반화’된 변환 방법\n\n예를 들어, 왜도가 큰 경우 변환을 통해 낮추면 성능향상에 도움\n\n양의 왜도(오른쪽으로 치우짐)의 경우 변환을 통해 그 정도를 낮춤\n음의 왜도인 경우, 적당히 큰 값에서 데이터를 빼준 뒤 다시 변환\n\n단순 로그변환시 더 커지므로, 적당히 큰 값(Min/Max)을 빼줌\n\n데이터범위가 너무 넓은 경우, 루트 변환으로 분산을 줄여줌\n캐글의 타이타닉 문제에서, y의 로그변환만 해도 정확도가 3%정도 오르는 케이스도 있음\nML은 통계학의 변환이라는 관점에서, 통계학은 정규성을 가정하는 경우가 많아 적절한 변환은 성능에 도움되는 경우가 많음\n\n\n\n\n인코딩\n\n수치형이 아닌 변수를 수치로 변환\n\nOne-hot Encoding : 범주형 변수의 Unique값을 변수(컬럼)로 변환\n\nSparcity증가의 문제(데이터 Loss는 없지만 학습이 오래걸림)\n\nLabel Encoding : 범주형 변수의 값을 숫자로 변환\n\n상하관계가 없는 데이터를 있다고 착각할 수 있음(상하관계없는 데이터에 1~3 부여 후 3이 높다고 판단할 수 있음)\n\n\n\n\n\n구간화\n\n수치형 변수를 구간으로 나누고, 범주화\n장점 : 일반적으로 모델의 복잡도와, 이상값의 영향을 줄이는 효과\n단점 : 구간 경계의 값은 해석의 손해가 있음\n\n구간의 크기/개수 설정은 초모수(하이퍼파라미터)이므로, 성능에 영향이 크므로 자주 사용되지는 않음(도메인 지식 필요)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#기초통계-등-확인하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#기초통계-등-확인하기",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "기초통계 등 확인하기",
    "text": "기초통계 등 확인하기\n\n데이터/패키지 로딩 및 자료형 등 확인\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# pandas설정\npd.set_option('display.max_rows', 500)\n\n# 데이터 로딩\ndata = pd.read_csv('application_train.csv')\ndata.head(5)\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nNAME_CONTRACT_TYPE\nCODE_GENDER\nFLAG_OWN_CAR\nFLAG_OWN_REALTY\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\n100002\n1\nCash loans\nM\nN\nY\n0\n202500.0\n406597.5\n24700.5\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n100003\n0\nCash loans\nF\nN\nN\n0\n270000.0\n1293502.5\n35698.5\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n100004\n0\nRevolving loans\nM\nY\nY\n0\n67500.0\n135000.0\n6750.0\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n100006\n0\nCash loans\nF\nN\nY\n0\n135000.0\n312682.5\n29686.5\n...\n0\n0\n0\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n100007\n0\nCash loans\nM\nN\nY\n0\n121500.0\n513000.0\n21865.5\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 122 columns\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 307511 entries, 0 to 307510\nColumns: 122 entries, SK_ID_CURR to AMT_REQ_CREDIT_BUREAU_YEAR\ndtypes: float64(65), int64(41), object(16)\nmemory usage: 286.2+ MB\n\n\n\n\n통계적 수치 확인\n\n기초 통계량 확인\n\npandas의 describe()\n\ncount : 결측치가 아닌 값의 개수\nunique : 범주형 변수에 대해, unique한 값의 개수\ntop : 범주형 변수에 대해, 가장 많은 값\nfreq : 범주형 변수에 대해, 가장 많은 값의 개수\nmean : 평균\nstd : 표준편차\nmin / 25% / 50% (median) / 75% / max : 최소값 / Q1 / 중앙값 / Q3 / 최대값\n\n아래의 코드(describe)를 통해 다음과 같은 점을 생각해 볼 수 있음\n\nUnique값이 있다는 것 → 범주형 변수라는 것\n대부분 컬럼이 정규화된 것으로 추정\n\nMin 0 / Max 1인 컬럼이 많음\n평균과 표준편차가 작은 편인 컬럼이 많음\n\n대부분 컬럼이 편향이 많이 있어보임\n\n예를 들어 NAME_CONTRACT_TYPE컬럼의 경우, unique 2 / count 307511 / frep 278232\n\n(향후 로그변환을 위해)변수가 음수인지 양수인지 봐두기\n\n\n\ndata.NAME_CONTRACT_TYPE.value_counts()\n\nNAME_CONTRACT_TYPE\nCash loans         278232\nRevolving loans     29279\nName: count, dtype: int64\n\n\n\nbasic_statistics = data.describe(include='all').transpose().reset_index()\nbasic_statistics.sort_values('unique')\n\n\n\n\n\n\n\n\nindex\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n2\nNAME_CONTRACT_TYPE\n307511\n2\nCash loans\n278232\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nFLAG_OWN_CAR\n307511\n2\nN\n202924\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\nFLAG_OWN_REALTY\n307511\n2\nY\n213312\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n90\nEMERGENCYSTATE_MODE\n161756\n2\nNo\n159428\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nCODE_GENDER\n307511\n3\nF\n202448\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n87\nHOUSETYPE_MODE\n153214\n3\nblock of flats\n150503\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n86\nFONDKAPREMONT_MODE\n97216\n4\nreg oper account\n73830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n13\nNAME_EDUCATION_TYPE\n307511\n5\nSecondary / secondary special\n218391\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n14\nNAME_FAMILY_STATUS\n307511\n6\nMarried\n196432\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n15\nNAME_HOUSING_TYPE\n307511\n6\nHouse / apartment\n272868\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11\nNAME_TYPE_SUITE\n306219\n7\nUnaccompanied\n248526\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n32\nWEEKDAY_APPR_PROCESS_START\n307511\n7\nTUESDAY\n53901\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n89\nWALLSMATERIAL_MODE\n151170\n7\nPanel\n66040\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n12\nNAME_INCOME_TYPE\n307511\n8\nWorking\n158774\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n28\nOCCUPATION_TYPE\n211120\n18\nLaborers\n55186\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n40\nORGANIZATION_TYPE\n307511\n58\nBusiness Entity Type 3\n67992\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n0\nSK_ID_CURR\n307511.0\nNaN\nNaN\nNaN\n278180.518577\n102790.175348\n100002.0\n189145.5\n278202.0\n367142.5\n456255.0\n\n\n1\nTARGET\n307511.0\nNaN\nNaN\nNaN\n0.080729\n0.272419\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n6\nCNT_CHILDREN\n307511.0\nNaN\nNaN\nNaN\n0.417052\n0.722121\n0.0\n0.0\n0.0\n1.0\n19.0\n\n\n7\nAMT_INCOME_TOTAL\n307511.0\nNaN\nNaN\nNaN\n168797.919297\n237123.146279\n25650.0\n112500.0\n147150.0\n202500.0\n117000000.0\n\n\n8\nAMT_CREDIT\n307511.0\nNaN\nNaN\nNaN\n599025.999706\n402490.776996\n45000.0\n270000.0\n513531.0\n808650.0\n4050000.0\n\n\n9\nAMT_ANNUITY\n307499.0\nNaN\nNaN\nNaN\n27108.573909\n14493.737315\n1615.5\n16524.0\n24903.0\n34596.0\n258025.5\n\n\n10\nAMT_GOODS_PRICE\n307233.0\nNaN\nNaN\nNaN\n538396.207429\n369446.46054\n40500.0\n238500.0\n450000.0\n679500.0\n4050000.0\n\n\n16\nREGION_POPULATION_RELATIVE\n307511.0\nNaN\nNaN\nNaN\n0.020868\n0.013831\n0.00029\n0.010006\n0.01885\n0.028663\n0.072508\n\n\n17\nDAYS_BIRTH\n307511.0\nNaN\nNaN\nNaN\n-16036.995067\n4363.988632\n-25229.0\n-19682.0\n-15750.0\n-12413.0\n-7489.0\n\n\n18\nDAYS_EMPLOYED\n307511.0\nNaN\nNaN\nNaN\n63815.045904\n141275.766519\n-17912.0\n-2760.0\n-1213.0\n-289.0\n365243.0\n\n\n19\nDAYS_REGISTRATION\n307511.0\nNaN\nNaN\nNaN\n-4986.120328\n3522.886321\n-24672.0\n-7479.5\n-4504.0\n-2010.0\n0.0\n\n\n20\nDAYS_ID_PUBLISH\n307511.0\nNaN\nNaN\nNaN\n-2994.202373\n1509.450419\n-7197.0\n-4299.0\n-3254.0\n-1720.0\n0.0\n\n\n21\nOWN_CAR_AGE\n104582.0\nNaN\nNaN\nNaN\n12.061091\n11.944812\n0.0\n5.0\n9.0\n15.0\n91.0\n\n\n22\nFLAG_MOBIL\n307511.0\nNaN\nNaN\nNaN\n0.999997\n0.001803\n0.0\n1.0\n1.0\n1.0\n1.0\n\n\n23\nFLAG_EMP_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.819889\n0.38428\n0.0\n1.0\n1.0\n1.0\n1.0\n\n\n24\nFLAG_WORK_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.199368\n0.399526\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n25\nFLAG_CONT_MOBILE\n307511.0\nNaN\nNaN\nNaN\n0.998133\n0.043164\n0.0\n1.0\n1.0\n1.0\n1.0\n\n\n26\nFLAG_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.281066\n0.449521\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n27\nFLAG_EMAIL\n307511.0\nNaN\nNaN\nNaN\n0.05672\n0.231307\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n29\nCNT_FAM_MEMBERS\n307509.0\nNaN\nNaN\nNaN\n2.152665\n0.910682\n1.0\n2.0\n2.0\n3.0\n20.0\n\n\n30\nREGION_RATING_CLIENT\n307511.0\nNaN\nNaN\nNaN\n2.052463\n0.509034\n1.0\n2.0\n2.0\n2.0\n3.0\n\n\n31\nREGION_RATING_CLIENT_W_CITY\n307511.0\nNaN\nNaN\nNaN\n2.031521\n0.502737\n1.0\n2.0\n2.0\n2.0\n3.0\n\n\n33\nHOUR_APPR_PROCESS_START\n307511.0\nNaN\nNaN\nNaN\n12.063419\n3.265832\n0.0\n10.0\n12.0\n14.0\n23.0\n\n\n34\nREG_REGION_NOT_LIVE_REGION\n307511.0\nNaN\nNaN\nNaN\n0.015144\n0.122126\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n35\nREG_REGION_NOT_WORK_REGION\n307511.0\nNaN\nNaN\nNaN\n0.050769\n0.219526\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n36\nLIVE_REGION_NOT_WORK_REGION\n307511.0\nNaN\nNaN\nNaN\n0.040659\n0.197499\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n37\nREG_CITY_NOT_LIVE_CITY\n307511.0\nNaN\nNaN\nNaN\n0.078173\n0.268444\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n38\nREG_CITY_NOT_WORK_CITY\n307511.0\nNaN\nNaN\nNaN\n0.230454\n0.421124\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n39\nLIVE_CITY_NOT_WORK_CITY\n307511.0\nNaN\nNaN\nNaN\n0.179555\n0.383817\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n41\nEXT_SOURCE_1\n134133.0\nNaN\nNaN\nNaN\n0.50213\n0.211062\n0.014568\n0.334007\n0.505998\n0.675053\n0.962693\n\n\n42\nEXT_SOURCE_2\n306851.0\nNaN\nNaN\nNaN\n0.514393\n0.19106\n0.0\n0.392457\n0.565961\n0.663617\n0.855\n\n\n43\nEXT_SOURCE_3\n246546.0\nNaN\nNaN\nNaN\n0.510853\n0.194844\n0.000527\n0.37065\n0.535276\n0.669057\n0.89601\n\n\n44\nAPARTMENTS_AVG\n151450.0\nNaN\nNaN\nNaN\n0.11744\n0.10824\n0.0\n0.0577\n0.0876\n0.1485\n1.0\n\n\n45\nBASEMENTAREA_AVG\n127568.0\nNaN\nNaN\nNaN\n0.088442\n0.082438\n0.0\n0.0442\n0.0763\n0.1122\n1.0\n\n\n46\nYEARS_BEGINEXPLUATATION_AVG\n157504.0\nNaN\nNaN\nNaN\n0.977735\n0.059223\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n\n\n47\nYEARS_BUILD_AVG\n103023.0\nNaN\nNaN\nNaN\n0.752471\n0.11328\n0.0\n0.6872\n0.7552\n0.8232\n1.0\n\n\n48\nCOMMONAREA_AVG\n92646.0\nNaN\nNaN\nNaN\n0.044621\n0.076036\n0.0\n0.0078\n0.0211\n0.0515\n1.0\n\n\n49\nELEVATORS_AVG\n143620.0\nNaN\nNaN\nNaN\n0.078942\n0.134576\n0.0\n0.0\n0.0\n0.12\n1.0\n\n\n50\nENTRANCES_AVG\n152683.0\nNaN\nNaN\nNaN\n0.149725\n0.100049\n0.0\n0.069\n0.1379\n0.2069\n1.0\n\n\n51\nFLOORSMAX_AVG\n154491.0\nNaN\nNaN\nNaN\n0.226282\n0.144641\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n\n\n52\nFLOORSMIN_AVG\n98869.0\nNaN\nNaN\nNaN\n0.231894\n0.16138\n0.0\n0.0833\n0.2083\n0.375\n1.0\n\n\n53\nLANDAREA_AVG\n124921.0\nNaN\nNaN\nNaN\n0.066333\n0.081184\n0.0\n0.0187\n0.0481\n0.0856\n1.0\n\n\n54\nLIVINGAPARTMENTS_AVG\n97312.0\nNaN\nNaN\nNaN\n0.100775\n0.092576\n0.0\n0.0504\n0.0756\n0.121\n1.0\n\n\n55\nLIVINGAREA_AVG\n153161.0\nNaN\nNaN\nNaN\n0.107399\n0.110565\n0.0\n0.0453\n0.0745\n0.1299\n1.0\n\n\n56\nNONLIVINGAPARTMENTS_AVG\n93997.0\nNaN\nNaN\nNaN\n0.008809\n0.047732\n0.0\n0.0\n0.0\n0.0039\n1.0\n\n\n57\nNONLIVINGAREA_AVG\n137829.0\nNaN\nNaN\nNaN\n0.028358\n0.069523\n0.0\n0.0\n0.0036\n0.0277\n1.0\n\n\n58\nAPARTMENTS_MODE\n151450.0\nNaN\nNaN\nNaN\n0.114231\n0.107936\n0.0\n0.0525\n0.084\n0.1439\n1.0\n\n\n59\nBASEMENTAREA_MODE\n127568.0\nNaN\nNaN\nNaN\n0.087543\n0.084307\n0.0\n0.0407\n0.0746\n0.1124\n1.0\n\n\n60\nYEARS_BEGINEXPLUATATION_MODE\n157504.0\nNaN\nNaN\nNaN\n0.977065\n0.064575\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n\n\n61\nYEARS_BUILD_MODE\n103023.0\nNaN\nNaN\nNaN\n0.759637\n0.110111\n0.0\n0.6994\n0.7648\n0.8236\n1.0\n\n\n62\nCOMMONAREA_MODE\n92646.0\nNaN\nNaN\nNaN\n0.042553\n0.074445\n0.0\n0.0072\n0.019\n0.049\n1.0\n\n\n63\nELEVATORS_MODE\n143620.0\nNaN\nNaN\nNaN\n0.07449\n0.132256\n0.0\n0.0\n0.0\n0.1208\n1.0\n\n\n64\nENTRANCES_MODE\n152683.0\nNaN\nNaN\nNaN\n0.145193\n0.100977\n0.0\n0.069\n0.1379\n0.2069\n1.0\n\n\n65\nFLOORSMAX_MODE\n154491.0\nNaN\nNaN\nNaN\n0.222315\n0.143709\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n\n\n66\nFLOORSMIN_MODE\n98869.0\nNaN\nNaN\nNaN\n0.228058\n0.16116\n0.0\n0.0833\n0.2083\n0.375\n1.0\n\n\n67\nLANDAREA_MODE\n124921.0\nNaN\nNaN\nNaN\n0.064958\n0.08175\n0.0\n0.0166\n0.0458\n0.0841\n1.0\n\n\n68\nLIVINGAPARTMENTS_MODE\n97312.0\nNaN\nNaN\nNaN\n0.105645\n0.09788\n0.0\n0.0542\n0.0771\n0.1313\n1.0\n\n\n69\nLIVINGAREA_MODE\n153161.0\nNaN\nNaN\nNaN\n0.105975\n0.111845\n0.0\n0.0427\n0.0731\n0.1252\n1.0\n\n\n70\nNONLIVINGAPARTMENTS_MODE\n93997.0\nNaN\nNaN\nNaN\n0.008076\n0.046276\n0.0\n0.0\n0.0\n0.0039\n1.0\n\n\n71\nNONLIVINGAREA_MODE\n137829.0\nNaN\nNaN\nNaN\n0.027022\n0.070254\n0.0\n0.0\n0.0011\n0.0231\n1.0\n\n\n72\nAPARTMENTS_MEDI\n151450.0\nNaN\nNaN\nNaN\n0.11785\n0.109076\n0.0\n0.0583\n0.0864\n0.1489\n1.0\n\n\n73\nBASEMENTAREA_MEDI\n127568.0\nNaN\nNaN\nNaN\n0.087955\n0.082179\n0.0\n0.0437\n0.0758\n0.1116\n1.0\n\n\n74\nYEARS_BEGINEXPLUATATION_MEDI\n157504.0\nNaN\nNaN\nNaN\n0.977752\n0.059897\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n\n\n75\nYEARS_BUILD_MEDI\n103023.0\nNaN\nNaN\nNaN\n0.755746\n0.112066\n0.0\n0.6914\n0.7585\n0.8256\n1.0\n\n\n76\nCOMMONAREA_MEDI\n92646.0\nNaN\nNaN\nNaN\n0.044595\n0.076144\n0.0\n0.0079\n0.0208\n0.0513\n1.0\n\n\n77\nELEVATORS_MEDI\n143620.0\nNaN\nNaN\nNaN\n0.078078\n0.134467\n0.0\n0.0\n0.0\n0.12\n1.0\n\n\n78\nENTRANCES_MEDI\n152683.0\nNaN\nNaN\nNaN\n0.149213\n0.100368\n0.0\n0.069\n0.1379\n0.2069\n1.0\n\n\n79\nFLOORSMAX_MEDI\n154491.0\nNaN\nNaN\nNaN\n0.225897\n0.145067\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n\n\n80\nFLOORSMIN_MEDI\n98869.0\nNaN\nNaN\nNaN\n0.231625\n0.161934\n0.0\n0.0833\n0.2083\n0.375\n1.0\n\n\n81\nLANDAREA_MEDI\n124921.0\nNaN\nNaN\nNaN\n0.067169\n0.082167\n0.0\n0.0187\n0.0487\n0.0868\n1.0\n\n\n82\nLIVINGAPARTMENTS_MEDI\n97312.0\nNaN\nNaN\nNaN\n0.101954\n0.093642\n0.0\n0.0513\n0.0761\n0.1231\n1.0\n\n\n83\nLIVINGAREA_MEDI\n153161.0\nNaN\nNaN\nNaN\n0.108607\n0.11226\n0.0\n0.0457\n0.0749\n0.1303\n1.0\n\n\n84\nNONLIVINGAPARTMENTS_MEDI\n93997.0\nNaN\nNaN\nNaN\n0.008651\n0.047415\n0.0\n0.0\n0.0\n0.0039\n1.0\n\n\n85\nNONLIVINGAREA_MEDI\n137829.0\nNaN\nNaN\nNaN\n0.028236\n0.070166\n0.0\n0.0\n0.0031\n0.0266\n1.0\n\n\n88\nTOTALAREA_MODE\n159080.0\nNaN\nNaN\nNaN\n0.102547\n0.107462\n0.0\n0.0412\n0.0688\n0.1276\n1.0\n\n\n91\nOBS_30_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n1.422245\n2.400989\n0.0\n0.0\n0.0\n2.0\n348.0\n\n\n92\nDEF_30_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n0.143421\n0.446698\n0.0\n0.0\n0.0\n0.0\n34.0\n\n\n93\nOBS_60_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n1.405292\n2.379803\n0.0\n0.0\n0.0\n2.0\n344.0\n\n\n94\nDEF_60_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n0.100049\n0.362291\n0.0\n0.0\n0.0\n0.0\n24.0\n\n\n95\nDAYS_LAST_PHONE_CHANGE\n307510.0\nNaN\nNaN\nNaN\n-962.858788\n826.808487\n-4292.0\n-1570.0\n-757.0\n-274.0\n0.0\n\n\n96\nFLAG_DOCUMENT_2\n307511.0\nNaN\nNaN\nNaN\n0.000042\n0.006502\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n97\nFLAG_DOCUMENT_3\n307511.0\nNaN\nNaN\nNaN\n0.710023\n0.453752\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\n98\nFLAG_DOCUMENT_4\n307511.0\nNaN\nNaN\nNaN\n0.000081\n0.009016\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n99\nFLAG_DOCUMENT_5\n307511.0\nNaN\nNaN\nNaN\n0.015115\n0.12201\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n100\nFLAG_DOCUMENT_6\n307511.0\nNaN\nNaN\nNaN\n0.088055\n0.283376\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n101\nFLAG_DOCUMENT_7\n307511.0\nNaN\nNaN\nNaN\n0.000192\n0.01385\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n102\nFLAG_DOCUMENT_8\n307511.0\nNaN\nNaN\nNaN\n0.081376\n0.273412\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n103\nFLAG_DOCUMENT_9\n307511.0\nNaN\nNaN\nNaN\n0.003896\n0.062295\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n104\nFLAG_DOCUMENT_10\n307511.0\nNaN\nNaN\nNaN\n0.000023\n0.004771\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n105\nFLAG_DOCUMENT_11\n307511.0\nNaN\nNaN\nNaN\n0.003912\n0.062424\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n106\nFLAG_DOCUMENT_12\n307511.0\nNaN\nNaN\nNaN\n0.000007\n0.00255\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n107\nFLAG_DOCUMENT_13\n307511.0\nNaN\nNaN\nNaN\n0.003525\n0.059268\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n108\nFLAG_DOCUMENT_14\n307511.0\nNaN\nNaN\nNaN\n0.002936\n0.05411\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n109\nFLAG_DOCUMENT_15\n307511.0\nNaN\nNaN\nNaN\n0.00121\n0.03476\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n110\nFLAG_DOCUMENT_16\n307511.0\nNaN\nNaN\nNaN\n0.009928\n0.099144\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n111\nFLAG_DOCUMENT_17\n307511.0\nNaN\nNaN\nNaN\n0.000267\n0.016327\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n112\nFLAG_DOCUMENT_18\n307511.0\nNaN\nNaN\nNaN\n0.00813\n0.089798\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n113\nFLAG_DOCUMENT_19\n307511.0\nNaN\nNaN\nNaN\n0.000595\n0.024387\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n114\nFLAG_DOCUMENT_20\n307511.0\nNaN\nNaN\nNaN\n0.000507\n0.022518\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n115\nFLAG_DOCUMENT_21\n307511.0\nNaN\nNaN\nNaN\n0.000335\n0.018299\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n116\nAMT_REQ_CREDIT_BUREAU_HOUR\n265992.0\nNaN\nNaN\nNaN\n0.006402\n0.083849\n0.0\n0.0\n0.0\n0.0\n4.0\n\n\n117\nAMT_REQ_CREDIT_BUREAU_DAY\n265992.0\nNaN\nNaN\nNaN\n0.007\n0.110757\n0.0\n0.0\n0.0\n0.0\n9.0\n\n\n118\nAMT_REQ_CREDIT_BUREAU_WEEK\n265992.0\nNaN\nNaN\nNaN\n0.034362\n0.204685\n0.0\n0.0\n0.0\n0.0\n8.0\n\n\n119\nAMT_REQ_CREDIT_BUREAU_MON\n265992.0\nNaN\nNaN\nNaN\n0.267395\n0.916002\n0.0\n0.0\n0.0\n0.0\n27.0\n\n\n120\nAMT_REQ_CREDIT_BUREAU_QRT\n265992.0\nNaN\nNaN\nNaN\n0.265474\n0.794056\n0.0\n0.0\n0.0\n0.0\n261.0\n\n\n121\nAMT_REQ_CREDIT_BUREAU_YEAR\n265992.0\nNaN\nNaN\nNaN\n1.899974\n1.869295\n0.0\n0.0\n1.0\n3.0\n25.0\n\n\n\n\n\n\n\n\n\n왜도 확인\n\n수치형 변수만, skew함수로 왜도를 구할 수 있음\n음의 왜도 확인하기 + 높은 양의 왜도 확인하기(10이 넘는 경우 왜도가 높은 편)\n아래 코드의 결과를 기준으로, 왜도가 심하게 높은 주요 변수 → 이상치 여부를 시각화로 확인 필요\n\nAMT_INCOME_TOTAL : 391.559654\nYEARS_BEGINEXPLUATATION_MEDI : -15.573124\nFLAG_CONT_MOBILE : -23.081172\nFLAG_MOBIL : -554.536744\n\n\n\nnumerical_data = data.select_dtypes(include=['float64', 'int64'])\nnumeric_skew = numerical_data.skew().reset_index() # skew() 함수로 왜도 구하기\nnumeric_skew.columns = ['index','skewness']\n\next_statistics = pd.merge(basic_statistics, numeric_skew, on='index', how='left')\next_statistics.sort_values(by='skewness', ascending=False)\n\n\n\n\n\n\n\n\nindex\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\nskewness\n\n\n\n\n106\nFLAG_DOCUMENT_12\n307511.0\nNaN\nNaN\nNaN\n0.000007\n0.00255\n0.0\n0.0\n0.0\n0.0\n1.0\n392.114779\n\n\n7\nAMT_INCOME_TOTAL\n307511.0\nNaN\nNaN\nNaN\n168797.919297\n237123.146279\n25650.0\n112500.0\n147150.0\n202500.0\n117000000.0\n391.559654\n\n\n104\nFLAG_DOCUMENT_10\n307511.0\nNaN\nNaN\nNaN\n0.000023\n0.004771\n0.0\n0.0\n0.0\n0.0\n1.0\n209.589054\n\n\n96\nFLAG_DOCUMENT_2\n307511.0\nNaN\nNaN\nNaN\n0.000042\n0.006502\n0.0\n0.0\n0.0\n0.0\n1.0\n153.791817\n\n\n120\nAMT_REQ_CREDIT_BUREAU_QRT\n265992.0\nNaN\nNaN\nNaN\n0.265474\n0.794056\n0.0\n0.0\n0.0\n0.0\n261.0\n134.365776\n\n\n98\nFLAG_DOCUMENT_4\n307511.0\nNaN\nNaN\nNaN\n0.000081\n0.009016\n0.0\n0.0\n0.0\n0.0\n1.0\n110.894364\n\n\n101\nFLAG_DOCUMENT_7\n307511.0\nNaN\nNaN\nNaN\n0.000192\n0.01385\n0.0\n0.0\n0.0\n0.0\n1.0\n72.174108\n\n\n111\nFLAG_DOCUMENT_17\n307511.0\nNaN\nNaN\nNaN\n0.000267\n0.016327\n0.0\n0.0\n0.0\n0.0\n1.0\n61.214140\n\n\n115\nFLAG_DOCUMENT_21\n307511.0\nNaN\nNaN\nNaN\n0.000335\n0.018299\n0.0\n0.0\n0.0\n0.0\n1.0\n54.612939\n\n\n114\nFLAG_DOCUMENT_20\n307511.0\nNaN\nNaN\nNaN\n0.000507\n0.022518\n0.0\n0.0\n0.0\n0.0\n1.0\n44.364897\n\n\n113\nFLAG_DOCUMENT_19\n307511.0\nNaN\nNaN\nNaN\n0.000595\n0.024387\n0.0\n0.0\n0.0\n0.0\n1.0\n40.956134\n\n\n109\nFLAG_DOCUMENT_15\n307511.0\nNaN\nNaN\nNaN\n0.00121\n0.03476\n0.0\n0.0\n0.0\n0.0\n1.0\n28.699333\n\n\n117\nAMT_REQ_CREDIT_BUREAU_DAY\n265992.0\nNaN\nNaN\nNaN\n0.007\n0.110757\n0.0\n0.0\n0.0\n0.0\n9.0\n27.043505\n\n\n108\nFLAG_DOCUMENT_14\n307511.0\nNaN\nNaN\nNaN\n0.002936\n0.05411\n0.0\n0.0\n0.0\n0.0\n1.0\n18.372533\n\n\n107\nFLAG_DOCUMENT_13\n307511.0\nNaN\nNaN\nNaN\n0.003525\n0.059268\n0.0\n0.0\n0.0\n0.0\n1.0\n16.753746\n\n\n70\nNONLIVINGAPARTMENTS_MODE\n93997.0\nNaN\nNaN\nNaN\n0.008076\n0.046276\n0.0\n0.0\n0.0\n0.0039\n1.0\n16.251819\n\n\n103\nFLAG_DOCUMENT_9\n307511.0\nNaN\nNaN\nNaN\n0.003896\n0.062295\n0.0\n0.0\n0.0\n0.0\n1.0\n15.927755\n\n\n105\nFLAG_DOCUMENT_11\n307511.0\nNaN\nNaN\nNaN\n0.003912\n0.062424\n0.0\n0.0\n0.0\n0.0\n1.0\n15.894229\n\n\n84\nNONLIVINGAPARTMENTS_MEDI\n93997.0\nNaN\nNaN\nNaN\n0.008651\n0.047415\n0.0\n0.0\n0.0\n0.0039\n1.0\n15.671995\n\n\n56\nNONLIVINGAPARTMENTS_AVG\n93997.0\nNaN\nNaN\nNaN\n0.008809\n0.047732\n0.0\n0.0\n0.0\n0.0039\n1.0\n15.541185\n\n\n116\nAMT_REQ_CREDIT_BUREAU_HOUR\n265992.0\nNaN\nNaN\nNaN\n0.006402\n0.083849\n0.0\n0.0\n0.0\n0.0\n4.0\n14.534062\n\n\n91\nOBS_30_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n1.422245\n2.400989\n0.0\n0.0\n0.0\n2.0\n348.0\n12.139598\n\n\n93\nOBS_60_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n1.405292\n2.379803\n0.0\n0.0\n0.0\n2.0\n344.0\n12.070829\n\n\n112\nFLAG_DOCUMENT_18\n307511.0\nNaN\nNaN\nNaN\n0.00813\n0.089798\n0.0\n0.0\n0.0\n0.0\n1.0\n10.955080\n\n\n110\nFLAG_DOCUMENT_16\n307511.0\nNaN\nNaN\nNaN\n0.009928\n0.099144\n0.0\n0.0\n0.0\n0.0\n1.0\n9.886111\n\n\n118\nAMT_REQ_CREDIT_BUREAU_WEEK\n265992.0\nNaN\nNaN\nNaN\n0.034362\n0.204685\n0.0\n0.0\n0.0\n0.0\n8.0\n9.293573\n\n\n99\nFLAG_DOCUMENT_5\n307511.0\nNaN\nNaN\nNaN\n0.015115\n0.12201\n0.0\n0.0\n0.0\n0.0\n1.0\n7.948322\n\n\n34\nREG_REGION_NOT_LIVE_REGION\n307511.0\nNaN\nNaN\nNaN\n0.015144\n0.122126\n0.0\n0.0\n0.0\n0.0\n1.0\n7.940276\n\n\n119\nAMT_REQ_CREDIT_BUREAU_MON\n265992.0\nNaN\nNaN\nNaN\n0.267395\n0.916002\n0.0\n0.0\n0.0\n0.0\n27.0\n7.804848\n\n\n57\nNONLIVINGAREA_AVG\n137829.0\nNaN\nNaN\nNaN\n0.028358\n0.069523\n0.0\n0.0\n0.0036\n0.0277\n1.0\n6.559012\n\n\n71\nNONLIVINGAREA_MODE\n137829.0\nNaN\nNaN\nNaN\n0.027022\n0.070254\n0.0\n0.0\n0.0011\n0.0231\n1.0\n6.522451\n\n\n85\nNONLIVINGAREA_MEDI\n137829.0\nNaN\nNaN\nNaN\n0.028236\n0.070166\n0.0\n0.0\n0.0031\n0.0266\n1.0\n6.508831\n\n\n62\nCOMMONAREA_MODE\n92646.0\nNaN\nNaN\nNaN\n0.042553\n0.074445\n0.0\n0.0072\n0.019\n0.049\n1.0\n5.620589\n\n\n48\nCOMMONAREA_AVG\n92646.0\nNaN\nNaN\nNaN\n0.044621\n0.076036\n0.0\n0.0078\n0.0211\n0.0515\n1.0\n5.457305\n\n\n76\nCOMMONAREA_MEDI\n92646.0\nNaN\nNaN\nNaN\n0.044595\n0.076144\n0.0\n0.0079\n0.0208\n0.0513\n1.0\n5.419238\n\n\n94\nDEF_60_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n0.100049\n0.362291\n0.0\n0.0\n0.0\n0.0\n24.0\n5.277878\n\n\n92\nDEF_30_CNT_SOCIAL_CIRCLE\n306490.0\nNaN\nNaN\nNaN\n0.143421\n0.446698\n0.0\n0.0\n0.0\n0.0\n34.0\n5.183518\n\n\n36\nLIVE_REGION_NOT_WORK_REGION\n307511.0\nNaN\nNaN\nNaN\n0.040659\n0.197499\n0.0\n0.0\n0.0\n0.0\n1.0\n4.651620\n\n\n53\nLANDAREA_AVG\n124921.0\nNaN\nNaN\nNaN\n0.066333\n0.081184\n0.0\n0.0187\n0.0481\n0.0856\n1.0\n4.458677\n\n\n67\nLANDAREA_MODE\n124921.0\nNaN\nNaN\nNaN\n0.064958\n0.08175\n0.0\n0.0166\n0.0458\n0.0841\n1.0\n4.377027\n\n\n81\nLANDAREA_MEDI\n124921.0\nNaN\nNaN\nNaN\n0.067169\n0.082167\n0.0\n0.0187\n0.0487\n0.0868\n1.0\n4.368292\n\n\n35\nREG_REGION_NOT_WORK_REGION\n307511.0\nNaN\nNaN\nNaN\n0.050769\n0.219526\n0.0\n0.0\n0.0\n0.0\n1.0\n4.092767\n\n\n27\nFLAG_EMAIL\n307511.0\nNaN\nNaN\nNaN\n0.05672\n0.231307\n0.0\n0.0\n0.0\n0.0\n1.0\n3.832853\n\n\n45\nBASEMENTAREA_AVG\n127568.0\nNaN\nNaN\nNaN\n0.088442\n0.082438\n0.0\n0.0442\n0.0763\n0.1122\n1.0\n3.566306\n\n\n73\nBASEMENTAREA_MEDI\n127568.0\nNaN\nNaN\nNaN\n0.087955\n0.082179\n0.0\n0.0437\n0.0758\n0.1116\n1.0\n3.553040\n\n\n59\nBASEMENTAREA_MODE\n127568.0\nNaN\nNaN\nNaN\n0.087543\n0.084307\n0.0\n0.0407\n0.0746\n0.1124\n1.0\n3.481533\n\n\n37\nREG_CITY_NOT_LIVE_CITY\n307511.0\nNaN\nNaN\nNaN\n0.078173\n0.268444\n0.0\n0.0\n0.0\n0.0\n1.0\n3.142781\n\n\n1\nTARGET\n307511.0\nNaN\nNaN\nNaN\n0.080729\n0.272419\n0.0\n0.0\n0.0\n0.0\n1.0\n3.078159\n\n\n102\nFLAG_DOCUMENT_8\n307511.0\nNaN\nNaN\nNaN\n0.081376\n0.273412\n0.0\n0.0\n0.0\n0.0\n1.0\n3.062241\n\n\n54\nLIVINGAPARTMENTS_AVG\n97312.0\nNaN\nNaN\nNaN\n0.100775\n0.092576\n0.0\n0.0504\n0.0756\n0.121\n1.0\n3.042198\n\n\n82\nLIVINGAPARTMENTS_MEDI\n97312.0\nNaN\nNaN\nNaN\n0.101954\n0.093642\n0.0\n0.0513\n0.0761\n0.1231\n1.0\n2.988291\n\n\n100\nFLAG_DOCUMENT_6\n307511.0\nNaN\nNaN\nNaN\n0.088055\n0.283376\n0.0\n0.0\n0.0\n0.0\n1.0\n2.907427\n\n\n68\nLIVINGAPARTMENTS_MODE\n97312.0\nNaN\nNaN\nNaN\n0.105645\n0.09788\n0.0\n0.0542\n0.0771\n0.1313\n1.0\n2.902672\n\n\n69\nLIVINGAREA_MODE\n153161.0\nNaN\nNaN\nNaN\n0.105975\n0.111845\n0.0\n0.0427\n0.0731\n0.1252\n1.0\n2.902491\n\n\n55\nLIVINGAREA_AVG\n153161.0\nNaN\nNaN\nNaN\n0.107399\n0.110565\n0.0\n0.0453\n0.0745\n0.1299\n1.0\n2.854736\n\n\n83\nLIVINGAREA_MEDI\n153161.0\nNaN\nNaN\nNaN\n0.108607\n0.11226\n0.0\n0.0457\n0.0749\n0.1303\n1.0\n2.848935\n\n\n88\nTOTALAREA_MODE\n159080.0\nNaN\nNaN\nNaN\n0.102547\n0.107462\n0.0\n0.0412\n0.0688\n0.1276\n1.0\n2.797572\n\n\n21\nOWN_CAR_AGE\n104582.0\nNaN\nNaN\nNaN\n12.061091\n11.944812\n0.0\n5.0\n9.0\n15.0\n91.0\n2.745422\n\n\n58\nAPARTMENTS_MODE\n151450.0\nNaN\nNaN\nNaN\n0.114231\n0.107936\n0.0\n0.0525\n0.084\n0.1439\n1.0\n2.703052\n\n\n44\nAPARTMENTS_AVG\n151450.0\nNaN\nNaN\nNaN\n0.11744\n0.10824\n0.0\n0.0577\n0.0876\n0.1485\n1.0\n2.641836\n\n\n72\nAPARTMENTS_MEDI\n151450.0\nNaN\nNaN\nNaN\n0.11785\n0.109076\n0.0\n0.0583\n0.0864\n0.1489\n1.0\n2.639256\n\n\n63\nELEVATORS_MODE\n143620.0\nNaN\nNaN\nNaN\n0.07449\n0.132256\n0.0\n0.0\n0.0\n0.1208\n1.0\n2.552281\n\n\n77\nELEVATORS_MEDI\n143620.0\nNaN\nNaN\nNaN\n0.078078\n0.134467\n0.0\n0.0\n0.0\n0.12\n1.0\n2.457824\n\n\n49\nELEVATORS_AVG\n143620.0\nNaN\nNaN\nNaN\n0.078942\n0.134576\n0.0\n0.0\n0.0\n0.12\n1.0\n2.439429\n\n\n50\nENTRANCES_AVG\n152683.0\nNaN\nNaN\nNaN\n0.149725\n0.100049\n0.0\n0.069\n0.1379\n0.2069\n1.0\n2.399717\n\n\n64\nENTRANCES_MODE\n152683.0\nNaN\nNaN\nNaN\n0.145193\n0.100977\n0.0\n0.069\n0.1379\n0.2069\n1.0\n2.392343\n\n\n78\nENTRANCES_MEDI\n152683.0\nNaN\nNaN\nNaN\n0.149213\n0.100368\n0.0\n0.069\n0.1379\n0.2069\n1.0\n2.387711\n\n\n6\nCNT_CHILDREN\n307511.0\nNaN\nNaN\nNaN\n0.417052\n0.722121\n0.0\n0.0\n0.0\n1.0\n19.0\n1.974604\n\n\n39\nLIVE_CITY_NOT_WORK_CITY\n307511.0\nNaN\nNaN\nNaN\n0.179555\n0.383817\n0.0\n0.0\n0.0\n0.0\n1.0\n1.669795\n\n\n18\nDAYS_EMPLOYED\n307511.0\nNaN\nNaN\nNaN\n63815.045904\n141275.766519\n-17912.0\n-2760.0\n-1213.0\n-289.0\n365243.0\n1.664346\n\n\n9\nAMT_ANNUITY\n307499.0\nNaN\nNaN\nNaN\n27108.573909\n14493.737315\n1615.5\n16524.0\n24903.0\n34596.0\n258025.5\n1.579777\n\n\n24\nFLAG_WORK_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.199368\n0.399526\n0.0\n0.0\n0.0\n0.0\n1.0\n1.504950\n\n\n16\nREGION_POPULATION_RELATIVE\n307511.0\nNaN\nNaN\nNaN\n0.020868\n0.013831\n0.00029\n0.010006\n0.01885\n0.028663\n0.072508\n1.488009\n\n\n10\nAMT_GOODS_PRICE\n307233.0\nNaN\nNaN\nNaN\n538396.207429\n369446.46054\n40500.0\n238500.0\n450000.0\n679500.0\n4050000.0\n1.349000\n\n\n38\nREG_CITY_NOT_WORK_CITY\n307511.0\nNaN\nNaN\nNaN\n0.230454\n0.421124\n0.0\n0.0\n0.0\n0.0\n1.0\n1.280138\n\n\n65\nFLOORSMAX_MODE\n154491.0\nNaN\nNaN\nNaN\n0.222315\n0.143709\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n1.244343\n\n\n121\nAMT_REQ_CREDIT_BUREAU_YEAR\n265992.0\nNaN\nNaN\nNaN\n1.899974\n1.869295\n0.0\n0.0\n1.0\n3.0\n25.0\n1.243590\n\n\n79\nFLOORSMAX_MEDI\n154491.0\nNaN\nNaN\nNaN\n0.225897\n0.145067\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n1.240185\n\n\n8\nAMT_CREDIT\n307511.0\nNaN\nNaN\nNaN\n599025.999706\n402490.776996\n45000.0\n270000.0\n513531.0\n808650.0\n4050000.0\n1.234778\n\n\n51\nFLOORSMAX_AVG\n154491.0\nNaN\nNaN\nNaN\n0.226282\n0.144641\n0.0\n0.1667\n0.1667\n0.3333\n1.0\n1.226454\n\n\n29\nCNT_FAM_MEMBERS\n307509.0\nNaN\nNaN\nNaN\n2.152665\n0.910682\n1.0\n2.0\n2.0\n3.0\n20.0\n0.987543\n\n\n26\nFLAG_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.281066\n0.449521\n0.0\n0.0\n0.0\n1.0\n1.0\n0.974083\n\n\n66\nFLOORSMIN_MODE\n98869.0\nNaN\nNaN\nNaN\n0.228058\n0.16116\n0.0\n0.0833\n0.2083\n0.375\n1.0\n0.963835\n\n\n80\nFLOORSMIN_MEDI\n98869.0\nNaN\nNaN\nNaN\n0.231625\n0.161934\n0.0\n0.0833\n0.2083\n0.375\n1.0\n0.960226\n\n\n52\nFLOORSMIN_AVG\n98869.0\nNaN\nNaN\nNaN\n0.231894\n0.16138\n0.0\n0.0833\n0.2083\n0.375\n1.0\n0.954197\n\n\n20\nDAYS_ID_PUBLISH\n307511.0\nNaN\nNaN\nNaN\n-2994.202373\n1509.450419\n-7197.0\n-4299.0\n-3254.0\n-1720.0\n0.0\n0.349327\n\n\n30\nREGION_RATING_CLIENT\n307511.0\nNaN\nNaN\nNaN\n2.052463\n0.509034\n1.0\n2.0\n2.0\n2.0\n3.0\n0.087468\n\n\n31\nREGION_RATING_CLIENT_W_CITY\n307511.0\nNaN\nNaN\nNaN\n2.031521\n0.502737\n1.0\n2.0\n2.0\n2.0\n3.0\n0.059730\n\n\n0\nSK_ID_CURR\n307511.0\nNaN\nNaN\nNaN\n278180.518577\n102790.175348\n100002.0\n189145.5\n278202.0\n367142.5\n456255.0\n-0.001200\n\n\n33\nHOUR_APPR_PROCESS_START\n307511.0\nNaN\nNaN\nNaN\n12.063419\n3.265832\n0.0\n10.0\n12.0\n14.0\n23.0\n-0.028024\n\n\n41\nEXT_SOURCE_1\n134133.0\nNaN\nNaN\nNaN\n0.50213\n0.211062\n0.014568\n0.334007\n0.505998\n0.675053\n0.962693\n-0.068755\n\n\n17\nDAYS_BIRTH\n307511.0\nNaN\nNaN\nNaN\n-16036.995067\n4363.988632\n-25229.0\n-19682.0\n-15750.0\n-12413.0\n-7489.0\n-0.115673\n\n\n43\nEXT_SOURCE_3\n246546.0\nNaN\nNaN\nNaN\n0.510853\n0.194844\n0.000527\n0.37065\n0.535276\n0.669057\n0.89601\n-0.409390\n\n\n19\nDAYS_REGISTRATION\n307511.0\nNaN\nNaN\nNaN\n-4986.120328\n3522.886321\n-24672.0\n-7479.5\n-4504.0\n-2010.0\n0.0\n-0.590872\n\n\n95\nDAYS_LAST_PHONE_CHANGE\n307510.0\nNaN\nNaN\nNaN\n-962.858788\n826.808487\n-4292.0\n-1570.0\n-757.0\n-274.0\n0.0\n-0.713606\n\n\n42\nEXT_SOURCE_2\n306851.0\nNaN\nNaN\nNaN\n0.514393\n0.19106\n0.0\n0.392457\n0.565961\n0.663617\n0.855\n-0.793576\n\n\n97\nFLAG_DOCUMENT_3\n307511.0\nNaN\nNaN\nNaN\n0.710023\n0.453752\n0.0\n0.0\n1.0\n1.0\n1.0\n-0.925725\n\n\n47\nYEARS_BUILD_AVG\n103023.0\nNaN\nNaN\nNaN\n0.752471\n0.11328\n0.0\n0.6872\n0.7552\n0.8232\n1.0\n-0.962485\n\n\n75\nYEARS_BUILD_MEDI\n103023.0\nNaN\nNaN\nNaN\n0.755746\n0.112066\n0.0\n0.6914\n0.7585\n0.8256\n1.0\n-0.962784\n\n\n61\nYEARS_BUILD_MODE\n103023.0\nNaN\nNaN\nNaN\n0.759637\n0.110111\n0.0\n0.6994\n0.7648\n0.8236\n1.0\n-1.002305\n\n\n23\nFLAG_EMP_PHONE\n307511.0\nNaN\nNaN\nNaN\n0.819889\n0.38428\n0.0\n1.0\n1.0\n1.0\n1.0\n-1.664886\n\n\n60\nYEARS_BEGINEXPLUATATION_MODE\n157504.0\nNaN\nNaN\nNaN\n0.977065\n0.064575\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n-14.755318\n\n\n46\nYEARS_BEGINEXPLUATATION_AVG\n157504.0\nNaN\nNaN\nNaN\n0.977735\n0.059223\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n-15.515264\n\n\n74\nYEARS_BEGINEXPLUATATION_MEDI\n157504.0\nNaN\nNaN\nNaN\n0.977752\n0.059897\n0.0\n0.9767\n0.9816\n0.9866\n1.0\n-15.573124\n\n\n25\nFLAG_CONT_MOBILE\n307511.0\nNaN\nNaN\nNaN\n0.998133\n0.043164\n0.0\n1.0\n1.0\n1.0\n1.0\n-23.081172\n\n\n22\nFLAG_MOBIL\n307511.0\nNaN\nNaN\nNaN\n0.999997\n0.001803\n0.0\n1.0\n1.0\n1.0\n1.0\n-554.536744\n\n\n2\nNAME_CONTRACT_TYPE\n307511\n2\nCash loans\n278232\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nCODE_GENDER\n307511\n3\nF\n202448\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nFLAG_OWN_CAR\n307511\n2\nN\n202924\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\nFLAG_OWN_REALTY\n307511\n2\nY\n213312\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11\nNAME_TYPE_SUITE\n306219\n7\nUnaccompanied\n248526\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n12\nNAME_INCOME_TYPE\n307511\n8\nWorking\n158774\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n13\nNAME_EDUCATION_TYPE\n307511\n5\nSecondary / secondary special\n218391\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n14\nNAME_FAMILY_STATUS\n307511\n6\nMarried\n196432\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n15\nNAME_HOUSING_TYPE\n307511\n6\nHouse / apartment\n272868\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n28\nOCCUPATION_TYPE\n211120\n18\nLaborers\n55186\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n32\nWEEKDAY_APPR_PROCESS_START\n307511\n7\nTUESDAY\n53901\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n40\nORGANIZATION_TYPE\n307511\n58\nBusiness Entity Type 3\n67992\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n86\nFONDKAPREMONT_MODE\n97216\n4\nreg oper account\n73830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n87\nHOUSETYPE_MODE\n153214\n3\nblock of flats\n150503\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n89\nWALLSMATERIAL_MODE\n151170\n7\nPanel\n66040\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n90\nEMERGENCYSTATE_MODE\n161756\n2\nNo\n159428\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n결측치 확인\n\nisnull()을 활용한 결측치 확인\n\n\ntotal = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['total', 'percent'])\nmissing_data[missing_data['percent']!=0]\n\n\n\n\n\n\n\n\ntotal\npercent\n\n\n\n\nCOMMONAREA_MEDI\n214865\n69.872297\n\n\nCOMMONAREA_AVG\n214865\n69.872297\n\n\nCOMMONAREA_MODE\n214865\n69.872297\n\n\nNONLIVINGAPARTMENTS_MODE\n213514\n69.432963\n\n\nNONLIVINGAPARTMENTS_AVG\n213514\n69.432963\n\n\nNONLIVINGAPARTMENTS_MEDI\n213514\n69.432963\n\n\nFONDKAPREMONT_MODE\n210295\n68.386172\n\n\nLIVINGAPARTMENTS_MODE\n210199\n68.354953\n\n\nLIVINGAPARTMENTS_AVG\n210199\n68.354953\n\n\nLIVINGAPARTMENTS_MEDI\n210199\n68.354953\n\n\nFLOORSMIN_AVG\n208642\n67.848630\n\n\nFLOORSMIN_MODE\n208642\n67.848630\n\n\nFLOORSMIN_MEDI\n208642\n67.848630\n\n\nYEARS_BUILD_MEDI\n204488\n66.497784\n\n\nYEARS_BUILD_MODE\n204488\n66.497784\n\n\nYEARS_BUILD_AVG\n204488\n66.497784\n\n\nOWN_CAR_AGE\n202929\n65.990810\n\n\nLANDAREA_MEDI\n182590\n59.376738\n\n\nLANDAREA_MODE\n182590\n59.376738\n\n\nLANDAREA_AVG\n182590\n59.376738\n\n\nBASEMENTAREA_MEDI\n179943\n58.515956\n\n\nBASEMENTAREA_AVG\n179943\n58.515956\n\n\nBASEMENTAREA_MODE\n179943\n58.515956\n\n\nEXT_SOURCE_1\n173378\n56.381073\n\n\nNONLIVINGAREA_MODE\n169682\n55.179164\n\n\nNONLIVINGAREA_AVG\n169682\n55.179164\n\n\nNONLIVINGAREA_MEDI\n169682\n55.179164\n\n\nELEVATORS_MEDI\n163891\n53.295980\n\n\nELEVATORS_AVG\n163891\n53.295980\n\n\nELEVATORS_MODE\n163891\n53.295980\n\n\nWALLSMATERIAL_MODE\n156341\n50.840783\n\n\nAPARTMENTS_MEDI\n156061\n50.749729\n\n\nAPARTMENTS_AVG\n156061\n50.749729\n\n\nAPARTMENTS_MODE\n156061\n50.749729\n\n\nENTRANCES_MEDI\n154828\n50.348768\n\n\nENTRANCES_AVG\n154828\n50.348768\n\n\nENTRANCES_MODE\n154828\n50.348768\n\n\nLIVINGAREA_AVG\n154350\n50.193326\n\n\nLIVINGAREA_MODE\n154350\n50.193326\n\n\nLIVINGAREA_MEDI\n154350\n50.193326\n\n\nHOUSETYPE_MODE\n154297\n50.176091\n\n\nFLOORSMAX_MODE\n153020\n49.760822\n\n\nFLOORSMAX_MEDI\n153020\n49.760822\n\n\nFLOORSMAX_AVG\n153020\n49.760822\n\n\nYEARS_BEGINEXPLUATATION_MODE\n150007\n48.781019\n\n\nYEARS_BEGINEXPLUATATION_MEDI\n150007\n48.781019\n\n\nYEARS_BEGINEXPLUATATION_AVG\n150007\n48.781019\n\n\nTOTALAREA_MODE\n148431\n48.268517\n\n\nEMERGENCYSTATE_MODE\n145755\n47.398304\n\n\nOCCUPATION_TYPE\n96391\n31.345545\n\n\nEXT_SOURCE_3\n60965\n19.825307\n\n\nAMT_REQ_CREDIT_BUREAU_HOUR\n41519\n13.501631\n\n\nAMT_REQ_CREDIT_BUREAU_DAY\n41519\n13.501631\n\n\nAMT_REQ_CREDIT_BUREAU_WEEK\n41519\n13.501631\n\n\nAMT_REQ_CREDIT_BUREAU_MON\n41519\n13.501631\n\n\nAMT_REQ_CREDIT_BUREAU_QRT\n41519\n13.501631\n\n\nAMT_REQ_CREDIT_BUREAU_YEAR\n41519\n13.501631\n\n\nNAME_TYPE_SUITE\n1292\n0.420148\n\n\nOBS_30_CNT_SOCIAL_CIRCLE\n1021\n0.332021\n\n\nDEF_30_CNT_SOCIAL_CIRCLE\n1021\n0.332021\n\n\nOBS_60_CNT_SOCIAL_CIRCLE\n1021\n0.332021\n\n\nDEF_60_CNT_SOCIAL_CIRCLE\n1021\n0.332021\n\n\nEXT_SOURCE_2\n660\n0.214626\n\n\nAMT_GOODS_PRICE\n278\n0.090403\n\n\nAMT_ANNUITY\n12\n0.003902\n\n\nCNT_FAM_MEMBERS\n2\n0.000650\n\n\nDAYS_LAST_PHONE_CHANGE\n1\n0.000325\n\n\n\n\n\n\n\n\n전체 데이터에서, 결측이 10% 이상인 컬럼 확인\n\n몇 퍼센트의 결측치를 볼지는 개인별 선택\n\n결측치가 많으므로 단순히 Drop할 것이 아니라 Imputation(대체)이 필요함\n\n\nmissing_data[missing_data.percent &gt;= 10].shape[0]/missing_data.shape[0]\n\n0.4672131147540984\n\n\n\n\n이상치 확인\n\n결측치와 달리, 이상치는 기준을 정해야 함\n\n일반적으로 IQR을 기준으로 많이 작업(모수의 분포를 모르므로)\nIQR(Q3-Q1)을 기준으로, \\(Q1 - 1.5*IQR\\) 보다 작거나, \\(Q3 + 1.5*IQR\\) 보다 크면 이상치라고 함\n\n분위수를 구하는 quantile함수를 활용해, Q1, Q3, IQR을 직접 구하는 함수를 구현할 수 있음\n이상치 확인할 때, 결측치 제외하는 것이 중요함\n이상치 확인하기 (결측치 제외되지 않은 사례)\n\n이상치 비율이 비정상적으로 큰 것을 볼 수 있음\n\n\n\ndef detect_outliers(df):\n  outlier_flags = df.copy()\n  for column in df.columns:\n    col_data = df[column].dropna()  # 결측치 제외   → 이상치를 구하는데 영향을 끼치므로\n    Q1 = col_data.quantile(0.25)\n    Q3 = col_data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outlier_flags[column] = df[column].apply(\n        lambda x: not lower_bound &lt;= x &lt;= upper_bound\n    )\n\n  return outlier_flags\n\noutliers_total = detect_outliers(numerical_data).sum().sort_values(ascending = False)\noutliers_percent = (detect_outliers(numerical_data).sum()/detect_outliers(numerical_data).count()*100).sort_values(ascending = False)\noutliers_data  = pd.concat([outliers_total, outliers_percent], axis=1, keys=['total', 'percent'])\noutliers_data.head(10)\n\n\n\n\n\n\n\n\ntotal\npercent\n\n\n\n\nNONLIVINGAPARTMENTS_AVG\n229094\n74.499449\n\n\nNONLIVINGAPARTMENTS_MEDI\n228729\n74.380754\n\n\nNONLIVINGAPARTMENTS_MODE\n227738\n74.058489\n\n\nCOMMONAREA_MEDI\n222869\n72.475131\n\n\nCOMMONAREA_AVG\n222807\n72.454969\n\n\nCOMMONAREA_MODE\n222803\n72.453668\n\n\nLIVINGAPARTMENTS_MEDI\n218126\n70.932747\n\n\nLIVINGAPARTMENTS_AVG\n218080\n70.917788\n\n\nLIVINGAPARTMENTS_MODE\n217668\n70.783809\n\n\nFLOORSMIN_MEDI\n208986\n67.960496\n\n\n\n\n\n\n\n\n이상치 확인하기 (결측치 제외된 사례)\n\n이상치의 비중이 26, 25%와 같이 높은 편으로, 단순히 제거하여 해결하는 것은 좋지 않음\n시각화까지 진행해보고, 이후 방향을 결정\n\n\n\ndef detect_outliers_wo_nan(df):\n  outlier_flags = df.copy()\n  for column in df.columns:\n    col_data = df[column].dropna()  # 결측치 제외\n    Q1 = col_data.quantile(0.25)\n    Q3 = col_data.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outlier_flags[column] = df[column].apply(\n        lambda x: not lower_bound &lt;= x &lt;= upper_bound if pd.notna(x) else False\n    )\n\n  return outlier_flags\n\n\noutliers_total = detect_outliers_wo_nan(numerical_data).sum().sort_values(ascending = False)\noutliers_percent = (detect_outliers_wo_nan(numerical_data).sum()/detect_outliers_wo_nan(numerical_data).count()*100).sort_values(ascending = False)\noutliers_data  = pd.concat([outliers_total, outliers_percent], axis=1, keys=['total', 'percent'])\noutliers_data.head(10)\n\n\n\n\n\n\n\n\ntotal\npercent\n\n\n\n\nREGION_RATING_CLIENT\n80527\n26.186706\n\n\nREGION_RATING_CLIENT_W_CITY\n78027\n25.373726\n\n\nDAYS_EMPLOYED\n72217\n23.484363\n\n\nREG_CITY_NOT_WORK_CITY\n70867\n23.045354\n\n\nFLAG_WORK_PHONE\n61308\n19.936848\n\n\nFLAG_EMP_PHONE\n55386\n18.011063\n\n\nLIVE_CITY_NOT_WORK_CITY\n55215\n17.955455\n\n\nAMT_REQ_CREDIT_BUREAU_QRT\n50575\n16.446566\n\n\nAMT_REQ_CREDIT_BUREAU_MON\n43759\n14.230060\n\n\nDEF_30_CNT_SOCIAL_CIRCLE\n35166\n11.435688\n\n\n\n\n\n\n\n\n\n심화1 : 벡터연산을 통한 최적화 (기존 이상치 연산과 비교)\n\npandas를 반복문 대신 벡터연산(numpy)으로 빠르게 처리할 수 있음\n\napply함수 : 각 행/열을 독립적으로 처리하는 벡터화 함수\n\n하단 코드는 벡터연산 적용한 이상치 찾는 함수 vs 기존의 이상치 찾는 함수의 비교\n\n\ndef detect_outliers_fast(df):\n    # Calculate IQR without NaN values\n    Q1 = df.quantile(0.25, interpolation='midpoint')\n    Q3 = df.quantile(0.75, interpolation='midpoint')\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Vectorized outlier detection with NaN handling\n    outlier_flags = df.apply(\n        lambda col: ~col.between(lower_bound[col.name], upper_bound[col.name]) & col.notna()\n    )\n    return outlier_flags\n\n\n%%time\ndetect_outliers_wo_nan(numerical_data).head(5)  # 기존의 (벡터연산 미적용된) 코드\n\nCPU times: total: 13.8 s\nWall time: 14.5 s\n\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 106 columns\n\n\n\n\n%%time\ndetect_outliers_fast(numerical_data).head(5)  # 벡터연산 적용한 코드\n\nCPU times: total: 797 ms\nWall time: 867 ms\n\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 106 columns\n\n\n\n\n\n심화2 : Polars\n\n대용량 데이터 처리에 특화된 Polars\n\npandas와 비슷한 문법, 대부분의 경우 pandas보다 빠른 연산\n광범위하게 쓰이진 않지만, 대용량 데이터 처리에는 유용함\n\n\n\n!pip install polars\n\nCollecting polars\n  Downloading polars-1.19.0-cp39-abi3-win_amd64.whl.metadata (15 kB)\nDownloading polars-1.19.0-cp39-abi3-win_amd64.whl (32.8 MB)\n   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n   -- ------------------------------------- 2.4/32.8 MB 11.2 MB/s eta 0:00:03\n   ----- ---------------------------------- 4.7/32.8 MB 11.4 MB/s eta 0:00:03\n   -------- ------------------------------- 7.3/32.8 MB 11.6 MB/s eta 0:00:03\n   --------- ------------------------------ 7.9/32.8 MB 9.2 MB/s eta 0:00:03\n   ----------- ---------------------------- 9.7/32.8 MB 9.2 MB/s eta 0:00:03\n   --------------- ------------------------ 12.3/32.8 MB 9.6 MB/s eta 0:00:03\n   ----------------- ---------------------- 14.7/32.8 MB 9.9 MB/s eta 0:00:02\n   --------------------- ------------------ 17.3/32.8 MB 10.2 MB/s eta 0:00:02\n   ----------------------- ---------------- 19.7/32.8 MB 10.3 MB/s eta 0:00:02\n   -------------------------- ------------- 22.0/32.8 MB 10.6 MB/s eta 0:00:02\n   ---------------------------- ----------- 23.6/32.8 MB 10.2 MB/s eta 0:00:01\n   ------------------------------- -------- 26.2/32.8 MB 10.3 MB/s eta 0:00:01\n   ---------------------------------- ----- 28.6/32.8 MB 10.4 MB/s eta 0:00:01\n   ------------------------------------- -- 31.2/32.8 MB 10.5 MB/s eta 0:00:01\n   ---------------------------------------- 32.8/32.8 MB 10.4 MB/s eta 0:00:00\nInstalling collected packages: polars\nSuccessfully installed polars-1.19.0\n\n\n\n# Polars 활용한 코드\nimport polars as pl\n\nnumerical_data_polars = pl.from_pandas(numerical_data)\n\ndef detect_outliers_polars(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n\n    outlier_flags = df.with_columns(\n        [\n            ~((pl.col(col) &gt;= lower_bound[col]) & (pl.col(col) &lt;= upper_bound[col])) # 범위 계산\n            .fill_null(False) # 결측치 제외   → null을 넣지않고 삭제\n            .alias(col) # 기존 열 이름 유지\n            for col in df.columns\n        ]\n    )\n    return outlier_flags\n\n\n%%time\ndetect_outliers_polars(numerical_data_polars).head(5)\n\nCPU times: total: 1.02 s\nWall time: 266 ms\n\n\n\nshape: (5, 106)\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\nDAYS_REGISTRATION\nDAYS_ID_PUBLISH\nOWN_CAR_AGE\nFLAG_MOBIL\nFLAG_EMP_PHONE\nFLAG_WORK_PHONE\nFLAG_CONT_MOBILE\nFLAG_PHONE\nFLAG_EMAIL\nCNT_FAM_MEMBERS\nREGION_RATING_CLIENT\nREGION_RATING_CLIENT_W_CITY\nHOUR_APPR_PROCESS_START\nREG_REGION_NOT_LIVE_REGION\nREG_REGION_NOT_WORK_REGION\nLIVE_REGION_NOT_WORK_REGION\nREG_CITY_NOT_LIVE_CITY\nREG_CITY_NOT_WORK_CITY\nLIVE_CITY_NOT_WORK_CITY\nEXT_SOURCE_1\nEXT_SOURCE_2\nEXT_SOURCE_3\nAPARTMENTS_AVG\nBASEMENTAREA_AVG\nYEARS_BEGINEXPLUATATION_AVG\nYEARS_BUILD_AVG\nCOMMONAREA_AVG\n…\nLANDAREA_MEDI\nLIVINGAPARTMENTS_MEDI\nLIVINGAREA_MEDI\nNONLIVINGAPARTMENTS_MEDI\nNONLIVINGAREA_MEDI\nTOTALAREA_MODE\nOBS_30_CNT_SOCIAL_CIRCLE\nDEF_30_CNT_SOCIAL_CIRCLE\nOBS_60_CNT_SOCIAL_CIRCLE\nDEF_60_CNT_SOCIAL_CIRCLE\nDAYS_LAST_PHONE_CHANGE\nFLAG_DOCUMENT_2\nFLAG_DOCUMENT_3\nFLAG_DOCUMENT_4\nFLAG_DOCUMENT_5\nFLAG_DOCUMENT_6\nFLAG_DOCUMENT_7\nFLAG_DOCUMENT_8\nFLAG_DOCUMENT_9\nFLAG_DOCUMENT_10\nFLAG_DOCUMENT_11\nFLAG_DOCUMENT_12\nFLAG_DOCUMENT_13\nFLAG_DOCUMENT_14\nFLAG_DOCUMENT_15\nFLAG_DOCUMENT_16\nFLAG_DOCUMENT_17\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\n…\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\nbool\n\n\n\n\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\n…\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\n…\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n…\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n…\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\n…\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\n\ny의 불균형도 확인\n\npandas의 value_counts : SQL의 group by + count\n\n\ndata['TARGET'].value_counts()\n\nTARGET\n0    282686\n1     24825\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#실습1-확인한-통계량을-가지고-시각화-고민하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#실습1-확인한-통계량을-가지고-시각화-고민하기",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "실습1 : 확인한 통계량을 가지고 시각화 고민하기",
    "text": "실습1 : 확인한 통계량을 가지고 시각화 고민하기\n\n지금까지 본 수치를 기반으로 각 변수별로 어떤 시각화가 필요할지 고민해봅시다.\n\n범주형 변수(describe기준 unique값 있는 경우)에 대해 barplot\n대부분 변수에 대해 Histogram으로 분포 확인\n결측치(Null)을 제외한 후 Box plot으로 이상치 확인\nHeatmap을 활용한 상관계수 확인\n\n\n\ndata.describe(include='all')\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nNAME_CONTRACT_TYPE\nCODE_GENDER\nFLAG_OWN_CAR\nFLAG_OWN_REALTY\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\ncount\n307511.000000\n307511.000000\n307511\n307511\n307511\n307511\n307511.000000\n3.075110e+05\n3.075110e+05\n307499.000000\n...\n307511.000000\n307511.000000\n307511.000000\n307511.000000\n265992.000000\n265992.000000\n265992.000000\n265992.000000\n265992.000000\n265992.000000\n\n\nunique\nNaN\nNaN\n2\n3\n2\n2\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nCash loans\nF\nN\nY\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\n278232\n202448\n202924\n213312\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n278180.518577\n0.080729\nNaN\nNaN\nNaN\nNaN\n0.417052\n1.687979e+05\n5.990260e+05\n27108.573909\n...\n0.008130\n0.000595\n0.000507\n0.000335\n0.006402\n0.007000\n0.034362\n0.267395\n0.265474\n1.899974\n\n\nstd\n102790.175348\n0.272419\nNaN\nNaN\nNaN\nNaN\n0.722121\n2.371231e+05\n4.024908e+05\n14493.737315\n...\n0.089798\n0.024387\n0.022518\n0.018299\n0.083849\n0.110757\n0.204685\n0.916002\n0.794056\n1.869295\n\n\nmin\n100002.000000\n0.000000\nNaN\nNaN\nNaN\nNaN\n0.000000\n2.565000e+04\n4.500000e+04\n1615.500000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n189145.500000\n0.000000\nNaN\nNaN\nNaN\nNaN\n0.000000\n1.125000e+05\n2.700000e+05\n16524.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n278202.000000\n0.000000\nNaN\nNaN\nNaN\nNaN\n0.000000\n1.471500e+05\n5.135310e+05\n24903.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n75%\n367142.500000\n0.000000\nNaN\nNaN\nNaN\nNaN\n1.000000\n2.025000e+05\n8.086500e+05\n34596.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.000000\n\n\nmax\n456255.000000\n1.000000\nNaN\nNaN\nNaN\nNaN\n19.000000\n1.170000e+08\n4.050000e+06\n258025.500000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n4.000000\n9.000000\n8.000000\n27.000000\n261.000000\n25.000000\n\n\n\n\n11 rows × 122 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#시각화",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#시각화",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "시각화",
    "text": "시각화\n\n일단 기본적인 histogram, barplot을 그려보기\n\n수치형 변수는 histogram, 범주형 변수는 bar plot\nselect_dtypes 활용\n\n\n\nHistogram\n\n수치형데이터에 대해 histogram 그리기\nMatplotlib와 Seaborn으로 나누어 실습\n\n\nnumeric_data = data.select_dtypes(include=['float64', 'int64'])\nnumeric_data.head(3)\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\n100002\n1\n0\n202500.0\n406597.5\n24700.5\n351000.0\n0.018801\n-9461\n-637\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n100003\n0\n0\n270000.0\n1293502.5\n35698.5\n1129500.0\n0.003541\n-16765\n-1188\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n100004\n0\n0\n67500.0\n135000.0\n6750.0\n135000.0\n0.010032\n-19046\n-225\n...\n0\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n3 rows × 106 columns\n\n\n\n\n# Matplotlib와 Seaborn 그래프\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 데이터 준비\nvar_idx = 1\nvar_nm = numeric_data.columns[var_idx]\n\n# 서브플롯 생성\nfig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n\n# 코드 1: Matplotlib Graph (좌측)\naxes[0].hist(numeric_data[var_nm], bins='auto')\naxes[0].set_title(f'Histogram of {var_nm}')\naxes[0].set_xlabel('Values')\naxes[0].set_ylabel('Frequency')\n\n# 코드 2: Seaborn Graph (우측)\nsns.histplot(data[var_nm], kde=True, bins='auto', ax=axes[1])\naxes[1].set_title(f'Histogram of {var_nm}')\naxes[1].set_xlabel('Values')\naxes[1].set_ylabel('Frequency')\n\n# 전체 그래프 간격 조정\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n0~1 사이에 분포하는 컬럼이 많음 → 많은 컬럼이 정규화 되어있음\n앞서 왜도가 높았던 컬럼의 확인 결과\n\nAMT_INCOME_TOTAL(391.559654) : 식별이 어려워 bin값 조정필요\nYEARS_BEGINEXPLUATATION_MEDI(-15.573124) : 왜도와 달리 값이 치우치지 않아보임\nFLAG_CONT_MOBILE(-23.081172) : 왜도와 달리 값이 치우치지 않아보임\nFLAG_MOBIL(-554.536744) : 왜도와 달리 값이 치우치지 않아보임\n\n\n\nimport math\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(numeric_data.columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(numeric_data.columns):\n    ax = axes[var_idx]\n    ax.hist(numeric_data.iloc[:, var_idx], bins='auto', color='skyblue', edgecolor='black')\n    ax.set_title(f'Histogram of {var_nm}')\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Frequency')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n실습2 : 시각화 실습\n\nAMT_INCOME_TOTAL을 다시 시각화해봅시다. bins 변수를 적절히 조정해서 잘 보이게 해봅시다.\n\n\nvar_idx = 3\n\nplt.figure(figsize=(5, 3))\nplt.hist(numeric_data.iloc[:,var_idx], bins=3) # 컬럼명 기준으로 인덱싱시 발생할 수 있는 오류 예방.\nplt.title(f'Histogram of {var_nm}')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBarplot\n\nplt.bar or sns.barplot으로 시각화\n\n\ncategorical_data = data.select_dtypes(include=['object', 'category'])\ncategorical_data.head(5)\n\n\n\n\n\n\n\n\nNAME_CONTRACT_TYPE\nCODE_GENDER\nFLAG_OWN_CAR\nFLAG_OWN_REALTY\nNAME_TYPE_SUITE\nNAME_INCOME_TYPE\nNAME_EDUCATION_TYPE\nNAME_FAMILY_STATUS\nNAME_HOUSING_TYPE\nOCCUPATION_TYPE\nWEEKDAY_APPR_PROCESS_START\nORGANIZATION_TYPE\nFONDKAPREMONT_MODE\nHOUSETYPE_MODE\nWALLSMATERIAL_MODE\nEMERGENCYSTATE_MODE\n\n\n\n\n0\nCash loans\nM\nN\nY\nUnaccompanied\nWorking\nSecondary / secondary special\nSingle / not married\nHouse / apartment\nLaborers\nWEDNESDAY\nBusiness Entity Type 3\nreg oper account\nblock of flats\nStone, brick\nNo\n\n\n1\nCash loans\nF\nN\nN\nFamily\nState servant\nHigher education\nMarried\nHouse / apartment\nCore staff\nMONDAY\nSchool\nreg oper account\nblock of flats\nBlock\nNo\n\n\n2\nRevolving loans\nM\nY\nY\nUnaccompanied\nWorking\nSecondary / secondary special\nSingle / not married\nHouse / apartment\nLaborers\nMONDAY\nGovernment\nNaN\nNaN\nNaN\nNaN\n\n\n3\nCash loans\nF\nN\nY\nUnaccompanied\nWorking\nSecondary / secondary special\nCivil marriage\nHouse / apartment\nLaborers\nWEDNESDAY\nBusiness Entity Type 3\nNaN\nNaN\nNaN\nNaN\n\n\n4\nCash loans\nM\nN\nY\nUnaccompanied\nWorking\nSecondary / secondary special\nSingle / not married\nHouse / apartment\nCore staff\nTHURSDAY\nReligion\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n하단의 범주형변수 시각화를 통해 알 수 있는 점\n\n대체로 불균형한 분포를 보임\n대부분 unique값이 많지 않아, one-hot인코딩에 크게 문제가 없어보임\nEducation_type은 순위가 있는 변수이므로 인코딩할 때 유의\n\n\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(categorical_data.columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(15, rows * 4))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(categorical_data.columns):\n    ax = axes[var_idx]\n    value_counts = categorical_data[var_nm].value_counts().reset_index()\n    value_counts.columns = [var_nm, 'Count']\n    sns.barplot(x=var_nm, y='Count', data=value_counts, ax=ax)\n    ax.set_title(f'Bar Plot of {var_nm}')\n    ax.set_xlabel(var_nm)\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTarget(y)값 기준으로 나누어서(Stack) 그릴 수도 있음\n\n\n# TARGET 데이터 합치기\ncombined_data = categorical_data.copy()\ncombined_data['TARGET'] = data['TARGET']  # 동일한 인덱스를 기준으로 TARGET 추가\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(categorical_data.columns)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 5))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(categorical_data.columns):\n    ax = axes[var_idx]\n    # 데이터 그룹화 및 카운트 계산\n    stacked_data = combined_data.groupby([var_nm, 'TARGET']).size().reset_index(name='Count')\n    sns.barplot(x=var_nm, y='Count', hue='TARGET', data=stacked_data, dodge=False, ax=ax)\n    ax.set_title(f'Stacked Bar Plot of {var_nm} by TARGET')\n    ax.set_xlabel(var_nm)\n    ax.set_ylabel('Count')\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(title='TARGET', loc='upper right')\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n실습3\n\ny도 시각화 해봅시다.\n\n\nsns.countplot(x='TARGET', data=data)\n\n&lt;Axes: xlabel='TARGET', ylabel='count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\nsns.boxplot or plt.boxplot로 시각화 가능\n이상치에 대해 시각화 (10%가 넘는 경우 시각화 해보기)\n아래 boxplot 시각화를 기준으로\n\n0이나 1이 너무 많아, 일반적인 boxplot형태는 아님\nDAYS_EMPLOYED는 음의 값이 존재\nDrop이 아닌 변환으로 진행\n\n\n\nimport math\n\noutliers_over_10 = outliers_data[outliers_data.percent &gt; 10]\n\n# 설정: 가로 그래프 개수\ncols = 3\nnum_vars = len(outliers_over_10.index)\nrows = math.ceil(num_vars / cols)  # 필요한 행(row) 수 계산\n\n# 서브플롯 생성\nfig, axes = plt.subplots(rows, cols, figsize=(18, rows * 4))  # 전체 figure 크기 조정\naxes = axes.flatten()  # 2D 배열을 1D로 변환하여 인덱싱 편리하게\n\n# 그래프 그리기\nfor var_idx, var_nm in enumerate(outliers_over_10.index):\n    ax = axes[var_idx]\n    sns.boxplot(x=data[var_nm], ax=ax, color='lightblue')\n    ax.set_title(f'Box Plot of {var_nm}')\n    ax.set_xlabel(var_nm)\n\n# 빈 서브플롯 숨기기\nfor i in range(num_vars, len(axes)):\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlineplot\n\n사용중인 데이터가 시계열 데이터가 아니므로, 임의의 데이터로 시각화\n\n\n# Generate random time series data\nnp.random.seed(42)\ntime_series_data = pd.DataFrame({\n    'Date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'Value': np.cumsum(np.random.randn(100))  # Cumulative sum of random values\n})\n\n# 서브플롯 생성\nfig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n\n# 코드 1: Matplotlib\naxes[0].plot(time_series_data['Date'], time_series_data['Value'], color='blue', label='Value')\naxes[0].set_title('Time Series Lineplot (Matplotlib)')\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Value')\naxes[0].legend()\naxes[0].grid(True)\n\n# 코드 2: Seaborn\nsns.lineplot(x='Date', y='Value', data=time_series_data, color='orange', ax=axes[1])\naxes[1].set_title('Time Series Lineplot (Seaborn)')\naxes[1].set_xlabel('Date')\naxes[1].grid(True)\n\n# 그래프 간격 조정\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHeatmap\n\ny변수와의 상관계수를 시각화, 수치형&수치형 변수일 때 사용\nseaborn만 가능(matplotlib불가)\n사용중인 데이터의 y가 binary이므로, 임의 데이터로 시각화\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Generate a random example dataset\nnp.random.seed(42)\nexample_data = pd.DataFrame(\n    np.random.rand(10, 10),\n    columns=[f'Feature_{i+1}' for i in range(10)]\n)\n\n# Calculate the correlation matrix\ncorrelation_matrix = example_data.corr()\n\n# Plot the heatmap using seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)\nplt.title('Correlation Heatmap')\nplt.show()"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#실습4-시각화-결과를-바탕으로-전처리-고민하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#실습4-시각화-결과를-바탕으로-전처리-고민하기",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "실습4 : 시각화 결과를 바탕으로, 전처리 고민하기",
    "text": "실습4 : 시각화 결과를 바탕으로, 전처리 고민하기\n\n지금까지의 시각화 결과를 바탕으로 변수별로 어떤 전처리가 필요할지, 전처리 계획을 세워봅시다.\n\n많은 컬럼이 정규화되어있음\n왜도가 높았던 변수가 있었지만, 시각화해보니 값이 치우치지는 않았음\n이상치 데이터가 많은 편으로, 단순히 제거하는 것은 옳지 않음\n순위가 있는 Education_type은 인코딩할 때 주의(순위있는 변수)\nunique값이 많지 않은 경우는 원핫인코딩도 고려"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#전처리",
    "href": "posts/meta-cm-sql_and_ml_xai-20250105/index.html#전처리",
    "title": "[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)",
    "section": "전처리",
    "text": "전처리\n\n이상값, 결측값 처리\n\nDrop 또는 변환을 통해 처리\n\n현재 사용중인 데이터는 Drop보다는 변환이 적절\npandas fillna()를 활용\n\n정규화 여부로 변수 나눠보기\n\n정규화 여부에 따라 채워넣을 값이 달라지기 때문\n\n\n\n# 결측치가 있는 컬럼 : 결측 데이터 비율 0% 초과\nmissing_data_cols = missing_data[missing_data.percent &gt; 0].index.tolist()\nmissing_data_cols\n\n['COMMONAREA_MEDI',\n 'COMMONAREA_AVG',\n 'COMMONAREA_MODE',\n 'NONLIVINGAPARTMENTS_MODE',\n 'NONLIVINGAPARTMENTS_AVG',\n 'NONLIVINGAPARTMENTS_MEDI',\n 'FONDKAPREMONT_MODE',\n 'LIVINGAPARTMENTS_MODE',\n 'LIVINGAPARTMENTS_AVG',\n 'LIVINGAPARTMENTS_MEDI',\n 'FLOORSMIN_AVG',\n 'FLOORSMIN_MODE',\n 'FLOORSMIN_MEDI',\n 'YEARS_BUILD_MEDI',\n 'YEARS_BUILD_MODE',\n 'YEARS_BUILD_AVG',\n 'OWN_CAR_AGE',\n 'LANDAREA_MEDI',\n 'LANDAREA_MODE',\n 'LANDAREA_AVG',\n 'BASEMENTAREA_MEDI',\n 'BASEMENTAREA_AVG',\n 'BASEMENTAREA_MODE',\n 'EXT_SOURCE_1',\n 'NONLIVINGAREA_MODE',\n 'NONLIVINGAREA_AVG',\n 'NONLIVINGAREA_MEDI',\n 'ELEVATORS_MEDI',\n 'ELEVATORS_AVG',\n 'ELEVATORS_MODE',\n 'WALLSMATERIAL_MODE',\n 'APARTMENTS_MEDI',\n 'APARTMENTS_AVG',\n 'APARTMENTS_MODE',\n 'ENTRANCES_MEDI',\n 'ENTRANCES_AVG',\n 'ENTRANCES_MODE',\n 'LIVINGAREA_AVG',\n 'LIVINGAREA_MODE',\n 'LIVINGAREA_MEDI',\n 'HOUSETYPE_MODE',\n 'FLOORSMAX_MODE',\n 'FLOORSMAX_MEDI',\n 'FLOORSMAX_AVG',\n 'YEARS_BEGINEXPLUATATION_MODE',\n 'YEARS_BEGINEXPLUATATION_MEDI',\n 'YEARS_BEGINEXPLUATATION_AVG',\n 'TOTALAREA_MODE',\n 'EMERGENCYSTATE_MODE',\n 'OCCUPATION_TYPE',\n 'EXT_SOURCE_3',\n 'AMT_REQ_CREDIT_BUREAU_HOUR',\n 'AMT_REQ_CREDIT_BUREAU_DAY',\n 'AMT_REQ_CREDIT_BUREAU_WEEK',\n 'AMT_REQ_CREDIT_BUREAU_MON',\n 'AMT_REQ_CREDIT_BUREAU_QRT',\n 'AMT_REQ_CREDIT_BUREAU_YEAR',\n 'NAME_TYPE_SUITE',\n 'OBS_30_CNT_SOCIAL_CIRCLE',\n 'DEF_30_CNT_SOCIAL_CIRCLE',\n 'OBS_60_CNT_SOCIAL_CIRCLE',\n 'DEF_60_CNT_SOCIAL_CIRCLE',\n 'EXT_SOURCE_2',\n 'AMT_GOODS_PRICE',\n 'AMT_ANNUITY',\n 'CNT_FAM_MEMBERS',\n 'DAYS_LAST_PHONE_CHANGE']\n\n\n\n# 최소 0 /최대 1인 값을 찾는 mask (정규화 되어있는)\nmin_max_mask = (numeric_data.min() == 0) & (numeric_data.max() == 1)\n\ndf_min_max_0_1 = numeric_data.loc[:, min_max_mask]\ndf_min_max_0_1.head(5)\n\n\n\n\n\n\n\n\nTARGET\nFLAG_MOBIL\nFLAG_EMP_PHONE\nFLAG_WORK_PHONE\nFLAG_CONT_MOBILE\nFLAG_PHONE\nFLAG_EMAIL\nREG_REGION_NOT_LIVE_REGION\nREG_REGION_NOT_WORK_REGION\nLIVE_REGION_NOT_WORK_REGION\n...\nFLAG_DOCUMENT_12\nFLAG_DOCUMENT_13\nFLAG_DOCUMENT_14\nFLAG_DOCUMENT_15\nFLAG_DOCUMENT_16\nFLAG_DOCUMENT_17\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\n\n\n\n\n0\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n1\n0\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 76 columns\n\n\n\n\ndf_min_max_0_1.columns\n\nIndex(['TARGET', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n       'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL',\n       'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n       'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n       'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'APARTMENTS_AVG',\n       'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n       'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG',\n       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n       'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE',\n       'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE',\n       'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE',\n       'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n       'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI',\n       'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI',\n       'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE',\n       'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4',\n       'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n       'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10',\n       'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n       'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16',\n       'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19',\n       'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'],\n      dtype='object')\n\n\n\n# 앞서 만든 mask의 값과 다른(~) dataframe 구하기 (정규화되어있지 않은)\ndf_other = numeric_data.loc[:, ~min_max_mask]\ndf_other.head(5)\n\n\n\n\n\n\n\n\nSK_ID_CURR\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\nDAYS_REGISTRATION\n...\nDEF_30_CNT_SOCIAL_CIRCLE\nOBS_60_CNT_SOCIAL_CIRCLE\nDEF_60_CNT_SOCIAL_CIRCLE\nDAYS_LAST_PHONE_CHANGE\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\n100002\n0\n202500.0\n406597.5\n24700.5\n351000.0\n0.018801\n-9461\n-637\n-3648.0\n...\n2.0\n2.0\n2.0\n-1134.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n100003\n0\n270000.0\n1293502.5\n35698.5\n1129500.0\n0.003541\n-16765\n-1188\n-1186.0\n...\n0.0\n1.0\n0.0\n-828.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n100004\n0\n67500.0\n135000.0\n6750.0\n135000.0\n0.010032\n-19046\n-225\n-4260.0\n...\n0.0\n0.0\n0.0\n-815.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n100006\n0\n135000.0\n312682.5\n29686.5\n297000.0\n0.008019\n-19005\n-3039\n-9833.0\n...\n0.0\n2.0\n0.0\n-617.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n100007\n0\n121500.0\n513000.0\n21865.5\n513000.0\n0.028663\n-19932\n-3038\n-4311.0\n...\n0.0\n0.0\n0.0\n-1106.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 30 columns\n\n\n\n\ndf_other.columns\n\nIndex(['SK_ID_CURR', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',\n       'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE',\n       'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',\n       'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n       'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START',\n       'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n       'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n       'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n       'AMT_REQ_CREDIT_BUREAU_YEAR'],\n      dtype='object')\n\n\n\n정규화 된 vs 되지 않은 변수에 대한 처리 방안\n\n정규화된 변수(df_min_max_0_1.columns)는 도시, 아파트 등 거주지에 관련된 변수\n\n평균이나 0으로 대체할 수 있음\n아예 다른 -1 등으로 대체하여, 모델에게 일반적인 값이 아니라고 알려줄 수 있음\n\n정규화되지 않은 변수(df_other.columns)는 공통적이지 않아 위처럼 일괄적용은 불가\n\n무난하게 평균으로 대체(Imputation)\n\n\n\n\n# 정규화된 데이터(df_min_max_0_1) : -1로 채우기\ndf_min_max_0_1_filled = df_min_max_0_1.fillna(-1)\n\n# 정규화되지 않은 데이터( : )df_other) : 평균으로 채우기\ndf_other_filled = df_other.fillna(df_other.mean())\n\n# 두 데이터 다시 합치기\nnumeric_data_filled = pd.concat([df_min_max_0_1_filled, df_other_filled], axis = 1)\nnumeric_data_filled = numeric_data_filled[numeric_data.columns]\nnumeric_data_filled.head(5)\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\n100002\n1\n0\n202500.0\n406597.5\n24700.5\n351000.0\n0.018801\n-9461\n-637\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n1\n100003\n0\n0\n270000.0\n1293502.5\n35698.5\n1129500.0\n0.003541\n-16765\n-1188\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n100004\n0\n0\n67500.0\n135000.0\n6750.0\n135000.0\n0.010032\n-19046\n-225\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\n100006\n0\n0\n135000.0\n312682.5\n29686.5\n297000.0\n0.008019\n-19005\n-3039\n...\n0\n0\n0\n0\n0.006402\n0.007\n0.034362\n0.267395\n0.265474\n1.899974\n\n\n4\n100007\n0\n0\n121500.0\n513000.0\n21865.5\n513000.0\n0.028663\n-19932\n-3038\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n5 rows × 106 columns\n\n\n\n\n# 결측치 존재여부 확인하기 : 없음\nnumeric_data_filled.isna().sum()\n\nSK_ID_CURR                      0\nTARGET                          0\nCNT_CHILDREN                    0\nAMT_INCOME_TOTAL                0\nAMT_CREDIT                      0\nAMT_ANNUITY                     0\nAMT_GOODS_PRICE                 0\nREGION_POPULATION_RELATIVE      0\nDAYS_BIRTH                      0\nDAYS_EMPLOYED                   0\nDAYS_REGISTRATION               0\nDAYS_ID_PUBLISH                 0\nOWN_CAR_AGE                     0\nFLAG_MOBIL                      0\nFLAG_EMP_PHONE                  0\nFLAG_WORK_PHONE                 0\nFLAG_CONT_MOBILE                0\nFLAG_PHONE                      0\nFLAG_EMAIL                      0\nCNT_FAM_MEMBERS                 0\nREGION_RATING_CLIENT            0\nREGION_RATING_CLIENT_W_CITY     0\nHOUR_APPR_PROCESS_START         0\nREG_REGION_NOT_LIVE_REGION      0\nREG_REGION_NOT_WORK_REGION      0\nLIVE_REGION_NOT_WORK_REGION     0\nREG_CITY_NOT_LIVE_CITY          0\nREG_CITY_NOT_WORK_CITY          0\nLIVE_CITY_NOT_WORK_CITY         0\nEXT_SOURCE_1                    0\nEXT_SOURCE_2                    0\nEXT_SOURCE_3                    0\nAPARTMENTS_AVG                  0\nBASEMENTAREA_AVG                0\nYEARS_BEGINEXPLUATATION_AVG     0\nYEARS_BUILD_AVG                 0\nCOMMONAREA_AVG                  0\nELEVATORS_AVG                   0\nENTRANCES_AVG                   0\nFLOORSMAX_AVG                   0\nFLOORSMIN_AVG                   0\nLANDAREA_AVG                    0\nLIVINGAPARTMENTS_AVG            0\nLIVINGAREA_AVG                  0\nNONLIVINGAPARTMENTS_AVG         0\nNONLIVINGAREA_AVG               0\nAPARTMENTS_MODE                 0\nBASEMENTAREA_MODE               0\nYEARS_BEGINEXPLUATATION_MODE    0\nYEARS_BUILD_MODE                0\nCOMMONAREA_MODE                 0\nELEVATORS_MODE                  0\nENTRANCES_MODE                  0\nFLOORSMAX_MODE                  0\nFLOORSMIN_MODE                  0\nLANDAREA_MODE                   0\nLIVINGAPARTMENTS_MODE           0\nLIVINGAREA_MODE                 0\nNONLIVINGAPARTMENTS_MODE        0\nNONLIVINGAREA_MODE              0\nAPARTMENTS_MEDI                 0\nBASEMENTAREA_MEDI               0\nYEARS_BEGINEXPLUATATION_MEDI    0\nYEARS_BUILD_MEDI                0\nCOMMONAREA_MEDI                 0\nELEVATORS_MEDI                  0\nENTRANCES_MEDI                  0\nFLOORSMAX_MEDI                  0\nFLOORSMIN_MEDI                  0\nLANDAREA_MEDI                   0\nLIVINGAPARTMENTS_MEDI           0\nLIVINGAREA_MEDI                 0\nNONLIVINGAPARTMENTS_MEDI        0\nNONLIVINGAREA_MEDI              0\nTOTALAREA_MODE                  0\nOBS_30_CNT_SOCIAL_CIRCLE        0\nDEF_30_CNT_SOCIAL_CIRCLE        0\nOBS_60_CNT_SOCIAL_CIRCLE        0\nDEF_60_CNT_SOCIAL_CIRCLE        0\nDAYS_LAST_PHONE_CHANGE          0\nFLAG_DOCUMENT_2                 0\nFLAG_DOCUMENT_3                 0\nFLAG_DOCUMENT_4                 0\nFLAG_DOCUMENT_5                 0\nFLAG_DOCUMENT_6                 0\nFLAG_DOCUMENT_7                 0\nFLAG_DOCUMENT_8                 0\nFLAG_DOCUMENT_9                 0\nFLAG_DOCUMENT_10                0\nFLAG_DOCUMENT_11                0\nFLAG_DOCUMENT_12                0\nFLAG_DOCUMENT_13                0\nFLAG_DOCUMENT_14                0\nFLAG_DOCUMENT_15                0\nFLAG_DOCUMENT_16                0\nFLAG_DOCUMENT_17                0\nFLAG_DOCUMENT_18                0\nFLAG_DOCUMENT_19                0\nFLAG_DOCUMENT_20                0\nFLAG_DOCUMENT_21                0\nAMT_REQ_CREDIT_BUREAU_HOUR      0\nAMT_REQ_CREDIT_BUREAU_DAY       0\nAMT_REQ_CREDIT_BUREAU_WEEK      0\nAMT_REQ_CREDIT_BUREAU_MON       0\nAMT_REQ_CREDIT_BUREAU_QRT       0\nAMT_REQ_CREDIT_BUREAU_YEAR      0\ndtype: int64\n\n\n\n\n표준화 / 정규화\n\n현재 이용중인 데이터는 정규화가 되어있어, 더미 데이터로 정규화 실습\n\n\n# 필요한 라이브러리 임포트\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# 1. 데이터 생성\n# make_classification을 사용해 극단적인 스케일 차이를 가진 데이터를 생성\nX_extreme, y_extreme = make_classification(\n    n_samples=10000,            # 데이터 샘플 수\n    n_features=40,             # 전체 피처 수\n    n_informative=10,           # 유용한 피처 수\n    scale=[10, 10000, 0.1, 0.001] * 10,  # 피처별 스케일 조정\n    random_state=42            # 재현 가능성을 위한 시드값\n)\n\n\npd.DataFrame(X_extreme).describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n0\n10000.0\n-0.000843\n0.099991\n-0.400455\n-0.068812\n-9.649203e-04\n0.065022\n0.345748\n\n\n1\n10000.0\n-0.049159\n0.203922\n-1.028082\n-0.186545\n-4.998679e-02\n0.087477\n0.696282\n\n\n2\n10000.0\n0.098310\n9.967478\n-37.084345\n-6.431134\n2.068342e-01\n6.805382\n36.569668\n\n\n3\n10000.0\n0.000092\n0.100792\n-0.376615\n-0.068290\n1.600637e-03\n0.066608\n0.370791\n\n\n4\n10000.0\n33.711066\n9968.009346\n-37924.222059\n-6660.572565\n2.665940e+01\n6718.807581\n38249.915049\n\n\n5\n10000.0\n-0.014777\n10.035188\n-44.042140\n-6.791118\n-1.207864e-03\n6.879935\n34.238262\n\n\n6\n10000.0\n0.004445\n0.348834\n-1.327898\n-0.224403\n4.139092e-03\n0.240916\n1.279965\n\n\n7\n10000.0\n-29.276123\n9901.953926\n-34920.171700\n-6780.256223\n5.289047e+01\n6660.789736\n35896.144788\n\n\n8\n10000.0\n5012.027982\n20322.028651\n-72656.674262\n-8177.712578\n5.592046e+03\n19130.519205\n87719.082190\n\n\n9\n10000.0\n5.235305\n21.577716\n-94.872061\n-8.381620\n6.028815e+00\n19.905182\n86.318571\n\n\n10\n10000.0\n-16.717119\n22438.781080\n-94469.882979\n-14865.590516\n7.735623e+02\n15595.529638\n82303.951337\n\n\n11\n10000.0\n-0.000006\n0.001006\n-0.004374\n-0.000688\n-2.374679e-07\n0.000671\n0.003626\n\n\n12\n10000.0\n105.978945\n10155.422965\n-37826.155102\n-6811.010474\n6.823802e+01\n6998.192954\n45621.147236\n\n\n13\n10000.0\n-0.001012\n0.210320\n-0.858273\n-0.143428\n5.860082e-04\n0.140071\n0.889246\n\n\n14\n10000.0\n-4858.457290\n21738.300654\n-94748.760660\n-19030.662537\n-5.041786e+03\n9388.088922\n78861.853678\n\n\n15\n10000.0\n-0.000007\n0.000999\n-0.004067\n-0.000675\n-5.164186e-06\n0.000670\n0.003677\n\n\n16\n10000.0\n-5.141587\n17.738662\n-71.054112\n-17.153315\n-5.349305e+00\n7.050539\n55.128501\n\n\n17\n10000.0\n-0.000369\n0.100389\n-0.348579\n-0.067523\n-3.805544e-04\n0.067480\n0.367819\n\n\n18\n10000.0\n-0.000006\n0.000996\n-0.003884\n-0.000686\n-1.293092e-05\n0.000666\n0.003644\n\n\n19\n10000.0\n-0.000172\n0.099557\n-0.353209\n-0.067392\n8.681804e-04\n0.067532\n0.340875\n\n\n20\n10000.0\n-0.000009\n0.098640\n-0.400360\n-0.066536\n5.128733e-04\n0.066323\n0.345383\n\n\n21\n10000.0\n-0.000014\n0.000989\n-0.003470\n-0.000682\n-2.495327e-05\n0.000648\n0.003575\n\n\n22\n10000.0\n58.343623\n9989.324997\n-37011.053323\n-6603.388513\n-2.738199e+01\n6687.434189\n37550.968936\n\n\n23\n10000.0\n-0.191571\n22.000080\n-76.276131\n-15.251958\n-1.285423e+00\n14.269217\n93.037809\n\n\n24\n10000.0\n-94.852822\n9900.227203\n-39993.322490\n-6837.457775\n-1.868981e+01\n6573.974126\n38262.551726\n\n\n25\n10000.0\n0.000006\n0.001006\n-0.003842\n-0.000672\n8.375875e-06\n0.000691\n0.004219\n\n\n26\n10000.0\n0.000290\n0.003541\n-0.012334\n-0.002022\n2.878352e-04\n0.002610\n0.015225\n\n\n27\n10000.0\n0.000841\n0.099419\n-0.399701\n-0.065312\n1.257511e-04\n0.067327\n0.354844\n\n\n28\n10000.0\n-0.088997\n9.912015\n-34.888492\n-6.821070\n-1.745981e-01\n6.518458\n37.321065\n\n\n29\n10000.0\n0.000400\n0.099395\n-0.356424\n-0.066818\n1.730892e-04\n0.066774\n0.386391\n\n\n30\n10000.0\n-79.666444\n10110.197747\n-37110.587943\n-6797.004214\n-2.879375e+01\n6750.676244\n37841.363336\n\n\n31\n10000.0\n0.112263\n10.151527\n-37.939557\n-6.815460\n6.569630e-02\n7.061795\n42.020259\n\n\n32\n10000.0\n0.059398\n10.005156\n-37.291744\n-6.652975\n1.770254e-01\n6.828899\n39.181850\n\n\n33\n10000.0\n-0.000011\n0.001012\n-0.003814\n-0.000696\n-1.729425e-05\n0.000668\n0.003775\n\n\n34\n10000.0\n0.112279\n9.979008\n-35.699687\n-6.730720\n1.131022e-01\n6.848352\n39.768682\n\n\n35\n10000.0\n0.000514\n0.001720\n-0.005501\n-0.000699\n4.901425e-04\n0.001649\n0.008263\n\n\n36\n10000.0\n-0.000020\n0.002119\n-0.007758\n-0.001392\n1.818282e-04\n0.001455\n0.006503\n\n\n37\n10000.0\n201.890197\n9859.993885\n-36573.006696\n-6408.351214\n1.730065e+02\n6872.631552\n34511.089264\n\n\n38\n10000.0\n-0.055936\n10.030580\n-37.936384\n-6.934862\n-5.052432e-02\n6.753313\n38.782169\n\n\n39\n10000.0\n-0.000004\n0.001007\n-0.004414\n-0.000690\n-1.970009e-05\n0.000669\n0.003569\n\n\n\n\n\n\n\n\n변수별로 scale차이가 큰 데이터가 만들어졌고, 로지스틱 회귀 모델에 넣어볼 예정\n표준화와 정규화 부분에 집중해서 보기(나머지는 추후 배움)\n표준화는 sklearn.preprocessing의 StandardScaler로, 정규화는 MinMaxScaler로 수행\n\n\n# 2. 데이터 분리\n# 학습 데이터와 테스트 데이터로 분리\nX_train_extreme, X_test_extreme, y_train_extreme, y_test_extreme = train_test_split(\n    X_extreme, y_extreme, test_size=0.3, random_state=42\n)\n\n# 3. 로지스틱 회귀 모델 초기화\nmodel_lr_extreme = LogisticRegression(random_state=42, max_iter=1000)\n\n# 4. 원본 데이터로 예측 (Raw Data)\n# 스케일링 없이 원본 데이터를 사용해 모델 학습\nmodel_lr_extreme.fit(X_train_extreme, y_train_extreme)\ny_pred_raw_extreme_lr = model_lr_extreme.predict(X_test_extreme)\naccuracy_raw_extreme_lr = accuracy_score(y_test_extreme, y_pred_raw_extreme_lr)\n\n# 5. 표준화된 데이터로 예측 (Standardized Data)\n# 표준화를 적용한 데이터를 사용해 모델 학습\nscaler_standard = StandardScaler()\nX_train_extreme_standardized = scaler_standard.fit_transform(X_train_extreme) # fit을 통해 0~4의 값이 min/max 0/4임을 구하고, transform으로 0~1로 변경\nX_test_extreme_standardized = scaler_standard.transform(X_test_extreme)\n\nmodel_lr_extreme.fit(X_train_extreme_standardized, y_train_extreme)\ny_pred_standardized_extreme_lr = model_lr_extreme.predict(X_test_extreme_standardized)\naccuracy_standardized_extreme_lr = accuracy_score(y_test_extreme, y_pred_standardized_extreme_lr)\n\n# 6. 정규화된 데이터로 예측 (Normalized Data)\n# 정규화를 적용한 데이터를 사용해 모델 학습\nscaler_minmax = MinMaxScaler()\nX_train_extreme_normalized = scaler_minmax.fit_transform(X_train_extreme)\nX_test_extreme_normalized = scaler_minmax.transform(X_test_extreme)\n\nmodel_lr_extreme.fit(X_train_extreme_normalized, y_train_extreme)\ny_pred_normalized_extreme_lr = model_lr_extreme.predict(X_test_extreme_normalized)\naccuracy_normalized_extreme_lr = accuracy_score(y_test_extreme, y_pred_normalized_extreme_lr)\n\n# 7. 결과 비교\n# 결과를 데이터프레임으로 정리\nresults_extreme_lr = {\n    \"Method\": [\"Raw Data\", \"Standardized Data\", \"Normalized Data\"],\n    \"Accuracy\": [accuracy_raw_extreme_lr, accuracy_standardized_extreme_lr, accuracy_normalized_extreme_lr],\n}\n\nresults_extreme_lr_df = pd.DataFrame(results_extreme_lr)\nresults_extreme_lr_df\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\n\n\n\n\nMethod\nAccuracy\n\n\n\n\n0\nRaw Data\n0.692333\n\n\n1\nStandardized Data\n0.803000\n\n\n2\nNormalized Data\n0.803667\n\n\n\n\n\n\n\n\n\n변환\n\n앞서 확인한 사실 : 일부 데이터의 치우침(왜도), 이상치, DAYS_EMPLOYED를 제외하면 전부 양수\n위와 같은 경우 변환을 할 수 있으며, 로그변환과 제곱근 변환 중 일반적으로 쓰이는 로그변환을 사용\nnp.log1p를 사용 : log(x + 1)를 정밀하게 연산하는 함수로, 값이 0에 가까울수록 무한대로 발산하는 것을 막기위해 log1p를 사용\n\n손실없이 계산하기 위해 log1p를 사용\n\n\n\noutliers_over_10\n\n\n\n\n\n\n\n\ntotal\npercent\n\n\n\n\nREGION_RATING_CLIENT\n80527\n26.186706\n\n\nREGION_RATING_CLIENT_W_CITY\n78027\n25.373726\n\n\nDAYS_EMPLOYED\n72217\n23.484363\n\n\nREG_CITY_NOT_WORK_CITY\n70867\n23.045354\n\n\nFLAG_WORK_PHONE\n61308\n19.936848\n\n\nFLAG_EMP_PHONE\n55386\n18.011063\n\n\nLIVE_CITY_NOT_WORK_CITY\n55215\n17.955455\n\n\nAMT_REQ_CREDIT_BUREAU_QRT\n50575\n16.446566\n\n\nAMT_REQ_CREDIT_BUREAU_MON\n43759\n14.230060\n\n\nDEF_30_CNT_SOCIAL_CIRCLE\n35166\n11.435688\n\n\n\n\n\n\n\n\nlog_transform_columns = outliers_over_10.index.tolist()\nlog_transform_columns.remove('DAYS_EMPLOYED')\nlog_transform_columns\n\n['REGION_RATING_CLIENT',\n 'REGION_RATING_CLIENT_W_CITY',\n 'REG_CITY_NOT_WORK_CITY',\n 'FLAG_WORK_PHONE',\n 'FLAG_EMP_PHONE',\n 'LIVE_CITY_NOT_WORK_CITY',\n 'AMT_REQ_CREDIT_BUREAU_QRT',\n 'AMT_REQ_CREDIT_BUREAU_MON',\n 'DEF_30_CNT_SOCIAL_CIRCLE']\n\n\n\nnumeric_data_filled[log_transform_columns]  = numeric_data_filled[log_transform_columns].apply(lambda x : np.log1p(x))\n\n\nnumeric_data_filled.head(5)\n\n\n\n\n\n\n\n\nSK_ID_CURR\nTARGET\nCNT_CHILDREN\nAMT_INCOME_TOTAL\nAMT_CREDIT\nAMT_ANNUITY\nAMT_GOODS_PRICE\nREGION_POPULATION_RELATIVE\nDAYS_BIRTH\nDAYS_EMPLOYED\n...\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_YEAR\n\n\n\n\n0\n100002\n1\n0\n202500.0\n406597.5\n24700.5\n351000.0\n0.018801\n-9461\n-637\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n1\n100003\n0\n0\n270000.0\n1293502.5\n35698.5\n1129500.0\n0.003541\n-16765\n-1188\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n100004\n0\n0\n67500.0\n135000.0\n6750.0\n135000.0\n0.010032\n-19046\n-225\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\n100006\n0\n0\n135000.0\n312682.5\n29686.5\n297000.0\n0.008019\n-19005\n-3039\n...\n0\n0\n0\n0\n0.006402\n0.007\n0.034362\n0.236964\n0.235447\n1.899974\n\n\n4\n100007\n0\n0\n121500.0\n513000.0\n21865.5\n513000.0\n0.028663\n-19932\n-3038\n...\n0\n0\n0\n0\n0.000000\n0.000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n5 rows × 106 columns\n\n\n\n\n\n실습 5\n\nDAYS_EMPOLYED 같이 음수의 값을 갖는 경우, 최소값 조정이라는 걸 통해 변환해줄 수 있습니다.\n쉽게 말해 최소값의 절대값보다 큰 수 C를 모든 데이터에 더하는 걸 말합니다.\n구현한 뒤에 로그변환을 해봅시다.\n\n\n# 최소값 조정 : numeric_data_filled['DAYS_EMPLOYED'].min() * -1\n(numeric_data_filled['DAYS_EMPLOYED'] + numeric_data_filled['DAYS_EMPLOYED'].min() * -1).apply(lambda x : np.log1p(x)).head(5)\n\n0    9.757074\n1    9.724660\n2    9.780642\n3    9.607370\n4    9.607437\nName: DAYS_EMPLOYED, dtype: float64\n\n\n\n\n인코딩\n\n각 카테고리의 값을 숫자로 바꾸는 Label인코딩은 순서에 대한 왜곡 발생\n\n메모리, 계산 부담이 없는 경우는 One-hot 인코딩이 이론상 더 성능이 좋음\n범주간의 순서가 존재하는 경우는 Label 인코딩을 하는게 오히려 좋음\nUnique한 값이 많으면 One-hot인코딩은 메모리 비효율 발생\n\nsklearn.preprocessing의 OneHotEncoder으로 Ont-hot 인코딩\n\n\n# One-hot 인코딩 대상 컬럼\n\none_hot_columns = categorical_data.describe().transpose()[categorical_data.describe().transpose().unique &lt;= 3].index.tolist()\none_hot_columns\n\n['NAME_CONTRACT_TYPE',\n 'CODE_GENDER',\n 'FLAG_OWN_CAR',\n 'FLAG_OWN_REALTY',\n 'HOUSETYPE_MODE',\n 'EMERGENCYSTATE_MODE']\n\n\n\n# One-hot 인코딩(fit_transform)을 통해, 각 카테고리의 값이  컬럼으로 올라감\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\nonehot_encoded = ohe.fit_transform(categorical_data[one_hot_columns])\nencoded_col_names = ohe.get_feature_names_out(input_features=one_hot_columns)\n# get_feature_names_out : ont-hot 인코딩된 컬럼을 반환\n\nonehot_df = pd.DataFrame(onehot_encoded, columns=encoded_col_names, index=categorical_data.index)\nonehot_df.head(5)\n\n\n\n\n\n\n\n\nNAME_CONTRACT_TYPE_Cash loans\nNAME_CONTRACT_TYPE_Revolving loans\nCODE_GENDER_F\nCODE_GENDER_M\nCODE_GENDER_XNA\nFLAG_OWN_CAR_N\nFLAG_OWN_CAR_Y\nFLAG_OWN_REALTY_N\nFLAG_OWN_REALTY_Y\nHOUSETYPE_MODE_block of flats\nHOUSETYPE_MODE_specific housing\nHOUSETYPE_MODE_terraced house\nHOUSETYPE_MODE_nan\nEMERGENCYSTATE_MODE_No\nEMERGENCYSTATE_MODE_Yes\nEMERGENCYSTATE_MODE_nan\n\n\n\n\n0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n3\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n순위가 불명확하거나 값이 다양한 경우 : Label Encoder 사용\nsklearn.preprocessing의 LabelEncoder로 Label인코딩\n\n컬럼별로 인코딩되며, 한번에 fit_transform불가\n벡터연산으로 적용해보기(.apply())\n\n\n\n# Label인코딩 대상 컬럼\n\nle_columns = categorical_data.describe().transpose()[categorical_data.describe().transpose().unique &gt; 3].index.tolist()\nle_columns.remove('NAME_EDUCATION_TYPE')\nle_columns\n\n['NAME_TYPE_SUITE',\n 'NAME_INCOME_TYPE',\n 'NAME_FAMILY_STATUS',\n 'NAME_HOUSING_TYPE',\n 'OCCUPATION_TYPE',\n 'WEEKDAY_APPR_PROCESS_START',\n 'ORGANIZATION_TYPE',\n 'FONDKAPREMONT_MODE',\n 'WALLSMATERIAL_MODE']\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_df = categorical_data[le_columns].apply(le.fit_transform)\nle_df.head(5)\n\n\n\n\n\n\n\n\nNAME_TYPE_SUITE\nNAME_INCOME_TYPE\nNAME_FAMILY_STATUS\nNAME_HOUSING_TYPE\nOCCUPATION_TYPE\nWEEKDAY_APPR_PROCESS_START\nORGANIZATION_TYPE\nFONDKAPREMONT_MODE\nWALLSMATERIAL_MODE\n\n\n\n\n0\n6\n7\n3\n1\n8\n6\n5\n2\n5\n\n\n1\n1\n4\n1\n1\n3\n1\n39\n2\n0\n\n\n2\n6\n7\n3\n1\n8\n1\n11\n4\n7\n\n\n3\n6\n7\n0\n1\n8\n6\n5\n4\n7\n\n\n4\n6\n7\n3\n1\n3\n4\n37\n4\n7\n\n\n\n\n\n\n\n\n\n실습6\n\n지금도 괜찮긴 한데, 이러면 나중에 저 값이 뭔지 궁금해도 찾아볼 수 없는 문제가 생깁니다.\nfit과 transform을 나눠서 적용하면 어떻게 될 것도 같은데, 해볼까요?\n\n\n# 고민해 본 방법1 : 각 딕셔너리에 .classes_를 활용해 보존 (GPT로 구현)\nfrom sklearn.preprocessing import LabelEncoder\n\n# 1. 각 열마다 별도의 LabelEncoder 생성\nlabel_encoders = {col: LabelEncoder() for col in le_columns}\n\n# 2. fit: LabelEncoder를 각 열에 학습\nle_df_encoder = categorical_data[le_columns].apply(\n    lambda col: label_encoders[col.name].fit(col)\n)\n\n# 학습된 카테고리와 매핑된 값 저장\ndict_classes = {}\nfor col in le_columns:\n    # 레이블과 변환된 값의 매핑 저장\n    dict_classes[col] = {label: idx for idx, label in enumerate(label_encoders[col].classes_)}\n\n# 3. transform: 학습된 LabelEncoder를 각 열에 적용\nle_df_value = categorical_data[le_columns].apply(\n    lambda col: label_encoders[col.name].transform(col)\n)\n\n# 결과 확인\ndict_classes\n\n{'NAME_TYPE_SUITE': {'Children': 0,\n  'Family': 1,\n  'Group of people': 2,\n  'Other_A': 3,\n  'Other_B': 4,\n  'Spouse, partner': 5,\n  'Unaccompanied': 6,\n  nan: 7},\n 'NAME_INCOME_TYPE': {'Businessman': 0,\n  'Commercial associate': 1,\n  'Maternity leave': 2,\n  'Pensioner': 3,\n  'State servant': 4,\n  'Student': 5,\n  'Unemployed': 6,\n  'Working': 7},\n 'NAME_FAMILY_STATUS': {'Civil marriage': 0,\n  'Married': 1,\n  'Separated': 2,\n  'Single / not married': 3,\n  'Unknown': 4,\n  'Widow': 5},\n 'NAME_HOUSING_TYPE': {'Co-op apartment': 0,\n  'House / apartment': 1,\n  'Municipal apartment': 2,\n  'Office apartment': 3,\n  'Rented apartment': 4,\n  'With parents': 5},\n 'OCCUPATION_TYPE': {'Accountants': 0,\n  'Cleaning staff': 1,\n  'Cooking staff': 2,\n  'Core staff': 3,\n  'Drivers': 4,\n  'HR staff': 5,\n  'High skill tech staff': 6,\n  'IT staff': 7,\n  'Laborers': 8,\n  'Low-skill Laborers': 9,\n  'Managers': 10,\n  'Medicine staff': 11,\n  'Private service staff': 12,\n  'Realty agents': 13,\n  'Sales staff': 14,\n  'Secretaries': 15,\n  'Security staff': 16,\n  'Waiters/barmen staff': 17,\n  nan: 18},\n 'WEEKDAY_APPR_PROCESS_START': {'FRIDAY': 0,\n  'MONDAY': 1,\n  'SATURDAY': 2,\n  'SUNDAY': 3,\n  'THURSDAY': 4,\n  'TUESDAY': 5,\n  'WEDNESDAY': 6},\n 'ORGANIZATION_TYPE': {'Advertising': 0,\n  'Agriculture': 1,\n  'Bank': 2,\n  'Business Entity Type 1': 3,\n  'Business Entity Type 2': 4,\n  'Business Entity Type 3': 5,\n  'Cleaning': 6,\n  'Construction': 7,\n  'Culture': 8,\n  'Electricity': 9,\n  'Emergency': 10,\n  'Government': 11,\n  'Hotel': 12,\n  'Housing': 13,\n  'Industry: type 1': 14,\n  'Industry: type 10': 15,\n  'Industry: type 11': 16,\n  'Industry: type 12': 17,\n  'Industry: type 13': 18,\n  'Industry: type 2': 19,\n  'Industry: type 3': 20,\n  'Industry: type 4': 21,\n  'Industry: type 5': 22,\n  'Industry: type 6': 23,\n  'Industry: type 7': 24,\n  'Industry: type 8': 25,\n  'Industry: type 9': 26,\n  'Insurance': 27,\n  'Kindergarten': 28,\n  'Legal Services': 29,\n  'Medicine': 30,\n  'Military': 31,\n  'Mobile': 32,\n  'Other': 33,\n  'Police': 34,\n  'Postal': 35,\n  'Realtor': 36,\n  'Religion': 37,\n  'Restaurant': 38,\n  'School': 39,\n  'Security': 40,\n  'Security Ministries': 41,\n  'Self-employed': 42,\n  'Services': 43,\n  'Telecom': 44,\n  'Trade: type 1': 45,\n  'Trade: type 2': 46,\n  'Trade: type 3': 47,\n  'Trade: type 4': 48,\n  'Trade: type 5': 49,\n  'Trade: type 6': 50,\n  'Trade: type 7': 51,\n  'Transport: type 1': 52,\n  'Transport: type 2': 53,\n  'Transport: type 3': 54,\n  'Transport: type 4': 55,\n  'University': 56,\n  'XNA': 57},\n 'FONDKAPREMONT_MODE': {'not specified': 0,\n  'org spec account': 1,\n  'reg oper account': 2,\n  'reg oper spec account': 3,\n  nan: 4},\n 'WALLSMATERIAL_MODE': {'Block': 0,\n  'Mixed': 1,\n  'Monolithic': 2,\n  'Others': 3,\n  'Panel': 4,\n  'Stone, brick': 5,\n  'Wooden': 6,\n  nan: 7}}\n\n\n\n# 고민해 본 방법2 : joblib로 인코더 저장 (타 프로젝트에서 해 본 방법)\n\nimport joblib\n\n# 0. 인코딩 전 데이터의 X, Y 확인\nprint(f\"\"\"X : {categorical_data[le_columns].index}\n      Y : {data['TARGET'].index}\n      \"\"\")\n\n# 1. 각 열마다 별도의 LabelEncoder 생성 및 학습\nlabel_encoders = {}\nfor col in le_columns:\n    le = LabelEncoder()\n    le.fit(categorical_data[col])  # LabelEncoder 학습\n    label_encoders[col] = le      # 학습된 인코더를 딕셔너리에 저장\n\n# 2. LabelEncoder 딕셔너리를 joblib으로 저장\njoblib.dump(label_encoders, 'label_encoders.pkl')\n\n# 3. 저장된 LabelEncoder를 사용해 transform 적용\nlabel_encoders_loaded = joblib.load('label_encoders.pkl')  # 인코더 불러오기\nencoded_data = categorical_data[le_columns].apply(\n    lambda col: label_encoders_loaded[col.name].transform(col)\n)\n\n# 결과 확인\nencoded_data.head(5)\n\nX : RangeIndex(start=0, stop=307511, step=1)\n      Y : RangeIndex(start=0, stop=307511, step=1)\n      \n\n\n\n\n\n\n\n\n\nNAME_TYPE_SUITE\nNAME_INCOME_TYPE\nNAME_FAMILY_STATUS\nNAME_HOUSING_TYPE\nOCCUPATION_TYPE\nWEEKDAY_APPR_PROCESS_START\nORGANIZATION_TYPE\nFONDKAPREMONT_MODE\nWALLSMATERIAL_MODE\n\n\n\n\n0\n6\n7\n3\n1\n8\n6\n5\n2\n5\n\n\n1\n1\n4\n1\n1\n3\n1\n39\n2\n0\n\n\n2\n6\n7\n3\n1\n8\n1\n11\n4\n7\n\n\n3\n6\n7\n0\n1\n8\n6\n5\n4\n7\n\n\n4\n6\n7\n3\n1\n3\n4\n37\n4\n7\n\n\n\n\n\n\n\n\n# 고민해 본 방법2-2 : joblib로 저장한 인코더 로딩하는 예제\n\nencoder_label = joblib.load('encoder_label.pkl')\nx_data_all = encoder_label.transform(df_dropped[x_column], df_dropped[y_column])\n\n\n# 답\n\n## 각 컬럼에 대한 LabelEncoder를 저장할 딕셔너리\nencoders = {}\n\n## apply를 사용해 벡터 연산으로 처리\ndef encode_column(column):\n    le = LabelEncoder()\n    encoders[column.name] = le  # 컬럼명을 key로 LabelEncoder 저장\n    return le.fit_transform(column)\n\ndf_encoded = categorical_data[le_columns].apply(encode_column)\n\n\n순위를 반영해야하는 경우 Ordinal인코딩\n\n순위가 명확한 학벌에 대해 적용\nOrder는 낮은 → 높은 순서로 반영\n\n\n\nnp.unique(categorical_data['NAME_EDUCATION_TYPE'])\n\narray(['Academic degree', 'Higher education', 'Incomplete higher',\n       'Lower secondary', 'Secondary / secondary special'], dtype=object)\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\norder = [\n    'Lower secondary',\n    'Secondary / secondary special',\n    'Incomplete higher',\n    'Higher education',\n    'Academic degree'\n]\n\noe = OrdinalEncoder(categories=[order])\noe_df = oe.fit_transform(categorical_data[['NAME_EDUCATION_TYPE']], )\noe_df = pd.Series(oe_df.flatten(), index=categorical_data.index, name='NAME_EDUCATION_TYPE_Encoded')\n\noe_df.head(5)\n\n0    1.0\n1    3.0\n2    1.0\n3    1.0\n4    1.0\nName: NAME_EDUCATION_TYPE_Encoded, dtype: float64\n\n\n\n\n구간화\n\n구간화를 할 만큼 제반지식(도메인)이 충분하지 않으므로, 임의의 더미 데이터로 구간화 실습\nsklearn.preprocessing의 KBinsDiscretizer\n아래 코드데이터를 보면, 실 성능에 도움이 되지 않을 수 있음\n  Accuracy without binning: 0.9600\n  Accuracy with binning: 0.9567\n구간화가 주는 메모리적 이득이 크지 않다면, 적용하는 것을 신중히 고려\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# 신용점수 데이터 생성\nnp.random.seed(42)\nnum_samples = 1000\ncredit_score = np.random.uniform(0, 1000, num_samples)\nannual_income = np.random.uniform(20_000, 100_000, num_samples)\n\n# 신용점수와 대출 승인 여부 설계\ntarget_credit = np.where(credit_score &gt;= 600, 1, 0)\nrandom_noise = np.random.choice([0, 1], size=num_samples, p=[0.2, 0.8])\ntarget_credit = np.where(random_noise == 0, 1 - target_credit, target_credit)\n\n# 더미 데이터 생성 (make_classification)\nX_dummy, y_dummy = make_classification(\n    n_samples=num_samples,         # 샘플 수\n    n_features=5,                  # 총 피처 수\n    n_informative=3,               # 유의미한 피처 수\n    n_redundant=2,                 # 중복된 피처 수\n    n_classes=2,                   # 클래스 수\n    random_state=42,               # 재현성을 위한 랜덤 시드\n    class_sep=1.0                  # 클래스 간 분리 정도\n)\n\n# 신용점수 데이터와 더미 데이터 결합\ndata = pd.DataFrame(X_dummy, columns=[f\"dummy_feature_{i+1}\" for i in range(X_dummy.shape[1])])\ndata[\"credit_score\"] = credit_score\ndata[\"annual_income\"] = annual_income\n\ndata[\"target\"] = np.where(target_credit == y_dummy, target_credit, y_dummy)  # 타겟 혼합\n\n# 데이터 분리\nX = data.drop(columns=[\"target\"])\ny = data[\"target\"]\n\n# 구간화를 적용하지 않은 경우\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\nnon_binned_preds = model.predict(X_test)\nnon_binned_accuracy = accuracy_score(y_test, non_binned_preds)\n\n# 신용점수에 구간화 적용 (고위험: 0~400, 중위험: 400~700, 저위험: 700~1000)\nbinner = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\nX_binned = X.copy()\nX_binned[\"credit_score\"] = binner.fit_transform(X[[\"credit_score\"]]).flatten()\n\n# 데이터 분리 (구간화된 데이터)\nX_train_binned, X_test_binned, y_train, y_test = train_test_split(X_binned, y, test_size=0.3, random_state=42)\nmodel_binned = RandomForestClassifier(random_state=42)\nmodel_binned.fit(X_train_binned, y_train)\nbinned_preds = model_binned.predict(X_test_binned)\nbinned_accuracy = accuracy_score(y_test, binned_preds)\n\n# 결과 출력\nprint(f\"Accuracy without binning: {non_binned_accuracy:.4f}\")\nprint(f\"Accuracy with binning: {binned_accuracy:.4f}\")\n\nAccuracy without binning: 0.9600\nAccuracy with binning: 0.9567\n\n\n\n\n과제\n\n개인/팀에 따라 주어진 과제 데이터에 대해, EDA와 전처리 진행 후, 데이터마트(csv파일) 만들기\n데이터를 처음보는(모르는) 사람에게 잘 설명하는 것이 앞으로의 (분석)업무임을 생각하며 진행\n위의 과정이 들어간 ipynb파일을 과제로 제출(+만든 csv파일은 계속 활용할 것이므로 잘 가지고 있기)\n과제 유형\n\n단체 과제 : Home Credit Default Risk (주택담보대출)\n\nhttps://www.kaggle.com/c/home-credit-default-risk/overview\n특징\n\n실제 현업처럼 데이터가 나뉘어 있음\n(분석용 마트를 만들기 위해)먼저 Merge를 한 후, 개인별로 EDA를 하는 경우가 많음\n이 과제의 모든 데이터를 쓸 필요는 없음 (좀 더 보고싶은 변수들을 Merge)\n\napplication_{train|test}.csv 파일만으로도 (y값)제출 진행 가능함\n\n인코딩을 제외하고는 필수 전처리는 아님. 다만 좋은 결과를 위해서는 추가해야 함\n기존에 소개한 산탄데르 과제는 너무 과한 과제로 보여 제외함\n\nhttps://www.kaggle.com/competitions/santander-customer-satisfaction/overview\n\n\n\n개인 과제 : 월간 데이콘 신용카드 사용자 연체 예측 AI 경진대회 (신용카드 연체예측)\n\nhttps://dacon.io/competitions/official/235713/overview/description\n특징\n\n단체 과제보다 컬럼의 수가 적음"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823_1/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240823_1/index.html",
    "title": "[DE스터디/2주차과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark (데이터셋 확인 및 스키마 저장/작성 실습)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터셋-확인-및-스키마-고민하기",
    "href": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터셋-확인-및-스키마-고민하기",
    "title": "[DE스터디/2주차과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터셋 확인 및 스키마 고민하기",
    "text": "데이터셋 확인 및 스키마 고민하기\n\n아래의 순서로 진행\n\nspark가 확인한 스키마를 json으로 저장\n저장된 스키마 확인 후, 수정 필요한 부분만 수정\n해당 json으로 spark.read.schema(schema_to_read).json(\"../data/*.json.gz\")해서 파일 읽고 확인하기\n저장한 스키마를 json파일로 저장하기\n\n\n\n스키마 확인, json변환\n\n# 최초 로딩 및 printSchema\ngithub = spark.read.json(\"../data/*.json.gz\")\ngithub.printSchema()\n\n                                                                                \n\n\nroot\n |-- actor: struct (nullable = true)\n |    |-- avatar_url: string (nullable = true)\n |    |-- display_login: string (nullable = true)\n |    |-- gravatar_id: string (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- login: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- created_at: string (nullable = true)\n |-- id: string (nullable = true)\n |-- org: struct (nullable = true)\n |    |-- avatar_url: string (nullable = true)\n |    |-- gravatar_id: string (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- login: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- payload: struct (nullable = true)\n |    |-- action: string (nullable = true)\n |    |-- before: string (nullable = true)\n |    |-- comment: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- commit_id: string (nullable = true)\n |    |    |-- created_at: string (nullable = true)\n |    |    |-- diff_hunk: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- in_reply_to_id: long (nullable = true)\n |    |    |-- issue_url: string (nullable = true)\n |    |    |-- line: long (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- original_commit_id: string (nullable = true)\n |    |    |-- original_line: long (nullable = true)\n |    |    |-- original_position: long (nullable = true)\n |    |    |-- original_start_line: long (nullable = true)\n |    |    |-- path: string (nullable = true)\n |    |    |-- performed_via_github_app: struct (nullable = true)\n |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- events: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- external_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- permissions: struct (nullable = true)\n |    |    |    |    |-- actions: string (nullable = true)\n |    |    |    |    |-- actions_variables: string (nullable = true)\n |    |    |    |    |-- administration: string (nullable = true)\n |    |    |    |    |-- attestations: string (nullable = true)\n |    |    |    |    |-- checks: string (nullable = true)\n |    |    |    |    |-- codespaces: string (nullable = true)\n |    |    |    |    |-- codespaces_metadata: string (nullable = true)\n |    |    |    |    |-- contents: string (nullable = true)\n |    |    |    |    |-- dependabot_secrets: string (nullable = true)\n |    |    |    |    |-- deployments: string (nullable = true)\n |    |    |    |    |-- discussions: string (nullable = true)\n |    |    |    |    |-- emails: string (nullable = true)\n |    |    |    |    |-- environments: string (nullable = true)\n |    |    |    |    |-- issues: string (nullable = true)\n |    |    |    |    |-- members: string (nullable = true)\n |    |    |    |    |-- merge_queues: string (nullable = true)\n |    |    |    |    |-- metadata: string (nullable = true)\n |    |    |    |    |-- organization_administration: string (nullable = true)\n |    |    |    |    |-- organization_custom_properties: string (nullable = true)\n |    |    |    |    |-- organization_custom_roles: string (nullable = true)\n |    |    |    |    |-- organization_events: string (nullable = true)\n |    |    |    |    |-- organization_hooks: string (nullable = true)\n |    |    |    |    |-- organization_plan: string (nullable = true)\n |    |    |    |    |-- organization_projects: string (nullable = true)\n |    |    |    |    |-- organization_secrets: string (nullable = true)\n |    |    |    |    |-- organization_self_hosted_runners: string (nullable = true)\n |    |    |    |    |-- organization_user_blocking: string (nullable = true)\n |    |    |    |    |-- packages: string (nullable = true)\n |    |    |    |    |-- pages: string (nullable = true)\n |    |    |    |    |-- plan: string (nullable = true)\n |    |    |    |    |-- pull_requests: string (nullable = true)\n |    |    |    |    |-- repository_hooks: string (nullable = true)\n |    |    |    |    |-- repository_projects: string (nullable = true)\n |    |    |    |    |-- secret_scanning_alerts: string (nullable = true)\n |    |    |    |    |-- secrets: string (nullable = true)\n |    |    |    |    |-- security_events: string (nullable = true)\n |    |    |    |    |-- single_file: string (nullable = true)\n |    |    |    |    |-- statuses: string (nullable = true)\n |    |    |    |    |-- team_discussions: string (nullable = true)\n |    |    |    |    |-- vulnerability_alerts: string (nullable = true)\n |    |    |    |    |-- workflows: string (nullable = true)\n |    |    |    |-- slug: string (nullable = true)\n |    |    |    |-- updated_at: string (nullable = true)\n |    |    |-- position: long (nullable = true)\n |    |    |-- pull_request_review_id: long (nullable = true)\n |    |    |-- pull_request_url: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- side: string (nullable = true)\n |    |    |-- start_line: long (nullable = true)\n |    |    |-- start_side: string (nullable = true)\n |    |    |-- subject_type: string (nullable = true)\n |    |    |-- updated_at: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- commits: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- author: struct (nullable = true)\n |    |    |    |    |-- email: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- distinct: boolean (nullable = true)\n |    |    |    |-- message: string (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- distinct_size: long (nullable = true)\n |    |-- forkee: struct (nullable = true)\n |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |-- archive_url: string (nullable = true)\n |    |    |-- archived: boolean (nullable = true)\n |    |    |-- assignees_url: string (nullable = true)\n |    |    |-- blobs_url: string (nullable = true)\n |    |    |-- branches_url: string (nullable = true)\n |    |    |-- clone_url: string (nullable = true)\n |    |    |-- collaborators_url: string (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- commits_url: string (nullable = true)\n |    |    |-- compare_url: string (nullable = true)\n |    |    |-- contents_url: string (nullable = true)\n |    |    |-- contributors_url: string (nullable = true)\n |    |    |-- created_at: string (nullable = true)\n |    |    |-- default_branch: string (nullable = true)\n |    |    |-- deployments_url: string (nullable = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- disabled: boolean (nullable = true)\n |    |    |-- downloads_url: string (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- fork: boolean (nullable = true)\n |    |    |-- forks: long (nullable = true)\n |    |    |-- forks_count: long (nullable = true)\n |    |    |-- forks_url: string (nullable = true)\n |    |    |-- full_name: string (nullable = true)\n |    |    |-- git_commits_url: string (nullable = true)\n |    |    |-- git_refs_url: string (nullable = true)\n |    |    |-- git_tags_url: string (nullable = true)\n |    |    |-- git_url: string (nullable = true)\n |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |-- has_issues: boolean (nullable = true)\n |    |    |-- has_pages: boolean (nullable = true)\n |    |    |-- has_projects: boolean (nullable = true)\n |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |-- homepage: string (nullable = true)\n |    |    |-- hooks_url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- is_template: boolean (nullable = true)\n |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |-- issue_events_url: string (nullable = true)\n |    |    |-- issues_url: string (nullable = true)\n |    |    |-- keys_url: string (nullable = true)\n |    |    |-- labels_url: string (nullable = true)\n |    |    |-- language: string (nullable = true)\n |    |    |-- languages_url: string (nullable = true)\n |    |    |-- license: struct (nullable = true)\n |    |    |    |-- key: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- merges_url: string (nullable = true)\n |    |    |-- milestones_url: string (nullable = true)\n |    |    |-- mirror_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- notifications_url: string (nullable = true)\n |    |    |-- open_issues: long (nullable = true)\n |    |    |-- open_issues_count: long (nullable = true)\n |    |    |-- owner: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- private: boolean (nullable = true)\n |    |    |-- public: boolean (nullable = true)\n |    |    |-- pulls_url: string (nullable = true)\n |    |    |-- pushed_at: string (nullable = true)\n |    |    |-- releases_url: string (nullable = true)\n |    |    |-- size: long (nullable = true)\n |    |    |-- ssh_url: string (nullable = true)\n |    |    |-- stargazers_count: long (nullable = true)\n |    |    |-- stargazers_url: string (nullable = true)\n |    |    |-- statuses_url: string (nullable = true)\n |    |    |-- subscribers_url: string (nullable = true)\n |    |    |-- subscription_url: string (nullable = true)\n |    |    |-- svn_url: string (nullable = true)\n |    |    |-- tags_url: string (nullable = true)\n |    |    |-- teams_url: string (nullable = true)\n |    |    |-- topics: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- trees_url: string (nullable = true)\n |    |    |-- updated_at: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- visibility: string (nullable = true)\n |    |    |-- watchers: long (nullable = true)\n |    |    |-- watchers_count: long (nullable = true)\n |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |-- head: string (nullable = true)\n |    |-- issue: struct (nullable = true)\n |    |    |-- active_lock_reason: string (nullable = true)\n |    |    |-- assignee: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- assignees: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- closed_at: string (nullable = true)\n |    |    |-- comments: long (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- created_at: string (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- labels: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- color: string (nullable = true)\n |    |    |    |    |-- default: boolean (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- labels_url: string (nullable = true)\n |    |    |-- locked: boolean (nullable = true)\n |    |    |-- milestone: struct (nullable = true)\n |    |    |    |-- closed_at: string (nullable = true)\n |    |    |    |-- closed_issues: long (nullable = true)\n |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |-- creator: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- due_on: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- number: long (nullable = true)\n |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- number: long (nullable = true)\n |    |    |-- performed_via_github_app: struct (nullable = true)\n |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- events: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- external_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- permissions: struct (nullable = true)\n |    |    |    |    |-- actions: string (nullable = true)\n |    |    |    |    |-- contents: string (nullable = true)\n |    |    |    |    |-- emails: string (nullable = true)\n |    |    |    |    |-- issues: string (nullable = true)\n |    |    |    |    |-- members: string (nullable = true)\n |    |    |    |    |-- metadata: string (nullable = true)\n |    |    |    |    |-- pull_requests: string (nullable = true)\n |    |    |    |-- slug: string (nullable = true)\n |    |    |    |-- updated_at: string (nullable = true)\n |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |-- diff_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- merged_at: string (nullable = true)\n |    |    |    |-- patch_url: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- repository_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- state_reason: string (nullable = true)\n |    |    |-- timeline_url: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- updated_at: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- master_branch: string (nullable = true)\n |    |-- member: struct (nullable = true)\n |    |    |-- avatar_url: string (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- followers_url: string (nullable = true)\n |    |    |-- following_url: string (nullable = true)\n |    |    |-- gists_url: string (nullable = true)\n |    |    |-- gravatar_id: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- login: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- organizations_url: string (nullable = true)\n |    |    |-- received_events_url: string (nullable = true)\n |    |    |-- repos_url: string (nullable = true)\n |    |    |-- site_admin: boolean (nullable = true)\n |    |    |-- starred_url: string (nullable = true)\n |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |-- number: long (nullable = true)\n |    |-- pages: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- action: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- page_name: string (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- summary: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |-- pull_request: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- comments: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- commits: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- issue: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- review_comment: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- review_comments: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- statuses: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- active_lock_reason: string (nullable = true)\n |    |    |-- additions: long (nullable = true)\n |    |    |-- assignee: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- assignees: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- auto_merge: struct (nullable = true)\n |    |    |    |-- commit_message: string (nullable = true)\n |    |    |    |-- commit_title: string (nullable = true)\n |    |    |    |-- enabled_by: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- merge_method: string (nullable = true)\n |    |    |-- base: struct (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |    |    |    |-- ref: string (nullable = true)\n |    |    |    |-- repo: struct (nullable = true)\n |    |    |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |    |    |-- archive_url: string (nullable = true)\n |    |    |    |    |-- archived: boolean (nullable = true)\n |    |    |    |    |-- assignees_url: string (nullable = true)\n |    |    |    |    |-- blobs_url: string (nullable = true)\n |    |    |    |    |-- branches_url: string (nullable = true)\n |    |    |    |    |-- clone_url: string (nullable = true)\n |    |    |    |    |-- collaborators_url: string (nullable = true)\n |    |    |    |    |-- comments_url: string (nullable = true)\n |    |    |    |    |-- commits_url: string (nullable = true)\n |    |    |    |    |-- compare_url: string (nullable = true)\n |    |    |    |    |-- contents_url: string (nullable = true)\n |    |    |    |    |-- contributors_url: string (nullable = true)\n |    |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |    |-- default_branch: string (nullable = true)\n |    |    |    |    |-- deployments_url: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- disabled: boolean (nullable = true)\n |    |    |    |    |-- downloads_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- fork: boolean (nullable = true)\n |    |    |    |    |-- forks: long (nullable = true)\n |    |    |    |    |-- forks_count: long (nullable = true)\n |    |    |    |    |-- forks_url: string (nullable = true)\n |    |    |    |    |-- full_name: string (nullable = true)\n |    |    |    |    |-- git_commits_url: string (nullable = true)\n |    |    |    |    |-- git_refs_url: string (nullable = true)\n |    |    |    |    |-- git_tags_url: string (nullable = true)\n |    |    |    |    |-- git_url: string (nullable = true)\n |    |    |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |    |    |-- has_issues: boolean (nullable = true)\n |    |    |    |    |-- has_pages: boolean (nullable = true)\n |    |    |    |    |-- has_projects: boolean (nullable = true)\n |    |    |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |    |    |-- homepage: string (nullable = true)\n |    |    |    |    |-- hooks_url: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- is_template: boolean (nullable = true)\n |    |    |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |    |    |-- issue_events_url: string (nullable = true)\n |    |    |    |    |-- issues_url: string (nullable = true)\n |    |    |    |    |-- keys_url: string (nullable = true)\n |    |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |    |-- language: string (nullable = true)\n |    |    |    |    |-- languages_url: string (nullable = true)\n |    |    |    |    |-- license: struct (nullable = true)\n |    |    |    |    |    |-- key: string (nullable = true)\n |    |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- merges_url: string (nullable = true)\n |    |    |    |    |-- milestones_url: string (nullable = true)\n |    |    |    |    |-- mirror_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notifications_url: string (nullable = true)\n |    |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |    |-- open_issues_count: long (nullable = true)\n |    |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- private: boolean (nullable = true)\n |    |    |    |    |-- pulls_url: string (nullable = true)\n |    |    |    |    |-- pushed_at: string (nullable = true)\n |    |    |    |    |-- releases_url: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- ssh_url: string (nullable = true)\n |    |    |    |    |-- stargazers_count: long (nullable = true)\n |    |    |    |    |-- stargazers_url: string (nullable = true)\n |    |    |    |    |-- statuses_url: string (nullable = true)\n |    |    |    |    |-- subscribers_url: string (nullable = true)\n |    |    |    |    |-- subscription_url: string (nullable = true)\n |    |    |    |    |-- svn_url: string (nullable = true)\n |    |    |    |    |-- tags_url: string (nullable = true)\n |    |    |    |    |-- teams_url: string (nullable = true)\n |    |    |    |    |-- topics: array (nullable = true)\n |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |-- trees_url: string (nullable = true)\n |    |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- visibility: string (nullable = true)\n |    |    |    |    |-- watchers: long (nullable = true)\n |    |    |    |    |-- watchers_count: long (nullable = true)\n |    |    |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- user: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- changed_files: long (nullable = true)\n |    |    |-- closed_at: string (nullable = true)\n |    |    |-- comments: long (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- commits: long (nullable = true)\n |    |    |-- commits_url: string (nullable = true)\n |    |    |-- created_at: string (nullable = true)\n |    |    |-- deletions: long (nullable = true)\n |    |    |-- diff_url: string (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- head: struct (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |    |    |    |-- ref: string (nullable = true)\n |    |    |    |-- repo: struct (nullable = true)\n |    |    |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |    |    |-- archive_url: string (nullable = true)\n |    |    |    |    |-- archived: boolean (nullable = true)\n |    |    |    |    |-- assignees_url: string (nullable = true)\n |    |    |    |    |-- blobs_url: string (nullable = true)\n |    |    |    |    |-- branches_url: string (nullable = true)\n |    |    |    |    |-- clone_url: string (nullable = true)\n |    |    |    |    |-- collaborators_url: string (nullable = true)\n |    |    |    |    |-- comments_url: string (nullable = true)\n |    |    |    |    |-- commits_url: string (nullable = true)\n |    |    |    |    |-- compare_url: string (nullable = true)\n |    |    |    |    |-- contents_url: string (nullable = true)\n |    |    |    |    |-- contributors_url: string (nullable = true)\n |    |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |    |-- default_branch: string (nullable = true)\n |    |    |    |    |-- deployments_url: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- disabled: boolean (nullable = true)\n |    |    |    |    |-- downloads_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- fork: boolean (nullable = true)\n |    |    |    |    |-- forks: long (nullable = true)\n |    |    |    |    |-- forks_count: long (nullable = true)\n |    |    |    |    |-- forks_url: string (nullable = true)\n |    |    |    |    |-- full_name: string (nullable = true)\n |    |    |    |    |-- git_commits_url: string (nullable = true)\n |    |    |    |    |-- git_refs_url: string (nullable = true)\n |    |    |    |    |-- git_tags_url: string (nullable = true)\n |    |    |    |    |-- git_url: string (nullable = true)\n |    |    |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |    |    |-- has_issues: boolean (nullable = true)\n |    |    |    |    |-- has_pages: boolean (nullable = true)\n |    |    |    |    |-- has_projects: boolean (nullable = true)\n |    |    |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |    |    |-- homepage: string (nullable = true)\n |    |    |    |    |-- hooks_url: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- is_template: boolean (nullable = true)\n |    |    |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |    |    |-- issue_events_url: string (nullable = true)\n |    |    |    |    |-- issues_url: string (nullable = true)\n |    |    |    |    |-- keys_url: string (nullable = true)\n |    |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |    |-- language: string (nullable = true)\n |    |    |    |    |-- languages_url: string (nullable = true)\n |    |    |    |    |-- license: struct (nullable = true)\n |    |    |    |    |    |-- key: string (nullable = true)\n |    |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- merges_url: string (nullable = true)\n |    |    |    |    |-- milestones_url: string (nullable = true)\n |    |    |    |    |-- mirror_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notifications_url: string (nullable = true)\n |    |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |    |-- open_issues_count: long (nullable = true)\n |    |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- private: boolean (nullable = true)\n |    |    |    |    |-- pulls_url: string (nullable = true)\n |    |    |    |    |-- pushed_at: string (nullable = true)\n |    |    |    |    |-- releases_url: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- ssh_url: string (nullable = true)\n |    |    |    |    |-- stargazers_count: long (nullable = true)\n |    |    |    |    |-- stargazers_url: string (nullable = true)\n |    |    |    |    |-- statuses_url: string (nullable = true)\n |    |    |    |    |-- subscribers_url: string (nullable = true)\n |    |    |    |    |-- subscription_url: string (nullable = true)\n |    |    |    |    |-- svn_url: string (nullable = true)\n |    |    |    |    |-- tags_url: string (nullable = true)\n |    |    |    |    |-- teams_url: string (nullable = true)\n |    |    |    |    |-- topics: array (nullable = true)\n |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |-- trees_url: string (nullable = true)\n |    |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- visibility: string (nullable = true)\n |    |    |    |    |-- watchers: long (nullable = true)\n |    |    |    |    |-- watchers_count: long (nullable = true)\n |    |    |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- user: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- issue_url: string (nullable = true)\n |    |    |-- labels: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- color: string (nullable = true)\n |    |    |    |    |-- default: boolean (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- locked: boolean (nullable = true)\n |    |    |-- maintainer_can_modify: boolean (nullable = true)\n |    |    |-- merge_commit_sha: string (nullable = true)\n |    |    |-- mergeable: boolean (nullable = true)\n |    |    |-- mergeable_state: string (nullable = true)\n |    |    |-- merged: boolean (nullable = true)\n |    |    |-- merged_at: string (nullable = true)\n |    |    |-- merged_by: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- milestone: struct (nullable = true)\n |    |    |    |-- closed_at: string (nullable = true)\n |    |    |    |-- closed_issues: long (nullable = true)\n |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |-- creator: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- due_on: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- number: long (nullable = true)\n |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- number: long (nullable = true)\n |    |    |-- patch_url: string (nullable = true)\n |    |    |-- rebaseable: boolean (nullable = true)\n |    |    |-- requested_reviewers: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- requested_teams: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- members_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notification_setting: string (nullable = true)\n |    |    |    |    |-- parent: string (nullable = true)\n |    |    |    |    |-- permission: string (nullable = true)\n |    |    |    |    |-- privacy: string (nullable = true)\n |    |    |    |    |-- repositories_url: string (nullable = true)\n |    |    |    |    |-- slug: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- review_comment_url: string (nullable = true)\n |    |    |-- review_comments: long (nullable = true)\n |    |    |-- review_comments_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- statuses_url: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- updated_at: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- push_id: long (nullable = true)\n |    |-- pusher_type: string (nullable = true)\n |    |-- ref: string (nullable = true)\n |    |-- ref_type: string (nullable = true)\n |    |-- release: struct (nullable = true)\n |    |    |-- assets: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- browser_download_url: string (nullable = true)\n |    |    |    |    |-- content_type: string (nullable = true)\n |    |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |    |-- download_count: long (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- label: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- state: string (nullable = true)\n |    |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |    |-- uploader: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- assets_url: string (nullable = true)\n |    |    |-- author: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- created_at: string (nullable = true)\n |    |    |-- discussion_url: string (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- is_short_description_html_truncated: boolean (nullable = true)\n |    |    |-- mentions: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- avatar_user_actor: boolean (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- profile_name: string (nullable = true)\n |    |    |    |    |-- profile_url: string (nullable = true)\n |    |    |-- mentions_count: long (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- prerelease: boolean (nullable = true)\n |    |    |-- published_at: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- short_description_html: string (nullable = true)\n |    |    |-- tag_name: string (nullable = true)\n |    |    |-- tarball_url: string (nullable = true)\n |    |    |-- target_commitish: string (nullable = true)\n |    |    |-- upload_url: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- zipball_url: string (nullable = true)\n |    |-- repository_id: long (nullable = true)\n |    |-- review: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- commit_id: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- pull_request_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- submitted_at: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- size: long (nullable = true)\n |-- public: boolean (nullable = true)\n |-- repo: struct (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n24/08/25 17:24:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n\n# Spark가 확인한 스키마를 json으로 저장\nimport json\n\ngithub_schema = json.loads(github.schema.json())\n\n\n\n컬럼명 기준 타입이 이상한 것 확인하고 타입 수정하기\n\n컬럼명을 입력하면 루트부터의 구조(‘payload.forkee.pushed_at’ 등)를 알려주는 함수 작성\nselect문으로 데이터 형태 확인 후 필요한 경우 데이터 타입 수정\n\n예를 들어, created_at은 string으로 되어있어 timestamp로 타입을 변환함\n\n\n\n# printSchema의 스키마를 보다가 의심되는 컬럼이 있으면 root부터의 구조를 보여주는 함수 by GPT\ndef find_parents(data, target_name, current_path=[]):\n    paths = []\n    if isinstance(data, dict):\n        if 'name' in data and data['name'] == target_name:\n            # Collect path to the target and the names of its ancestors\n            paths.append(current_path)\n        # Recursively search within this dictionary\n        for key, value in data.items():\n            if isinstance(value, (dict, list)):\n                paths.extend(find_parents(value, target_name, current_path + [data.get('name', None)]))\n    elif isinstance(data, list):\n        for index, item in enumerate(data):\n            paths.extend(find_parents(item, target_name, current_path))\n    return paths\n\ndef format_paths(paths):\n    formatted_paths = []\n    for path in paths:\n        # Filter out None values and join the path\n        formatted_path = '.'.join(filter(None, path))\n        formatted_paths.append(formatted_path)\n    return formatted_paths\n\n# Format and print the names of the ancestors\nname_of_column = 'pushed_at'\nfor path in format_paths(find_parents(github_schema, name_of_column)):\n    print(f'{path}.{name_of_column}')\n\npayload.forkee.pushed_at\npayload.pull_request.base.repo.pushed_at\npayload.pull_request.head.repo.pushed_at\n\n\n\n# 의심되는 컬럼의 데이터 형태 확인 with select문\ngithub.select(\n    col('payload.forkee.pushed_at')\n).distinct().show()\n\n\n[Stage 86:&gt;                                                         (0 + 1) / 1]\n\n\n+--------------------+\n|           pushed_at|\n+--------------------+\n|2024-02-20T15:01:04Z|\n|2024-05-19T04:16:24Z|\n|2024-05-19T03:32:00Z|\n|2024-05-10T16:43:22Z|\n|2024-05-08T13:46:33Z|\n|2021-11-14T06:52:01Z|\n|2024-05-14T14:26:38Z|\n|2024-05-09T13:46:19Z|\n|2024-05-01T22:05:15Z|\n|2024-05-18T16:06:07Z|\n|2024-02-07T03:31:38Z|\n|2024-05-11T19:54:42Z|\n|2024-05-15T12:37:12Z|\n|2024-05-19T14:51:30Z|\n|2024-05-18T18:56:16Z|\n|2024-05-17T13:05:15Z|\n|2024-05-19T10:01:48Z|\n|2024-05-18T04:16:44Z|\n|2024-02-01T07:50:08Z|\n|2024-05-19T13:56:05Z|\n+--------------------+\nonly showing top 20 rows\n\n\n\n\n                                                                                \n\n\n\n# 수정 필요한 컬럼명은 스키마 수정\ndict_typetable = {'timestamp':['created_at', 'updated_at','closed_at','merged_at','pushed_at']}\n\ndef update_type(data, dict_typetable):\n    if isinstance(data, dict):\n        # Check if the dictionary contains 'fields'\n        if 'fields' in data:\n            for item in data['fields']:\n                if isinstance(item['type'], dict) and 'fields' in item['type']:\n                    # Recursively process nested 'fields'\n                    update_type(item['type'], dict_typetable)\n                else:\n                    for key, list_column in dict_typetable.items():\n                        if item.get('name') in list_column:\n                            item['type'] = key # key=type (timestamp 등)\n        else:\n            # Handle nested structures\n            for key, value in data.items():\n                update_type(value)\n    elif isinstance(data, list):\n        for item in data:\n            update_type(item)\n\nupdate_type(github_schema, dict_typetable)\n\n\n\n수정한 스키마로 파일 읽어서 확인하기\n\nspark.read.schema(schema_to_read).json('file.json')으로 읽기\n변경된 데이터 확인하기 : 2024-02-20T15:01:04Z → 2024-05-19 12:00:33\nprintSchema로 변경된 타입 확인하기\n\n\n# 저장한 스키마로 파일 읽기 (빠른 확인을 위해 1개의 gh archive데이터만 사용)\nfrom pyspark.sql.types import StructType\n\nschema_to_read = StructType.fromJson(github_schema)\ndf = spark.read.schema(schema_to_read).json(\"../data/gh_archive/2024-07-01-14.json.gz\")\n\n\n# 변경된 데이터 확인\nimport pyspark.sql.functions as F\ndf.select(\n    F.col(\"payload.forkee.pushed_at\")\n).distinct().show()\n\n\n[Stage 1:&gt;                                                          (0 + 1) / 1]\n\n\n+-------------------+\n|          pushed_at|\n+-------------------+\n|2024-05-19 12:00:33|\n|2024-05-13 22:33:41|\n|2021-11-26 09:57:24|\n|2024-05-03 16:34:07|\n|2024-04-30 03:16:52|\n|2024-03-17 20:21:22|\n|2024-05-18 13:28:47|\n|2024-05-19 14:25:45|\n|2024-04-11 23:36:42|\n|2021-01-06 17:18:27|\n|2023-11-11 18:39:26|\n|2024-05-19 05:03:02|\n|2021-11-13 05:20:38|\n|2024-05-13 23:53:03|\n|2024-05-16 01:11:03|\n|2024-05-17 21:07:06|\n|2021-10-29 01:29:12|\n|2024-05-08 12:18:27|\n|2024-03-18 05:24:37|\n|2024-02-28 19:55:17|\n+-------------------+\nonly showing top 20 rows\n\n\n\n\n                                                                                \n\n\n\n# 변경된 스키마 확인\ndf.printSchema()\n\nroot\n |-- actor: struct (nullable = true)\n |    |-- avatar_url: string (nullable = true)\n |    |-- display_login: string (nullable = true)\n |    |-- gravatar_id: string (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- login: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- created_at: timestamp (nullable = true)\n |-- id: string (nullable = true)\n |-- org: struct (nullable = true)\n |    |-- avatar_url: string (nullable = true)\n |    |-- gravatar_id: string (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- login: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- payload: struct (nullable = true)\n |    |-- action: string (nullable = true)\n |    |-- before: string (nullable = true)\n |    |-- comment: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- commit_id: string (nullable = true)\n |    |    |-- created_at: timestamp (nullable = true)\n |    |    |-- diff_hunk: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- in_reply_to_id: long (nullable = true)\n |    |    |-- issue_url: string (nullable = true)\n |    |    |-- line: long (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- original_commit_id: string (nullable = true)\n |    |    |-- original_line: long (nullable = true)\n |    |    |-- original_position: long (nullable = true)\n |    |    |-- original_start_line: long (nullable = true)\n |    |    |-- path: string (nullable = true)\n |    |    |-- performed_via_github_app: struct (nullable = true)\n |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- events: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- external_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- permissions: struct (nullable = true)\n |    |    |    |    |-- actions: string (nullable = true)\n |    |    |    |    |-- actions_variables: string (nullable = true)\n |    |    |    |    |-- administration: string (nullable = true)\n |    |    |    |    |-- attestations: string (nullable = true)\n |    |    |    |    |-- checks: string (nullable = true)\n |    |    |    |    |-- codespaces: string (nullable = true)\n |    |    |    |    |-- codespaces_metadata: string (nullable = true)\n |    |    |    |    |-- contents: string (nullable = true)\n |    |    |    |    |-- dependabot_secrets: string (nullable = true)\n |    |    |    |    |-- deployments: string (nullable = true)\n |    |    |    |    |-- discussions: string (nullable = true)\n |    |    |    |    |-- emails: string (nullable = true)\n |    |    |    |    |-- environments: string (nullable = true)\n |    |    |    |    |-- issues: string (nullable = true)\n |    |    |    |    |-- members: string (nullable = true)\n |    |    |    |    |-- merge_queues: string (nullable = true)\n |    |    |    |    |-- metadata: string (nullable = true)\n |    |    |    |    |-- organization_administration: string (nullable = true)\n |    |    |    |    |-- organization_custom_properties: string (nullable = true)\n |    |    |    |    |-- organization_custom_roles: string (nullable = true)\n |    |    |    |    |-- organization_events: string (nullable = true)\n |    |    |    |    |-- organization_hooks: string (nullable = true)\n |    |    |    |    |-- organization_plan: string (nullable = true)\n |    |    |    |    |-- organization_projects: string (nullable = true)\n |    |    |    |    |-- organization_secrets: string (nullable = true)\n |    |    |    |    |-- organization_self_hosted_runners: string (nullable = true)\n |    |    |    |    |-- organization_user_blocking: string (nullable = true)\n |    |    |    |    |-- packages: string (nullable = true)\n |    |    |    |    |-- pages: string (nullable = true)\n |    |    |    |    |-- plan: string (nullable = true)\n |    |    |    |    |-- pull_requests: string (nullable = true)\n |    |    |    |    |-- repository_hooks: string (nullable = true)\n |    |    |    |    |-- repository_projects: string (nullable = true)\n |    |    |    |    |-- secret_scanning_alerts: string (nullable = true)\n |    |    |    |    |-- secrets: string (nullable = true)\n |    |    |    |    |-- security_events: string (nullable = true)\n |    |    |    |    |-- single_file: string (nullable = true)\n |    |    |    |    |-- statuses: string (nullable = true)\n |    |    |    |    |-- team_discussions: string (nullable = true)\n |    |    |    |    |-- vulnerability_alerts: string (nullable = true)\n |    |    |    |    |-- workflows: string (nullable = true)\n |    |    |    |-- slug: string (nullable = true)\n |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- position: long (nullable = true)\n |    |    |-- pull_request_review_id: long (nullable = true)\n |    |    |-- pull_request_url: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- side: string (nullable = true)\n |    |    |-- start_line: long (nullable = true)\n |    |    |-- start_side: string (nullable = true)\n |    |    |-- subject_type: string (nullable = true)\n |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- commits: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- author: struct (nullable = true)\n |    |    |    |    |-- email: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- distinct: boolean (nullable = true)\n |    |    |    |-- message: string (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- distinct_size: long (nullable = true)\n |    |-- forkee: struct (nullable = true)\n |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |-- archive_url: string (nullable = true)\n |    |    |-- archived: boolean (nullable = true)\n |    |    |-- assignees_url: string (nullable = true)\n |    |    |-- blobs_url: string (nullable = true)\n |    |    |-- branches_url: string (nullable = true)\n |    |    |-- clone_url: string (nullable = true)\n |    |    |-- collaborators_url: string (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- commits_url: string (nullable = true)\n |    |    |-- compare_url: string (nullable = true)\n |    |    |-- contents_url: string (nullable = true)\n |    |    |-- contributors_url: string (nullable = true)\n |    |    |-- created_at: timestamp (nullable = true)\n |    |    |-- default_branch: string (nullable = true)\n |    |    |-- deployments_url: string (nullable = true)\n |    |    |-- description: string (nullable = true)\n |    |    |-- disabled: boolean (nullable = true)\n |    |    |-- downloads_url: string (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- fork: boolean (nullable = true)\n |    |    |-- forks: long (nullable = true)\n |    |    |-- forks_count: long (nullable = true)\n |    |    |-- forks_url: string (nullable = true)\n |    |    |-- full_name: string (nullable = true)\n |    |    |-- git_commits_url: string (nullable = true)\n |    |    |-- git_refs_url: string (nullable = true)\n |    |    |-- git_tags_url: string (nullable = true)\n |    |    |-- git_url: string (nullable = true)\n |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |-- has_issues: boolean (nullable = true)\n |    |    |-- has_pages: boolean (nullable = true)\n |    |    |-- has_projects: boolean (nullable = true)\n |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |-- homepage: string (nullable = true)\n |    |    |-- hooks_url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- is_template: boolean (nullable = true)\n |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |-- issue_events_url: string (nullable = true)\n |    |    |-- issues_url: string (nullable = true)\n |    |    |-- keys_url: string (nullable = true)\n |    |    |-- labels_url: string (nullable = true)\n |    |    |-- language: string (nullable = true)\n |    |    |-- languages_url: string (nullable = true)\n |    |    |-- license: struct (nullable = true)\n |    |    |    |-- key: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- merges_url: string (nullable = true)\n |    |    |-- milestones_url: string (nullable = true)\n |    |    |-- mirror_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- notifications_url: string (nullable = true)\n |    |    |-- open_issues: long (nullable = true)\n |    |    |-- open_issues_count: long (nullable = true)\n |    |    |-- owner: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- private: boolean (nullable = true)\n |    |    |-- public: boolean (nullable = true)\n |    |    |-- pulls_url: string (nullable = true)\n |    |    |-- pushed_at: timestamp (nullable = true)\n |    |    |-- releases_url: string (nullable = true)\n |    |    |-- size: long (nullable = true)\n |    |    |-- ssh_url: string (nullable = true)\n |    |    |-- stargazers_count: long (nullable = true)\n |    |    |-- stargazers_url: string (nullable = true)\n |    |    |-- statuses_url: string (nullable = true)\n |    |    |-- subscribers_url: string (nullable = true)\n |    |    |-- subscription_url: string (nullable = true)\n |    |    |-- svn_url: string (nullable = true)\n |    |    |-- tags_url: string (nullable = true)\n |    |    |-- teams_url: string (nullable = true)\n |    |    |-- topics: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- trees_url: string (nullable = true)\n |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- visibility: string (nullable = true)\n |    |    |-- watchers: long (nullable = true)\n |    |    |-- watchers_count: long (nullable = true)\n |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |-- head: string (nullable = true)\n |    |-- issue: struct (nullable = true)\n |    |    |-- active_lock_reason: string (nullable = true)\n |    |    |-- assignee: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- assignees: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- closed_at: timestamp (nullable = true)\n |    |    |-- comments: long (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- created_at: timestamp (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- labels: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- color: string (nullable = true)\n |    |    |    |    |-- default: boolean (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- labels_url: string (nullable = true)\n |    |    |-- locked: boolean (nullable = true)\n |    |    |-- milestone: struct (nullable = true)\n |    |    |    |-- closed_at: timestamp (nullable = true)\n |    |    |    |-- closed_issues: long (nullable = true)\n |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |-- creator: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- due_on: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- number: long (nullable = true)\n |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- number: long (nullable = true)\n |    |    |-- performed_via_github_app: struct (nullable = true)\n |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- events: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- external_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- permissions: struct (nullable = true)\n |    |    |    |    |-- actions: string (nullable = true)\n |    |    |    |    |-- contents: string (nullable = true)\n |    |    |    |    |-- emails: string (nullable = true)\n |    |    |    |    |-- issues: string (nullable = true)\n |    |    |    |    |-- members: string (nullable = true)\n |    |    |    |    |-- metadata: string (nullable = true)\n |    |    |    |    |-- pull_requests: string (nullable = true)\n |    |    |    |-- slug: string (nullable = true)\n |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |-- diff_url: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- merged_at: timestamp (nullable = true)\n |    |    |    |-- patch_url: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- repository_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- state_reason: string (nullable = true)\n |    |    |-- timeline_url: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- master_branch: string (nullable = true)\n |    |-- member: struct (nullable = true)\n |    |    |-- avatar_url: string (nullable = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- followers_url: string (nullable = true)\n |    |    |-- following_url: string (nullable = true)\n |    |    |-- gists_url: string (nullable = true)\n |    |    |-- gravatar_id: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- login: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- organizations_url: string (nullable = true)\n |    |    |-- received_events_url: string (nullable = true)\n |    |    |-- repos_url: string (nullable = true)\n |    |    |-- site_admin: boolean (nullable = true)\n |    |    |-- starred_url: string (nullable = true)\n |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |-- number: long (nullable = true)\n |    |-- pages: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- action: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- page_name: string (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- summary: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |-- pull_request: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- comments: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- commits: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- issue: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- review_comment: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- review_comments: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- self: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- statuses: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- active_lock_reason: string (nullable = true)\n |    |    |-- additions: long (nullable = true)\n |    |    |-- assignee: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- assignees: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- auto_merge: struct (nullable = true)\n |    |    |    |-- commit_message: string (nullable = true)\n |    |    |    |-- commit_title: string (nullable = true)\n |    |    |    |-- enabled_by: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- merge_method: string (nullable = true)\n |    |    |-- base: struct (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |    |    |    |-- ref: string (nullable = true)\n |    |    |    |-- repo: struct (nullable = true)\n |    |    |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |    |    |-- archive_url: string (nullable = true)\n |    |    |    |    |-- archived: boolean (nullable = true)\n |    |    |    |    |-- assignees_url: string (nullable = true)\n |    |    |    |    |-- blobs_url: string (nullable = true)\n |    |    |    |    |-- branches_url: string (nullable = true)\n |    |    |    |    |-- clone_url: string (nullable = true)\n |    |    |    |    |-- collaborators_url: string (nullable = true)\n |    |    |    |    |-- comments_url: string (nullable = true)\n |    |    |    |    |-- commits_url: string (nullable = true)\n |    |    |    |    |-- compare_url: string (nullable = true)\n |    |    |    |    |-- contents_url: string (nullable = true)\n |    |    |    |    |-- contributors_url: string (nullable = true)\n |    |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |    |-- default_branch: string (nullable = true)\n |    |    |    |    |-- deployments_url: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- disabled: boolean (nullable = true)\n |    |    |    |    |-- downloads_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- fork: boolean (nullable = true)\n |    |    |    |    |-- forks: long (nullable = true)\n |    |    |    |    |-- forks_count: long (nullable = true)\n |    |    |    |    |-- forks_url: string (nullable = true)\n |    |    |    |    |-- full_name: string (nullable = true)\n |    |    |    |    |-- git_commits_url: string (nullable = true)\n |    |    |    |    |-- git_refs_url: string (nullable = true)\n |    |    |    |    |-- git_tags_url: string (nullable = true)\n |    |    |    |    |-- git_url: string (nullable = true)\n |    |    |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |    |    |-- has_issues: boolean (nullable = true)\n |    |    |    |    |-- has_pages: boolean (nullable = true)\n |    |    |    |    |-- has_projects: boolean (nullable = true)\n |    |    |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |    |    |-- homepage: string (nullable = true)\n |    |    |    |    |-- hooks_url: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- is_template: boolean (nullable = true)\n |    |    |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |    |    |-- issue_events_url: string (nullable = true)\n |    |    |    |    |-- issues_url: string (nullable = true)\n |    |    |    |    |-- keys_url: string (nullable = true)\n |    |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |    |-- language: string (nullable = true)\n |    |    |    |    |-- languages_url: string (nullable = true)\n |    |    |    |    |-- license: struct (nullable = true)\n |    |    |    |    |    |-- key: string (nullable = true)\n |    |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- merges_url: string (nullable = true)\n |    |    |    |    |-- milestones_url: string (nullable = true)\n |    |    |    |    |-- mirror_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notifications_url: string (nullable = true)\n |    |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |    |-- open_issues_count: long (nullable = true)\n |    |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- private: boolean (nullable = true)\n |    |    |    |    |-- pulls_url: string (nullable = true)\n |    |    |    |    |-- pushed_at: timestamp (nullable = true)\n |    |    |    |    |-- releases_url: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- ssh_url: string (nullable = true)\n |    |    |    |    |-- stargazers_count: long (nullable = true)\n |    |    |    |    |-- stargazers_url: string (nullable = true)\n |    |    |    |    |-- statuses_url: string (nullable = true)\n |    |    |    |    |-- subscribers_url: string (nullable = true)\n |    |    |    |    |-- subscription_url: string (nullable = true)\n |    |    |    |    |-- svn_url: string (nullable = true)\n |    |    |    |    |-- tags_url: string (nullable = true)\n |    |    |    |    |-- teams_url: string (nullable = true)\n |    |    |    |    |-- topics: array (nullable = true)\n |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |-- trees_url: string (nullable = true)\n |    |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- visibility: string (nullable = true)\n |    |    |    |    |-- watchers: long (nullable = true)\n |    |    |    |    |-- watchers_count: long (nullable = true)\n |    |    |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- user: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- changed_files: long (nullable = true)\n |    |    |-- closed_at: timestamp (nullable = true)\n |    |    |-- comments: long (nullable = true)\n |    |    |-- comments_url: string (nullable = true)\n |    |    |-- commits: long (nullable = true)\n |    |    |-- commits_url: string (nullable = true)\n |    |    |-- created_at: timestamp (nullable = true)\n |    |    |-- deletions: long (nullable = true)\n |    |    |-- diff_url: string (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- head: struct (nullable = true)\n |    |    |    |-- label: string (nullable = true)\n |    |    |    |-- ref: string (nullable = true)\n |    |    |    |-- repo: struct (nullable = true)\n |    |    |    |    |-- allow_forking: boolean (nullable = true)\n |    |    |    |    |-- archive_url: string (nullable = true)\n |    |    |    |    |-- archived: boolean (nullable = true)\n |    |    |    |    |-- assignees_url: string (nullable = true)\n |    |    |    |    |-- blobs_url: string (nullable = true)\n |    |    |    |    |-- branches_url: string (nullable = true)\n |    |    |    |    |-- clone_url: string (nullable = true)\n |    |    |    |    |-- collaborators_url: string (nullable = true)\n |    |    |    |    |-- comments_url: string (nullable = true)\n |    |    |    |    |-- commits_url: string (nullable = true)\n |    |    |    |    |-- compare_url: string (nullable = true)\n |    |    |    |    |-- contents_url: string (nullable = true)\n |    |    |    |    |-- contributors_url: string (nullable = true)\n |    |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |    |-- default_branch: string (nullable = true)\n |    |    |    |    |-- deployments_url: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- disabled: boolean (nullable = true)\n |    |    |    |    |-- downloads_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- fork: boolean (nullable = true)\n |    |    |    |    |-- forks: long (nullable = true)\n |    |    |    |    |-- forks_count: long (nullable = true)\n |    |    |    |    |-- forks_url: string (nullable = true)\n |    |    |    |    |-- full_name: string (nullable = true)\n |    |    |    |    |-- git_commits_url: string (nullable = true)\n |    |    |    |    |-- git_refs_url: string (nullable = true)\n |    |    |    |    |-- git_tags_url: string (nullable = true)\n |    |    |    |    |-- git_url: string (nullable = true)\n |    |    |    |    |-- has_discussions: boolean (nullable = true)\n |    |    |    |    |-- has_downloads: boolean (nullable = true)\n |    |    |    |    |-- has_issues: boolean (nullable = true)\n |    |    |    |    |-- has_pages: boolean (nullable = true)\n |    |    |    |    |-- has_projects: boolean (nullable = true)\n |    |    |    |    |-- has_wiki: boolean (nullable = true)\n |    |    |    |    |-- homepage: string (nullable = true)\n |    |    |    |    |-- hooks_url: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- is_template: boolean (nullable = true)\n |    |    |    |    |-- issue_comment_url: string (nullable = true)\n |    |    |    |    |-- issue_events_url: string (nullable = true)\n |    |    |    |    |-- issues_url: string (nullable = true)\n |    |    |    |    |-- keys_url: string (nullable = true)\n |    |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |    |-- language: string (nullable = true)\n |    |    |    |    |-- languages_url: string (nullable = true)\n |    |    |    |    |-- license: struct (nullable = true)\n |    |    |    |    |    |-- key: string (nullable = true)\n |    |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- spdx_id: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- merges_url: string (nullable = true)\n |    |    |    |    |-- milestones_url: string (nullable = true)\n |    |    |    |    |-- mirror_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notifications_url: string (nullable = true)\n |    |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |    |-- open_issues_count: long (nullable = true)\n |    |    |    |    |-- owner: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- private: boolean (nullable = true)\n |    |    |    |    |-- pulls_url: string (nullable = true)\n |    |    |    |    |-- pushed_at: timestamp (nullable = true)\n |    |    |    |    |-- releases_url: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- ssh_url: string (nullable = true)\n |    |    |    |    |-- stargazers_count: long (nullable = true)\n |    |    |    |    |-- stargazers_url: string (nullable = true)\n |    |    |    |    |-- statuses_url: string (nullable = true)\n |    |    |    |    |-- subscribers_url: string (nullable = true)\n |    |    |    |    |-- subscription_url: string (nullable = true)\n |    |    |    |    |-- svn_url: string (nullable = true)\n |    |    |    |    |-- tags_url: string (nullable = true)\n |    |    |    |    |-- teams_url: string (nullable = true)\n |    |    |    |    |-- topics: array (nullable = true)\n |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |-- trees_url: string (nullable = true)\n |    |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- visibility: string (nullable = true)\n |    |    |    |    |-- watchers: long (nullable = true)\n |    |    |    |    |-- watchers_count: long (nullable = true)\n |    |    |    |    |-- web_commit_signoff_required: boolean (nullable = true)\n |    |    |    |-- sha: string (nullable = true)\n |    |    |    |-- user: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- issue_url: string (nullable = true)\n |    |    |-- labels: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- color: string (nullable = true)\n |    |    |    |    |-- default: boolean (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- locked: boolean (nullable = true)\n |    |    |-- maintainer_can_modify: boolean (nullable = true)\n |    |    |-- merge_commit_sha: string (nullable = true)\n |    |    |-- mergeable: boolean (nullable = true)\n |    |    |-- mergeable_state: string (nullable = true)\n |    |    |-- merged: boolean (nullable = true)\n |    |    |-- merged_at: timestamp (nullable = true)\n |    |    |-- merged_by: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- milestone: struct (nullable = true)\n |    |    |    |-- closed_at: timestamp (nullable = true)\n |    |    |    |-- closed_issues: long (nullable = true)\n |    |    |    |-- created_at: timestamp (nullable = true)\n |    |    |    |-- creator: struct (nullable = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- description: string (nullable = true)\n |    |    |    |-- due_on: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- labels_url: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- number: long (nullable = true)\n |    |    |    |-- open_issues: long (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- title: string (nullable = true)\n |    |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- number: long (nullable = true)\n |    |    |-- patch_url: string (nullable = true)\n |    |    |-- rebaseable: boolean (nullable = true)\n |    |    |-- requested_reviewers: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- requested_teams: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- members_url: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- notification_setting: string (nullable = true)\n |    |    |    |    |-- parent: string (nullable = true)\n |    |    |    |    |-- permission: string (nullable = true)\n |    |    |    |    |-- privacy: string (nullable = true)\n |    |    |    |    |-- repositories_url: string (nullable = true)\n |    |    |    |    |-- slug: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- review_comment_url: string (nullable = true)\n |    |    |-- review_comments: long (nullable = true)\n |    |    |-- review_comments_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- statuses_url: string (nullable = true)\n |    |    |-- title: string (nullable = true)\n |    |    |-- updated_at: timestamp (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- push_id: long (nullable = true)\n |    |-- pusher_type: string (nullable = true)\n |    |-- ref: string (nullable = true)\n |    |-- ref_type: string (nullable = true)\n |    |-- release: struct (nullable = true)\n |    |    |-- assets: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- browser_download_url: string (nullable = true)\n |    |    |    |    |-- content_type: string (nullable = true)\n |    |    |    |    |-- created_at: string (nullable = true)\n |    |    |    |    |-- download_count: long (nullable = true)\n |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |-- label: string (nullable = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |-- size: long (nullable = true)\n |    |    |    |    |-- state: string (nullable = true)\n |    |    |    |    |-- updated_at: string (nullable = true)\n |    |    |    |    |-- uploader: struct (nullable = true)\n |    |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |    |    |-- id: long (nullable = true)\n |    |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- assets_url: string (nullable = true)\n |    |    |-- author: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- created_at: timestamp (nullable = true)\n |    |    |-- discussion_url: string (nullable = true)\n |    |    |-- draft: boolean (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- is_short_description_html_truncated: boolean (nullable = true)\n |    |    |-- mentions: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |    |-- avatar_user_actor: boolean (nullable = true)\n |    |    |    |    |-- login: string (nullable = true)\n |    |    |    |    |-- profile_name: string (nullable = true)\n |    |    |    |    |-- profile_url: string (nullable = true)\n |    |    |-- mentions_count: long (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- prerelease: boolean (nullable = true)\n |    |    |-- published_at: string (nullable = true)\n |    |    |-- reactions: struct (nullable = true)\n |    |    |    |-- +1: long (nullable = true)\n |    |    |    |-- -1: long (nullable = true)\n |    |    |    |-- confused: long (nullable = true)\n |    |    |    |-- eyes: long (nullable = true)\n |    |    |    |-- heart: long (nullable = true)\n |    |    |    |-- hooray: long (nullable = true)\n |    |    |    |-- laugh: long (nullable = true)\n |    |    |    |-- rocket: long (nullable = true)\n |    |    |    |-- total_count: long (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |-- short_description_html: string (nullable = true)\n |    |    |-- tag_name: string (nullable = true)\n |    |    |-- tarball_url: string (nullable = true)\n |    |    |-- target_commitish: string (nullable = true)\n |    |    |-- upload_url: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- zipball_url: string (nullable = true)\n |    |-- repository_id: long (nullable = true)\n |    |-- review: struct (nullable = true)\n |    |    |-- _links: struct (nullable = true)\n |    |    |    |-- html: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |    |-- pull_request: struct (nullable = true)\n |    |    |    |    |-- href: string (nullable = true)\n |    |    |-- author_association: string (nullable = true)\n |    |    |-- body: string (nullable = true)\n |    |    |-- commit_id: string (nullable = true)\n |    |    |-- html_url: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- node_id: string (nullable = true)\n |    |    |-- pull_request_url: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- submitted_at: string (nullable = true)\n |    |    |-- user: struct (nullable = true)\n |    |    |    |-- avatar_url: string (nullable = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- followers_url: string (nullable = true)\n |    |    |    |-- following_url: string (nullable = true)\n |    |    |    |-- gists_url: string (nullable = true)\n |    |    |    |-- gravatar_id: string (nullable = true)\n |    |    |    |-- html_url: string (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- login: string (nullable = true)\n |    |    |    |-- node_id: string (nullable = true)\n |    |    |    |-- organizations_url: string (nullable = true)\n |    |    |    |-- received_events_url: string (nullable = true)\n |    |    |    |-- repos_url: string (nullable = true)\n |    |    |    |-- site_admin: boolean (nullable = true)\n |    |    |    |-- starred_url: string (nullable = true)\n |    |    |    |-- subscriptions_url: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |-- size: long (nullable = true)\n |-- public: boolean (nullable = true)\n |-- repo: struct (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- url: string (nullable = true)\n |-- type: string (nullable = true)\n\n\n\n\n\n확인완료 후 재사용을 위해 스키마를 json파일로 저장\n\n# 스키마 json으로 저장\noutput_file_path = 'github_schema.json'\n\nwith open(output_file_path, 'w') as json_file:\n    json.dump(github_schema, json_file, indent=4)  # `indent` for pretty-printing\n\nprint(f\"Data has been saved to {output_file_path}\")"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터셋-저장-스키마-결정-공유",
    "href": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터셋-저장-스키마-결정-공유",
    "title": "[DE스터디/2주차과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터셋 저장 스키마 결정 & 공유",
    "text": "데이터셋 저장 스키마 결정 & 공유\n\n#json파일로 저장해 둔 스키마 불러오기\n\nimport json\nfrom pyspark.sql.types import StructType\n\n# Define the path to the JSON file\ninput_file_path = 'github_schema.json'\n\n# Open and load the JSON file\nwith open(input_file_path, 'r') as json_file:\n    github_schema = json.load(json_file)\n\n\n# (선택1)저장한 스키마로 파일 읽기 (빠른 확인을 위해 일부 gh archive데이터만 사용)\nfrom pyspark.sql.types import StructType\n\nschema_to_read = StructType.fromJson(github_schema)\ndf = spark.read.schema(schema_to_read).json(\"../data/gh_archive/2024-08-24-*.json.gz\")\n\n24/09/01 14:30:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n\n# (선택2)저장한 스키마로 파일 읽기\nfrom pyspark.sql.types import StructType\n\nschema_to_read = StructType.fromJson(github_schema)\ndf = spark.read.schema(schema_to_read).json(\"../data/gh_archive/*.json.gz\")"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터-정제방법-고민하기",
    "href": "posts/meta-de-spark_and_airflow-20240823_1/index.html#데이터-정제방법-고민하기",
    "title": "[DE스터디/2주차과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터 정제방법 고민하기",
    "text": "데이터 정제방법 고민하기\n\n데이터 확인하기\n\n전체 컬럼명을 보여주는 함수 작성 및 확인\nselect문으로 해당 컬럼의 데이터 확인하기\nselect문으로 해당 컬럼의 데이터 중 filter해서 확인하기\n\n\ndef find_all_names(data, current_path=[]):\n    paths = []\n    if isinstance(data, dict):\n        if 'name' in data:\n            # Collect the path to the current 'name'\n            paths.append('.'.join(current_path + [data['name']]))\n        # Recursively search within this dictionary\n        for key, value in data.items():\n            if isinstance(value, (dict, list)):\n                paths.extend(find_all_names(value, current_path + [data.get('name', key)]))\n    elif isinstance(data, list):\n        for item in data:\n            paths.extend(find_all_names(item, current_path))\n    return paths\n\nlist_all_column = []\nfor i, each_line in enumerate(find_all_names(github_schema)):\n    print(each_line.replace('fields.',''))\n    list_all_column.append(f\"{each_line.replace('fields.','')}\")\n    if i == 11:\n        break    \n\nactor\nactor.avatar_url\nactor.display_login\nactor.gravatar_id\nactor.id\nactor.login\nactor.url\ncreated_at\nid\norg\norg.avatar_url\norg.gravatar_id\n\n\n\n# 데이터 확인 (컬럼의 전체데이터 distinct)\nimport pyspark.sql.functions as F\n\ncolumns = ['type']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf.select(*select_exprs).distinct().show(10,False)\n\n                                                                                \n\n\n+-----------------------------+\n|type                         |\n+-----------------------------+\n|PullRequestReviewEvent       |\n|PushEvent                    |\n|GollumEvent                  |\n|ReleaseEvent                 |\n|CommitCommentEvent           |\n|CreateEvent                  |\n|PullRequestReviewCommentEvent|\n|IssueCommentEvent            |\n|DeleteEvent                  |\n|IssuesEvent                  |\n+-----------------------------+\nonly showing top 10 rows\n\n\n\n\n# 데이터 확인 (컬럼의 전체데이터 distinct)\nimport pyspark.sql.functions as F\n\ncolumns = ['payload.issue.assignee.login']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf.select(*select_exprs).distinct().show(10,False)\n\n[Stage 10:=================================================&gt;        (6 + 1) / 7]\n\n\n+----------------------------+\n|payload.issue.assignee.login|\n+----------------------------+\n|Vladimir563                 |\n|Z3rio                       |\n|albar965                    |\n|mrbubbles-src               |\n|0xSaksham                   |\n|deepak1556                  |\n|Artem-Ter                   |\n|jonatfoodgroup              |\n|GbCyber                     |\n|IsaiahHarvi                 |\n+----------------------------+\nonly showing top 10 rows\n\n\n\n\n                                                                                \n\n\n\n# 데이터 확인 (컬럼의 전체데이터 distinct)\nimport pyspark.sql.functions as F\n\ncolumns = ['actor.login']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf.select(*select_exprs).distinct().show(10,False)\n\n[Stage 42:=================================================&gt;        (6 + 1) / 7]\n\n\n+-----------------+\n|actor.login      |\n+-----------------+\n|ItalloK          |\n|youmomlmao       |\n|InayatUllahKhan10|\n|Zireael07        |\n|smsrkursat       |\n|BiancaDavey      |\n|rr-weiyi-yu      |\n|burstknight      |\n|Shabirahmad1676  |\n|Ritiky23         |\n+-----------------+\nonly showing top 10 rows\n\n\n\n\n                                                                                \n\n\n\n# 데이터 확인 (컬럼의 filtered 데이터 distnct)\nimport pyspark.sql.functions as F\n\ncolumns = ['payload.pull_request.base.repo.language']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf.select(*select_exprs).distinct().filter(F.col('payload.pull_request.base.repo.language') == 'Python').show(10)\n\n[Stage 21:=================================&gt;                      (45 + 1) / 75]\n\n\n+---------------------------------------+\n|payload.pull_request.base.repo.language|\n+---------------------------------------+\n|                                 Python|\n+---------------------------------------+\n\n\n\n\n[Stage 21:=================================================&gt;      (66 + 2) / 75]\n\n                                                                                \n\n\n\n# 데이터 확인 (컬럼의 전체데이터 distinct)\nimport pyspark.sql.functions as F\n\ncolumns = ['repo.name','type']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf.select(*select_exprs).distinct().filter(F.col('repo.name').contains('pytorch/pytorch')).show(10, False)\n\n                                                                                \n\n\n+-------------------------+-----------------------------+\n|repo.name                |type                         |\n+-------------------------+-----------------------------+\n|pytorch/pytorch          |DeleteEvent                  |\n|pytorch/pytorch          |PullRequestEvent             |\n|pytorch/pytorch          |PullRequestReviewCommentEvent|\n|pytorch/pytorch          |IssuesEvent                  |\n|pytorch/pytorch          |ForkEvent                    |\n|pytorch/pytorch          |WatchEvent                   |\n|pytorch/pytorch.github.io|PullRequestEvent             |\n|pytorch/pytorch          |CreateEvent                  |\n|pytorch/pytorch          |IssueCommentEvent            |\n|pytorch/pytorch          |PullRequestReviewEvent       |\n+-------------------------+-----------------------------+\nonly showing top 10 rows\n\n\n\n\n[Stage 31:======================================================&gt; (73 + 1) / 75]\n\n                                                                                \n\n\n\n# 데이터 확인 (컬럼의 전체데이터 distinct)\nimport pyspark.sql.functions as F\n\ncolumns = ['repo.name','type']\nselect_exprs = [F.col(col_path).alias(col_path) for col_path in columns]\n\ndf1 = df.select(*select_exprs).distinct().filter(F.col('repo.name').contains('pytorch/pytorch'))\ndf1.filter(F.col('type').contains('IssuesEvent')).show(10, False)\n\n                                                                                \n\n\n+---------------+-----------+\n|repo.name      |type       |\n+---------------+-----------+\n|pytorch/pytorch|IssuesEvent|\n+---------------+-----------+\n\n\n\n\n\n산출할 데이터 모델결정 및 정제방법 고민\n\n기본과제 데이터 모델\n\nTop 10 Repo\n\n\nid\n@timestamp\nrepo_url\nrepo_name\npush_count\ncommit_count\npr_count\nfork_count\nissue_count\nwatch_count\n\n\nTop 10 User\n\n\nid\n@timestamp\nuser_name\npush_count\ncommit_count\npr_count\nissue_count\nissue_comment_count\n\n\nDaily Stats\n\n\nid\n@timestamp\ndistinct_user_cnt\ndistinct_repo_cnt\npush_count\ncommit_count\npr_count\nissue_count\nissue_comment_count\nrelease_count\n\n\n\n추가 데이터 모델 (가능하면 진행..)\n\npytorch 레포지토리에서 가장 Issue를 활발히 진행한 Top 10 User\n\n\nrepo.name\n\nrepo_name : 에서 가져온 ID/레포이름 형식 데이터가 pytorch/pytorch로 일치하는 조건만 filter\n\nactor.login\n\nis_bot : 값이 github-actions[bot]인 경우 1 (bot 제거용)\nuser_made_issue : 유저이름\n\ntype\n\nissue_count : 횟수 집계 (type=IssuesEvent)\n\n추가 데이터 모델에 대한 스키마 작성해보기\n\n\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType\n\nrepo_schema = StructType([\n    StructField('name', StringType(), True) # repo.name\n])\n\nactor_schema = StructType([\n    StructField('login', StringType(), True), # actor.login\n])\n\ntype_schema = StructField('type', StringType(), True) # type\n\n# payload.issue.assignee.login (로직확인 후 불필요해졌지만 다중 StructField참고용 샘플로 남겨둠)\npayload_schema = StructType([\n    StructField('issue',  StructType([StructField('assignee', StructType([StructField('login',StringType(), True)]))]))])"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240908/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240908/index.html",
    "title": "[DE스터디/5주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Airflow(+SlackAPI)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240908/index.html#다운로드-정제-저장을-하나의-task로-작성할지-별도의-task-를-두고-dependency-를-설정할지-결정해보세요",
    "href": "posts/meta-de-spark_and_airflow-20240908/index.html#다운로드-정제-저장을-하나의-task로-작성할지-별도의-task-를-두고-dependency-를-설정할지-결정해보세요",
    "title": "[DE스터디/5주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "다운로드, 정제, 저장을 하나의 task로 작성할지, 별도의 task 를 두고 dependency 를 설정할지 결정해보세요",
    "text": "다운로드, 정제, 저장을 하나의 task로 작성할지, 별도의 task 를 두고 dependency 를 설정할지 결정해보세요\n\n아래의 Task로 구분해서 진행\n\n다운로드\n오래된 파일 삭제\n정제 및 저장\n\nDependency는 원본 파일의 다운/삭제가 완료된 후에 정제/저장이 되도록 설정"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240908/index.html#데이터-다운로드-정제-저장-기능을-수행하는-dag-를-작성하고-스케줄링을-적용해보세요",
    "href": "posts/meta-de-spark_and_airflow-20240908/index.html#데이터-다운로드-정제-저장-기능을-수행하는-dag-를-작성하고-스케줄링을-적용해보세요",
    "title": "[DE스터디/5주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터 다운로드, 정제, 저장 기능을 수행하는 DAG 를 작성하고 스케줄링을 적용해보세요",
    "text": "데이터 다운로드, 정제, 저장 기능을 수행하는 DAG 를 작성하고 스케줄링을 적용해보세요\n\n진행방안\n\n작업 중 오류 발생시 Slack으로 로그 전송\n\n작업명, 실행시간, 로그 등 전송\n\n기준일자 계산\n\nAirflow의 작업일자를 기준으로 파일을 다운로드/삭제할 기준일자 계산\nAirflow작업실패시 catchup=True로 기준일자 자동계산하여 변수 수정 등 없이 자동진행\n\n파일에 대한 작업을 먼저 진행한 뒤 Slack으로 결과 전송\n\n다운로드와 삭제는 어느 것이 먼저 진행되어도 무방\n다운로드와 삭제함수가 내역을 리턴하고, Slack이 해당 내용(파일내역)으로 작업완료 알림\n\nSpark로 데이터를 정제한 뒤 저장하고, Slack으로 결과 전송\n\n데이터를 정제한 뒤 저장하고, Text파일로 Dataframe결과표를 저장\n\nairflow variable에 저장하는 것으로 변경\n\nText파일의 표를 Slack으로 전송하여 작업결과를 공유\n\nairflow variable에서 가져오는 것으로 변경\n\n\n\n\n\nAirflow Graph\n\n\n\nwork_w5_airflow_graph.jpg\n\n\n\n\nSlack 결과 샘플\n \n\n\n코드\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.slack.operators.slack import SlackAPIPostOperator\nfrom airflow.models.baseoperator import chain\nfrom airflow.models import Variable\nfrom datetime import datetime, timedelta\n\nimport sys\nsys.path.append('/opt/airflow/jobs')\nfrom airflowjob_down_new_delete_old import down_from_gharchive, del_old_file_gharchive\n\n# 오류시 메시지를 보낼 함수 정의\ndef slack_failure_callback(context):\n    slack_msg = f\"\"\"\n    :red_circle: DAG Failed\n    *Task*: {context.get('task_instance').task_id}\n    *Dag*: {context.get('task_instance').dag_id}\n    *Execution Time*: {context.get('execution_date')}\n    *Log URL*: {str(context.get('task_instance').log_url).replace(':8080',':8082')}\n    \"\"\"\n    \n    slack_alert = SlackAPIPostOperator(\n            slack_conn_id=\"slack_pkb\",\n            task_id='send_error_with_slack',\n            channel='#alarm',  # 전송할 Slack 채널\n            dag=dag,\n            blocks=[\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": (\n                            slack_msg\n                        ),\n                    },\n                }\n            ],\n            text=\"Airflow Error\",  # 필수 fallback 메시지\n        )\n    return slack_alert.execute(context=context)\n\n# DAG\ndag = DAG(\"gharchive-down_new_delete_old\", \n          default_args={\n            \"owner\": \"airflow\",\n            \"depends_on_past\": False, # 과거 실행에 의존\n            \"start_date\": datetime(2024, 9, 11),\n            \"retries\": 1,             # retry 횟수\n            \"retry_delay\": timedelta(minutes=3), # retry주기\n            'on_failure_callback': slack_failure_callback,\n            },\n          catchup=True, \n          tags=['PKB','gharchive','down&delete'])\n\n# Task0 : target_date 설정 (catchup 등으로 실행하는 부분 고려)\ndef set_down_date(**kwargs):\n    print(f\"Logical_date : {kwargs['logical_date']}\")\n    target_date_down = (kwargs['logical_date']- timedelta(days=1)).strftime('%Y-%m-%d')\n    print(f'Calculated target_date_down : {target_date_down}')\n    return target_date_down\n\ndown_date_calculate = PythonOperator(\n    task_id='calculate_down_date',\n    python_callable=set_down_date,\n    provide_context=True,\n    dag=dag,\n)\n## 기존 target_date_down 산출로직 (참고용)\n# target_date_down = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n\ndef set_delete_date(**kwargs):\n    print(f\"Logical_date : {kwargs['logical_date']}\")\n    target_date_delete = (kwargs['logical_date']- timedelta(days=30)).strftime('%Y-%m-%d')\n    print(f'Calculated target_date_delete : {target_date_delete}')\n    return target_date_delete\n\ndelete_date_calculate = PythonOperator(\n    task_id='calculate_delete_date',\n    python_callable=set_delete_date,\n    provide_context=True,\n    dag=dag,\n)\n## 기존 target_date_delete 산출로직 (참고용)\n# target_date_delete = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n\n\n# Task1 : 다운로드\ntarget_path = '/opt/bitnami/spark/data/gh_archive/'\n\ndownload_from_gharchive = PythonOperator(\n    task_id=\"file_down_gharchive\",\n    op_kwargs={\"target_date_down\": \"{{task_instance.xcom_pull(task_ids='calculate_down_date')}}\",\"target_path\":target_path},\n    python_callable = down_from_gharchive,\n    dag=dag\n)\n\n\n# Task2 : 1달지난 파일 삭제\n\ndelete_old_of_filepath = PythonOperator(\n    task_id=\"file_delete_old_gharchive\",\n    op_kwargs={\"target_date_delete\": \"{{task_instance.xcom_pull(task_ids='calculate_delete_date')}}\",\"target_path\":target_path},\n    python_callable = del_old_file_gharchive,\n    dag=dag\n)\n\n# Task3 : 결과 전송 with Slack (위 Task에서 리턴받은 결과를 출력)\nsend_result_with_slack__file = SlackAPIPostOperator(\n    slack_conn_id=\"slack_pkb\",\n    task_id='slack_file_down_delete',\n    channel='#alarm',  # 전송할 Slack 채널\n    dag=dag,\n    blocks=[\n        {\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": (\n                    \"*[gharchive다운로드]*\\n{{ task_instance.xcom_pull(task_ids='file_down_gharchive') }}\\n\\n\"\n                    \"*[gharchive삭제(30일이상)]*\\n{{ task_instance.xcom_pull(task_ids='file_delete_old_gharchive') }}\"\n                ),\n            },\n        }\n    ],\n    text=\"gharchive_file_downloaded_and_deleted\",  # 필수 fallback 메시지\n)\n\n\n# Task4 : Spark 데이터 정제\nspark_filter_gh = SparkSubmitOperator(\n        task_id='spark_filter_gharchive',\n        application=\"jobs/main_modified.py\", # 절대경로라면 /opt/airflow/jobs/main.py\n        application_args=[\"--target_date\", \"{{ task_instance.xcom_pull(task_ids='calculate_down_date') }}\"],\n        name=\"spark_filter_gh\",\n        conf={\n            'spark.master': 'spark://spark-master:7077',  # master 설정\n            'spark.dynamicAllocation.enabled': 'true',\n            'spark.dynamicAllocation.executorIdleTimeout': '2m',\n            'spark.dynamicAllocation.minExecutors': '1',\n            'spark.dynamicAllocation.maxExecutors': '3',\n            'spark.dynamicAllocation.initialExecutors': '1',\n            'spark.memory.offHeap.enabled': 'true',\n            'spark.memory.offHeap.size': '2G',\n            'spark.shuffle.service.enabled': 'true',\n            'spark.executor.memory': '2G',\n            'spark.driver.memory': '2G',\n            'spark.driver.maxResultSize': '0',\n        },\n        conn_id=\"spark-conn\", # 필수값. UI에서 conenctivity 설정해둔 기준\n        jars=\"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\",\n        executor_cores=1,\n        num_executors=2,\n        verbose=1,\n        dag=dag\n    )\n\n\n# Task5 : 결과 전송 with Slack\nsend_result_with_slack__spark = SlackAPIPostOperator(\n    slack_conn_id=\"slack_pkb\",\n    task_id='slack_spark_filter',\n    channel='#alarm',  # 전송할 Slack 채널\n    dag=dag,\n    blocks=[\n        {\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": f'{Variable.get(\"gharchive_df\")}',\n            },\n        }\n    ],\n    text=\"gharchive_spark_result\",  # 필수 fallback 메시지\n)\n\n\n# Flow\nchain([down_date_calculate, delete_date_calculate],\n      [download_from_gharchive, delete_old_of_filepath],\n      send_result_with_slack__file,\n      spark_filter_gh,\n      send_result_with_slack__spark)\n\n#[down_date_calculate, delete_date_calculate] &gt;&gt; [download_from_gharchive, delete_old_of_filepath] &gt;&gt; send_result_with_slack__file &gt;&gt; spark_filter_gh &gt;&gt; send_result_with_slack__spark"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240905/index.html",
    "href": "posts/meta-de-spark_and_airflow-20240905/index.html",
    "title": "[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "",
    "text": "데이터 엔지니어링 스터디 내용정리 - Spark, Elasticsearch\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240905/index.html#데이터-포맷에-적합한-elasticsearch-index-생성하기",
    "href": "posts/meta-de-spark_and_airflow-20240905/index.html#데이터-포맷에-적합한-elasticsearch-index-생성하기",
    "title": "[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "데이터 포맷에 적합한 elasticsearch index 생성하기",
    "text": "데이터 포맷에 적합한 elasticsearch index 생성하기\n\n기본코드\n\nimport argparse\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime, timedelta\nimport sys\nsys.path.append('/home/jovyan/jobs')\nfrom base import read_input, init_df, df_with_meta\nfrom filter import DailyStatFilter, TopRepoFilter, TopUserFilter, PytorchTopIssuerFilter\nfrom es import Es\n\n# SparkSession\nspark = (SparkSession\n    .builder\n    .master(\"local\")\n    .appName(\"spark-sql\")\n    .config(\"spark.driver.extraClassPath\", \"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n    .config(\"spark.jars\", \"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n    # for jupyter\n    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n    .config(\"spark.jars\", \"/home/jovyan/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")   \n    # 옵션추가 시작\n    .config(\"spark.executor.memory\",\"3G\")\n    .config(\"spark.driver.memory\",\"3G\")\n    .config(\"spark.executor.cores\",2)\n    # 옵션추가 끝\n    .getOrCreate())\n\n# 제출용 파일이므로 로그는 미출력되게 조정 (ALL,DEBUG,ERROR,FATAL,TRACE,WARN,INFO,OFF)\nspark.sparkContext.setLogLevel(\"OFF\")\n\nclass Args:\n    def __init__(self):\n        self.target_date = None\n        self.input_path = None\n        self.spark = None\n\nargs = Args()\nargs.spark = spark\n\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n24/09/05 14:06:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n\n# for Jupyter test\nargs.input_path = f\"/home/jovyan/data/gh_archive/2024-08-24-*.json.gz\"\n\n\n\n실습코드 데이터 확인\n\ndf = read_input(args.spark, args.input_path)\ndf = init_df(df)\n\n\n# DailyStatFilter 산출데이터 확인\nstat_filter = DailyStatFilter(args)\nstat_df = stat_filter.filter(df)\nstat_df.show()\n\n                                                                                \n\n\n+------------+------------+----------+--------+----------+--------------------+\n|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|\n+------------+------------+----------+--------+----------+--------------------+\n|317111      |255241      |1103808   |106155  |21821     |1480                |\n+------------+------------+----------+--------+----------+--------------------+\n\n\n\n[Stage 27:=============================================&gt;        (169 + 1) / 200]\n\n\n+------------+------------+----------+--------+----------+--------------------+\n|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|\n+------------+------------+----------+--------+----------+--------------------+\n|      317111|      255241|   1103808|  106155|     21821|                1480|\n+------------+------------+----------+--------+----------+--------------------+\n\n\n\n\n                                                                                \n\n\n\n# DailyStatFilter 산출데이터 with metadata(timestamp) 확인\nstat_df = df_with_meta(stat_df, args.target_date)\nstat_df.show()\n\n                                                                                \n\n\n+------------+------------+----------+--------+----------+--------------------+----------+\n|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|@timestamp|\n+------------+------------+----------+--------+----------+--------------------+----------+\n|      317111|      255241|   1103808|  106155|     21821|                1480|      null|\n+------------+------------+----------+--------+----------+--------------------+----------+\n\n\n\n\n\neleasticsearch index생성할 데이터 확인\n\nimport pyspark.sql.functions as F\n\n\n# Filter : repo_name = pytorch\nbase_df = df.filter(F.col('userid_and_repo_name') == 'pytorch/pytorch')\n\nissues_event_exists = base_df.filter(base_df[\"type\"] == \"IssuesEvent\").count() &gt; 0\nif issues_event_exists:\n    filtered_df = base_df.filter(F.col('type') == 'IssuesEvent')\nelse:\n    filtered_df is None\n\nif filtered_df is not None:\n    filtered_df.show()\n\n[Stage 5:=============================&gt;                             (1 + 1) / 2]\n\n\n+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n|        user_name|                 url|         created_at|         id|repository_id|size|distinct_size|comment|       type|            repo_url|userid_and_repo_name|repo_name|\n+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n|         hyperkai|https://api.githu...|2024-08-24 16:21:24|41307929891|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|       phanicoder|https://api.githu...|2024-08-24 16:48:56|41308170756|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|samuele-bortolato|https://api.githu...|2024-08-24 14:56:17|41307163891|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|  pytorchmergebot|https://api.githu...|2024-08-24 17:04:08|41308298815|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|       stevenvana|https://api.githu...|2024-08-24 19:20:50|41309466695|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|  pytorchmergebot|https://api.githu...|2024-08-24 20:33:26|41310013328|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n|  pytorchmergebot|https://api.githu...|2024-08-24 21:09:31|41310280894|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n\n\n\n\n                                                                                \n\n\n\n# groupby : \nresult_df = filtered_df.groupBy('user_name').pivot('type').count()\nresult_df = result_df.cache()\nresult_df.where((~F.col('user_name').contains('[bot]'))) \\\n            .orderBy(F.desc('IssuesEvent')) \\\n            .limit(10)\nresult_df.show()\n\n                                                                                \n\n\n+-----------------+-----------+\n|        user_name|IssuesEvent|\n+-----------------+-----------+\n|         hyperkai|          1|\n|       phanicoder|          1|\n|  pytorchmergebot|          3|\n|       stevenvana|          1|\n|samuele-bortolato|          1|\n+-----------------+-----------+\n\n\n\n\nresult_df.printSchema()\n\nroot\n |-- user_name: string (nullable = true)\n |-- IssuesEvent: long (nullable = true)\n\n\n\n\n\nelasticsearch index 생성해보기\n\nGPT에서 스키마를 주고 index 생성\n\n강의내용에 따라 수정하고자 했으나, GPT의 의도가 내 사용목적에 부합함\n\nuser_name필드\n\ntype:text로 검색가능한(full-text search) 텍스트 데이터(analyzer 적용)\n정렬을 위한 fields.keyword사용(analyzer 미적용)\n\nIssuesEvent필드\n\ntype:long\n\nPUT /pytorch_top_issuer\n{\n\"mappings\": {\n\"properties\": {\n\"user_name\": {\n  \"type\": \"text\",\n  \"fields\": {\n    \"keyword\": {\n      \"type\": \"keyword\",\n      \"ignore_above\": 256\n    }\n  }\n},\n\"IssuesEvent\": {\n  \"type\": \"long\"\n}\n}\n}\n}\n\n\n\n\n# 만든 index넣어보기 (코드 by GPT)\nimport requests\nimport json\n\n# Elasticsearch 클러스터의 URL 및 포트\nes_host = 'localhost'\nes_port = 9200\nindex_name = 'pytorch_top_issuer'\n\n# 인덱스 생성에 사용할 JSON 데이터\nmapping = {\n  \"mappings\": {\n    \"properties\": {\n      \"user_name\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"IssuesEvent\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n\n# Elasticsearch에 인덱스 생성 요청\nurl = f'http://es:9200/{index_name}'\nheaders = {'Content-Type': 'application/json'}\n\nresponse = requests.put(url, headers=headers, data=json.dumps(mapping))\n\n# 응답 출력\nprint(response.status_code)\nprint(response.json())\n\n200\n{'acknowledged': True, 'shards_acknowledged': True, 'index': 'pytorch_top_issuer'}"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240905/index.html#spark-dataframe-을-elasticsearch-에-저장해보기",
    "href": "posts/meta-de-spark_and_airflow-20240905/index.html#spark-dataframe-을-elasticsearch-에-저장해보기",
    "title": "[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "spark dataframe 을 elasticsearch 에 저장해보기",
    "text": "spark dataframe 을 elasticsearch 에 저장해보기\n\n# 강의에 사용된 elasticsearch 저장용 코드 그대로 사용\nclass Es(object):\n    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n        self.es_hosts = es_hosts\n        self.es_mode = mode\n        self.es_write_operation = write_operation\n        self.es_index_auto_create = \"yes\"\n        # self.es_mapping_id\n\n    def write_df(self, df, es_resource):\n        df.write.format(\"org.elasticsearch.spark.sql\") \\\n          .mode(self.es_mode) \\\n          .option(\"es.nodes\", self.es_hosts) \\\n          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n          .option(\"es.resource\", es_resource) \\\n          .save()\n\n# 호스트 지정 후 저장함수 활용\nes = Es(\"http://es:9200\")\nes.write_df(result_df, \"pytorch_top_issuer\")\n\n                                                                                \n\n\n\n# 앞서 만들어본 elasticsearch index 적용시의 코드\n## 위 실습에서 만들어본 pytorch_top_issuer 인덱스로 테스트. 이후부터는 auto_create옵션으로 진행할 예정\nclass Es(object):\n    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n        self.es_hosts = es_hosts\n        self.es_mode = mode\n        self.es_write_operation = write_operation\n        self.es_index_auto_create = \"no\"  # 기존 인덱스 사용을 위해 \"no\"로 설정\n        # self.es_mapping_id\n\n    def write_df(self, df, es_resource):\n        df.write.format(\"org.elasticsearch.spark.sql\") \\\n          .mode(self.es_mode) \\\n          .option(\"es.nodes\", self.es_hosts) \\\n          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n          .option(\"es.resource\", es_resource) \\\n          .save()\n\n# 호스트 지정 후 저장함수 활용\nes = Es(\"http://es:9200\")\nes.write_df(df, \"pytorch_top_issuer\")\n\n\n# 앞서 만들어본 elasticsearch index 적용시의 코드\n## 위 실습에서 만들어본 pytorch_top_issuer 인덱스로 테스트. 이후부터는 auto_create옵션으로 진행할 예정\n## 결과 확인을 위해 spark의 로그레벨을 다시 조정\n\nclass Es(object):\n    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n        self.es_hosts = es_hosts\n        self.es_mode = mode\n        self.es_write_operation = write_operation\n        self.es_index_auto_create = \"no\"  # 기존 인덱스 사용을 위해 \"no\"로 설정\n        # self.es_mapping_id\n\n    def write_df(self, df, es_resource):\n        df.write.format(\"org.elasticsearch.spark.sql\") \\\n          .mode(self.es_mode) \\\n          .option(\"es.nodes\", self.es_hosts) \\\n          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n          .option(\"es.resource\", es_resource) \\\n          .save()\n        \n# 결과확인을 위해 임시로 로그레벨 조정 (ALL,DEBUG,ERROR,FATAL,TRACE,WARN,INFO,OFF)\nspark.sparkContext.setLogLevel(\"WARN\")\n\n# 호스트 지정 후 저장함수 활용\nes = Es(\"http://es:9200\")\nes.write_df(df, \"pytorch_top_issuer\")"
  },
  {
    "objectID": "posts/meta-de-spark_and_airflow-20240905/index.html#저장된-데이터-확인해보기",
    "href": "posts/meta-de-spark_and_airflow-20240905/index.html#저장된-데이터-확인해보기",
    "title": "[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링",
    "section": "저장된 데이터 확인해보기",
    "text": "저장된 데이터 확인해보기\n\nfrom elasticsearch import Elasticsearch\n\n\n# Connect to Elasticsearch by GPT\nes = Elasticsearch(['http://es:9200'])\n\n# Define the index\nindex_name = 'pytorch_top_issuer'\n\n# Example search query to retrieve all documents (with a size limit)\nquery = {\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n\n# Execute the search query\nresponse = es.search(index=index_name, body=query, size=10)  # Adjust 'size' to retrieve more documents\n\n/tmp/ipykernel_69/3314748211.py:15: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n  response = es.search(index=index_name, body=query, size=10)  # Adjust 'size' to retrieve more documents\n\n\n\n# Parse and print the search results\nfor hit in response['hits']['hits']:\n    print(hit['_source'])\n\n{'user_name': 'hyperkai', 'IssuesEvent': 1}\n{'user_name': 'phanicoder', 'IssuesEvent': 1}\n{'user_name': 'pytorchmergebot', 'IssuesEvent': 3}\n{'user_name': 'stevenvana', 'IssuesEvent': 1}\n{'user_name': 'samuele-bortolato', 'IssuesEvent': 1}"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 4주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형-데이터-처리",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형-데이터-처리",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "불균형 데이터 처리",
    "text": "불균형 데이터 처리\n\n불균형이 많은 금융데이터의 경우, 단순히 하나의 값만 예측해내더라도 accuracy는 99%가 될 수 있음\n\n하지만 목표는 1%의 사기/이상 데이터를 찾아내는 것임\n이러한 문제를 막기위해 불균형 데이터의 처리가 필요함\n\n불균형데이터의 처리 방법\n\n데이터 수준 방법(학습전 데이터 처리)\n\nUnder-Sampling : RUS, Tomek Links, ENN 등\nOver-Sampling : SMOTE, ADASYN 등\nHybrid Method : SMOTETomek(SMOTE-Tomek), SMOTEENN(SMOTE-ENN)\n\n알고리즘 수준 방법(학습시 가중치 조정)\n\n가중치 조정\n\n\n\n\n불균형 데이터 처리(데이터수준) - Under-Sampling\n\nUnder-Sampling : 수가 더 많은 class의 값을 줄이는 것\nUnder-Sampling 종류\n\nRUS(Random Under Sampling) : 랜덤하게 제거\n\n데이터가 커지면 알고리즘적 방법 적용이 어려워,1차적으로 많이 사용됨\n파이썬에서도 빠르게 가능하며, SQL에서는 더 빠르게 가능(ORDER BY RAND() LIMIT)\n\nENN(Edited Nearest Neighbors) : KNN알고리즘 기반의 노이즈 제거\n\n근접한 N개의 class분포 내에서, 해당 class와 다른 값을 제거\n데이터의 분포가 정돈되는 효과\n정확한 비율을 설정할수는 없음\n\nTomeklinks : class의 경계를 기반으로 노이즈 제거\n\n클러스터링 한 2개의 그룹에서, 알고리즘으로 경계를 찾아서, 많은 쪽의 class를 제거\n경계선 구분이 명확해지는 효과\n\n\nUnder-Sampling 방법별 결과 비교\n\nRUS : 수가 많은 class 임의 제거\nENN : 오분류로 추정되는 값 제거 (예:주황색 그룹 안의 파란점 제거)\nTomeklinks : 경계선에 있는 값 제거 (경계를 찾아낸 후, 경계에서 많은 점을 제거)\n\n\n\nNote_week4_1.jpg\n\n\n\n\n\n\n불균형 데이터 처리(데이터수준) - Over-Sampling\n\nOver-Sampling : 수가 적은 class의 값 늘리는 것\n\n아래의 사유들로, 잘 쓰이지 않음\n\n전반적으로 과적합 위험이 있으며, 삭제가 아닌 생성으로 시간이 오래 걸림\n\n\nOver-Sampling 종류\n\nROS(Random Over Sampling) : (RUS와 같이) 랜덤하게 제거\n\n실제로 쓰이지는 않으며, 빠르지만 과적합의 위험\n\nSMOTE : KNN알고리즘 기반, 인접한 이웃과의 선형보간을 통한 데이터 생성\n\n기존 데이터의 단순 재생산이 아니므로 (상대적으로) 과적합 위험 적음\n\nADASYN : SMOTE의 개량형. 학습이 어려운 샘플(분류상 경계에 있는)에 더 많은 데이터를 생성\n\n(Tomeklink와 유사하게)경계 근처의 데이터에 더 많은 샘플을 생성\n\nSMOTE에 Weight를 추가한 개념(구분이 어려운 경계 근처 샘플에 Weight를 더 줌)\n\n경계 구분이 어려울 때 이점. 그러나 특정 데이터에 과적합될 위험\n\n\nOver-Sampling 방법별 결과 비교\n\nROS : 수가 적은 CLASS 임의 추가\nSMOTE : (ROS와 달리)기존 분포를 어느정보 반영하여 추가 (SMOTE알고리즘 활용)\nADASYN : 경계선에 걸친 값 위주로 값 추가 (ADASYN알고리즘 활용)\n\n\n\nNote_week4_2.jpg\n\n\n\n\n\n\n불균형 데이터 처리(데이터수준) - Hybrid Method\n\nHybrid Method : Over와 Under Sampling을 합친 Combined Method\nHybrid Method의 종류\n\nSMOTETomeks : SMOTE로 소수 클래스 증강 후, Tomeklinks로 제거\n\n경계가 뚜렷하지 않은 경우에 적합\nSMOTE + Tomeklinks\n\nSMOTEEN : SMOTE로 소수 클래스 증강 후, ENN으로 제거\n\n경계 근처에 노이즈가 많은 경우 적합\nSMOTE + ENN\n\n\nADASYN이 아닌, SMOTE로 먼저 보강하는 이유\n\n(ADASYN에 비해) SMOTE가 간단히/직관적으로 데이터를 증강해 안정적\n소수 클래스에 대한 균일한 보강으로(ADASYN은 경계값 위주), 일관적인 성능 발휘\n\nTest데이터의 적중이 목표이므로 Train과적합이 ADASYN보다 SMOTE가 덜함\n\n오래/넓게 쓰여왔던 방법론으로, 선호도가 높음\n\nHybrid Method 방법별 결과 비교\n\nSMOTETomeks : 제거된 값이 주로 경계에 모여있음\nSMOTEEN : 제거된 값이 경계 이외에도, 노이즈들에 해당\n추가를 의미하는 파란점은 유사하고[SMOTE공통사용]. 삭제를 의미하는 빨간점은 분포가 다름 \n\n\n\n\n불균형 데이터 처리(알고리즘 수준) - 가중치 조절\n\n가중치 조절 : 데이터 수정시가 아닌, 알고리즘 학습시에 가중치를 조절\n\n데이터 수준에서의 처리(Oversampling 등)는 시간이 오래걸리므로 학습시 처리하자는 관점\n데이터 처리를 하지 않아도 된다는 장점이 있어, 실제로 많이 사용\n가중치는 분석가의 직감이 아닌, 데이터의 비중을 참고하여 반영(a가 b보다 10배많다면 b에 가중치 10 부여)\n불균형이 극단적인 경우, 데이터 처리(Oversampling 등)로 비율을 맞춘 후 가중치부여\n\nLogloss 예시\n\n\\(y_i\\)와 \\((1 - y_i)\\) 앞에 가중치를 두고, 더 중요한 부분에 가중치를 높게 설정\n조정 전 Logloss \\[\\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\]\n조정 후 Logloss : Class가중치(Class weight)인 \\(w0\\), \\(w1\\)을 추가 \\[\\text{Weighted LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\omega_1 \\cdot y_i \\cdot \\log(\\hat{y}_i) + \\omega_0 \\cdot (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\\]\n1일 때 \\(w1\\), 0일 때 \\(w0\\)을 곱해 가중치를 부여했음"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#파생변수",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#파생변수",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "파생변수",
    "text": "파생변수\n\n파생변수 : 기존 데이터에서 새로운 정보를 추출하여 생성한 변수\n\nEDA 후 파생변수 생성이 필요하다고 판단될 때, 데이터 전처리를 완료한 후 진행\n\n전처리전 파생변수를 만들면, 전처리할 양도 많아지고 오래걸리게 됨\n\n전처리시 진행했던 변환, 인코딩, 군집화도 파생변수의 일종\n도메인 지식이 매우 중요함함\n\n파생변수의 유형\n\n집계 관련 변수 : 추가적인 집계가 필요하다면, SQL 등을 통해 생성 가능\n\n특정 집단/기간 등을 기준으로 추가할 수 있음\n연령별 평균 교통비 등의 변수. 보통 SQL에서 처리하여 가져옴\n\n날짜 관련 변수 : 시간/요일/휴일/공휴일 등을 추가\n\n요일, 휴일 등의 경우 도움이 될 수 있음\n\n예를 들어, 물동량 예측시 요일의존도가 높은데, 날짜기반으로 전주대비 물동량 차이 등을 파생변수로\n\n\n\n상호작용 변수 : 변수간의 상호작용 가능 & 본인의 도메인지식이 충분한 경우, SQL 등으로 생성가능\n\nPolynomial Interation(오래된 통계모델링 코드 등에서 보임)\n\n변수 간 상호작용에서 발생하는 비선형적 관계를 설명하기 위해, 기존 변수들의 곱을 변수로 추가한 것. 기존 통계적 모델링에서 활용됨\n차원의 저주에 쉽게 빠지는 단점과, 트리기반 모델에서 자체적으로 비선형관계를 처리하는 등(다중공선성 문제에서 자유롭다) ML로 넘어오며 이제는 거의 사용되지 않음\n\n트리 기반 모델이 Best가 되며 다중공선성은 덜 고려해도되지 않나 하는 경향이 있음\n\nsklearn.preprocessing으로 가능"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#모델-선택",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#모델-선택",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "모델 선택",
    "text": "모델 선택\n\n모델 선택 : 유형에 따라 적절한 모델을 선택해야 함.\n\n베이스라인 모델로 Regression 등을 먼저 돌려보기도 함\n\nRegression모델이 계수로 나와 해석이 쉽고 보고하기 좋음, 다만 성능이 좋지 않음\n\n\n모델 선택 유형\n\n문제 유형에 따른 분류 : 회귀 or 분류\n\n회귀 : 선형 회귀 등 Regressor로 끝나는 모델\n분류 : 로지스틱 회귀 등 Classifier로 끝나는 모델\n\n데이터 특성에 따른 분류 :\n\n데이터 크기 : 데이터가 클수록 계산효율이 좋은 모델\n\n큰 데이터에 약한 SVM모델 vs 상대적으로 나은 트리 기반 모델\n\n데이터 형태 : 데이터에 선형성이 없는 경우, Linear모델 지양\n\n반대라면 빠르게 결과를 볼 수 있는 선형 모델 사용\n\n\n해석 가능성에 따른 분류 : 통계적 회귀모델 vs 트리기반 모델 vs SVM/DNN\n\n통계적 회귀모델 : 가장 해석이 쉬움\n트리기반 모델 : feature importance 반환으로 어느정도 해석 가능\nSVN/DNN 모델 : 해석이 거의 불가한 Black box\n\n\n모델별 장단점\n\n\n\n\n\n\n\n\n\n구분\n선형/로지스틱 회귀\nSVM\n트리 기반 모델\n\n\n\n\n해석 가능성\n높음\n매우 낮음\n낮음\n\n\n데이터 크기\n소규모에서 효과적\n소규모에서 효과적\n대규모 데이터 처리 가능\n\n\n데이터 전처리\n고차원에서 매우 비효율적다중공선성 등 취약\n고차원 데이터에서 효율적많은 컬럼에 강해 SOTA였음많은 행은 어려움\n상대적으로 고차원에서 나쁘지 않음\n\n\n정확 / 이상치\n취약\n취약\n강건(노드가 나뉘므로)\n\n\n학습 속도\n빠름\n데이터 크기가 클수록 급격하게 느림\n보통\n\n\n\n트리기반 모델 장단점 비교\n\nLightGBM이 가장 빠르고, 이후는 병렬학습인 Bagging모델이 다음으로 빠름\n\n\n\n\n\n\n\n\n\n\n\n구분\nRandom Forest\nXGBoost\nLightGBM\nCATBOOST\nExtra Trees\n\n\n\n\n학습방식\nBagging(다수 트리의 평균)\nBoosting(순차적트리학습)\nBoosting(순차적트리학습)\nBoosting(순차적트리학습)\nBagging(다수 트리의 평균)\n\n\n속도\n빠름\n상대적으로 느림\n가장 빠름\n중간\n빠름\n\n\n특성중요도\n제공\n제공\n제공\n제공\n제공\n\n\n장점\n가장 간단\n(충분한 시간/리소스 있다면)성능이 뛰어감\n가장 빠름/ 대규모 데이터처리에 능함\n범주형 변수처리 특화. 자동처리 가능\nRF에서 더 랜덤성을 강화하여 강건한 모델 생성\n\n\n단점\n가장 간단\n느리고 메모리소모 많음\n메모리사용량 높음\n일부 설정에서 느림\nRF보다 성능 떨어지는 경우가 많음\n\n\n\n\n배깅(Bagging)과 부스팅(Boosting)\n\n보통은 부스팅이 이기는 경우가 많으며, 분산이 높고 불안정한 경우 배깅이 이기기도 함\n\n\n\n\n\n\n\n\n구분\n배깅(Bagging)\n부스팅(Boosting)\n\n\n\n\n개요\n여러 약한 모델의 결과를 결합→하나의 결과\n이전 약한 모델의 틀린 결과를 개선하며→순차적 학습\n\n\n학습방식\n원본 데이터에서 여러 하위집합(Bag) 생성하고,개별 Bag에 대해 약한 모델이 병렬적으로 학습됨\n원본 데이터에서 하위 집합을 생성하고,기본 모델을 생성해당 기본모델로 예측 후 틀린 예측에 더 높은 가중치를 부여하며 점차 개선\n\n\n특징\n모델의 분산을 줄이고 과적합을 피하는데 도움특히 분산을 줄이는데 중점\n모델의 분산을 줄이고 어려운 문제를 푸는데 도움특히 편향을 줄이는데 중점\n\n\n사용처\n분류기가 불안정하고 분산이 높을 때 유용\n모델의 안정성보다 문제 난이도가 어려울 때 유용\n\n\n\n\n배깅(Bagging)과 부스팅(Boosting)의 작동방식 차이"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형데이터imbalnced-data-처리-실습-샘플데이터",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형데이터imbalnced-data-처리-실습-샘플데이터",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "불균형데이터(Imbalnced Data) 처리 실습 (샘플데이터)",
    "text": "불균형데이터(Imbalnced Data) 처리 실습 (샘플데이터)\n\n# 샘플데이터 생성 : 3개 클래스의 2차원 데이터 생성 (더 겹치도록 분산 조정함)\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nnp.random.seed(42)\nclass_0 = np.random.multivariate_normal([2, 2], [[1, 0.8], [0.8, 1]], 300)\nclass_1 = np.random.multivariate_normal([4, 4], [[1, 0.6], [0.6, 1]], 150)\nclass_2 = np.random.multivariate_normal([6, 2], [[1, 0.5], [0.5, 1]], 50)\n\ndata = np.vstack([class_0, class_1, class_2])\nlabels = np.array([0] * 300 + [1] * 150 + [2] * 50)\n\ndf = pd.DataFrame(data, columns=[\"feature1\", \"feature2\"])\ndf[\"TARGET\"] = labels\n\n# 원본 클래스 분포\noriginal_counts = Counter(df[\"TARGET\"])\noriginal_counts\n\nCounter({0: 300, 1: 150, 2: 50})\n\n\n\nUndersampling - RUS\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# RUS 적용\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\n\n# RUS 제거된 데이터 계산\nrus_removed = df.merge(pd.DataFrame(X_rus, columns=[\"feature1\", \"feature2\"]), how=\"outer\", indicator=True)\nrus_removed = rus_removed[rus_removed[\"_merge\"] == \"left_only\"]\n\n# 시각화\n# plt.figure(figsize=(3, 3))\n\n# 원본 데이터\nplt.subplot(2, 1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\n# RUS 데이터\nplt.subplot(2, 1, 2)\nfor target_class in np.unique(y_rus):\n    subset = pd.DataFrame(X_rus, columns=[\"feature1\", \"feature2\"])[y_rus == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(rus_removed[\"feature1\"], rus_removed[\"feature2\"], color=\"red\", label=\"Removed by RUS\", alpha=0.6, marker='x')\nplt.title(\"RUS (Random Under Sampling)\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nUndersampling - ENN\n\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\n# ENN 적용\nenn = EditedNearestNeighbours()\nX_enn, y_enn = enn.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\n\n# 제거된 데이터 계산\nenn_removed = df.merge(pd.DataFrame(X_enn, columns=[\"feature1\", \"feature2\"]), how=\"outer\", indicator=True)\nenn_removed = enn_removed[enn_removed[\"_merge\"] == \"left_only\"]\n\n# 원본 데이터\nplt.subplot(2,1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\n# ENN 데이터\nplt.subplot(2,1, 2)\nfor target_class in np.unique(y_enn):\n    subset = pd.DataFrame(X_enn, columns=[\"feature1\", \"feature2\"])[y_enn == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(enn_removed[\"feature1\"], enn_removed[\"feature2\"], color=\"red\", label=\"Removed by ENN\", alpha=0.6, marker='x')\nplt.title(\"ENN (Edited Nearest Neighbours)\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUndersampling - Tomek Links\n\nfrom imblearn.under_sampling import TomekLinks\n\n# Tomek Links 적용\ntomek = TomekLinks()\nX_tomek, y_tomek = tomek.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\n\ntomek_removed = df.merge(pd.DataFrame(X_tomek, columns=[\"feature1\", \"feature2\"]), how=\"outer\", indicator=True)\ntomek_removed = tomek_removed[tomek_removed[\"_merge\"] == \"left_only\"]\n\n# 원본 데이터\nplt.subplot(2, 1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\n\n# Tomek Links 데이터\nplt.subplot(2,1, 2)\nfor target_class in np.unique(y_tomek):\n    subset = pd.DataFrame(X_tomek, columns=[\"feature1\", \"feature2\"])[y_tomek == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(tomek_removed[\"feature1\"], tomek_removed[\"feature2\"], color=\"red\", label=\"Removed by Tomek Links\", alpha=0.6, marker='x')\nplt.title(\"Tomek Links\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOversampling - ROS\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Original class distribution\noriginal_counts = Counter(df[\"TARGET\"])\n\n# Random Oversampling\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\nros_added = pd.DataFrame(X_ros, columns=[\"feature1\", \"feature2\"]).iloc[len(df):]\n\n# Visualization for Random Oversampling\nplt.subplot(2, 1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nfor target_class in np.unique(y_ros):\n    subset = pd.DataFrame(X_ros, columns=[\"feature1\", \"feature2\"])[y_ros == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(ros_added[\"feature1\"], ros_added[\"feature2\"], color=\"red\", label=\"Added by Random Oversampling\", alpha=0.6, marker='x')\nplt.title(\"Random Oversampling\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOversampling - SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\n# SMOTE Oversampling\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\nsmote_added = pd.DataFrame(X_smote, columns=[\"feature1\", \"feature2\"]).iloc[len(df):]\n\n\n# Visualization for SMOTE\nplt.subplot(2, 1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nfor target_class in np.unique(y_smote):\n    subset = pd.DataFrame(X_smote, columns=[\"feature1\", \"feature2\"])[y_smote == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(smote_added[\"feature1\"], smote_added[\"feature2\"], color=\"red\", label=\"Added by SMOTE\", alpha=0.6, marker='x')\nplt.title(\"SMOTE Oversampling\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOversampling - ADASYN\n\nfrom imblearn.over_sampling import ADASYN\n\n# ADASYN Oversampling\nadasyn = ADASYN(random_state=42)\nX_adasyn, y_adasyn = adasyn.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\nadasyn_added = pd.DataFrame(X_adasyn, columns=[\"feature1\", \"feature2\"]).iloc[len(df):]\n\n\n# Visualization for ADASYN\nplt.subplot(2, 1, 1)\nfor target_class in original_counts.keys():\n    subset = df[df[\"TARGET\"] == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.title(\"Original Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nfor target_class in np.unique(y_adasyn):\n    subset = pd.DataFrame(X_adasyn, columns=[\"feature1\", \"feature2\"])[y_adasyn == target_class]\n    plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\nplt.scatter(adasyn_added[\"feature1\"], adasyn_added[\"feature2\"], color=\"red\", label=\"Added by ADASYN\", alpha=0.6, marker='x')\nplt.title(\"ADASYN Oversampling\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHybrid Method - SMOTEENN\n\nfrom imblearn.combine import SMOTEENN\n\n# Original class distribution\noriginal_counts = Counter(df[\"TARGET\"])\n\n# SMOTEENN Oversampling\nsmoteenn = SMOTEENN(random_state=42)\nX_smoteenn, y_smoteenn = smoteenn.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\n\n# Identify added and deleted data for SMOTEENN\nsmoteenn_all = pd.DataFrame(X_smoteenn, columns=[\"feature1\", \"feature2\"])\nsmoteenn_deleted = df[~df[[\"feature1\", \"feature2\"]].apply(tuple, axis=1).isin(smoteenn_all.apply(tuple, axis=1))]\nsmoteenn_added = smoteenn_all[~smoteenn_all.apply(tuple, axis=1).isin(df[[\"feature1\", \"feature2\"]].apply(tuple, axis=1))]\n\n# Visualization function\ndef visualize(title, original_df, processed_df, added_df, deleted_df):\n    # Original data\n    plt.subplot(2, 1, 1)\n    for target_class in original_counts.keys():\n        subset = original_df[original_df[\"TARGET\"] == target_class]\n        plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\n    plt.title(\"Original Data\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.legend()\n    plt.grid(True)\n\n    # Processed data\n    plt.subplot(2, 1, 2)\n    for target_class in np.unique(processed_df[\"TARGET\"]):\n        subset = processed_df[processed_df[\"TARGET\"] == target_class]\n        plt.scatter(subset[\"feature1\"], subset[\"feature2\"], label=f\"Class {target_class}\", alpha=0.6)\n    plt.scatter(added_df[\"feature1\"], added_df[\"feature2\"], color=\"blue\", label=\"Added Samples\", alpha=0.6, marker='x')\n    plt.scatter(deleted_df[\"feature1\"], deleted_df[\"feature2\"], color=\"red\", label=\"Deleted Samples\", alpha=0.6, marker='x')\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Visualize SMOTEENN\nvisualize(\"SMOTEENN Oversampling\", df, smoteenn_all.assign(TARGET=y_smoteenn), smoteenn_added, smoteenn_deleted)\n\n\n\n\n\n\n\n\n\n\nHybrid Method - SMOTETomek\n\nfrom imblearn.combine import SMOTETomek\n\n# SMOTETomek Oversampling\nsmotetomek = SMOTETomek(random_state=42)\nX_smotetomek, y_smotetomek = smotetomek.fit_resample(df[[\"feature1\", \"feature2\"]], df[\"TARGET\"])\n\n# Identify added and deleted data for SMOTETomek\nsmotetomek_all = pd.DataFrame(X_smotetomek, columns=[\"feature1\", \"feature2\"])\nsmotetomek_deleted = df[~df[[\"feature1\", \"feature2\"]].apply(tuple, axis=1).isin(smotetomek_all.apply(tuple, axis=1))]\nsmotetomek_added = smotetomek_all[~smotetomek_all.apply(tuple, axis=1).isin(df[[\"feature1\", \"feature2\"]].apply(tuple, axis=1))]\n\n# Visualize SMOTETomek\nvisualize(\"SMOTETomek Oversampling\", df, smotetomek_all.assign(TARGET=y_smotetomek), smotetomek_added, smotetomek_deleted)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형-데이터-처리방법별-성능비교-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#불균형-데이터-처리방법별-성능비교-실습",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "불균형 데이터 처리방법별 성능비교 실습",
    "text": "불균형 데이터 처리방법별 성능비교 실습\n\n데이터 로딩 및 train/test 나누기\n\n# data split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\n# 지난수업에서 전처리한 csv파일 로딩\ndata = pd.read_csv('data_preprocessed.csv')\n\n# Train/Test셋 데이터 분할 (stratify : y비율을 비슷하게 유지하는 층화추출출)\ntrain , test = train_test_split(data, test_size = 0.1, random_state = 42, stratify = data['TARGET'])\n\n\n\n성능비교 : Vanilla(별도 처리하지 않음)\n\nRandomForestClassifier 활용\n아래 코드에서 사용된 predict_proba() : predict가 확률계산 후 확률에 따라 속하는 y를 산출했다면, predict_proba는 확률까지만 계산\n\nAUC를 계산하기 위해 사용함 (Imbalance data는 단순히 accuracy로 평가할 수 없음)\nAUC/ROC 이해를 돕기위한 움직이는 그래프 자료 : https://angeloyeo.github.io/2020/08/05/ROC.html\n\n\n\nX_vanila = train.drop(['TARGET'], axis = 1)\ny_vanila = train['TARGET']\n\n\n%%time\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_vanila = RandomForestClassifier(random_state = 42)\nrf_vanila.fit(X_vanila, y_vanila)\n\ny_vanila_pred = rf_vanila.predict(test.drop(['TARGET'], axis = 1))\ny_vanila_proba = rf_vanila.predict_proba(test.drop(['TARGET'], axis = 1)) # predict_proba : 0.5보다 높고 낮음까지 계산하지말고, 그전의 확률까지만계산\n\ny_test = test['TARGET']\n\nCPU times: total: 32.3 s\nWall time: 1min 54s\n\n\n\nvanila_accuracy = accuracy_score(y_test, y_vanila_pred)\nvanila_cf = confusion_matrix(y_test, y_vanila_pred)\nvanila_roc_auc = roc_auc_score(y_test, y_vanila_proba[:,1])\n\nprint(f\"* Vanila_accuracy : {vanila_accuracy}\")\nprint(f\"* Vanila_roc_auc{vanila_roc_auc}\")\nprint(f\"* Vanila_confusion materix :\\n {vanila_cf}\")\n\n* Vanila_accuracy : 0.9193223204994797\n* Vanila_roc_auc0.7084669152907\n* Vanila_confusion materix :\n [[28266     3]\n [ 2478     5]]\n\n\n\n\n심화 1 : Rapids\n\nsklearn은 GPU미지원 시기에 개발되어, sklearn의 RandomForest는 GPU연산 미지원\n\nXGB나 LGBM은 지원 (XGB의 RandomForest는 지원원)\n\nGPU연산 미지원 라이브러리를 GPU연산 하고싶을 때, RAPIDS라이브러리의 cuml 사용 가능\n\ncuml은 이외에도 다양한 sklearn의 기능을 대체 가능\n\nRAPIDS 설치\n\nCUDA버전을 먼저 확인\n!nvcc --version\n공식git(설치법 등) : https://github.com/rapidsai/cuml?tab=readme-ov-file\nthe RAPIDS Release Selector : https://docs.rapids.ai/install/#selector\n\n환경에 맞춰 선택하면, 아래와 같이 Command가 생성됨\n  pip install \\\n      --extra-index-url=https://pypi.nvidia.com \\\n      \"cudf-cu12==24.12.*\" \"dask-cudf-cu12==24.12.*\" \"cuml-cu12==24.12.*\" \\\n      \"cugraph-cu12==24.12.*\" \"nx-cugraph-cu12==24.12.*\" \"cuspatial-cu12==24.12.*\" \\\n      \"cuproj-cu12==24.12.*\" \"cuxfilter-cu12==24.12.*\" \"cucim-cu12==24.12.*\" \\\n      \"pylibraft-cu12==24.12.*\" \"raft-dask-cu12==24.12.*\" \"cuvs-cu12==24.12.*\" \\\n      \"nx-cugraph-cu12==24.12.*\"\n\n\nRAPIDS cuml 사용 예시 ``` from cuml.ensemble import RandomForestClassifier as cuml_RandomForestClassifier\nrf_vanila_rapid = cuml_RandomForestClassifier(random_state=42) rf_vanila_rapid.fit(X_vanila, y_vanila) y_vanila_rapid_pred = rf_vanila_rapid.predict(test.drop([‘TARGET’], axis = 1)) y_vanila_rapid_proba = rf_vanila_rapid.predict_proba(test.drop([‘TARGET’], axis = 1)) y_test = test[‘TARGET’] ```\nCUDA버전확인 샘플\n\n\n# CUDA버전확인 샘플\n!nvcc --version\n\n\ncuml의 RandomForestClassifier 샘플\n\n\n%%time\nfrom cuml.ensemble import RandomForestClassifier as cuml_RandomForestClassifier\n\nrf_vanila_rapid = cuml_RandomForestClassifier(random_state=42)\nrf_vanila_rapid.fit(X_vanila, y_vanila)\ny_vanila_rapid_pred = rf_vanila_rapid.predict(test.drop(['TARGET'], axis = 1))\ny_vanila_rapid_proba = rf_vanila_rapid.predict_proba(test.drop(['TARGET'], axis = 1))\ny_test = test['TARGET']\n\n\n\n성능비교 : RUS\n\nWall time: 1min 54s → 16.2 s\nVanila_accuracy : 0.9193223204994797 → 0.694686524453694\nVanila_roc_auc0.7084669152907 → 0.7370645914878501\nVanila_confusion materix : [[28266 3] → [[19723 8546] [ 2478 5]] → [ 843 1640]]\n\n\n%%time\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# RUS 적용\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(train.drop(['TARGET'], axis = 1), train[\"TARGET\"])\nX_rus.shape\n\nCPU times: total: 156 ms\nWall time: 513 ms\n\n\n(44684, 131)\n\n\n\n%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_rus = RandomForestClassifier(random_state = 42)\nrf_rus.fit(X_rus, y_rus)\ny_rus_pred = rf_rus.predict(test.drop(['TARGET'], axis = 1))\ny_rus_proba = rf_rus.predict_proba(test.drop(['TARGET'], axis = 1))\ny_test = test['TARGET']\n\nCPU times: total: 4.41 s\nWall time: 16.2 s\n\n\n\nrus_accuracy = accuracy_score(y_test, y_rus_pred)\nrus_cf = confusion_matrix(y_test, y_rus_pred)\nrus_roc_auc = roc_auc_score(y_test, y_rus_proba[:,1])\n\nsampling_method = 'RUS'\nprint(f\"* {sampling_method}_accuracy : {rus_accuracy}\")\nprint(f\"* {sampling_method}_roc_auc : {rus_roc_auc}\")\nprint(f\"*{sampling_method}_confusion materix :\\n {rus_cf}\")\n\n* RUS_accuracy : 0.694686524453694\n* RUS_roc_auc : 0.7370645914878501\n*RUS_confusion materix :\n [[19723  8546]\n [  843  1640]]\n\n\n\n함수화 (이후부터 함수로 진행)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\n\ndef compute_for_samplers(train, test, sampler, return_metric_only= False):\n  X_sampled, y_sampled = sampler.fit_resample(train.drop(['TARGET'], axis = 1), train[\"TARGET\"])\n  model = RandomForestClassifier(random_state = 42)\n  model.fit(X_sampled, y_sampled)\n  y_pred = model.predict(test.drop(['TARGET'], axis = 1))\n  y_proba = model.predict_proba(test.drop(['TARGET'], axis = 1))\n  y_test = test['TARGET']\n\n  accuracy = accuracy_score(y_test, y_pred)\n  auc = roc_auc_score(y_test, y_proba[:,1])\n  cf = confusion_matrix(y_test, y_pred)\n  if return_metric_only:\n    return accuracy, cf, auc\n  else:\n    return {'acc':accuracy,\n            'auc':auc,\n            'cf':cf,\n            'X_sampled' : X_sampled,\n            'y_sampled' : y_sampled,\n            'model' : model,\n            'y_pred' : y_pred,\n            'y_proba' : y_proba}\n\n\n\n성능비교 : ENN\n\n요약\n\n시간이 더 오래걸리고, 성능 도움도 되지 않았음\n비율을 맞춰주는 알고리즘이 아니고, 연산시간이 길어짐\n\nWall time: 1min 54s → 3min 37s\nVanila_accuracy : 0.9193223204994797 → 0.9192572840790842\nVanila_roc_auc : 0.7084669152907 → 0.7138916844382973\nVanila_confusion materix : [[28266 3] → [[28240 29] [ 2478 5]] → [ 2454 29]]\n\n\n%%time\n\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nenn = EditedNearestNeighbours()\nenn_result = compute_for_samplers(train, test, enn)\n\nprint(enn_result['acc'])\nprint(enn_result['auc'])\nprint(enn_result['cf'])\n\n0.9192572840790842\n0.7138916844382973\n[[28240    29]\n [ 2454    29]]\nCPU times: total: 6min 13s\nWall time: 3min 37s\n\n\n\nImbalance를 적절히 제거했다면, y에 맞춰 2만여개의 데이터가 남아있어야 함\n실제로는 22만개가 남아있는데, ENN은 비율을 맞추는 알고리즘이 아닌 이상한 것(노이즈)을 걸러내기 때문\nk값을 늘릴수록 없애는 값이 늘어남(knn기반이므로로)\n\n\n# X_train 샘플 차이\nprint(f\"\"\"* ENN전 : {train.shape}\n* ENN후 : \"{enn_result['X_sampled'].shape}\"\"\")\n\n* ENN전 : (276759, 132)\n* ENN후 : \"(221430, 131)\n\n\n\n# y_train 값 분포\nprint(f\"\"\"\n* ENN 전\n{train.groupby('TARGET').size()}\n\n* ENN 후\n{enn_result['y_sampled'].value_counts()}\"\"\")\n\n\n* ENN 전\nTARGET\n0    254417\n1     22342\ndtype: int64\n\n* ENN 후\nTARGET\n0    199088\n1     22342\nName: count, dtype: int64\n\n\n\n\n성능비교 : Tomek-Links\n\n요약\n\n오래 걸리고, 성능도 개선되지 않음\n\nWall time: 1min 54s → 4min 8s\nVanila_accuracy : 0.9193223204994797 → 0.9194198751300728\nVanila_roc_auc : 0.7084669152907 → 0.7135753731337223\nVanila_confusion materix : [[28266 3] → [[28266 3] [ 2478 5]] → [ 2475 8]]\n\n\n%%time\n\nfrom imblearn.under_sampling import TomekLinks\n\ntml = TomekLinks()\ntml_result = compute_for_samplers(train, test, tml)\n\nprint(tml_result['acc'])\nprint(tml_result['auc'])\nprint(tml_result['cf'])\n\n0.9194198751300728\n0.7135753731337223\n[[28266     3]\n [ 2475     8]]\nCPU times: total: 7min 37s\nWall time: 4min 8s\n\n\n\n# X_train 샘플 차이\nprint(f\"\"\"* TomekLinks 전 : {train.shape}\n* TomekLinks 후 : \"{tml_result['X_sampled'].shape}\"\"\")\n\n* TomekLinks 전 : (276759, 132)\n* TomekLinks 후 : \"(266325, 131)\n\n\n\n# y_train 값 분포\nprint(f\"\"\"\n* TomekLinks 전\n{train.groupby('TARGET').size()}\n\n* TomekLinks 후\n{tml_result['y_sampled'].value_counts()}\"\"\")\n\n\n* TomekLinks 전\nTARGET\n0    254417\n1     22342\ndtype: int64\n\n* TomekLinks 후\nTARGET\n0    243983\n1     22342\nName: count, dtype: int64\n\n\n\n\n성능비교 : ROS\n\nWall time: 1min 54s → 3min 4s\nVanila_accuracy : 0.9193223204994797 → 0.919062174817898\nVanila_roc_auc : 0.7084669152907 → 0.7287776769542174\nVanila_confusion materix : [[28266 3] → [[28223 46] [ 2478 5]] → [ 2443 40]]\n\n\n%%time\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\nros_result = compute_for_samplers(train, test, ros)\n\nprint(ros_result['acc'])\nprint(ros_result['auc'])\nprint(ros_result['cf'])\n\n0.919062174817898\n0.7287776769542174\n[[28223    46]\n [ 2443    40]]\nCPU times: total: 1min 1s\nWall time: 3min 4s\n\n\n\n# X_train 샘플 차이\nprint(f\"\"\"* ROS 전 : {train.shape}\n* ROS 후 : {ros_result['X_sampled'].shape}\"\"\")\n\n* ROS 전 : (276759, 132)\n* ROS 후 : (508834, 131)\n\n\n\n# y_train 값 분포\nprint(f\"\"\"\n* ROS 전\n{train.groupby('TARGET').size()}\n\n* ROS 후\n{ros_result['y_sampled'].value_counts()}\"\"\")\n\n\n* ROS 전\nTARGET\n0    254417\n1     22342\ndtype: int64\n\n* ROS 후\nTARGET\n0    254417\n1    254417\nName: count, dtype: int64\n\n\n\n\n성능비교 : SMOTE\n\nWall time: 1min 54s → 3min 23s\nVanila_accuracy : 0.9193223204994797 → 0.9192572840790842\nVanila_roc_auc : 0.7084669152907 → 0.6958898421466617\nVanila_confusion materix : [[28266 3] → [[28268 1] [ 2478 5]] → [ 2482 1]]\n\n\n%%time\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nsmote_result = compute_for_samplers(train, test, smote)\n\nprint(smote_result['acc'])\nprint(smote_result['auc'])\nprint(smote_result['cf'])\n\n0.9192572840790842\n0.6958898421466617\n[[28268     1]\n [ 2482     1]]\nCPU times: total: 1min 16s\nWall time: 3min 23s\n\n\n\n# X_train 샘플 차이\nprint(f\"\"\"* SMOTE 전 : {train.shape}\n* SMOTE 후 : \"{smote_result['X_sampled'].shape}\"\"\")\n\n* SMOTE 전 : (276759, 132)\n* SMOTE 후 : \"(508834, 131)\n\n\n\n# y_train 값 분포\nprint(f\"\"\"\n* SMOTE 전\n{train.groupby('TARGET').size()}\n\n* SMOTE 후\n{smote_result['y_sampled'].value_counts()}\"\"\")\n\n\n* SMOTE 전\nTARGET\n0    254417\n1     22342\ndtype: int64\n\n* SMOTE 후\nTARGET\n0    254417\n1    254417\nName: count, dtype: int64\n\n\n\n\n성능비교 결과\n\nimblearn의 성능개선에 비해 들어가는 노력(시간)이 큼\n데이터 사이즈와 변수가 적었던 과거와 달리, 유의미한 결과를 내지 못하게 되었음\n어느정도 비율을 맞추는데 사용되고, class weights를 통한 가중치 조절에 자리를 내어줌\n\n\n# 이제 모델 성능 비교\n# 선형 회귀 vs SVR vs RFR\n# 로지스틱회귀 vs svc vs RFC\n# RFC vs XGBC vs LGBMC vs CATBOOSTC vs EXTRATREEC"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#model별-성능-비교-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#model별-성능-비교-실습",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "Model별 성능 비교 실습",
    "text": "Model별 성능 비교 실습\n\nSVM의 실행시간문제로 데이터 사이즈를 줄여서 실습\n\n\n라이브러리 설치 및 데이터 분할\n\n!pip install catboost xgboost lightgbm\n# 라이브러리 설치\n\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\nimport time\n\ntrain_, _  = train_test_split(train, random_state = 42, test_size = 0.9)\ntrain_.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 27675 entries, 105024 to 224936\nColumns: 132 entries, SK_ID_CURR to EMERGENCYSTATE_MODE_nan\ndtypes: float64(89), int64(43)\nmemory usage: 28.1 MB\n\n\n\n\n모델 성능 비교\n\n# 모델 리스트\nmodels = {\n    \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n    \"SVC\": SVC(class_weight=\"balanced\", probability = True, random_state=42),\n    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", random_state=42),\n    \"XGBoost\": XGBClassifier(scale_pos_weight=10, eval_metric=\"logloss\", random_state=42),\n    \"CatBoost\": CatBoostClassifier(class_weights=[1, 10], verbose=0, random_state=42),\n    \"Extra Trees\": ExtraTreesClassifier(class_weight=\"balanced\", random_state=42),\n}\n\nX_train = train_.drop(['TARGET'], axis = 1)\ny_train = train_['TARGET']\nX_test = test.drop(['TARGET'], axis = 1)\ny_test = test['TARGET']\nrsts = {}\n# 학습 및 평가\nfor name, model in models.items():\n    print(f\"\\n{name}\")\n    start = time.time()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_proba[:,1])\n    cf = confusion_matrix(y_test, y_pred)\n    rsts[name] = {'acc':accuracy,\n                  'auc':auc,\n                  'cf':cf,\n                  'model' : model,\n                  'y_pred' : y_pred,\n                  'y_proba' : y_proba,\n                  'time' : time.time() - start }\n    \n    print(f\"\"\"\n{classification_report(y_test, y_pred)}\"\"\")\n\n\nRandom Forest\n\n              precision    recall  f1-score   support\n\n           0       0.92      1.00      0.96     28269\n           1       0.50      0.00      0.00      2483\n\n    accuracy                           0.92     30752\n   macro avg       0.71      0.50      0.48     30752\nweighted avg       0.89      0.92      0.88     30752\n\n\nSVC\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.53      0.67     28269\n           1       0.10      0.60      0.17      2483\n\n    accuracy                           0.53     30752\n   macro avg       0.52      0.56      0.42     30752\nweighted avg       0.87      0.53      0.63     30752\n\n\nLightGBM\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 2199, number of negative: 25476\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10852\n[LightGBM] [Info] Number of data points in the train set: 27675, number of used features: 120\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.80      0.87     28269\n           1       0.19      0.52      0.28      2483\n\n    accuracy                           0.78     30752\n   macro avg       0.57      0.66      0.57     30752\nweighted avg       0.89      0.78      0.82     30752\n\n\nXGBoost\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.89      0.91     28269\n           1       0.21      0.33      0.26      2483\n\n    accuracy                           0.85     30752\n   macro avg       0.57      0.61      0.58     30752\nweighted avg       0.88      0.85      0.86     30752\n\n\nCatBoost\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.88      0.91     28269\n           1       0.22      0.38      0.28      2483\n\n    accuracy                           0.84     30752\n   macro avg       0.58      0.63      0.59     30752\nweighted avg       0.88      0.84      0.86     30752\n\n\nExtra Trees\n\n              precision    recall  f1-score   support\n\n           0       0.92      1.00      0.96     28269\n           1       0.00      0.00      0.00      2483\n\n    accuracy                           0.92     30752\n   macro avg       0.46      0.50      0.48     30752\nweighted avg       0.85      0.92      0.88     30752\n\n\n\n\n# 파일로 저장\nimport pickle\nwith open('rsts.pkl', 'wb') as f:\n  pickle.dump(rsts, f)\n\n\ndf_results = pd.DataFrame(rsts).loc[['acc','auc','cf','time'],:]\ndf_results\n\n\n\n\n\n\n\n\nRandom Forest\nSVC\nLightGBM\nXGBoost\nCatBoost\nExtra Trees\n\n\n\n\nacc\n0.919257\n0.531835\n0.779429\n0.846839\n0.839588\n0.91916\n\n\nauc\n0.707628\n0.596337\n0.736252\n0.703948\n0.735909\n0.689785\n\n\ncf\n[[28266, 3], [2480, 3]]\n[[14872, 13397], [1000, 1483]]\n[[22670, 5599], [1184, 1299]]\n[[25235, 3034], [1676, 807]]\n[[24872, 3397], [1536, 947]]\n[[28266, 3], [2483, 0]]\n\n\ntime\n9.124638\n762.844463\n0.5796\n0.665159\n9.372309\n5.709153\n\n\n\n\n\n\n\n\nLGBM으로 Model selection\n\nAUC기준 LGBM(0.736252), CatBoost(0.735909) 순으로 좋음\nCF기준 LightGBM과 XGBoost는 Tradeoff가 있음\n\nLightGBM과는 실제 1인 데이터를 잘 찾고(1299), XGBoost는 1이 아닌데 1로 분류하는 경우가 적다(3034)\n과제의 성격에 따라 모델을 선택\n\nTime기준 LGBM과 XGBoost가 빠름\n현재 과제는 실제 1을 찾아내는 것이 중요하다는 관점으로, LGBM선택"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#model-selection-후-케이스별-비교",
    "href": "posts/meta-cm-sql_and_ml_xai-20250112/index.html#model-selection-후-케이스별-비교",
    "title": "[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택",
    "section": "Model selection 후 케이스별 비교",
    "text": "Model selection 후 케이스별 비교\n\n비교할 2가지 Case\n\nLightGBM with RUS\nLightGBM(Weighted) without RUS\n\nWeight에 대한 설정방법은 모델별로 다름\n  models = {\n  \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\"),\n  \"SVC\": SVC(class_weight=\"balanced\", probability = True),\n  \"LightGBM\": LGBMClassifier(class_weight=\"balanced\"),\n  \"XGBoost\": XGBClassifier(scale_pos_weight=10, eval_metric=\"logloss\"),\n  \"CatBoost\": CatBoostClassifier(class_weights=[1, 10], verbose=0),\n  \"Extra Trees\": ExtraTreesClassifier(class_weight=\"balanced\"),\n  }\n\n\n# Base model with RUS\nX_train_rus, y_train_rus = X_rus, y_rus\n\nrus_model = LGBMClassifier(random_state=42).fit(X_train_rus, y_train_rus)\ny_pred_rus = rus_model.predict(X_test)\ny_proba_rus = rus_model.predict_proba(X_test)\n\naccuracy_rus = accuracy_score(y_test, y_pred_rus)\nauc_rus = roc_auc_score(y_test, y_proba_rus[:,1])\ncf_rus = confusion_matrix(y_test, y_pred_rus)\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 22342, number of negative: 22342\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005589 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 11024\n[LightGBM] [Info] Number of data points in the train set: 44684, number of used features: 121\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=0.000000\n\n\n\n# Weight model without RUS\nweights_model = LGBMClassifier(random_state=42, class_weight='balanced').fit(X_train, y_train)\ny_pred_weights = weights_model.predict(X_test)\ny_proba_weights = weights_model.predict_proba(X_test)\n\naccuracy_weights = accuracy_score(y_test, y_pred_weights)\nauc_weights = roc_auc_score(y_test, y_proba_weights[:,1])\ncf_weights = confusion_matrix(y_test, y_pred_weights)\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 2199, number of negative: 25476\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003091 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 10852\n[LightGBM] [Info] Number of data points in the train set: 27675, number of used features: 120\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -&gt; initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n\n\n\n# Base model with RUS\nprint(f'accuracy of LGBM with RUS : {round(accuracy_rus,4)}')\nprint(f'auc of LGBM with RUS : {round(auc_rus,4)}')\nprint('-'*100)\nprint('CF of LGBM With RUS')\nprint(cf_rus)\n\naccuracy of LGBM with RUS : 0.6924\nauc of LGBM with RUS : 0.7565\n----------------------------------------------------------------------------------------------------\nCF of LGBM With RUS\n[[19587  8682]\n [  777  1706]]\n\n\n\n# Weight model without RUS\nprint(f'accuracy of LGBM with Class Weight : {round(accuracy_weights,4)}')\nprint(f'auc of LGBM with Class Weight : {round(auc_weights,4)}')\nprint('-'*100)\nprint('CF of LGBM With Class Weight')\nprint(cf_weights)\n\naccuracy of LGBM with Class Weight : 0.7794\nauc of LGBM with Class Weight : 0.7363\n----------------------------------------------------------------------------------------------------\nCF of LGBM With Class Weight\n[[22670  5599]\n [ 1184  1299]]\n\n\n\n결과 비교 및 선택\n\nTest데이터에 대해 강건(Robust)한 모델은, 데이터를 많이 본 모델\n특히 장기적으로 쓴다면 더 많은 데이터를 본 강건한 모델이 더 유지하기 쉬움\n위 기준으로는 후자의 모델(Weight model without RUS)을 선택\n\n참고\n\n현재의 모델도 100% 완벽하다고는 볼 수 없으나, 연체 위험자 등을 찾아낼 수 있다는 점에서 의의\nHybrid method를 실제로 쓸 일은 잘 없음(SMOTE의 오버피팅 문제로)"
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html",
    "href": "posts/coach-ds-20231107/index.html",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "",
    "text": "공공데이터포털 자료로 인터렉티브맵(folium) 시각화 연습\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html#개요",
    "href": "posts/coach-ds-20231107/index.html#개요",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "개요",
    "text": "개요"
  },
  {
    "objectID": "posts/coach-ds-20231107/index.html#데이터-불러오기",
    "href": "posts/coach-ds-20231107/index.html#데이터-불러오기",
    "title": "[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)",
    "section": "데이터 불러오기",
    "text": "데이터 불러오기\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport koreanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('medical_201909.csv', encoding=\"cp949\", low_memory=False)\n\n\nfolium 시각화 연습\n\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# 데이터 추출\ndf_elder = df[df['상권업종소분류명'] == '노인/치매병원'].copy()\n\n# folium 지도 생성 (fit_bounds에 위도/경도를 넣어 적정 zoom 상태로 시작)\nlatitude = df_elder['위도']\nlongitude = df_elder['경도']\nmain_folium_map = folium.Map(location=[latitude.mean(),longitude.mean()])\nmain_folium_map.fit_bounds([[latitude.min(), longitude.min()],[latitude.max(),longitude.max()]])\n\n# 시/도별 색상을 다르게 하기위해 색상표 추가 (별도 확인한 folium색상리스트와 데이터의 시도명 활용)\nindex_color= ['red', 'blue', 'green', 'purple', 'orange', 'darkred',\n         'lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue',\n         'darkpurple', 'white', 'pink', 'lightblue', 'lightgreen',\n         'gray', 'lightgray']\nindex_address = df_elder['시도명'].value_counts().index.tolist()\ncolor_table = {}\n\nfor i, area in enumerate(index_address):\n  color_table[area] = index_color[i]\n\n# 위도/경도 기준으로 지도에 마커 추가\n\n# 상호명은 아이콘 위에 마우스 올리면 보이게 함 (tooltip)\n# 주소는 클릭하면 조이게 하고, 세로로 뜨지않게 Popup(address, max_width=200) 을 활용\n# 시도별 색상부여를 위해 위에서 만든 색상표 사용, 시도명 null값은 딕셔터리.get(,'black)으로 처리\n# 요양병원에 맞는 아이콘 부여 (prefix='fa', icon='blind') prefix fa에 참고한 사이트 : https://glyphsearch.com/\n# MarkerCluster를 활용해 클러스터화(묶음) 수행\ncluster = MarkerCluster()\nfor idx in df_elder.index:\n  location = [df_elder.loc[idx, '위도'], df_elder.loc[idx, '경도']]\n  name = df_elder.loc[idx, '상호명']\n  address = df_elder.loc[idx, '도로명주소']\n  icon_color = color_table.get(df_elder.loc[idx, '시도명'],'black')\n\n  cluster.add_child(\n      folium.Marker(location,\n                    popup=folium.Popup(address, max_width=200),\n                    tooltip=name,\n                    icon=folium.Icon(color=icon_color, prefix='fa', icon='blind')\n                    )\n  ).add_to(main_folium_map)\n\nmain_folium_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n썸네일용 이미지 업로드"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html",
    "href": "posts/meta-dl-creditcard-20240710/index.html",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "",
    "text": "Kaggle(Credit Card Fraud detection) Keras Tuner실습\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#데이터-전처리",
    "href": "posts/meta-dl-creditcard-20240710/index.html#데이터-전처리",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\nimport pandas as pd\nimport sqlite3\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Create a connection to the SQLite database\nconn = sqlite3.connect('creditcard.db')\n\n# Read the data from the database into a pandas DataFrame\ndf = pd.read_sql_query(\"SELECT * FROM creditcard\", conn)\n\n# Close the connection\nconn.close()\n\n# 거래액 0인 값 제거\ndf_filtered1 = df[df['Amount'] != 0].copy()\ndf_filtered1\n\n# Dataset 나누고 Scaler 적용\ndf_x = df_filtered1.drop(['Time', 'Class'], axis=1).copy()\ndf_y = df_filtered1['Class'].copy()\n\nscaler_minmax = MinMaxScaler()\ndf_x_scaled = scaler_minmax.fit_transform(df_x)\n\nscaler_std = StandardScaler()\ndf_x_scaled = scaler_std.fit_transform(df_x_scaled)\n\n# train + test\nx_train, x_test = train_test_split(df_x_scaled, test_size=0.3)\ny_train, y_test = train_test_split(df_y, test_size=0.3)\n\n# train + validation\nx_train, x_validate = train_test_split(x_train, test_size=0.3)\ny_train, y_validate = train_test_split(y_train, test_size=0.3)"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---optimizer-설정-예제",
    "href": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---optimizer-설정-예제",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "Keras Tuner - Optimizer 설정 예제",
    "text": "Keras Tuner - Optimizer 설정 예제\n\nimport tensorflow as tf\nimport keras_tuner as kt\n\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input((29,1)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1, activation='sigmoid') # 이진분류이므로 Sigmoid사용\n    ])\n\n    # For optimizer\n    optimizer=hp.Choice('optimizer',values=['Nadam','adam','sgd','rmsprop', 'Ftrl','Adamax','adadelta','Adagrad'])\n     \n    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\ntuner=kt.RandomSearch(build_model,\n                     objective=kt.Objective('val_loss', direction='min'),\n                     overwrite=True,\n                     max_trials=9)\n\ntuner.search(x_train,y_train,epochs=5,validation_data=(x_test,y_test))\n\nTrial 2 Complete [00h 00m 56s]\nval_F1Score: 0.002878308529034257\n\nBest val_F1Score So Far: 0.002878308529034257\nTotal elapsed time: 00h 01m 57s\n\n\n\nBest Optimizer 출력\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'optimizer': 'Nadam'}"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---neuron-수-설정-예제",
    "href": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---neuron-수-설정-예제",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "Keras Tuner - Neuron 수 설정 예제",
    "text": "Keras Tuner - Neuron 수 설정 예제\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # For Neurons\n    units=hp.Int('units',min_value=5,max_value=150,step=5)\n\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n\n    model.compile(optimizer='Nadam', loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\ntuner=kt.RandomSearch(build_model,\n                     objective=kt.Objective('val_loss', direction='min'),# accuracy 미사용\n                     overwrite=True,\n                     max_trials=9,\n                     project_name='randomsearch_neuron')\n\ntuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 00m 44s]\nval_loss: 0.012561993673443794\n\nBest val_loss So Far: 0.012475043535232544\nTotal elapsed time: 00h 06m 42s\n\n\n\nBest Neuron의 수 출력\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'units': 130}"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---hidden-layer수-설정-예제",
    "href": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---hidden-layer수-설정-예제",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "Keras Tuner - Hidden layer수 설정 예제",
    "text": "Keras Tuner - Hidden layer수 설정 예제\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(130, activation='relu'))\n\n    # For Hidden Layers\n    for i in range(hp.Int('number_of_layers',min_value=1,max_value=10)):\n        model.add(tf.keras.layers.Dense(130,activation='relu'))\n\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n\n    model.compile(optimizer='Adamax', loss='binary_crossentropy',metrics=['F1Score'])\n    return model\n\ntuner=kt.RandomSearch(build_model,\n                     objective=kt.Objective('val_loss', direction='min'),# accuracy 미사용\n                     overwrite=True,\n                     max_trials=9,\n                     project_name='randomsearch_hidden_layer')\n\ntuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 01m 35s]\nval_loss: 0.012566009536385536\n\nBest val_loss So Far: 0.012515497393906116\nTotal elapsed time: 00h 10m 45s\n\n\n\nBest Hidden Layer의 수 출력\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'number_of_layers': 8}"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---optimizer-neuron-hidden-layer-등-동시설정-예제",
    "href": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---optimizer-neuron-hidden-layer-등-동시설정-예제",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "Keras Tuner - Optimizer, Neuron, Hidden layer 등 동시설정 예제",
    "text": "Keras Tuner - Optimizer, Neuron, Hidden layer 등 동시설정 예제\n\ndef build_model(hp):\n    model = tf.keras.models.Sequential()\n\n    # Input & Flatten\n    model.add(tf.keras.layers.Input((29,1)))\n    model.add(tf.keras.layers.Flatten())\n\n    # Hidden Layers\n    for i in range(hp.Int('num_layers',min_value=1,max_value=20)):\n\n        # For Dense\n        units = hp.Int('units',min_value=5,max_value=150,step=5) # For Neurons\n        activation = hp.Choice('activation'+str(i),values=['relu','elu']) # For Activation\n\n        model.add(tf.keras.layers.Dense(units, activation=activation))\n\n        # For Dropout\n        dropout_rate = hp.Choice('dropout'+str(i),values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n\n        model.add(tf.keras.layers.Dropout(dropout_rate))\n\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 이진분류이므로 Sigmoid사용\n        \n    \n    optimizer=hp.Choice('optimizer',values=['Nadam'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['F1Score'])\n    \n    return model\n\nwith tf.device('/device:GPU:0'):\n    tuner=kt.RandomSearch(build_model,\n                        objective=kt.Objective('val_F1Score', direction='max'),# accuracy 미사용\n                        overwrite=True,\n                        max_trials=9,\n                        project_name='randomsearch_model')\n\n\n    tuner.search(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n\nTrial 9 Complete [00h 01m 10s]\nval_F1Score: 0.0028702165000140667\n\nBest val_F1Score So Far: 0.0028861388564109802\nTotal elapsed time: 00h 18m 28s\n\n\n\ntuner.get_best_hyperparameters()[0].values\n\n{'num_layers': 12,\n 'units': 90,\n 'activation0': 'relu',\n 'dropout0': 0.9,\n 'optimizer': 'Nadam',\n 'activation1': 'relu',\n 'dropout1': 0.1,\n 'activation2': 'relu',\n 'dropout2': 0.1,\n 'activation3': 'relu',\n 'dropout3': 0.1,\n 'activation4': 'relu',\n 'dropout4': 0.1,\n 'activation5': 'relu',\n 'dropout5': 0.1,\n 'activation6': 'relu',\n 'dropout6': 0.1,\n 'activation7': 'relu',\n 'dropout7': 0.1,\n 'activation8': 'relu',\n 'dropout8': 0.1,\n 'activation9': 'relu',\n 'dropout9': 0.1,\n 'activation10': 'relu',\n 'dropout10': 0.1,\n 'activation11': 'relu',\n 'dropout11': 0.1}"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---모델-저장",
    "href": "posts/meta-dl-creditcard-20240710/index.html#keras-tuner---모델-저장",
    "title": "[M_Study_별도 공부] Keras Tuner",
    "section": "Keras Tuner - 모델 저장",
    "text": "Keras Tuner - 모델 저장\n\nmodel_2= tuner.get_best_models(num_models=1)[0]\nmodel_2.summary()\n\nc:\\Users\\kibok\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 55 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 29)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 90)             │         2,700 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (Dense)                 │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (Dropout)             │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (Dropout)            │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (Dense)                │ (None, 90)             │         8,190 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (Dropout)            │ (None, 90)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (Dense)                │ (None, 1)              │            91 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 92,881 (362.82 KB)\n\n\n\n Trainable params: 92,881 (362.82 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# 모델 학습\nhistory = model_2.fit(x_train, y_train, epochs=10)\n\n# 모델 평가\nprint('* 모델평가')\nloss, f1score = model_2.evaluate(x_train, y_train, verbose=2)\nloss, f1score = model_2.evaluate(x_test, y_test, verbose=2)\n\nEpoch 1/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 15s 2ms/step - F1Score: 0.0031 - loss: 0.0463\nEpoch 2/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0222\nEpoch 3/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0031 - loss: 0.0144\nEpoch 4/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0032 - loss: 0.0202\nEpoch 5/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0200\nEpoch 6/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0034 - loss: 0.0166\nEpoch 7/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 10s 2ms/step - F1Score: 0.0036 - loss: 0.0167\nEpoch 8/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 12s 3ms/step - F1Score: 0.0032 - loss: 0.0131\nEpoch 9/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 11s 3ms/step - F1Score: 0.0033 - loss: 0.0229\nEpoch 10/10\n4334/4334 ━━━━━━━━━━━━━━━━━━━━ 11s 3ms/step - F1Score: 0.0032 - loss: 0.0216\n* 모델평가\n4334/4334 - 4s - 1ms/step - F1Score: 0.0034 - loss: 0.0125\n2653/2653 - 2s - 787us/step - F1Score: 0.0029 - loss: 0.0113"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 5주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#과제-설명",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#과제-설명",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "과제 설명",
    "text": "과제 설명\n\n과제 : 월간 데이콘 신용카드 사용자 연체 예측 AI 경진대회\n\nhttps://dacon.io/competitions/official/235713/overview/description\n\n아래 내용 진행해보기\n\n지난 과제에서 학습시켜 본 모델에 대해, SHAP를 통해 FI구하기\nFeature Selection 후 재학습해보기\n재학습한 모델에 대해 SHAP의 다양한 시각화 적용 & 변수의 설명력 구해보기"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#전처리-해둔-데이터-읽고-데이터셋-나누기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#전처리-해둔-데이터-읽고-데이터셋-나누기",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "전처리 해둔 데이터 읽고 데이터셋 나누기",
    "text": "전처리 해둔 데이터 읽고 데이터셋 나누기\n\nfrom pkb_sqlite3 import DB_sqlite3\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndb_controller = DB_sqlite3('Dacon_creditcard_overdue.db')\ndf_train = db_controller.search_db_show_df('SELECT * FROM train')\ndf_train_pre = db_controller.search_db_show_df('SELECT * FROM train_pre')\ndf_test = db_controller.search_db_show_df('SELECT * FROM test_pre')\ndf_sample_submission = db_controller.search_db_show_df('SELECT * FROM sample_submission')\n\ntrain = pd.concat([df_train_pre, df_train['credit']], axis=1)\nx_test = df_test.copy()\n\n# index컬럼 삭제\ntrain = train.drop(columns=['index'])\nx_test = x_test.drop(columns=['index'])\n\nx_train, x_validate = train_test_split(train, test_size=0.3, random_state=42, stratify=train['credit'])"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#shap로-feature-importance-구하기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#shap로-feature-importance-구하기",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "SHAP로 Feature Importance 구하기",
    "text": "SHAP로 Feature Importance 구하기\n\nSHAP를 위한 모델 학습\n\n\nfrom imblearn.over_sampling import ADASYN\nfrom lightgbm import LGBMClassifier, early_stopping\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, log_loss\nimport shap\n\n\ndef model_practice_with_LGBMClassifier(X_train, y_train, X_validate, y_validate):\n    model = LGBMClassifier(class_weight=\"balanced\", random_state=42)\n    model.fit(X_train, \n            y_train,\n            eval_set=[(X_validate, y_validate)],\n            eval_metric='logloss',callbacks=[early_stopping(stopping_rounds=10)],\n            )\n\n    y_pred = model.predict(X_validate)\n    y_proba = model.predict_proba(X_validate)\n\n    accuracy = accuracy_score(y_validate, y_pred)\n    logloss = log_loss(y_validate, y_proba)\n    cf_matrix = confusion_matrix(y_validate, y_pred)\n    classify_report = classification_report(y_validate, y_pred)\n\n    result_dict = {'model':model,\n                   'y_pred':y_pred,\n                   'y_proba':y_proba,\n                   'accuracy':accuracy,\n                   'logloss':logloss,\n                   'cf_matrix':cf_matrix,\n                   'classify_report':classify_report\n                   }\n    \n    return result_dict\n\n\n# 오버샘플링\nADASYN_sampler = ADASYN(random_state=42)\nX_train, y_train = ADASYN_sampler.fit_resample(x_train.drop(['credit'], axis = 1), x_train[\"credit\"])\nX_validate, y_validate = ADASYN_sampler.fit_resample(x_validate.drop(['credit'], axis = 1), x_validate[\"credit\"])\n\n# 모델학습\nresult_default = model_practice_with_LGBMClassifier(X_train, y_train, X_validate, y_validate)\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 7336\n[LightGBM] [Info] Number of data points in the train set: 35665, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\nTraining until validation scores don't improve for 10 rounds\nDid not meet early stopping. Best iteration is:\n[95]    valid_0's multi_logloss: 0.749591\n\n\n\nSHAP Feature Importance : SHAP 절대값의 평균\n\n각 feature에 대해 절대값의 평균을 구함\naxis 설명\n\naxis 0 : row (각 샘플)\naxis 1 : feature\naxis 2 : class (출력값, 위 모델에서의 credit)\n\nnp.mean(np.abs(shap_values_train), axis=(0, 2))\n\nrow와 class를 따라 계산해, feature importance를 계산\n\n\n\n\nimport numpy as np\n\n# Explainer를 활용한 SHAP 계산\nexplainer = shap.TreeExplainer(result_default['model'])\nshap_values_train = explainer.shap_values(X_train)\n\n# 전체 SHAP값의 절대값의 평균으로 Feature Importance 구하기\nshap_values_mean = np.mean(np.abs(shap_values_train), axis=(0, 2))  # (35,) 크기\n\nprint(f\"\"\"SHAP value의 Shape              : {shap_values_train.shape}\nSHAP Feature Importance의 Shape : {shap_values_mean.shape}\"\"\")\n\nSHAP value의 Shape              : (35665, 35, 3)\nSHAP Feature Importance의 Shape : (35,)\n\n\n\n데이터프레임 & 오름차순 표기\n\n\nimport pandas as pd\n\nfeature_importance = pd.DataFrame({\n    'Feature': [f'{X_train.columns[i]}' for i in range(shap_values_mean.shape[0])],\n    'Importance': shap_values_mean\n}).sort_values(by='Importance', ascending=False)\n\nfeature_importance\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n34\nbegin_month\n0.208791\n\n\n12\nincome_type_Working\n0.182263\n\n\n2\ngender_F\n0.169896\n\n\n0\nedu_type\n0.160190\n\n\n4\ncar_N\n0.154671\n\n\n6\nreality_N\n0.153865\n\n\n14\nfamily_type_Married\n0.110945\n\n\n25\nchild_num\n0.083134\n\n\n8\nincome_type_Commercial associate\n0.074013\n\n\n9\nincome_type_Pensioner\n0.069626\n\n\n27\nDAYS_BIRTH\n0.060860\n\n\n26\nincome_total\n0.043608\n\n\n13\nfamily_type_Civil marriage\n0.039357\n\n\n24\nindex\n0.038752\n\n\n28\nDAYS_EMPLOYED\n0.034525\n\n\n33\nfamily_size\n0.033940\n\n\n19\nhouse_type_House / apartment\n0.033131\n\n\n16\nfamily_type_Single / not married\n0.028371\n\n\n1\noccyp_type\n0.023017\n\n\n10\nincome_type_State servant\n0.014559\n\n\n3\ngender_M\n0.013786\n\n\n31\nphone\n0.011512\n\n\n5\ncar_Y\n0.011273\n\n\n32\nemail\n0.010337\n\n\n7\nreality_Y\n0.010111\n\n\n15\nfamily_type_Separated\n0.010072\n\n\n17\nfamily_type_Widow\n0.006757\n\n\n22\nhouse_type_Rented apartment\n0.006367\n\n\n20\nhouse_type_Municipal apartment\n0.004178\n\n\n30\nwork_phone\n0.003070\n\n\n23\nhouse_type_With parents\n0.003047\n\n\n21\nhouse_type_Office apartment\n0.001704\n\n\n18\nhouse_type_Co-op apartment\n0.000524\n\n\n11\nincome_type_Student\n0.000000\n\n\n29\nFLAG_MOBIL\n0.000000"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#feature-selection-후-재학습",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#feature-selection-후-재학습",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "Feature Selection 후 재학습",
    "text": "Feature Selection 후 재학습\n\n# 0보다 큰 Feature Select\nlist_selected_feature = feature_importance[feature_importance['Importance']&gt;0]['Feature'].tolist()\n\n# Feature Selection 적용한 데이터셋\nX_train_filtered = X_train.loc[:, list_selected_feature]\nX_validate_filtered = X_validate.loc[:, list_selected_feature]\n\n# 모델학습\nresult_feature_selected = model_practice_with_LGBMClassifier(X_train_filtered, y_train, X_validate_filtered, y_validate)\n\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001743 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 7336\n[LightGBM] [Info] Number of data points in the train set: 35665, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\n[LightGBM] [Info] Start training from score -1.098612\nTraining until validation scores don't improve for 10 rounds\nDid not meet early stopping. Best iteration is:\n[95]    valid_0's multi_logloss: 0.749591"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#재학습한-모델에-대해-shap의-다양한-시각화-적용-변수의-설명력-구해보기",
    "href": "posts/meta-cm-sql_and_ml_xai-20250121/index.html#재학습한-모델에-대해-shap의-다양한-시각화-적용-변수의-설명력-구해보기",
    "title": "[DA스터디/5주차/과제] SHAP시각화 & 변수 설명",
    "section": "재학습한 모델에 대해 SHAP의 다양한 시각화 적용 & 변수의 설명력 구해보기",
    "text": "재학습한 모델에 대해 SHAP의 다양한 시각화 적용 & 변수의 설명력 구해보기\n\n재학습한 모델에 대한 SHAP계산\n\n\nimport numpy as np\n\n# Explainer를 활용한 SHAP 계산\nexplainer_featured = shap.TreeExplainer(result_feature_selected['model'])\nshap_values_train_featured = explainer_featured.shap_values(X_train_filtered)\n\n# 전체 SHAP값의 절대값의 평균으로 Feature Importance 구하기\nshap_values_mean_featured = np.mean(np.abs(shap_values_train_featured), axis=(0, 2))  # (35,) 크기\n\nprint(f\"\"\"SHAP value의 Shape              : {shap_values_train_featured.shape}\nSHAP Feature Importance의 Shape : {shap_values_mean_featured.shape}\"\"\")\n\nSHAP value의 Shape              : (35665, 33, 3)\nSHAP Feature Importance의 Shape : (33,)\n\n\n\nSHAP Summary plot\n\nClass 0 : 신용도가 높음\n\n일해서 돈을 버는 사람일수록(income_type_Working높음) / 증가\n결혼하지 않은 사람일수록(family_type_Married) / 증가\n신용카드 발급이 오래된 사람일수록(begin_month) / 증가\n\n데이터수집일을 기준으로 0부터 역으로 센 음수변수(값이 작을수록 오래됨)\n\nincome_total…?\n\n\n\nshap.summary_plot(shap_values_train_featured[:, :, 0], \n                  X_train_filtered, \n                  feature_names=X_train_filtered.columns,)\n\n\n\n\n\n\n\n\n\nClass 2 : 신용도가 낮음\n\n신용카드 발급이 오래된 사람일수록(begin_month) / 증가\n\n데이터수집일을 기준으로 0부터 역으로 센 음수변수(값이 작을수록 오래됨)\n\n\n\n\nshap.summary_plot(shap_values_train_featured[:, :, 2], \n                  X_train_filtered, \n                  feature_names=X_train_filtered.columns,)\n\n\n\n\n\n\n\n\n\n\nSHAP Feature Importance plot\n\nClass 0 : 신용도가 높음\n\nincome_type_Working / 영향력이 큼\n\n\n\nshap.summary_plot(shap_values_train_featured[:, :, 0],\n                  X_train_filtered,\n                  plot_type='bar')\n\n\n\n\n\n\n\n\n\nClass 2 : 신용도가 낮음\n\n성별, 교육수준, 신용카드 발급기간, 자동차 및 부동산 소유 여부의 영향력 큼\n\n\n\nshap.summary_plot(shap_values_train_featured[:, :, 2],\n                  X_train_filtered,\n                  plot_type='bar')\n\n\n\n\n\n\n\n\n\n\nSHAP Depenence plot\n\nFeature importance에서 특정 경향이 강했던 변수 위주의 확인(Class 0 : 신용도가 높음)\n\nincome_type_Working : 전 구간에서 양의 영향을 끼침\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"income_type_Working\", \n                     shap_values_train_featured[:, :, 0], \n                     X_train_filtered, \n                     interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\nFeature importance에서 특정 경향이 강했던 변수 위주의 확인(Class 2 : 신용도가 낮음)\n\ngender_F : 전 구간에서 음의 영향을 끼침\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"gender_F\", \n                     shap_values_train_featured[:, :, 2], \n                     X_train_filtered, \n                     interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\nedu_type : 1을 임계점으로하여, 낮은 경우 양의 영향 / 높은 경우 음의 영향\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"edu_type\", \n                     shap_values_train_featured[:, :, 2], \n                     X_train_filtered, \n                     interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\nbegin_month : 약 1.2를 임계점으로하며, 이보다 높은 경우 음의 영향\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10,7))\nshap.dependence_plot(\"begin_month\", \n                     shap_values_train_featured[:, :, 2], \n                     X_train_filtered, \n                     interaction_index=None, ax=ax)\n\n\n\n\n\n\n\n\n\n\nSHAP Force plot\n\n샘플로 1가지만 실습\n\n0번(사람)에 대해, 모델이 예측값을 산출할 때\n\nincome_type_Working이 음의 영향을 줌(0일수록 높은 신용도)\n컬럼에 index가 있던 것을 발견함. 데이터 전처리로직에서 index컬럼삭제해도록 반영해 둠\n\n\n\n\nclass_label = 0 # credit 0, 1, 2 중 택1\nrow_number = 0  # 확인해보고 싶은 데이터 row\n\nshap.force_plot(explainer_featured.expected_value[class_label], \n                shap_values_train_featured[row_number, :, class_label], \n                X_train_filtered.iloc[row_number, :],\n                matplotlib=True)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "",
    "text": "금융권 데이터를 활용한 분석 스터디 - 1주차\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#sql문-실행순서",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#sql문-실행순서",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "SQL문 실행순서",
    "text": "SQL문 실행순서\n\nSELECT\n\nFROM\n\nJOIN ON\n\nWHERE\n\nGROUP BY\n\nHAVING\nORDER BY\n\nLIMIT"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#select",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#select",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "SELECT",
    "text": "SELECT\n\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect('my_database.db')\n\n\n# 컬럼 지정\nselected_df = pd.read_sql_query(\"SELECT idx, y, job FROM my_table;\", conn)\nselected_df.head(5)\n\n\n\n\n\n\n\n\nidx\ny\njob\n\n\n\n\n0\n0\nno\nhousemaid\n\n\n1\n1\nno\nservices\n\n\n2\n2\nno\nservices\n\n\n3\n3\nno\nadmin.\n\n\n4\n4\nno\nservices\n\n\n\n\n\n\n\n\n# 전체 컬럼 (*)\ndf_all = pd.read_sql_query(\"SELECT * FROM my_table\", conn)\ndf_all.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#select-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#select-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "SELECT 실습",
    "text": "SELECT 실습\n\nmy_table에서 idx, martial, education, y를 조회하여 df_trial_1에 할당해봅시다.\n\n\ndf_trial_1 = pd.read_sql_query(\"SELECT idx, marital, education, y FROM my_table\", conn)\ndf_trial_1.head(5)\n\n\n\n\n\n\n\n\nidx\nmarital\neducation\ny\n\n\n\n\n0\n0\nmarried\nbasic.4y\nno\n\n\n1\n1\nmarried\nhigh.school\nno\n\n\n2\n2\nmarried\nhigh.school\nno\n\n\n3\n3\nmarried\nbasic.6y\nno\n\n\n4\n4\nmarried\nhigh.school\nno"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "WHERE",
    "text": "WHERE\n\nWHERE에서의 1=1의 사용\n\n없어도 되지만 편의를 위해 사용. 실무적으로 많이 활용되는 편\n괄호를 활용해 우선순위를 부여(명시)해주는 것이 좋음\n\nAND(*)가 OR(+)보다 우선순위가 높으나, 헷갈리지 않게 명시\n\n\n\n\nq = \"\"\"SELECT\n    *\nFROM my_table\nWHERE 1=1\n    AND y = 'no'\n    AND (\n        is_default = 'no'\n        OR campaign &gt; 0\n    );\"\"\"\ndf_multicon_2 = pd.read_sql_query(q, conn)\ndf_multicon_2.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where-심화",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where-심화",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "WHERE 심화",
    "text": "WHERE 심화\n\nLIKE : 문자열에서 찾기 (%와 함께 사용해 특정 문자열을 포함한 경우를 찾기도 함)\nIN : 특정 목록에 포함된 경우 찾기\nBETWEEN : 범위 안에 포함된 경우 찾기\n\n\n# 'LIKE'를 활용해 9가 포함된 값을 찾는 예제\nq = \"\"\"SELECT\n    *\nFROM my_table\nWHERE idx LIKE '%9%';\"\"\"\n\ndf_idcode_9 = pd.read_sql_query(q, conn)\ndf_idcode_9.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n9\n25\nservices\nsingle\nhigh.school\nno\nyes\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n19\n39\nmanagement\nsingle\nbasic.9y\nunknown\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n29\n55\nunknown\nmarried\nuniversity.degree\nunknown\nunknown\nunknown\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n39\n56\ntechnician\nmarried\nbasic.4y\nno\nyes\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n49\n45\nblue-collar\nmarried\nbasic.9y\nno\nyes\nno\ntelephone\nmay\n...\n2\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# 'IN'을 활용해 특정 목록의 데이터가 포함된 값을 찾는 예제\nIDS = ('11003','12903')\n\nq = f\"\"\"SELECT\n    *\nFROM my_table\nWHERE idx in {IDS};\"\"\"\n\ndf_given_ids = pd.read_sql_query(q, conn)\ndf_given_ids\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n11003\n56\nretired\nmarried\nbasic.4y\nno\nyes\nno\ntelephone\njun\n...\n30\n999\n0\nnonexistent\n1.4\n94.465\n-41.8\n4.962\n5228.1\nno\n\n\n1\n12903\n23\nadmin.\nsingle\nhigh.school\nno\nno\nno\ncellular\njul\n...\n1\n999\n0\nnonexistent\n1.4\n93.918\n-42.7\n4.962\n5228.1\nno\n\n\n\n\n2 rows × 22 columns\n\n\n\n\n# 'BETWEEN'을 활용해 특정 범위의 데이터를 찾는 예제\nq =\"\"\"SELECT\n    *\nFROM my_table\nWHERE campaign BETWEEN 10 AND 20;\"\"\"\n\ndf_between = pd.read_sql_query(q, conn)\ndf_between.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n2183\n58\nmanagement\nmarried\nuniversity.degree\nno\nno\nno\ntelephone\nmay\n...\n10\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n2189\n56\nadmin.\nmarried\nbasic.9y\nunknown\nno\nno\ntelephone\nmay\n...\n11\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n2234\n47\nmanagement\nmarried\nuniversity.degree\nno\nno\nno\ntelephone\nmay\n...\n12\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n2553\n54\nretired\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\n...\n10\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.856\n5191.0\nno\n\n\n4\n2554\n31\nadmin.\nmarried\nhigh.school\nunknown\nyes\nno\ntelephone\nmay\n...\n13\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.856\n5191.0\nno\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#where-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "WHERE 실습",
    "text": "WHERE 실습\n\n문제\n\nidx에 7이 들어가며\nemp_var_rate가 1보다 크고\n연령이 20세 이하거나\n연령이 70대 이상인\n고객을 구하세요.\n\n\n\nq =\"\"\"SELECT\n    *\nFROM my_table\nWHERE 1=1\nAND idx Like '%7%'\nAND emp_var_rate &gt; 1\nAND (age &lt;= 20 or age &gt;= 70);\"\"\"\n\ndf_q2 = pd.read_sql_query(q, conn)\ndf_q2\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n6575\n20\nentrepreneur\nsingle\nhigh.school\nno\nno\nyes\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n13407\n20\nadmin.\nsingle\nhigh.school\nno\nno\nno\ncellular\njul\n...\n4\n999\n0\nnonexistent\n1.4\n93.918\n-42.7\n4.962\n5228.1\nno\n\n\n2\n15798\n19\nstudent\nsingle\nbasic.9y\nunknown\nyes\nno\ncellular\njul\n...\n4\n999\n0\nnonexistent\n1.4\n93.918\n-42.7\n4.960\n5228.1\nno\n\n\n\n\n3 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#group-by-집계함수",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#group-by-집계함수",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "GROUP BY (+집계함수)",
    "text": "GROUP BY (+집계함수)\n\nGROUP BY를 활용해 정보를 그룹핑하여 볼 수 있음\nSELECT문에 집계함수(SUM 등)를 적어, 해당 컬럼별 집계 가능\n\n주의 : 컬럼(변수)이 집계함수나 GROUP BY절 중 한 곳에는 있어야 함\n추천 : 집계함수를 적용한 컬럼은 AS로 이름을 지정하는 것이 좋음\n\n집계함수의 종류\n\nSUM : 합\nAVG (MEAN) : 평균\nMIN / MAX : 최소 / 최대값\nCOUNT : 개수\n\n\n\nq =\"\"\"SELECT\n    y, SUM(duration) as sum_dur\nFROM my_table\nGROUP BY y;\"\"\"\n\ndf_sum_0 = pd.read_sql_query(q, conn)\ndf_sum_0\n\n\n\n\n\n\n\n\ny\nsum_dur\n\n\n\n\n0\nno\n8071436\n\n\n1\nyes\n2566807"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#group-by-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#group-by-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "GROUP BY 실습",
    "text": "GROUP BY 실습\n\n문제\n\ny별로\nidx에 7이 들어가는 대상에 대해\nduration의 합과\ncampaign의 평균, 최대, 최소값을 구해봅시다\n\n\n\nq =\"\"\"SELECT\n    y, SUM(duration) as sum_dur, AVG(campaign) as avg_camp, MIN(campaign) as min_camp, MAX(campaign) as max_camp\nFROM my_table\nWHERE idx like '%7%'\nGROUP BY y;\"\"\"\n\ndf_sum_0 = pd.read_sql_query(q, conn)\ndf_sum_0\n\n\n\n\n\n\n\n\ny\nsum_dur\navg_camp\nmin_camp\nmax_camp\n\n\n\n\n0\nno\n2680014\n2.786460\n1\n56\n\n\n1\nyes\n911542\n2.091711\n1\n17"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#having",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#having",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "HAVING",
    "text": "HAVING\n\n집계함수의 결과의 필터링에 HAVING 사용\n\n집계함수는 WHERE절 이후 실행되어 WHERE절에서의 집계함수 필터링은 불가함\n\n\n\nq =\"\"\"SELECT\n    y, SUM(duration) as sum_dur\nFROM my_table\nGROUP BY y\nHAVING SUM(duration) &gt; 5000000;\"\"\"\n\ndf_sum_over_1m = pd.read_sql_query(q, conn)\ndf_sum_over_1m\n\n\n\n\n\n\n\n\ny\nsum_dur\n\n\n\n\n0\nno\n8071436"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#having-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#having-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "HAVING 실습",
    "text": "HAVING 실습\n\n문제\n\ny별로\nvar_0의 평균값이 11보다 큰 y 대해\nvar_0의 평균을 avg_var_0,\nvar_100의 최대값을 max_var_100,\nvar_100의 최소값을 min_var_100\n으로 구하는 쿼리를 써봅시다.\n\n\n\nq =\"\"\"SELECT\n    y, SUM(duration) as sum_dur\nFROM my_table\nGROUP BY y\nHAVING SUM(duration) &gt; 5000000;\"\"\"\n\ndf_sum_over_1m = pd.read_sql_query(q, conn)\ndf_sum_over_1m\n\n\n\n\n\n\n\n\ny\nsum_dur\n\n\n\n\n0\nno\n8071436"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#order-by정렬",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#order-by정렬",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "ORDER BY(정렬)",
    "text": "ORDER BY(정렬)\n\nORDER BY로 정렬 (SQL마다 다를 수 있으나 기본값은 보통 오름차순)\n\n내림차순을 원하는 경우 뒤에 DESC를 붙임\n\n\n\n# 오름차순 (기본값) 예제\nq =\"\"\"SELECT\n    *\nFROM my_table\nORDER BY idx;\"\"\"\n\ndf_var0_order = pd.read_sql_query(q, conn)\ndf_var0_order.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# 내림차순 예제제\nq =\"\"\"SELECT\n    *\nFROM my_table\nORDER BY idx;\"\"\"\n\ndf_var0_order = pd.read_sql_query(q, conn)\ndf_var0_order.head(5)"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#limit",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#limit",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "LIMIT",
    "text": "LIMIT\n\n위에서부터 N개의 값을 반환하는, pandas의 .head(N)과 같은 함수\n\n\nq = \"\"\"SELECT\n    *\nFROM my_table\nLIMIT 100\n\"\"\"\n\ndf_100 = pd.read_sql_query(q, conn)\nlen(df_100)\n\n100"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#order-by-limit-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#order-by-limit-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "ORDER BY & LIMIT 실습",
    "text": "ORDER BY & LIMIT 실습\n\npdays가 999가 아닌 경우에 한해서, pdays값이 상위 100개에 해당하는 데이터를 반환하는 쿼리를 짜 봅시다.\n\n\n# Write your code\nq = \"\"\"SELECT\n    *\nFROM my_table\nWHERE pdays != 999\nORDER BY pdays desc\nLIMIT 100\n\"\"\"\n\ndf_100 = pd.read_sql_query(q, conn)\n\nprint(len(df_100))\ndf_100.head(5)\n\n100\n\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n40874\n41\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\noct\n...\n2\n27\n1\nsuccess\n-1.1\n94.601\n-49.5\n0.959\n4963.6\nyes\n\n\n1\n40243\n24\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\njul\n...\n4\n26\n1\nsuccess\n-1.7\n94.215\n-40.3\n0.885\n4991.6\nyes\n\n\n2\n40234\n60\nadmin.\nmarried\nbasic.9y\nno\nno\nno\ncellular\njul\n...\n2\n25\n2\nfailure\n-1.7\n94.215\n-40.3\n0.884\n4991.6\nyes\n\n\n3\n39883\n28\nmanagement\nsingle\nuniversity.degree\nno\nyes\nno\ncellular\njun\n...\n1\n22\n1\nsuccess\n-1.7\n94.055\n-39.8\n0.729\n4991.6\nyes\n\n\n4\n40489\n30\nadmin.\nsingle\nhigh.school\nno\nno\nno\ntelephone\naug\n...\n1\n22\n1\nsuccess\n-1.7\n94.027\n-38.3\n0.899\n4991.6\nno\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#결과를-테이블로-저장",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#결과를-테이블로-저장",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "결과를 테이블로 저장",
    "text": "결과를 테이블로 저장\nCREATE TABLE (테이블이름) AS SELECT  FROM TABLE\n\n# 특정 쿼리를 테이블 저장\n\nq =\"\"\"CREATE TABLE MY_TABLE_ORDERED AS\nSELECT\n    *\nFROM MY_TABLE\nORDER BY idx DESC;\"\"\"\nconn.execute(q)\n\n# 저정한 테이블 조회\nq2 =\"\"\"SELECT * FROM MY_TABLE_ORDERED;\"\"\"\n\ndf_var0_desc_order = pd.read_sql_query(q2, conn)\ndf_var0_desc_order.head(5)\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n41187\n74\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\n...\n3\n999\n1\nfailure\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n1\n41186\n44\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n2\n41185\n56\nretired\nmarried\nuniversity.degree\nno\nyes\nno\ncellular\nnov\n...\n2\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n3\n41184\n46\nblue-collar\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n4\n41183\n73\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# 저장한 테이블 삭제\nq =\"\"\"DROP TABLE MY_TABLE_ORDERED;\"\"\"\n\nconn.execute(q)\n\n&lt;sqlite3.Cursor at 0x1eb6a498cc0&gt;"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#결과를-테이블로-저장하기-실습",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#결과를-테이블로-저장하기-실습",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "결과를 테이블로 저장하기 실습",
    "text": "결과를 테이블로 저장하기 실습\n\n# 테이블 저장\nq =\"\"\"CREATE TABLE question6 AS\nSELECT\n    *\nFROM my_table\nWHERE pdays != 999\nORDER BY pdays desc\nLIMIT 100;\"\"\"\nconn.execute(q)\n\n# 저장한 테이블 확인\nq2 = \"\"\"SELECT * FROM question6\"\"\"\nresult1 = pd.read_sql_query(q2, conn)\nresult1\n\n\n\n\n\n\n\n\nidx\nage\njob\nmarital\neducation\nis_default\nhousing\nloan\ncontact\nmonth\n...\ncampaign\npdays\nprevious\npoutcome\nemp_var_rate\ncons_price_idx\ncons_conf_idx\neuribor3m\nnr_employed\ny\n\n\n\n\n0\n40874\n41\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\noct\n...\n2\n27\n1\nsuccess\n-1.1\n94.601\n-49.5\n0.959\n4963.6\nyes\n\n\n1\n40243\n24\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\njul\n...\n4\n26\n1\nsuccess\n-1.7\n94.215\n-40.3\n0.885\n4991.6\nyes\n\n\n2\n40234\n60\nadmin.\nmarried\nbasic.9y\nno\nno\nno\ncellular\njul\n...\n2\n25\n2\nfailure\n-1.7\n94.215\n-40.3\n0.884\n4991.6\nyes\n\n\n3\n39883\n28\nmanagement\nsingle\nuniversity.degree\nno\nyes\nno\ncellular\njun\n...\n1\n22\n1\nsuccess\n-1.7\n94.055\n-39.8\n0.729\n4991.6\nyes\n\n\n4\n40489\n30\nadmin.\nsingle\nhigh.school\nno\nno\nno\ntelephone\naug\n...\n1\n22\n1\nsuccess\n-1.7\n94.027\n-38.3\n0.899\n4991.6\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n39157\n38\nblue-collar\nsingle\nhigh.school\nno\nyes\nno\ncellular\nmar\n...\n1\n13\n2\nfailure\n-1.8\n93.369\n-34.8\n0.655\n5008.7\nyes\n\n\n96\n39243\n39\nadmin.\ndivorced\nuniversity.degree\nno\nyes\nno\ncellular\nmar\n...\n1\n13\n1\nsuccess\n-1.8\n93.369\n-34.8\n0.650\n5008.7\nno\n\n\n97\n39315\n29\ntechnician\nsingle\nprofessional.course\nno\nyes\nyes\ncellular\nmar\n...\n2\n13\n1\nsuccess\n-1.8\n93.369\n-34.8\n0.643\n5008.7\nyes\n\n\n98\n39556\n38\nadmin.\nsingle\nhigh.school\nno\nyes\nyes\ncellular\napr\n...\n1\n13\n1\nsuccess\n-1.8\n93.749\n-34.6\n0.659\n5008.7\nyes\n\n\n99\n39651\n30\nadmin.\nsingle\nhigh.school\nno\nno\nno\ncellular\nmay\n...\n1\n13\n1\nsuccess\n-1.8\n93.876\n-40.0\n0.682\n5008.7\nno\n\n\n\n\n100 rows × 22 columns"
  },
  {
    "objectID": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#프로그래머스-과제-풀기",
    "href": "posts/meta-cm-sql_and_ml_xai-20241222/index.html#프로그래머스-과제-풀기",
    "title": "[DA스터디/1주차] SQL기초 및 실습",
    "section": "프로그래머스 과제 풀기",
    "text": "프로그래머스 과제 풀기\n\n저작권 관련 프로그래머스 링크\n\n  코딩 테스트 연습 문제\n  코딩테스트 연습에 공개된 문제는 (주)그렙이 저작권을 가지고 있습니다.\n  (지문 하단에 별도 저작권 표시 문제 제외)\n  코딩테스트 연습 문제의 지문, 테스트케이스, 풀이 등과 같은 정보는 비상업적, 비영리적 용도로 게시할 수 있습니다.\n  다만 문제의 지문, 풀이 등과 같은 정보를 단순히 게시하는 것을 넘어, \n  이를 바탕으로 문제를 풀고 채점이 가능하도록 하는 등의 방식으로 활용하는 것은 제한됩니다.\n\n  ※ 2020 KAKAO BLIND RECRUITMENT, Summer/Winter Coding 등의 문제는 기업 코딩 테스트에 나온 문제이나, \n  코딩테스트 연습에 공개된 문제이기 때문에 마찬가지로 비상업적, 비영리적 용도로 게시할 수 있습니다.\n\n  (2021. 01. 08 업데이트)\n\nhttps://school.programmers.co.kr/learn/courses/30/lessons/59040 \nSELECT ANIMAL_TYPE, COUNT(*)\nFROM ANIMAL_INS\nGROUP BY ANIMAL_TYPE\nORDER BY ANIMAL_TYPE\nhttps://school.programmers.co.kr/learn/courses/30/lessons/133024 \nSELECT FLAVOR\nFROM FIRST_HALF \nORDER BY TOTAL_ORDER DESC, SHIPMENT_ID\nhttps://school.programmers.co.kr/learn/courses/30/lessons/59405 \nSELECT NAME\nFROM ANIMAL_INS \nORDER BY DATETIME\nLIMIT 1\nhttps://school.programmers.co.kr/learn/courses/30/lessons/131697 \nSELECT MAX(PRICE) AS MAX_PRICE\nFROM PRODUCT"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240714/index.html",
    "href": "posts/meta-dl-creditcard-20240714/index.html",
    "title": "[M_Study_최종과제 피드백/이론] 신용카드 이상거래 탐지 모델링",
    "section": "",
    "text": "Kaggle CreditCard Fraud Detection (과제 피드백 및 관련 이론, 개선계획)\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240714/index.html#내-제출과제에-대한-피드백",
    "href": "posts/meta-dl-creditcard-20240714/index.html#내-제출과제에-대한-피드백",
    "title": "[M_Study_최종과제 피드백/이론] 신용카드 이상거래 탐지 모델링",
    "section": "내 제출과제에 대한 피드백",
    "text": "내 제출과제에 대한 피드백\n\nTrain, Validation, Test 데이터의 비율은 8:1:1 정도가 보통 \nFlatten은 사진 등 이미지에 많이 사용, 필요 없을 수 있다\nmodel.compile의 metric\n\nf1-score는 여러 종류가 있음\n\nimbalance한 상황에서 이상거래 등에 가중치를 주는 weighted f1-score\nclass별로 하는 macro f1-score\nmicro f1-score\n\n이런 상황에서 지원되는 metric이 vanila f1-score라면 점수가 안좋게 나온 것일수도 있다\n\nweighted f1-score와 같은 것을 사용하는 편이 적절히 평가될 수 있음\nrecall, precision을 모두 넣고 볼 수도 있음\nmodel build 대비 점수가 낮게 나와서, metric 문제일 가능성이 높아보임 \n\n\nHyper parameter Optimizer (발표과제에서는 Keras Tuner 사용)\n\n개념\n\n예를 들어 10개 파라미터에 100개 선택지가 있다면 \\(100^{10}\\)과 같이 계산량이 많음\n\n(Brute force방식) 모든 조합을 계산하여 최적조합을 찾는 방식은 계산량 문제로 어려움\n\n전체 계산 중 일부만 계산하는 방식으로, 전문가만큼의 성능은 안나오지만 초보의 수준에서는 좋음\n\nRandom search, Grid search\n\nGrid search : 나머지 파라미터를 고정시킨 채 최적 파라미터를 도출하는 것을 반복하여, 최적 파라미터만 조합\n\n최적이 아닌 파라미터들의 조합으로 최적 성능이 나올 수 있다는 단점\n\nRandom Search : 파라미터들의 ’조합’이라는 부분을 Grid search보다 좀 더 보기위한 방법\n\n추가용도\n\n모델을 확정한 후, 모델의 성능을 좀 더 높이기 위해서도 사용 가능\n\n예를 들면 모델을 분석하고 보고서를 쓰는 코딩테스트도 있는데, 이럴 때 조금이라도 성능을 올리기 위해 사용가능\n\n\nHyper parameter관련 보고서를 쓴다면, 어떤 기법을 추가하여 유용하고 좋았는지 비교하는 것이 좋음 \n\nEarly stopping 설정\n\nepoch수 대비 3%정도로 설정하기도 함\n실질적으로는 전체 epoch을 돌리고, 가장 성능이 좋았던 포인트를 사용하는 방법을 사용(모든 포인트의 고려) \n\n참고\n\n이후 새로운 거래가 생겨 모델로 판별하려면, 학습데이터와 같이 PCA처리 후 input 해야함\n매번 현재의 과제처럼 튜닝을 새로하기보다는, 전 모델의 튜닝을 지속하는 것이 효율적임\n\nLayer수, Activation function 등은 변경 불가한 요소\nLearning rate scheduler, early stopping 등의 변경 가능"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240714/index.html#과제-보완-계획",
    "href": "posts/meta-dl-creditcard-20240714/index.html#과제-보완-계획",
    "title": "[M_Study_최종과제 피드백/이론] 신용카드 이상거래 탐지 모델링",
    "section": "과제 보완 계획",
    "text": "과제 보완 계획\n\nData set 분석\n\nt-sne를 통해 데이터의 분포 등을 시각화하여 파악해봄\n내가 어느정도의 분류는 가능하겠다는 정도를 파악할 수 있음\n\n\n\nData set 구성\n\nTrain/Validation/Test를 8:1:1로 구성\ntrain_test_split(stratify=실제Label)을 통한 Label 비중 맞추기 검토\nValidation set은 StratifiedKFold 적용 검토\n컬럼별 이상치 처리 : Box plot의 Upperbound나 Underbound값으로 대체 고려(제거X)\n\nRobust scaler 고려\n\n\n\n\nParameter 등 모델구성\n\noptuna적용고려, Randomsearch, GridSearch 확인 후 선택\nmetric을 weighted f1-score나 recall&precision 등을 고려\n\n예시 : metrics = [f1_score, Precision(), Recall()]\n\n  import tensorflow_addons as tfa\n  f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n  model.compile(loss=SigmoidFocalCorssEntropy(),\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                metrics=[f1_score, Pricision(), Recall()])\nEarly stopping은 코드구현은 하되 실질적으로는 무효가되는 patience인 경우도 추가로 설정\n\n전체 epoch을 돌려 가장 성능이 좋은 포인트도 확인한 후 대조\n\nLearning rate scheduling은 기존의 별도 함수를 만드는 방식 외, ReduceLROnPlateau 함수 이용도 고려\n\n  from tf.keras.callbacks import ReduceLROnPlateau\n  reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                factor=0.2, # lr감소치. 현재 lr이 0.01이라면 0.01*0.2를 적용\n                patience=10, # 개선에 대한 허용치. 10 epoch까지 개선이 없다면 적용\n                mode='min', # auto, max, min 옵션 있음\n                min_lr=0.001)\n  model.fit(x_train, y_train, callbacks=[reduce_lr])\n\nBatch size는 가능한 선에서는 늘려서 빠른 학습을 도모\n평가시 ROC Curve나 PR Curve 시각화도 고려\nCross validation 활용시, 여러번의 모델Score가 나오므로 Box plot을 통해 안정적인 모델인지 시각화\n\n\n\n기타\n\n시각화를 통해 데이터 설명 보강\n\nROC Curve나 PR Curve 등의 사용\n\n\n\n\n추가진행\n\n머신러닝 기법 활용\n\n모델 : XGB, Random forest 고려\n데이터셋 : 샘플링(오버 또는 언더) 적용 검토\n\n오버샘플링 한다면 SMOTE대신 ADASYN적용검토\n샘플링 적용/미적용 성능 비교\n\n위의 모델 또는 Logistic regression으로 비교\n\n\n모델학습 : Cross validation을 통한 과적합 방지\nThreshold adjustment 고려"
  },
  {
    "objectID": "posts/meta-dl-creditcard-20240714/index.html#주차-발표과제들-보완에-필요한-추가지식",
    "href": "posts/meta-dl-creditcard-20240714/index.html#주차-발표과제들-보완에-필요한-추가지식",
    "title": "[M_Study_최종과제 피드백/이론] 신용카드 이상거래 탐지 모델링",
    "section": "7주차 발표과제들 보완에 필요한 추가지식",
    "text": "7주차 발표과제들 보완에 필요한 추가지식\n\n불균형한 데이터를 맞춰주는 샘플링 기법 : 오버샘플링, 언더샘플링\n\n주의점 : 두 기법 모두 Test data를 먼저 분리해둔 후 나머지에 대해 적용. Test data는 건드리지 않는다\n언더샘플링과 오버샘플링은 둘 중 하나만 사용하는게 나음(함께 사용시 둘의 단점을 모두 가질수도 있다)\n참고사항\n\n가급적이면 언더샘플링보다는 오버샘플링을 많이 씀 (데이터는 자산이라는 관점에서, 손실 방지)\n추세는 발전된 알고리즘의 모델에 맡기고, 샘플링을 적용하지 않는 방향\n\n다만 상황에 따라서는 오버샘플링이 훨씬 좋을 때가 있음\n\n로지스틱회귀는 최신의 복잡한 알고리즘이 아닌 단순한 모델로, 편중된 데이터에 취약한데 여기에는 궁합이 좋음\nXGBoost나 Neural Network같은 복잡한 모델에는 오히려 안좋을 수 있음\n\n\n\n오버샘플링 & 언더샘플링\n\n오버샘플링 : 부족한 데이터를 늘림\n\n장점 : 데이터의 손실이 없음\n단점 : 데이터를 늘리기 위해 같은 데이터를 반복하면서 노이즈 발생 : Precision이 낮아진다(Recall은 높아진다)\n\nSMOTE가 이를 보완하기 위한 오버샘플링 기법이지만 근본적으로는 데이터가 반복되는 것은 같음\n(별도확인) ADASYN으로 SMOTE의 단점을 보완가능(SMOTE와 달리 다른 클래스의 데이터 수도 고려하여 증강)\n\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n언더샘플링 : 많은 쪽의 데이터를 부족한 데이터만큼 남기고 버림\n\n단점 : 데이터를 버린다는 손실\nfunction이 아니라 직접 구현한다면, 순서대로 가져오지 않고 Random하게 가져오도록 구현\n\n샘플링을 통해, 이상거래에 더 잘 반응할 수 있는 모델로 만들어 줄 수 있음\n\n다만 고객 입장에서는 정상거래인데 이상거래로 판단되게 되는 불편함이 생길 수 있음\n\n\n\nStratifiedKFold : 특정 Label이 많거나 적어서 분포가 Imbalance한 경우 활용\n\nValidation set을 나누는 기법이므로, Test set 분리 후 적용\nfold: 같은 Data set에서, 각 부분을 모두 Validation으로 활용할 수 있도록 반복\n\n데이터가 적을 때, 있는 데이터를 최대한 활용해, Validation set을 다양하게 만들 수 있다\n예를 들어 100개 데이터에 5 fold를 적용한다면, 아래와 같이 구성됨(굵은 부분이 validation set)\n\n120, 21~100 / 1~20, 2140, 41~100 / 1~40, 4160, 61~100 / 1~60, 6180, 81~100 / 1~80, 81~100\n\n\n참고 코드\n\n  skfold = StratifiedKFold(n_split=5, random_state=None, Shuffle=False)\n  for train_index, test_index in skfold.split(X,Y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n이상치 데이터에 대한 처리 : 제거 vs 대체\n\n제거는 데이터의 손실이므로 대체가 더 좋을 때도 있음\n\nBox plot 기준으로 Upper bound이상의 값들은 Upper bound값으로 치환하는 것도 방법\n\n\n차원축소(t-sne, PCA , SVD등)\n\n차원축소 : Feature들을 상관관계가 없는 Feature로 바꾸는 것을, 수학적인 방법으로 적용\n\n예를 들어 100개의 feature 중 연봉과 월급은 중복(상관관계있음)이므로 차원축소 적용 후 10개의 feature로 mapping\n\n대표적으로 PCA가 있음, 현재의 데이터는 이미 PCA가 적용되어 있음\n종류\n\nPCA : 상관관계를 없애며 전처리 가능\nSVD(간단하게 PCA를 확장한 것으로 볼 수 있음. 전처리 가능)\nt-sne : 저차원으로 mapping 하여 시각화 용이 (전처리는 불가)\n\n유의사항 : PCA나 SVD는 선형적인 관계가 있을때만 적용한다(비선형적인 정보는 손실되므로 모델성능하락이 있을 수 있다)\n\nPlot을 그렸을 때 fold의 수만큼 점으로 나오게 됨(5fold면 5개)  \nRecall, Precision은 Trade-off, 2개 모두 준수한 Sweet spot을 찾기 위해 함께 볼 수 있는 방법\n\nf1-score : 조화평균을 활용한 하나의 숫자로 판단 가능\nROC Curve : TP와 FP를 x, y축으로 두고 면적으로 판단 가능(넓을수록 좋음, 0~1값 가짐)\nPR(Precision-Recall) Curve : Precision과 Recall을 x, y축으로 두고 면적으로 판단 (넓을수록 좋음)\n\nBinary 문제의 경우, Simoid 대신 2개 뉴런의 Softmax도 가능함\nBatch size는 관례(2의 배수)를 맞추는 것이 좋음 (보고서 확인의 관점에서는 안좋은 시선으로 보게될 수 있다)\n이상치 Robustscaler\ntrain_test_split(stratify=실제Label)옵션으로 Label의 비율을 비슷하게 샘플링 가능\n오버샘플링의 단점 : precision이 낮아진다\n\nprecision은 판정한 건 중 실제의 비율이고, 오버샘플링으로 정상거래를 이상거래로 판정하는건이 많아짐\nrecall의 측면에서는 오버샘플링을 통해 판정을 늘리는게 나을 수 있음(암환자 사례)\n\n데이터 불균형에서의 가중치 설정 : Class(label)분포 기반 가중치 설정\n\n더 중요하게 보는 것에 패널티를 더 크게 줌\n정상거래(0) 284315건, 이상거래(1) 492건인 경우의 예시\n  total_samples = 284315 + 492\n  weight_for_0 = total_samples / ( 2 * 284315)\n  weight_for_1 = total_samples / (2 * 492)\n  class_weights = {0:weight_for_0, 1:weight_for_1}\n\n  model.fit(x_train, y_train, class_weight=class_weights)\nNeural network에서 해결할 수 있는 부분이어서 오히려 혼동을 줄 수도 있다\n\n모델 알고리즘이 복잡한 경우는, Weight에서 패널티를 다르게 주는 focal loss나 오버샘플링을 잘 쓰지 않음\n\nNeural network에 Sigmoid focal CrossEntrophy를 통해 패널티를 높게 주어 긍정적 효과를 기대했지만, 오히려 성능하락될 수 있음\n\n모델 알고리즘이 단순한 Logistic regression 등은, 이러한 Weighted loss가 효과가 좋을 수 있음\n\n\nLearning rate scheduler 함수로 구현 예시\n\n  from tf.keras.callbacks import ReduceLROnPlateau\n  reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                factor=0.2, # lr감소치. 현재 lr이 0.01이라면 0.01*0.2를 적용\n                patience=10, # 개선에 대한 허용치. 10 epoch까지 개선이 없다면 적용\n                mode='min', # auto, max, min 옵션 있음\n                min_lr=0.001)\n  model.fit(x_train, y_train, callbacks=[reduce_lr])\n\nDropout은 간단한 모델이라면 높게 설정할지 않아도 괜찮음\nBatch size는 데이터가 적거나 장비(메모리) 상황이 좋다면 늘려서 빠르게 학습\n과적합(Overfitting)의 원인\n\n모델 자체의 문제, 앞단계인 데이터 전처리나 샘플링, Data split 등의 문제일 수 있음\n단순히 성능이 안나왔다해서 과적합으로 판단은 금물\n\nLDA (Linear discriminant Anaylsis) : 분류문제에서는 Logistic Regression 좋은 성능\nKNN (K-Nearest Neighbor) : 근접(이웃)한 데이터들을 분류\nCart : 의사결정나무(Decision Tree)의 기본 모델\nRandom forest : 여러개의 트리 모델을 만들고, 다수결로 정함. 앙상블 모델\nBoosting 계열 모델 : 학습 후, 틀린 부분에 가중치를 두면서 학습 반복\n\n처음에는 이상거래에 대해 많이 틀리다가, 점점 잘 잡아내도록 학습됨.\nXGB를 많이 사용\n\n이외에 Catboost 등도 있음\n현재와 같은 20만건 정도의 데이터는, 최신계열의 복잡한 머신러닝에서 더 성능이 좋을 수 있음\nCross validation 활용시, 여러번의 모델Score가 나오므로 Box plot을 통해 안정적인 모델인지 확인 가능\n모델의 선택 (SOTA가 기준이 되어야 하는지에 대한 질문)\n\n문제 정의(상황이 어떤 문제인지 정의할 능력 필요)\n어떤 계열의 모델이 적합한지 선택 (전반적인 머신러닝 등 모델 지식 필요)\nSOTA(State of the art)는 이후의 문제\n\n논문 등을 볼 때는 문제 Setup까지 본 후, 바로 풀이보지말고 먼저 고민 후 보기\nRandom forest모델 사용시 두가지 고려사항\n\nCross validation을 통한 과적합 방지\n\n앙상블모델은 복잡한 머신러닝 기법에 속하고, XGB나 Random forest는 과적합 일어날 수 있음\n\nHyper parameter tuning을 통한 개선\n\nThresholding(임계값 조정)\n\nPrecision과 recall은 trade-off. 원하는 것은 둘 다 높은 Spot\n\n이러한 Sweet spot을 시각적으로는 ROC/PR Curve로 확인, 코드로는 Threshold adjustment 조절\nThreshold 예시 일부\n\n  y_scores = model.predict_proba(X_test)[:, 1]\n  precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n  f1_score = 2 * precision * recall / (precision + recall)\n  optimal_idx = np.argmax(f1_scores)\n  optimal_threshold = thresholds[optimal_idx]"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html",
    "href": "posts/prgms-sql-20240318/index.html",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#개요",
    "href": "posts/prgms-sql-20240318/index.html#개요",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "",
    "text": "프로그래머스 SQL 문제풀이 연습(Oracle기준, Mysql아님)입니다  (비상업적, 비영리적 용도)\n문제링크 게시가능여부"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#문제-가장-비싼-상품-구하기",
    "href": "posts/prgms-sql-20240318/index.html#문제-가장-비싼-상품-구하기",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "문제 : 가장 비싼 상품 구하기",
    "text": "문제 : 가장 비싼 상품 구하기\n\n\n\n문제 이미지"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#작성답안",
    "href": "posts/prgms-sql-20240318/index.html#작성답안",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "작성답안",
    "text": "작성답안\n\n\n\nSELECT MAX(PRICE) AS MAX_PRICE\nFROM PRODUCT\n\n\nFigure 1"
  },
  {
    "objectID": "posts/prgms-sql-20240318/index.html#정리",
    "href": "posts/prgms-sql-20240318/index.html#정리",
    "title": "[프로그래머스SQL] 가장 비싼 상품 구하기",
    "section": "정리",
    "text": "정리\n\nMAX(컬럼명) : 최대값\nMIN(컬럼명) : 최소값"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "[DA스터디/최종과제] Autogluon활용한 베이스모델 생성 및 고도화 실험\n\n\n\nPython\n\n\nAutogluon\n\n\nCUDA\n\n\nEDA\n\n\nSHAP\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-02-02\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/6주차] optuna, Autogluon\n\n\n\nPython\n\n\noptuna\n\n\nAutogluon\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-26\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/5주차/과제] SHAP시각화 & 변수 설명\n\n\n\nPython\n\n\nMetric\n\n\nXAI\n\n\nExplainableAI\n\n\nSHAP\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-21\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/5주차] 평가Metric, XAI, SHAP\n\n\n\nPython\n\n\nMetric\n\n\nXAI\n\n\nExplainableAI\n\n\nSHAP\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/4주차/과제] 불균형데이터 처리(오버샘플링, 가중치 조절 등)\n\n\n\nPython\n\n\nUnder-sampling\n\n\nOver-sampling\n\n\nHybrid-sampling\n\n\nModel selection\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/4주차] 불균형데이터 처리, 파생변수, 모델선택\n\n\n\nPython\n\n\nUnder-sampling\n\n\nOver-sampling\n\n\nHybrid-sampling\n\n\nModel selection\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-12\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/3주차/과제] EDA 및 전처리\n\n\n\nPython\n\n\nEDA\n\n\nPreprocessing\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-07\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/3주차] EDA 및 전처리 이론(+금융데이터 특징)\n\n\n\nPython\n\n\nEDA\n\n\nPreprocessing\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2025-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/2주차] SQL심화\n\n\n\nPython\n\n\nSQL\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n[인프콘2024] ‘혹시 당신은 데이터를 모르는 백엔드 개발자인가요’ 세션 정리\n\n\n\n인프콘\n\n\n인프콘2024\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-12-25\n\n\n\n\n\n\n\n\n\n\n\n\n[DA스터디/1주차] SQL기초 및 실습\n\n\n\nPython\n\n\nSQL\n\n\n202412Study_DataAnalysis\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/최종과제_피드백정리] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nairflow\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/최종과제1-gharchive] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nairflow\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/최종과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nairflow\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/6주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nKibana\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/5주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nairflow\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/5주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nAirflow\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nelasticsearch\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-05\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/4주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nElasticsearch\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-09-02\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/3주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-30\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/3주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/2주차과제2] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-23\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/2주차과제1] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-23\n\n\n\n\n\n\n\n\n\n\n\n\n[KOSIS데이터분석] 화장품이나 육아용품은 서울시 어떤 구에서 가장 많이 받았을까?\n\n\n\n파이썬\n\n\n분석\n\n\nfolium\n\n\nchoropleth\n\n\ngeojson\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-21\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/2주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-19\n\n\n\n\n\n\n\n\n\n\n\n\n[DE스터디/1주차강의] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링\n\n\n\nSpark\n\n\nPySpark\n\n\nDocker\n\n\nDocker-compose\n\n\n202408Study_DataEngineering\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n[한빛앤] 데이터 웨어하우스로 효율적인 분석시스템 만들기 세미나 정리\n\n\n\nDataWarehouse\n\n\nDataLake\n\n\nDataMart\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n[MStudy_과제개선3] 신용카드 이상거래 탐지 모델링 with ML\n\n\n\n파이썬\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\noptuna\n\n\nOptunaSearchCV\n\n\nRandomForest\n\n\nScikit-learn\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n[MStudy_과제개선2] 신용카드 이상거래 탐지 모델링\n\n\n\n파이썬\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\nKeras tuner\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n[MStudy_과제개선1] 신용카드 이상거래 탐지 모델링\n\n\n\n파이썬\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\nStratifiedKFold\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_최종과제 피드백/이론] 신용카드 이상거래 탐지 모델링\n\n\n\n파이썬\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_최종과제] 신용카드 이상거래 탐지 모델링\n\n\n\n파이썬\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_별도 공부] Keras Tuner\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nFDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-07-10\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_6주차] 자연어처리 및 RNN관련 기초내용\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-30\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-7(광물 전체 제안배경 작성)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-24\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_5주차] Overfitting Control & Hyper-Parameter\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_4주차과제] CNN으로 MNIST다루기\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nMNIST\n\n\nTensorflow\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-22\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-6(니켈기준 Plot짜보기)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-21\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-5(UN Comtrade API)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\nUN ComtradeAPI\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-16\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_4주차] Convolutional Neural Network\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-16\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_3주차과제1] Softmax로 MNIST다루기\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nMNIST\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-15\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_3주차과제2] Neural Network로 MNIST다루기\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\nMNIST\n\n\nTensorflow\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-15\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-4(모델에 대한 Feature개발 및 평가지표)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-15\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-3(사용할 피쳐 재분석)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\ngithub\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-10\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_3주차] Multi-class Classification / Artificial Neural Network\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-09\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-2(github온라인db)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\ngithub\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-08\n\n\n\n\n\n\n\n\n\n\n\n\n[공모전] 공공데이터 공모전-1(대상광물 분석, 공공데이터API)\n\n\n\n공모전\n\n\n공공데이터\n\n\n분석\n\n\n공공데이터API\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-06\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_2주차] Multiple Regression / Logistic Regression\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-06-02\n\n\n\n\n\n\n\n\n\n\n\n\n[M_Study_1주차] Tensorflow / Linear Regression\n\n\n\n파이썬\n\n\n머신러닝\n\n\n딥러닝\n\n\n202406Study_FDS\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-05-26\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 3월에 태어난 여성 회원 목록 출력하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-21\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 조건에 부합하는 중고거래 댓글 조회하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 조건에 맞는 도서 리스트 출력하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-19\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 가장 비싼 상품 구하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-18\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스SQL] 평균 일일 대여 요금 구하기\n\n\n\n프로그래머스/SQL\n\n\n\n\n\n\n\nKibok Park\n\n\n2024-03-17\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] 공공데이터 상권정보로 인터랙티브 맵 시각화(folium)\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n[한빛앤] GitHub Copilot 세미나 정리\n\n\n\nCopilot\n\n\nCodeium\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-09-13\n\n\n\n\n\n\n\n\n\n\n\n\n[Pycon2023] 짠내나는 데이터 다루기 세션 정리\n\n\n\n파이썬\n\n\n파이콘\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-08-13\n\n\n\n\n\n\n\n\n\n\n\n\n[Scikit-learn] Kaggle 집값예측 실습\n\n\n\n파이썬\n\n\n머신러닝\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-05-06\n\n\n\n\n\n\n\n\n\n\n\n\n[Pytorch] MNIST 실습\n\n\n\n파이썬\n\n\n머신러닝\n\n\n\n\n\n\n\nKibok Park\n\n\n2023-02-19\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] KOSIS 온라인쇼핑 해외직접판매액 간단분석 (타 주제 Choropleth시각화)\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2022-11-13\n\n\n\n\n\n\n\n\n\n\n\n\n[간단분석] 공공데이터포털 건강검진정보 활용\n\n\n\n파이썬\n\n\n분석\n\n\n\n\n\n\n\nKibok Park\n\n\n2022-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "index_dashboards.html",
    "href": "index_dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "No matching items\n\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "긴급하게 현재 가지고 있는 데이터로, 예측관련하여 만들 수 있는 머신러닝 모델을 요청받음\nKaggle에서 운송수단을 예측하는 KNN모델 샘플을 발견하여 적용\n\n\n\n\n\n국내외의 이동 데이터를 학습한 머신러닝을 활용하여, 조건(국가, 거래업체 등)입력시 어떤 운송수단으로 수출할지 예측(항공/해상)\n\n[모델1] KNeighborsClassifier 활용한 분류\n\n모델 정확성을 위해 7개의 Feature로 학습\n업무 사용시에는 사용자가 알 수 있는 4가지 정보를 필수로 받음 (‘Loading Country’, ‘Final Destination country’, ‘Sold-to party’, ‘Ship To Party’)\n\n3가지 추가정보 제공시 더 정확한 예측제공, 없는 경우 기존데이터의 최빈값으로 대체하여 예측 (Sales Organization, Incoterms, Dangerous goods)\n\n\n\n예측된 운송수단을 기반으로, 어느정도의 공간(부피)가 필요한지 확인 후 부킹(운송을 위한 공간예약)에 활용\n\n[모델2] 활용한 예측\n\n모델1에서 입력/예측된 값을 기반으로 부피(Volumn)을 예측\n\n‘Loading Country’, ‘Final Destination country’, ‘Sold-to party’, ‘Ship To Party’ (+ ‘AIR/VESSEL’, ‘Dangerous goods’)\n\n\n\n예측된 값과 함께, 기존 데이터의 History를 제공해 어떤 출발지에서 나갔었는지 함께 공유하여 업무에 활용\n\n어떤 출발지인지에 따라, 사용하면 안되는 선사(해상운송업체)나 항공사가 정해질 수 있음\n\nKNN 참조한 샘플코드 : https://www.kaggle.com/code/sergeifursa/shipment-type-prediction-with-knn\n\n\n\n\n\n특정 조건(출/도착국가, 상대업체)을 기반으로, 항공기와 배 중 어떤 수단으로 얼마만큼 물량을 선적해야할 지 예측\n예측한 데이터를 기반으로, 조기에 Space(선적공간)을 수배하고 선점하여 대응력 강화\n\n\n\n\n\n별도의 github레포로 정리하지 않고, 하단에 코드로만 기록\n\n\n\n\n\ndb로 전환하여 sqlite3으로 데이터 보관\n데이터 전처리는 아래의 로직에 따라 pandas로 처리\n\n\nDrop\n\n예측 대상인 중량(GrossWT), 항공중량(ChargeableWT), 부피(Volumn)이 모두 없으면 Drop\n배로 선적하는데 부피가 없는 경우 Drop\n\n변환(단위 통일)\n\n중량은 G/KG 중 KG으로, 부피는 CCM/CBM중 CBM으로 설정\n\n대체(Null보완)\n\n항공중량(ChargeableWT)만 없는 경우, 별도의 계산식으로 보완처리 (Max(중량, 부피*167))\n\n수치변환(To Numeric)\n\nY값으로 사용할 값 중 Categorical한 AIR/VESSEL컬럼은 dict로 관리 및 보관\nX값으로 사용할 값 중 Categorical한 컬럼들은 Scikit-learn의 category_encoders로 변환\n\n향후 데이터가 추가/삭제될 상황을 위해 사용(업체코드 등은 추가가 빈번할 것으로 예상)\nHigh Cardinality하여 OnehotEncoding으로 인한 차원문제도 방지\n최초 실행 후 joblib로 encoder값을 저장해두고, 이후에는 로딩하여 사용\n\n\n\n\n데이터셋 분할은 Scikit-learn의 train_test_split으로 train/test 8:2로 split\n모델 파라미터는 optuna를 활용하여 세팅\n\n\n첫 사용 모델로 일부 특이사항만 지정하여 OptunaSearchCV로 파라미터 지정\n\n서열형(Ordinal)이 아닌 Categorical한 Y값이어서 Hamming을 metric으로 지정\n일부 Null값이 있어 kd_tree에서는 적용불가 메시지가 떠 kd_tree는 대상에서 제외\n이외 파라미터는 공식문서의 default값을 기준으로 일부 buffer를 두어 세팅함\n\n\n\nScikit-learn의 cross_val_score으로 모델평가 (accuracy사용)\n모델예측은 앞서 category_encoders으로 변환 후 학습했으므로, predict시에도 변환 후 적용\njoblib로 모델저장\n이후 내용 추가 예정\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#추진배경",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "긴급하게 현재 가지고 있는 데이터로, 예측관련하여 만들 수 있는 머신러닝 모델을 요청받음\nKaggle에서 운송수단을 예측하는 KNN모델 샘플을 발견하여 적용"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#계획",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#계획",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "국내외의 이동 데이터를 학습한 머신러닝을 활용하여, 조건(국가, 거래업체 등)입력시 어떤 운송수단으로 수출할지 예측(항공/해상)\n\n[모델1] KNeighborsClassifier 활용한 분류\n\n모델 정확성을 위해 7개의 Feature로 학습\n업무 사용시에는 사용자가 알 수 있는 4가지 정보를 필수로 받음 (‘Loading Country’, ‘Final Destination country’, ‘Sold-to party’, ‘Ship To Party’)\n\n3가지 추가정보 제공시 더 정확한 예측제공, 없는 경우 기존데이터의 최빈값으로 대체하여 예측 (Sales Organization, Incoterms, Dangerous goods)\n\n\n\n예측된 운송수단을 기반으로, 어느정도의 공간(부피)가 필요한지 확인 후 부킹(운송을 위한 공간예약)에 활용\n\n[모델2] 활용한 예측\n\n모델1에서 입력/예측된 값을 기반으로 부피(Volumn)을 예측\n\n‘Loading Country’, ‘Final Destination country’, ‘Sold-to party’, ‘Ship To Party’ (+ ‘AIR/VESSEL’, ‘Dangerous goods’)\n\n\n\n예측된 값과 함께, 기존 데이터의 History를 제공해 어떤 출발지에서 나갔었는지 함께 공유하여 업무에 활용\n\n어떤 출발지인지에 따라, 사용하면 안되는 선사(해상운송업체)나 항공사가 정해질 수 있음\n\nKNN 참조한 샘플코드 : https://www.kaggle.com/code/sergeifursa/shipment-type-prediction-with-knn"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#효과",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#효과",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "특정 조건(출/도착국가, 상대업체)을 기반으로, 항공기와 배 중 어떤 수단으로 얼마만큼 물량을 선적해야할 지 예측\n예측한 데이터를 기반으로, 조기에 Space(선적공간)을 수배하고 선점하여 대응력 강화"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#github-repository",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "별도의 github레포로 정리하지 않고, 하단에 코드로만 기록"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "",
    "text": "db로 전환하여 sqlite3으로 데이터 보관\n데이터 전처리는 아래의 로직에 따라 pandas로 처리\n\n\nDrop\n\n예측 대상인 중량(GrossWT), 항공중량(ChargeableWT), 부피(Volumn)이 모두 없으면 Drop\n배로 선적하는데 부피가 없는 경우 Drop\n\n변환(단위 통일)\n\n중량은 G/KG 중 KG으로, 부피는 CCM/CBM중 CBM으로 설정\n\n대체(Null보완)\n\n항공중량(ChargeableWT)만 없는 경우, 별도의 계산식으로 보완처리 (Max(중량, 부피*167))\n\n수치변환(To Numeric)\n\nY값으로 사용할 값 중 Categorical한 AIR/VESSEL컬럼은 dict로 관리 및 보관\nX값으로 사용할 값 중 Categorical한 컬럼들은 Scikit-learn의 category_encoders로 변환\n\n향후 데이터가 추가/삭제될 상황을 위해 사용(업체코드 등은 추가가 빈번할 것으로 예상)\nHigh Cardinality하여 OnehotEncoding으로 인한 차원문제도 방지\n최초 실행 후 joblib로 encoder값을 저장해두고, 이후에는 로딩하여 사용\n\n\n\n\n데이터셋 분할은 Scikit-learn의 train_test_split으로 train/test 8:2로 split\n모델 파라미터는 optuna를 활용하여 세팅\n\n\n첫 사용 모델로 일부 특이사항만 지정하여 OptunaSearchCV로 파라미터 지정\n\n서열형(Ordinal)이 아닌 Categorical한 Y값이어서 Hamming을 metric으로 지정\n일부 Null값이 있어 kd_tree에서는 적용불가 메시지가 떠 kd_tree는 대상에서 제외\n이외 파라미터는 공식문서의 default값을 기준으로 일부 buffer를 두어 세팅함\n\n\n\nScikit-learn의 cross_val_score으로 모델평가 (accuracy사용)\n모델예측은 앞서 category_encoders으로 변환 후 학습했으므로, predict시에도 변환 후 적용\njoblib로 모델저장\n이후 내용 추가 예정"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#load-with-dbsqlite3",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#load-with-dbsqlite3",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "Load with DB(sqlite3)",
    "text": "Load with DB(sqlite3)\n\nimport pandas as pd\nimport sqlite3\n\n# Connect to the SQLite database\nconn = sqlite3.connect('sample.db')\n\n# Write the DataFrame to the database\nquery = \"SELECT * FROM `table`\"\ndf = pd.read_sql_query(query, conn)\n\n# Close the database connection\nconn.close()\ndf"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#drop-및-값-변환",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#drop-및-값-변환",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "Drop 및 값 변환",
    "text": "Drop 및 값 변환\n\n[Drop] 운임 또는 Space확보의 기준되는 값이 없는 것이므로 Drop\n\n해상/항공여부를 예측하더라도, Space정보가 없는 경우는 이후 대응이 불가하므로 Drop\n\n[변환] 중량 등 수치에 대한 단위 일치\n\n중량(KG, G), 부피(CBM, CCM)일치시키기\n\n[Null대체] 주어진 값으로 계산하여 대체 가능한 경우 반영\n\nChargeable WT : 중량과 부피를 기준으로 계산가능. Null인 경우 계산하여 대체\n\n\n\n# 판다스 컬럼생략하지 않게하는 옵션설정\npd.set_option('display.max_columns', None)\n\n\ndef drop_fillna_convert_df(df):\n    # Drop\n    df_dropped = df.drop(df[(df['AIR/VESSEL']==2) & df['Charg. Unit'].isna()].index).copy()\n    ## 모든 중량/부피가 없음\n    df_dropped = df_dropped.drop(df_dropped[df_dropped['Gross weight'].isna() & df_dropped['Volume'].isna() & df_dropped['Charg. weigh'].isna()].index)\n    ## 해상인데 부피 또는 부피단위가 없음\n    df_dropped = df_dropped.drop(df_dropped[(df_dropped['AIR/VESSEL']==1) & df_dropped['Volume'].isna()].index)\n    df_dropped = df_dropped.drop(df_dropped[(df_dropped['AIR/VESSEL']==1) & df_dropped['Volume unit'].isna()].index)\n    ## 항공인데, ChargeableWT가 없는데, 계산할 기초값인 Groww weight나 Volumn 중 하나가 없음\n    df_dropped = df_dropped.drop(df_dropped[(df_dropped['AIR/VESSEL']==2) & df_dropped['Charg. weigh'].isna() & df_dropped['Gross weight'].isna()].index)\n    df_dropped = df_dropped.drop(df_dropped[(df_dropped['AIR/VESSEL']==2) & df_dropped['Charg. weigh'].isna() & df_dropped['Volume'].isna()].index)\n\n    # Convert Unit\n    df_dropped.loc[df_dropped['Gross unit'] == 'G', 'Gross weight'] *= 1000\n    df_dropped.loc[df_dropped['Volume unit'] == 'CCM', 'Volume'] /= 1000\n\n    # Null대체\n    crit_replace_na = (df_dropped['Charg. weigh'].isnull()) & (df_dropped['AIR/VESSEL'] == 2)\n    df_dropped.loc[crit_replace_na, 'Charg. weigh'] = np.maximum(df_dropped[crit_replace_na]['Gross weight'], df_dropped[crit_replace_na]['Volume'] * 167)\n\n    return df_dropped\n\ndf_dropped = drop_fillna_convert_df(df)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#x-y값-변환to-numeric",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#x-y값-변환to-numeric",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "X, Y값 변환(To numeric)",
    "text": "X, Y값 변환(To numeric)\n\nY값 변환\n\n사용할 3개의 Y값(AIR/VESSEL, Volumn, Charg. weigh)중, 한가지는 Categorical이므로 숫자로 변경\n\nAIR/VESSEL(Categorical이므로), Volumn(Numeric), Charg. weigh(Numeric)\n\n해상/항공만 사용할 예정이므로 해상과 항공에 대해 매핑, 나머지는 9\n\nX값 변환\n\nDangerous goods는 2가지 값(O,X)밖에 없으므로 0,1로 변환\n\n\n\nmapping_airvessel_dict = dict()\n\nfor i in df['AIR/VESSEL'].unique():\n    if 'S' in i:\n        mapping_airvessel_dict[i] = 1\n    elif 'P' in i or 'M' in i:\n        mapping_airvessel_dict[i] = 2\n    else:\n        mapping_airvessel_dict[i] = 9\nmapping_airvessel_dict\n\n\n# y값 매핑\ndf_dropped['AIR/VESSEL'] = df_dropped['AIR/VESSEL'].map(mapping_airvessel_dict)\n# X값 매핑\ndf_dropped['Dangerous goods'] = df_dropped['Dangerous goods'].map({None:0,'X':1})"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#x값-변환to-numeric-with-leaveoneout-encoder",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#x값-변환to-numeric-with-leaveoneout-encoder",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "X값 변환(To numeric with LeaveOneOut Encoder)",
    "text": "X값 변환(To numeric with LeaveOneOut Encoder)\n\n머신러닝학습을 위해 숫자로 변환 필요\nLeaveOneOut Encoder를 활용하는 것으로 결정\n\n국가코드는 거의 바뀔 일이 없지만, 아프리카 지역 등 일부 Minor한 국가로 판매할 경우 추가될 수 있음\n업체코드는 추가/삭제될 가능성이 높음\nUnique값이 많으므로 One-hot Encoding시 차원이 너무 많아질 위험 있음(High Cardinality)\n처음에는 아래와 같이 dict로 각 컬럼별 매핑값을 관리하고자했으나, 번거로움과 대상이 많아짐에 따라 Encoder를 활용\n  party_code_reverse = {v: k for k, v in party_code.items()}\n  country_code_map_reverse = {v: k for k, v in country_code_map.items()}\n\n  x_data_encoded = pd.DataFrame()\n  for each_column in x_column:\n      if each_column in ['Loading Country', 'Final Destination country']:\n          x_data_encoded[each_column] = x_data[each_column].map(country_code_map_reverse)\n      elif each_column in ['Sold-to party', 'Ship To Party']:\n          x_data_encoded[each_column] = x_data[each_column].map(party_code_reverse)\n\nSci-kit learn LeaveOneOut Encoder 공식문서\n\nhttps://contrib.scikit-learn.org/category_encoders/leaveoneout.html\n\n\n\n# 학습할 Feature\nx_column = ['Loading Country', 'Final Destination country', 'Sold-to party', 'Ship To Party', 'Dangerous goods','Sales Organization','Incoterms']\ny_column = 'AIR/VESSEL'\n\n\n# LeaveOneOutEncoder (최초 실행시)\nimport category_encoders as ce\nimport joblib\n\nencoder_leave_one_out = ce.LeaveOneOutEncoder(cols=x_column, sigma=0.1, return_df=True)\nx_data_all = encoder_leave_one_out.fit_transform(df_dropped[x_column], df_dropped[y_column])\n\njoblib.dump(encoder_leave_one_out, 'encoder_leave_one_out.pkl')\n\nx_data_all\n\n\n\n\n\n\n\n\nLoading Country\nFinal Destination country\nSold-to party\nShip To Party\nDangerous goods\nSales Organization\nIncoterms\n\n\n\n\n0\n7.047596\n3.679223\n3.861376\n8.134116\n2.765859\n2.988212\n3.206009\n\n\n1\n1.501914\n1.956254\n1.396398\n1.010231\n2.410832\n3.551036\n2.268934\n\n\n2\n1.246987\n1.247269\n1.076889\n1.069540\n2.266524\n3.929307\n1.368437\n\n\n3\n7.557798\n3.385827\n4.343922\n5.881165\n2.453504\n3.253987\n3.082357\n\n\n4\n1.711316\n1.644219\n1.418986\n1.081862\n2.604062\n1.513666\n1.461735\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n68896\n1.805997\n1.211290\n1.620240\n0.923400\n2.128367\n4.805674\n1.252805\n\n\n68897\n1.738878\n1.543501\n1.904886\n0.850793\n2.625204\n2.538548\n2.008110\n\n\n68898\n1.916489\n1.649348\n1.104931\n1.064014\n2.018030\n2.863359\n1.569362\n\n\n68899\n1.903145\n2.041549\n2.386892\n0.954536\n2.133451\n1.841485\n1.714075\n\n\n68900\n1.963608\n1.810246\n1.429540\n1.208997\n2.474078\n2.198915\n1.706463\n\n\n\n\n68880 rows × 7 columns\n\n\n\n\n# LeaveOneOutEncoder (실행내역 있는 경우)\n\nencoder_leave_one_out = joblib.load('encoder_leave_one_out.pkl')\nx_data_all = encoder_leave_one_out.transform(df_dropped[x_column], df_dropped[y_column])\n\njoblib.dump(encoder_leave_one_out, 'encoder_leave_one_out.pkl')\n\nx_data_all\n\n\n\n\n\n\n\n\nLoading Country\nFinal Destination country\nSold-to party\nShip To Party\nDangerous goods\nSales Organization\nIncoterms\n\n\n\n\n0\n7.041479\n2.808034\n3.644974\n10.297530\n2.271593\n3.376236\n2.761253\n\n\n1\n1.203195\n1.595750\n1.537957\n1.012471\n2.542451\n4.059490\n1.791253\n\n\n2\n1.509811\n1.355136\n1.029430\n0.941528\n2.484937\n3.039443\n1.278045\n\n\n3\n8.488335\n4.329470\n5.074977\n5.995380\n2.378870\n3.362426\n2.950839\n\n\n4\n1.371701\n1.669704\n1.662936\n1.251279\n2.103728\n1.285509\n1.482815\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n68896\n1.759238\n1.000788\n1.684182\n1.115493\n2.130670\n2.516951\n1.334155\n\n\n68897\n2.000342\n1.765736\n1.469979\n1.130242\n2.640289\n2.358999\n1.532509\n\n\n68898\n1.901704\n1.555484\n1.386904\n0.944819\n2.572425\n2.429427\n1.479614\n\n\n68899\n2.164074\n2.300433\n2.976153\n1.097528\n2.626010\n1.895660\n1.757920\n\n\n68900\n1.721231\n1.691727\n1.582234\n1.200996\n2.586225\n1.986513\n1.549286\n\n\n\n\n68880 rows × 7 columns\n\n\n\n\ny_data_all = df_dropped['AIR/VESSEL']\ny_data_all\n\n0        9\n1        1\n2        1\n3        9\n4        1\n        ..\n68896    1\n68897    1\n68898    1\n68899    1\n68900    1\nName: AIR/VESSEL, Length: 68880, dtype: int64"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#데이터셋-분할split-traintest",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#데이터셋-분할split-traintest",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "데이터셋 분할(Split Train/Test)",
    "text": "데이터셋 분할(Split Train/Test)\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_data_all, y_data_all, train_size=0.8, shuffle=True, stratify=y_data_all)"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-학습",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-학습",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "KNeighborsClassifier 학습",
    "text": "KNeighborsClassifier 학습\n\n처음 사용해보는 모델이면서 마감기한이 촉박하여, optuna를 활용하여 파라미터 세팅\nScikit-learn KNeighborsClassifier 공식문서\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n\nMetric의 경우, hamming distance를 채택\n\nKaggle 샘플코드에서 Categorical인 경우는 hamming distance를 기준한다고 되어있음을 참고\n시간관계상 간단히 알아보니, Simple matching이라면 hamming, 중요도가 있다면 jacard라고 함\n이외에 변수가 서열형(ordinal)인 경우에도 다른 방법이 있다고 하나, 현재의 데이터는 서열형으로 볼 수는 없어 제외\n\n\n\nimport optuna\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(metric='hamming')\n\nparam_distributions = {\n    \"n_neighbors\": optuna.distributions.IntDistribution(3, 5),\n    \"weights\": optuna.distributions.CategoricalDistribution(['uniform', 'distance']),\n    \"algorithm\": optuna.distributions.CategoricalDistribution(['auto', 'ball_tree', 'brute']), #'kd_tree'는 nan인 경우 문제가 있어 제외\n    \"leaf_size\" : optuna.distributions.IntDistribution(20, 40),\n}\n\noptuna_search = optuna.integration.OptunaSearchCV(\n    clf, \n    param_distributions, \n    n_jobs=-1, # Number of parallel jobs. -1 means using all processors.\n    cv=5, #  estimator가 classifier & label이 binary or multiclass라면 sklearn.model_selection.StratifiedKFold 적용 (이외는 sklearn.model_selection.KFold)\n    n_trials=100, \n    timeout=600, \n    verbose=2,\n    scoring=None, # If None, score on the estimator is used.\n    refit=True # Best Parameter로 refit. refitted estimator는 best_estimator_ attribute로 바로 predict가능\n)\n\noptuna_search.fit(x_train, y_train)\n\nprint(\"Best trial:\")\ntrial = optuna_search.study_.best_trial\n\nprint(\"  Value: \", trial.value)\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\npy:13: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n  optuna_search = optuna.integration.OptunaSearchCV(\n[I 2024-08-04 22:23:30,602] A new study created in memory with name: no-name-8095b4dc-5e03-4ae4-942f-6fa9c1f72a9c\n[I 2024-08-04 22:24:37,815] Trial 4 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 24}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:24:39,034] Trial 5 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 36}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:24:51,518] Trial 6 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 23}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:26:20,120] Trial 3 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 39}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:26:20,993] Trial 2 finished with value: 0.42937162859026257 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 25}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:26:51,430] Trial 1 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 30}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:26:52,521] Trial 7 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 32}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:26:52,625] Trial 0 finished with value: 0.42937162859026257 and parameters: {'n_neighbors': 3, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 35}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:27:15,381] Trial 8 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 35}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:27:16,046] Trial 9 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 25}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:28:09,880] Trial 14 finished with value: 0.4293897757638404 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 36}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:28:10,904] Trial 10 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 36}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:28:12,691] Trial 15 finished with value: 0.4293897757638404 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 29}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:28:26,116] Trial 17 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 20}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:16,651] Trial 12 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'auto', 'leaf_size': 24}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:17,018] Trial 11 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 34}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:33,118] Trial 19 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 20}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:33,664] Trial 18 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 20}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:37,861] Trial 20 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 20}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:29:46,067] Trial 21 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:30:10,551] Trial 13 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 32}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:30:11,709] Trial 16 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 31}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:30:29,747] Trial 22 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:30:30,705] Trial 23 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 40}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:31:25,150] Trial 29 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 27}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:31:40,271] Trial 30 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 23}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:31:41,349] Trial 31 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 23}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:31:42,531] Trial 28 finished with value: 0.5235917538057604 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 23}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:32:32,403] Trial 27 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 28}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:33:00,736] Trial 24 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 28}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:33:02,806] Trial 26 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 40}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:33:05,471] Trial 25 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 28}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:04,143] Trial 32 finished with value: 0.42937162859026257 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 23}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:13,049] Trial 33 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 22}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:14,021] Trial 34 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 27}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:16,306] Trial 37 finished with value: 0.4293897757638404 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 26}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:22,248] Trial 38 finished with value: 0.4293897757638404 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 22}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:22,899] Trial 39 finished with value: 0.4293897757638404 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'brute', 'leaf_size': 22}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:30,938] Trial 35 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 27}. Best is trial 4 with value: 0.5235917538057604.\n[I 2024-08-04 22:34:37,208] Trial 36 finished with value: 0.3665977063092387 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'ball_tree', 'leaf_size': 27}. Best is trial 4 with value: 0.5235917538057604.\n\n\nBest trial:\n  Value:  0.5235917538057604\n  Params: \n    n_neighbors: 5\n    weights: distance\n    algorithm: brute\n    leaf_size: 24\n\n\n\noptuna_search.best_params_\n\n{'n_neighbors': 5,\n 'weights': 'distance',\n 'algorithm': 'brute',\n 'leaf_size': 24}\n\n\n\nbest_model_knclassifier = optuna_search.best_estimator_\nbest_model_knclassifier\n\nKNeighborsClassifier(algorithm='brute', leaf_size=24, metric='hamming',\n                     weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(algorithm='brute', leaf_size=24, metric='hamming',\n                     weights='distance')"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-모델평가",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-모델평가",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "KNeighborsClassifier 모델평가",
    "text": "KNeighborsClassifier 모델평가\n\nimport sklearn.model_selection\n\nsklearn.model_selection.cross_val_score(best_model_knclassifier, x_test, y_test, scoring='accuracy', cv=5, \n                                        n_jobs=None, verbose=0)\n\narray([0.52358491, 0.52377495, 0.52377495, 0.52341198, 0.52341198])"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-예측prediction",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-예측prediction",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "KNeighborsClassifier 예측(Prediction)",
    "text": "KNeighborsClassifier 예측(Prediction)\n\n# 예측대상 입력 후 변환\n## 입력\nx_test_input = pd.DataFrame({'Loading Country':'MX',\n                             'Final Destination country':'US',\n                             'Sold-to party':'C310-00',\n                             'Ship To Party':'4891788',\n                             'Dangerous goods':'0',\n                             'Sales Organization':'R001',\n                             'Incoterms':'CIP'},\n                             index=[0]\n)\n## LeaveOneOutEncoder변환\nencoder_leave_one_out = joblib.load('encoder_leave_one_out.pkl')\nx_data_to_predict = encoder_leave_one_out.transform(x_test_input[x_column])\nx_data_to_predict['Dangerous goods'] = x_data_to_predict['Dangerous goods'].map({np.nan:0})\n\nx_data_to_predict\n\n\n\n\n\n\n\n\nLoading Country\nFinal Destination country\nSold-to party\nShip To Party\nDangerous goods\nSales Organization\nIncoterms\n\n\n\n\n0\n7.339717\n3.507706\n4.077912\n9.0\n0\n3.443488\n3.025576\n\n\n\n\n\n\n\n\n# 예측값 {1:'VESSEL',2:'AIR'}\n\nprediction = best_model_knclassifier.predict(x_data_to_predict)\nprediction\n\narray([2], dtype=int64)\n\n\n\narray_to_conver = prediction\nnp.where(array_to_conver == 1, 'VESSEL', np.where(array_to_conver == 2, 'AIR', 'UNKNOWN'))\n\narray(['AIR'], dtype='&lt;U7')"
  },
  {
    "objectID": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-모델저장",
    "href": "posts_miniprojects/sel-py-ml_predict_shiptype-20240801/index.html#kneighborsclassifier-모델저장",
    "title": "[Python] AIR/VESSEL 예측을 위한 머신러닝 모델(진행중)",
    "section": "KNeighborsClassifier 모델저장",
    "text": "KNeighborsClassifier 모델저장\n\nimport joblib\n\njoblib_file = \"model_knn_1st_fitted.joblib\"\njoblib.dump(best_model_knclassifier, joblib_file)\n\n['model_knn_1st_fitted.joblib']\n\n\n\njoblib_file = \"model_knn_1st_fitted.joblib\"\nknn_loaded = joblib.load(joblib_file)\n\nprediction = knn_loaded.predict(x_data_to_predict)\nprediction\n\narray([2], dtype=int64)\n\n\n\narray_to_conver = prediction\nnp.where(array_to_conver == 1, 'VESSEL', np.where(array_to_conver == 2, 'AIR', 'UNKNOWN'))\n\narray(['AIR'], dtype='&lt;U7')"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "",
    "text": "50~120건의 pdf서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n서류제공은 해외의 시차, 고객의 주문시점으로 인해 아침에 제공\n생산을 하기위한 공장또한 다른 국가에 있고, 생산계획 마감시간 문제\n각자의 이유(고객이 다양해 시점을 조정하기 어려움, 생산투입자원 조정을 위한 마감시간의 존재)로 조정 어려움\n\n문제 개선을 위해 아래의 포인트를 확인하였음\n\n제공되는 pdf서류는 모두 1장의 동일한 양식이며, 드래그가 가능한 형태\n\n드래그가 가능하다면 컴퓨터가 인식하는데도 무리가 없을테니 자동화 도입이 가능할 것이라는 판단\n\n입력작업은 엑셀VBA를 활용한 자동화Tool이 개발되어있음\n시스템 글자수 제한으로 주문번호를 축약하는 별도작업 수행\n주문번호별로 지불조건 등이 내부시스템의 코드로 매칭되어있는 별도의 관리시스템이 있음\n\n확인한 사항을 바탕으로 아래의 개선을 수행\n\n관리시스템에 이미 있는 정확도가 높은 정보를 main으로 가져옴\n시스템에서 확인할 수 없는 pdf의 정보들을 크롤링하여 필요한 정보만 식별\n이미 개발되어있는 자동화입력Tool(엑셀VBA)에 연계가능한 형태로 데이터 가공\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#추진배경",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "",
    "text": "50~120건의 pdf서류를 아침제공 후 오전 내 입력하도록 요청받아 다른 업무가 불가능할 정도의 피크타임 발생\n\n서류제공은 해외의 시차, 고객의 주문시점으로 인해 아침에 제공\n생산을 하기위한 공장또한 다른 국가에 있고, 생산계획 마감시간 문제\n각자의 이유(고객이 다양해 시점을 조정하기 어려움, 생산투입자원 조정을 위한 마감시간의 존재)로 조정 어려움\n\n문제 개선을 위해 아래의 포인트를 확인하였음\n\n제공되는 pdf서류는 모두 1장의 동일한 양식이며, 드래그가 가능한 형태\n\n드래그가 가능하다면 컴퓨터가 인식하는데도 무리가 없을테니 자동화 도입이 가능할 것이라는 판단\n\n입력작업은 엑셀VBA를 활용한 자동화Tool이 개발되어있음\n시스템 글자수 제한으로 주문번호를 축약하는 별도작업 수행\n주문번호별로 지불조건 등이 내부시스템의 코드로 매칭되어있는 별도의 관리시스템이 있음\n\n확인한 사항을 바탕으로 아래의 개선을 수행\n\n관리시스템에 이미 있는 정확도가 높은 정보를 main으로 가져옴\n시스템에서 확인할 수 없는 pdf의 정보들을 크롤링하여 필요한 정보만 식별\n이미 개발되어있는 자동화입력Tool(엑셀VBA)에 연계가능한 형태로 데이터 가공"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#효과",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#효과",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "효과",
    "text": "효과\n\n고객의 주문을 당기거나, 생산계획마감이 지연되거나, 담당자의 과도한 업무가중이 발생하지 않고 문제해결\n이미 개발된 자원(엑셀VBA)에 연동하여 큰 시간을 들이지 않고 개발했으며 업무 투입시간 또한 감소\n\n기존에는 4시간 이내의 투입시간이 있었지만, 이번 도입으로 5~10분 정도로 작업이 완료됨\n기존대비 빠른 완수로 생산담당자 만족, 생산계획이 미뤄지지 않아 납기 등 제품수령 고객도 만족"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#github-repository",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-readPIAR-20231102/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] Peak타임 대응용 수출계약서pdf tabula리딩",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\njson으로 파일을 저장할 경로정보 및 변환할 코드정보를 관리\nxlwings로 Excel로 저장해둔 기본정보를 열람\n\nDRM암호화와 관계없이 파일을 읽을 수 있기 때문에 xlwings를 채택\n\ntabula로 pdf를 표 형태로 읽어, 지정된 자리의 정보를 읽고 json형태로 저장\njson형태로 저장된 정보를 pandas DataFrame으로 concat처리 후 저장\n시스템 등록을 위해 사용중인 별도의 VBA Tool에 저장된 Excel을 넘기면 업무 완료"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "",
    "text": "각 주문서에 나뉘어있는 사용빈도가 높은 정보가 여러 탭에 나뉘어 있음\n\n탭 내에서도 많은 정보를 보여주기 위해 좁은 프레임(4행만 보임)에 많은 정보를 넣어 복사 등을 하기엔 불편함\n4행만 보이는 물품내역 프레임에 50여건의 물품이 있는 경우 많은 시간 소요\n\n크게 조회가능한 모드로 보는 경우에는 제품/모델명/HSCODE의 형식으로 문단형식으로 혼재되어있어 중복제거 등 가공 필수\n\n출발/도착지/품명 등을 전체 주문에 대해 확인하고자 하는 경우 건별로 메뉴진입 필요 (일괄로 조회하는 메뉴는 일부 정보 제외되어있음)\n\n아래의 방안으로 해결하고자 함\n\n조회속도가 빠르므로 필요한 정보를 필요할때마다 일괄 크롤링하도록 설계\n유저의 복사/가공/중복제거의 작업이 패턴화되어있어 미리 진행하여 결과물만 제공\n\n대상정보 : Sales Org, Plant(Code,Name), POL(출발지), POD(도착지), HSCODE, Description(물품명세)\n\n\n\n\n[용어설명] SR : 하나의 기본 선적 단위, Shipping request HSCODE : 해외로 물건을 보내기 위해 수출신고할 때, (의약품, 전자기기 등)물품 종류를 알 수 있는 제품 코드\nCopyright © 2024 Kibok Park All rights reserved."
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#추진배경",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#추진배경",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "",
    "text": "각 주문서에 나뉘어있는 사용빈도가 높은 정보가 여러 탭에 나뉘어 있음\n\n탭 내에서도 많은 정보를 보여주기 위해 좁은 프레임(4행만 보임)에 많은 정보를 넣어 복사 등을 하기엔 불편함\n4행만 보이는 물품내역 프레임에 50여건의 물품이 있는 경우 많은 시간 소요\n\n크게 조회가능한 모드로 보는 경우에는 제품/모델명/HSCODE의 형식으로 문단형식으로 혼재되어있어 중복제거 등 가공 필수\n\n출발/도착지/품명 등을 전체 주문에 대해 확인하고자 하는 경우 건별로 메뉴진입 필요 (일괄로 조회하는 메뉴는 일부 정보 제외되어있음)\n\n아래의 방안으로 해결하고자 함\n\n조회속도가 빠르므로 필요한 정보를 필요할때마다 일괄 크롤링하도록 설계\n유저의 복사/가공/중복제거의 작업이 패턴화되어있어 미리 진행하여 결과물만 제공\n\n대상정보 : Sales Org, Plant(Code,Name), POL(출발지), POD(도착지), HSCODE, Description(물품명세)\n\n\n\n\n[용어설명] SR : 하나의 기본 선적 단위, Shipping request HSCODE : 해외로 물건을 보내기 위해 수출신고할 때, (의약품, 전자기기 등)물품 종류를 알 수 있는 제품 코드"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#효과",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#효과",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "효과",
    "text": "효과\n\n단건 또는 여러건의 주문(SR)에 대해 건당 1~2초 이내로 필요한 정보 수집\n클립보드 복사가 가능한 텍스트, 엑셀형태로 제공하여 요구사항에 대해 즉시대응 가능"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#github-repository",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#github-repository",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "github repository",
    "text": "github repository\n관련 github레포"
  },
  {
    "objectID": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "href": "posts_miniprojects/sel-py-crawl_sr-20240306/index.html#세부내용-구현내용-사용한-언어패키지-등",
    "title": "[Python] ERP(SAP) 특정 메뉴의 주요정보 크롤링 & 정리 Tool",
    "section": "[세부내용] 구현내용 & 사용한 언어/패키지 등",
    "text": "[세부내용] 구현내용 & 사용한 언어/패키지 등\n\n단건 확인시, 코드셀에 붙여넣기 후 실행, 텍스트로 출력하며 pandas dataframe으로도 저장하여 필요시 엑셀도 제공\n여러건 확인시, 엑셀 등에서 복사한 표를 코드셀에 바로 붙여넣도록 설계(자동 분할, 편의성 고려함) 이후 작업은 단건 확인과 동일\n필요시 엑셀로 저장 (기존 업무유형상 출력텍스트가 더 많이 활용될 것으로 보여 별도 기능으로 추가함)"
  }
]