{
  "cells": [
    {
      "cell_type": "raw",
      "id": "de6ec63f",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: '[DE스터디/4주차과제] 다양한 데이터처리 플랫폼을 사용한 데이터 수집~모니터링'\n",
        "author: 'Kibok Park'\n",
        "date: '2024-09-05'\n",
        "categories: [Spark, PySpark, elasticsearch, 202408Study_DataEngineering]\n",
        "execute:\n",
        "  freeze: auto\n",
        "toc: true\n",
        "draft: false\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "comments:\n",
        "  giscus:\n",
        "    repo: kr9268/giscus_for_blog\n",
        "---\n",
        "데이터 엔지니어링 스터디 내용정리 - Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e73fb7a",
      "metadata": {},
      "source": [
        "# 개요\n",
        "* 참여중인 데이터 엔지니어링 스터디에서 배우는 내용 정리\n",
        "  * 데이터 수집, 정제 : pyspark, airflow\n",
        "  * 저장 : elasticsearch\n",
        "  * 시각화 : kibana\n",
        "\n",
        "* 4주차 과제 : elasticsearch관련 실습(index만들기, pyspark dataframe저장 등)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab30bb21",
      "metadata": {},
      "source": [
        "# 과제 - 4주차"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dfe818",
      "metadata": {},
      "source": [
        "## 데이터 포맷에 적합한 elasticsearch index 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb0ccd2",
      "metadata": {},
      "source": [
        "### 기본코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35bc64a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "24/09/05 14:06:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime, timedelta\n",
        "import sys\n",
        "sys.path.append('/home/jovyan/jobs')\n",
        "from base import read_input, init_df, df_with_meta\n",
        "from filter import DailyStatFilter, TopRepoFilter, TopUserFilter, PytorchTopIssuerFilter\n",
        "from es import Es\n",
        "\n",
        "# SparkSession\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .master(\"local\")\n",
        "    .appName(\"spark-sql\")\n",
        "    .config(\"spark.driver.extraClassPath\", \"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n",
        "    .config(\"spark.jars\", \"/opt/bitnami/spark/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n",
        "    # for jupyter\n",
        "    .config(\"spark.driver.extraClassPath\", \"/home/jovyan/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")\n",
        "    .config(\"spark.jars\", \"/home/jovyan/resources/elasticsearch-spark-30_2.12-8.4.3.jar\")   \n",
        "    # 옵션추가 시작\n",
        "    .config(\"spark.executor.memory\",\"3G\")\n",
        "    .config(\"spark.driver.memory\",\"3G\")\n",
        "    .config(\"spark.executor.cores\",2)\n",
        "    # 옵션추가 끝\n",
        "    .getOrCreate())\n",
        "\n",
        "# 제출용 파일이므로 로그는 미출력되게 조정 (ALL,DEBUG,ERROR,FATAL,TRACE,WARN,INFO,OFF)\n",
        "spark.sparkContext.setLogLevel(\"OFF\")\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.target_date = None\n",
        "        self.input_path = None\n",
        "        self.spark = None\n",
        "\n",
        "args = Args()\n",
        "args.spark = spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62023cb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for Jupyter test\n",
        "args.input_path = f\"/home/jovyan/data/gh_archive/2024-08-24-*.json.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ab5766",
      "metadata": {},
      "source": [
        "### 실습코드 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb769f9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = read_input(args.spark, args.input_path)\n",
        "df = init_df(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f91ef5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|\n",
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "|317111      |255241      |1103808   |106155  |21821     |1480                |\n",
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 27:=============================================>        (169 + 1) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|\n",
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "|      317111|      255241|   1103808|  106155|     21821|                1480|\n",
            "+------------+------------+----------+--------+----------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# DailyStatFilter 산출데이터 확인\n",
        "stat_filter = DailyStatFilter(args)\n",
        "stat_df = stat_filter.filter(df)\n",
        "stat_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdde401a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+----------+--------+----------+--------------------+----------+\n",
            "|d_user_count|d_repo_count|push_count|pr_count|fork_count|commit_comment_count|@timestamp|\n",
            "+------------+------------+----------+--------+----------+--------------------+----------+\n",
            "|      317111|      255241|   1103808|  106155|     21821|                1480|      null|\n",
            "+------------+------------+----------+--------+----------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DailyStatFilter 산출데이터 with metadata(timestamp) 확인\n",
        "stat_df = df_with_meta(stat_df, args.target_date)\n",
        "stat_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c04db4",
      "metadata": {},
      "source": [
        "### eleasticsearch index생성할 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3987c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10345ead",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 5:=============================>                             (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n",
            "|        user_name|                 url|         created_at|         id|repository_id|size|distinct_size|comment|       type|            repo_url|userid_and_repo_name|repo_name|\n",
            "+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n",
            "|         hyperkai|https://api.githu...|2024-08-24 16:21:24|41307929891|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|       phanicoder|https://api.githu...|2024-08-24 16:48:56|41308170756|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|samuele-bortolato|https://api.githu...|2024-08-24 14:56:17|41307163891|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|  pytorchmergebot|https://api.githu...|2024-08-24 17:04:08|41308298815|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|       stevenvana|https://api.githu...|2024-08-24 19:20:50|41309466695|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|  pytorchmergebot|https://api.githu...|2024-08-24 20:33:26|41310013328|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "|  pytorchmergebot|https://api.githu...|2024-08-24 21:09:31|41310280894|         null|null|         null|   null|IssuesEvent|https://api.githu...|     pytorch/pytorch|  pytorch|\n",
            "+-----------------+--------------------+-------------------+-----------+-------------+----+-------------+-------+-----------+--------------------+--------------------+---------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Filter : repo_name = pytorch\n",
        "base_df = df.filter(F.col('userid_and_repo_name') == 'pytorch/pytorch')\n",
        "\n",
        "issues_event_exists = base_df.filter(base_df[\"type\"] == \"IssuesEvent\").count() > 0\n",
        "if issues_event_exists:\n",
        "    filtered_df = base_df.filter(F.col('type') == 'IssuesEvent')\n",
        "else:\n",
        "    filtered_df is None\n",
        "\n",
        "if filtered_df is not None:\n",
        "    filtered_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b410be",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------+\n",
            "|        user_name|IssuesEvent|\n",
            "+-----------------+-----------+\n",
            "|         hyperkai|          1|\n",
            "|       phanicoder|          1|\n",
            "|  pytorchmergebot|          3|\n",
            "|       stevenvana|          1|\n",
            "|samuele-bortolato|          1|\n",
            "+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# groupby : \n",
        "result_df = filtered_df.groupBy('user_name').pivot('type').count()\n",
        "result_df = result_df.cache()\n",
        "result_df.where((~F.col('user_name').contains('[bot]'))) \\\n",
        "            .orderBy(F.desc('IssuesEvent')) \\\n",
        "            .limit(10)\n",
        "result_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e045d31",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- IssuesEvent: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b810105",
      "metadata": {},
      "source": [
        "### elasticsearch index 생성해보기\n",
        "\n",
        "* GPT에서 스키마를 주고 index 생성\n",
        "  * 강의내용에 따라 수정하고자 했으나, GPT의 의도가 내 사용목적에 부합함\n",
        "    * user_name필드\n",
        "      * `type:text`로 검색가능한(full-text search) 텍스트 데이터(analyzer 적용)\n",
        "      * 정렬을 위한 `fields.keyword`사용(analyzer 미적용)\n",
        "    * IssuesEvent필드\n",
        "      * type:long\n",
        "```\n",
        "PUT /pytorch_top_issuer\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"properties\": {\n",
        "      \"user_name\": {\n",
        "        \"type\": \"text\",\n",
        "        \"fields\": {\n",
        "          \"keyword\": {\n",
        "            \"type\": \"keyword\",\n",
        "            \"ignore_above\": 256\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"IssuesEvent\": {\n",
        "        \"type\": \"long\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a4aa05",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'pytorch_top_issuer'}\n"
          ]
        }
      ],
      "source": [
        "# 만든 index넣어보기 (코드 by GPT)\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Elasticsearch 클러스터의 URL 및 포트\n",
        "es_host = 'localhost'\n",
        "es_port = 9200\n",
        "index_name = 'pytorch_top_issuer'\n",
        "\n",
        "# 인덱스 생성에 사용할 JSON 데이터\n",
        "mapping = {\n",
        "  \"mappings\": {\n",
        "    \"properties\": {\n",
        "      \"user_name\": {\n",
        "        \"type\": \"text\",\n",
        "        \"fields\": {\n",
        "          \"keyword\": {\n",
        "            \"type\": \"keyword\",\n",
        "            \"ignore_above\": 256\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"IssuesEvent\": {\n",
        "        \"type\": \"long\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Elasticsearch에 인덱스 생성 요청\n",
        "url = f'http://es:9200/{index_name}'\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "response = requests.put(url, headers=headers, data=json.dumps(mapping))\n",
        "\n",
        "# 응답 출력\n",
        "print(response.status_code)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0470a9",
      "metadata": {},
      "source": [
        "## spark dataframe 을 elasticsearch 에 저장해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965775ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 강의에 사용된 elasticsearch 저장용 코드 그대로 사용\n",
        "class Es(object):\n",
        "    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n",
        "        self.es_hosts = es_hosts\n",
        "        self.es_mode = mode\n",
        "        self.es_write_operation = write_operation\n",
        "        self.es_index_auto_create = \"yes\"\n",
        "        # self.es_mapping_id\n",
        "\n",
        "    def write_df(self, df, es_resource):\n",
        "        df.write.format(\"org.elasticsearch.spark.sql\") \\\n",
        "          .mode(self.es_mode) \\\n",
        "          .option(\"es.nodes\", self.es_hosts) \\\n",
        "          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n",
        "          .option(\"es.resource\", es_resource) \\\n",
        "          .save()\n",
        "\n",
        "# 호스트 지정 후 저장함수 활용\n",
        "es = Es(\"http://es:9200\")\n",
        "es.write_df(result_df, \"pytorch_top_issuer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128ecec5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 앞서 만들어본 elasticsearch index 적용시의 코드\n",
        "## 위 실습에서 만들어본 pytorch_top_issuer 인덱스로 테스트. 이후부터는 auto_create옵션으로 진행할 예정\n",
        "class Es(object):\n",
        "    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n",
        "        self.es_hosts = es_hosts\n",
        "        self.es_mode = mode\n",
        "        self.es_write_operation = write_operation\n",
        "        self.es_index_auto_create = \"no\"  # 기존 인덱스 사용을 위해 \"no\"로 설정\n",
        "        # self.es_mapping_id\n",
        "\n",
        "    def write_df(self, df, es_resource):\n",
        "        df.write.format(\"org.elasticsearch.spark.sql\") \\\n",
        "          .mode(self.es_mode) \\\n",
        "          .option(\"es.nodes\", self.es_hosts) \\\n",
        "          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n",
        "          .option(\"es.resource\", es_resource) \\\n",
        "          .save()\n",
        "\n",
        "# 호스트 지정 후 저장함수 활용\n",
        "es = Es(\"http://es:9200\")\n",
        "es.write_df(df, \"pytorch_top_issuer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4514f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 앞서 만들어본 elasticsearch index 적용시의 코드\n",
        "## 위 실습에서 만들어본 pytorch_top_issuer 인덱스로 테스트. 이후부터는 auto_create옵션으로 진행할 예정\n",
        "## 결과 확인을 위해 spark의 로그레벨을 다시 조정\n",
        "\n",
        "class Es(object):\n",
        "    def __init__(self, es_hosts, mode=\"append\", write_operation=\"overwrite\"):\n",
        "        self.es_hosts = es_hosts\n",
        "        self.es_mode = mode\n",
        "        self.es_write_operation = write_operation\n",
        "        self.es_index_auto_create = \"no\"  # 기존 인덱스 사용을 위해 \"no\"로 설정\n",
        "        # self.es_mapping_id\n",
        "\n",
        "    def write_df(self, df, es_resource):\n",
        "        df.write.format(\"org.elasticsearch.spark.sql\") \\\n",
        "          .mode(self.es_mode) \\\n",
        "          .option(\"es.nodes\", self.es_hosts) \\\n",
        "          .option(\"es.index.auto.create\", self.es_index_auto_create) \\\n",
        "          .option(\"es.resource\", es_resource) \\\n",
        "          .save()\n",
        "        \n",
        "# 결과확인을 위해 임시로 로그레벨 조정 (ALL,DEBUG,ERROR,FATAL,TRACE,WARN,INFO,OFF)\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# 호스트 지정 후 저장함수 활용\n",
        "es = Es(\"http://es:9200\")\n",
        "es.write_df(df, \"pytorch_top_issuer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64304c68",
      "metadata": {},
      "source": [
        "## 저장된 데이터 확인해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c7f0b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f123887d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_69/3314748211.py:15: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  response = es.search(index=index_name, body=query, size=10)  # Adjust 'size' to retrieve more documents\n"
          ]
        }
      ],
      "source": [
        "# Connect to Elasticsearch by GPT\n",
        "es = Elasticsearch(['http://es:9200'])\n",
        "\n",
        "# Define the index\n",
        "index_name = 'pytorch_top_issuer'\n",
        "\n",
        "# Example search query to retrieve all documents (with a size limit)\n",
        "query = {\n",
        "    \"query\": {\n",
        "        \"match_all\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Execute the search query\n",
        "response = es.search(index=index_name, body=query, size=10)  # Adjust 'size' to retrieve more documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29f69c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'user_name': 'hyperkai', 'IssuesEvent': 1}\n",
            "{'user_name': 'phanicoder', 'IssuesEvent': 1}\n",
            "{'user_name': 'pytorchmergebot', 'IssuesEvent': 3}\n",
            "{'user_name': 'stevenvana', 'IssuesEvent': 1}\n",
            "{'user_name': 'samuele-bortolato', 'IssuesEvent': 1}\n"
          ]
        }
      ],
      "source": [
        "# Parse and print the search results\n",
        "for hit in response['hits']['hits']:\n",
        "    print(hit['_source'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
