{
  "cells": [
    {
      "cell_type": "raw",
      "id": "de6ec63f",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: '[GPT활용강의/2일차] beautifulsoup활용한 크롤링'\n",
        "author: 'Kibok Park'\n",
        "date: '2025-05-27'\n",
        "categories: [ChatGPT, beautifulsoup]\n",
        "execute:\n",
        "  freeze: auto\n",
        "toc: true\n",
        "draft: false\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "comments:\n",
        "  giscus:\n",
        "    repo: kr9268/giscus_for_blog\n",
        "---\n",
        "GPT활용강의 내용정리 - beautifulsoup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e73fb7a",
      "metadata": {},
      "source": [
        "# 개요\n",
        "* GPT활용교육을 들은 내용을 별도로 정리\n",
        "  * 1일차 : 네이버API 활용 : GPT를 활용한 변수 분석 등\n",
        "  * 2일차 : beautifulsoup활용한 뉴스 및 증권사리포트 크롤링\n",
        "  * 3일차 : 뉴스 및 증권사리포트 데이터 전처리 \n",
        "  * 4일차 : 감성점수 변환\n",
        "  * 5일차 : 백테스팅 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fd03ec",
      "metadata": {},
      "source": [
        "# 2일차 실습내용 정리"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7c5d9c",
      "metadata": {},
      "source": [
        "## 실습 : BeautifulSoup 객체생성 및 태크/클래스로 크롤링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f96e75",
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_sample = \"\"\"\n",
        "<html>\n",
        "  <head>\n",
        "    <title>테스트 페이지</title>\n",
        "  </head>\n",
        "  <body>\n",
        "    <h1 id=\"main-title\">hello, world</h1>\n",
        "    <p class=\"description\">웹페이지 크롤링중</p>\n",
        "    <a href=\"https://example.com\" class=\"link\">example</a>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# BeautifulSoup 객체생성\n",
        "soup = BeautifulSoup(html_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bfcbedb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<title>테스트 페이지</title>\n",
            "테스트 페이지\n"
          ]
        }
      ],
      "source": [
        "## 제목 태그 (title)\n",
        "title_tag = soup.title\n",
        "\n",
        "print(title_tag)\n",
        "print(title_tag.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eab7fbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h1 id=\"main-title\">hello, world</h1>\n",
            "hello, world\n"
          ]
        }
      ],
      "source": [
        "## 특정 태그 (h1)\n",
        "h1 = soup.find('h1')\n",
        "\n",
        "print(h1)\n",
        "print(h1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5612442",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p class=\"description\">웹페이지 크롤링중</p>\n",
            "웹페이지 크롤링중\n"
          ]
        }
      ],
      "source": [
        "## 클래스 (description)\n",
        "description = soup.find('p', class_='description')\n",
        "\n",
        "print(description)\n",
        "print(description.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af272bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<a class=\"link\" href=\"https://example.com\">example</a>\n",
            "https://example.com\n"
          ]
        }
      ],
      "source": [
        "## 태그 속성 접근 (href)\n",
        "link = soup.find('a')\n",
        "\n",
        "print(link)\n",
        "print(link['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285d4ddf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<a class=\"link\" href=\"https://example.com\">example</a>] \n",
            "\n",
            "https://example.com\n"
          ]
        }
      ],
      "source": [
        "## 모든 태그 찾기\n",
        "all_links = soup.find_all('a')\n",
        "\n",
        "print(all_links, '\\n')\n",
        "\n",
        "for link in all_links:\n",
        "    print(link['href'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa4bc82",
      "metadata": {},
      "source": [
        "## requests와 함께 사용하는 샘플 예제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ac50aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97803dc9",
      "metadata": {},
      "source": [
        "## 실습 : GPT를 활용한 뉴스크롤링\n",
        "\n",
        "* GPT에 아래와 같은 프롬프트를 입력 (좋은 값을 뽑기위해 먼저 GPT가 생각하게 함)\n",
        "\n",
        "    ```\n",
        "    url과 html을 참고해서, url에 어떤 값을 줄 수 있고, html에서 어떤 걸 크롤링할 수 있는지 고민\n",
        "\n",
        "    url : https://search.naver.com/search.naver?ssc=tab.news.all&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2023.01.01&de=2023.02.27&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20230101to20230131&is_sug_officeid=0&office_category=&service_area=\n",
        "\n",
        "    html : <copyElement로 긁어낸 실제 html을 복사. 너무 길어서 여기서는 생략. (copyElement는 하위항목의 태그를 포함하여 복사함)>\n",
        "    ```\n",
        "* GPT가 URL분석(검색 파라미터의 의미)과 html에서 추출가능한 데이터를 알려줌 (하단은 결과샘플)\n",
        "  * URL분석\n",
        "    ```\n",
        "    https://search.naver.com/search.naver?ssc=tab.news.all&query=삼성전자&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2023.01.01&de=2023.02.27&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20230101to20230131&is_sug_officeid=0&office_category=&service_area=\n",
        "\n",
        "    ```\n",
        "  * 결과샘플\n",
        "\n",
        "    | 파라미터                                                                                                                                                     | 의미                                      |\n",
        "    | -------------- | --------------------------------------- |\n",
        "    | `query`                                                                                                                                                  | 검색 키워드 (`삼성전자`)                         |\n",
        "    | `sort`                                                                                                                                                   | 정렬 기준 (0: 관련도, 1: 최신순, 2: 오래된순)         |\n",
        "    | `pd`                                                                                                                                                     | 날짜 범위 (0: 전체, 1: 1일, 2: 1주일, 3: 직접입력)   |\n",
        "    | `ds` / `de`                                                                                                                                              | 시작일 (`2023.01.01`) / 종료일 (`2023.02.27`) |\n",
        "    | `nso`                                                                                                                                                    | 날짜 범위 정제 필터, `p:from~to` 형식             |\n",
        "    | `photo`, `field`, `mynews`, <br>`office_type`, `related`, <br>`office_section_code`, <br>`news_office_checked`,<br> `office_category`, <br>`service_area`, <br>`is_sug_officeid` | 검색 필터 관련 옵션 (필요 시 확장 가능)                |\n",
        "  * HTML에서 크롤링 가능한 정보\n",
        "\n",
        "    | 정보 종류    | 설명                       | 예시                                             |\n",
        "    | -------------- | ------------------------ | ---------------------------------------------- |\n",
        "    | 언론사      | 기사 발행 언론사 이름             | `조선비즈`, `이데일리` 등                               |\n",
        "    | 날짜       | 기사 발행일                   | `2023.01.01.`                                  |\n",
        "    | 제목       | 기사 제목 텍스트                | `\"삼성·LG전자 4분기 실적발표\"`                           |\n",
        "    | 요약       | 기사 요약 텍스트                | `\"삼성전자와 LG전자의 4분기 잠정실적...\"`                    |\n",
        "    | 기사 원문 링크 | `href` 속성에 포함된 뉴스 원문 URL | `https://biz.chosun.com/...`                   |\n",
        "    | 썸네일 이미지  | 이미지 링크                   | `https://imgnews.pstatic.net/image/origin/...` |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c32ae0",
      "metadata": {},
      "source": [
        "* GPT에 아래와 같은 프롬프트를 입력 (샘플 코드 생성)\n",
        "\n",
        "    ```\n",
        "    아래 조건에 맞는 python requests, bs4를 사용한 코드 작성\n",
        "    - url에서 query랑 ds, de를 값으로 주기\n",
        "    - html에서는 기사 제목, 요약, 언론사이름, 날짜를 크롤링\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5ef3d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlencode\n",
        "import datetime\n",
        "\n",
        "def get_news(query: str, ds: str, de: str, display_cnt: int = 1):\n",
        "    \"\"\"\n",
        "    네이버 뉴스 검색에서 기사 제목, 요약, 언론사 이름, 날짜 크롤링\n",
        "\n",
        "    Parameters:\n",
        "        query (str): 검색어\n",
        "        ds (str): 시작일자 ('2025.05.01' 형식)\n",
        "        de (str): 종료일자 ('2025.05.22' 형식)\n",
        "        display_cnt (int): 가져올 페이지 수 (1페이지 = 10건)\n",
        "    \"\"\"\n",
        "    base_url = 'https://search.naver.com/search.naver'\n",
        "    \n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36',\n",
        "        'Referer': 'https://www.naver.com/'\n",
        "    }\n",
        "\n",
        "    all_data = []\n",
        "    \n",
        "    for page in range(display_cnt):\n",
        "        params = {\n",
        "            'where': 'news',\n",
        "            'query': query,\n",
        "            'sm': 'tab_opt',\n",
        "            'sort': 0,\n",
        "            'photo': 0,\n",
        "            'field': 0,\n",
        "            'pd': 3,  # 기간 직접 설정\n",
        "            'ds': ds,\n",
        "            'de': de,\n",
        "            'start': page * 10 + 1\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, headers=headers, params=params)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        news_list = soup.select('div.sds-comps-full-layout.fender-news-item-list-tab > div > div[data-sds-comp=\"Profile\"]')\n",
        "\n",
        "        for news in news_list:\n",
        "            title_tag = news.select_one('a.news_tit')\n",
        "            summary_tag = news.select_one('div.news_dsc')\n",
        "            press_tag = news.select_one('a.info_group > span.press')\n",
        "            date_tag = news.select_one('span.info')\n",
        "\n",
        "            title = title_tag.get_text(strip=True) if title_tag else ''\n",
        "            link = title_tag['href'] if title_tag and title_tag.has_attr('href') else ''\n",
        "            summary = summary_tag.get_text(strip=True) if summary_tag else ''\n",
        "            press = press_tag.get_text(strip=True) if press_tag else ''\n",
        "            date = date_tag.get_text(strip=True) if date_tag else ''\n",
        "\n",
        "            all_data.append({\n",
        "                '제목': title,\n",
        "                '요약': summary,\n",
        "                '언론사': press,\n",
        "                '날짜': date,\n",
        "                '링크': link\n",
        "            })\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "# 예시 사용\n",
        "if __name__ == '__main__':\n",
        "    query = '삼성전자'\n",
        "    ds = '2025.05.01'\n",
        "    de = '2025.05.01'\n",
        "    result = get_news(query, ds, de, display_cnt=2)\n",
        "\n",
        "    for idx, item in enumerate(result, 1):\n",
        "        print(f\"\\n[{idx}]\")\n",
        "        for key, value in item.items():\n",
        "            print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4471b178",
      "metadata": {},
      "source": [
        "## 유의사항\n",
        "\n",
        "* 네이버에는 로봇 차단 방지 기능이 있음\n",
        "  * User-Agent설정, 요청간간딜레이를 추가하여 대응\n",
        "\n",
        "* 데이터 사용 윤리\n",
        "  * 공개된 데이터만 활용하고, 과도한 요청 피하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14933145",
      "metadata": {},
      "source": [
        "## 처음 제공된 기초 코드 샘플"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2adf02a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "963\n"
          ]
        }
      ],
      "source": [
        "from pykrx import stock\n",
        "\n",
        "# 코스피 상장 종목 코드 리스트 가져오기\n",
        "tickers = stock.get_market_ticker_list(market=\"KOSPI\")\n",
        "\n",
        "kospi_name = []\n",
        "\n",
        "# 종목 코드와 종목명 출력\n",
        "for ticker in tickers:\n",
        "    name = stock.get_market_ticker_name(ticker)\n",
        "    kospi_name.append(name)\n",
        "\n",
        "print(len(kospi_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa6d431",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b999dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import time\n",
        "\n",
        "def get_naver_news_titles(company_name, start_date, end_date):\n",
        "    # 네이버 뉴스 검색 URL 생성\n",
        "    base_url = \"https://search.naver.com/search.naver\"\n",
        "    query = urllib.parse.quote(company_name)\n",
        "    start_date_clean = start_date.replace('.', '')\n",
        "    end_date_clean = end_date.replace('.', '')\n",
        "\n",
        "    params = {\n",
        "        'where': 'news',\n",
        "        'query': query,\n",
        "        'sm': 'tab_opt',\n",
        "        'sort': '0',\n",
        "        'photo': '0',\n",
        "        'field': '0',\n",
        "        'pd': '3',\n",
        "        'ds': start_date,\n",
        "        'de': end_date,\n",
        "        'nso': f'so:r,p:from{start_date_clean}to{end_date_clean}',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36',\n",
        "        'Referer': 'https://www.naver.com/'\n",
        "    }\n",
        "\n",
        "    # 세션 객체 생성\n",
        "    session = requests.Session()\n",
        "\n",
        "    try:\n",
        "        response = session.get(base_url, params=params, headers=headers)\n",
        "\n",
        "        # 응답 상태 코드 확인\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "        # HTML 파싱\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        news_data = []\n",
        "\n",
        "        # 뉴스 제목, 날짜, 요약 찾기\n",
        "        for news in soup.select(\"div.news_area\"):\n",
        "            title_tag = news.select_one(\"a.news_tit\")\n",
        "            date_tag = news.select_one(\"span.info\")\n",
        "            summary_tag = news.select_one(\"div.dsc_wrap\")\n",
        "\n",
        "            if title_tag and date_tag and summary_tag:\n",
        "                title = title_tag.get('title')\n",
        "                date = date_tag.text\n",
        "                summary = summary_tag.text.strip()\n",
        "                news_data.append((title, date, summary))\n",
        "\n",
        "            # 딜레이 추가 (너무 빠른 요청 방지)\n",
        "            time.sleep(1)\n",
        "\n",
        "        return news_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b842edaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 한달마다 데이터 뽑아오기\n",
        "\n",
        "month_list = ['2023.01.01', '2023.02.01', '2023.03.01', '2023.04.01', '2023.05.01', '2023.06.01']\n",
        "month_end_list = ['2023.01.31', '2023.02.28', '2023.03.31', '2023.04.30', '2023.05.31', '2023.06.30']\n",
        "\n",
        "\n",
        "# company_name = \"LG\"\n",
        "# start_date = \"2023.10.01\"\n",
        "# end_date = \"2023.10.31\"\n",
        "\n",
        "\n",
        "for month in range(6):\n",
        "    news_df = pd.DataFrame(columns=['company', 'title', 'date', 'summary'])\n",
        "    for company in tqdm(kospi_name):\n",
        "        print(f\"News for {company} from {month_list[month]} to {month_end_list[month]}:\")\n",
        "        # 만약 company의 30개정도 돌았으면, 10초정도 쉬어준다\n",
        "        if len(news_df) % 300 == 0:\n",
        "            time.sleep(10)\n",
        "        news_data = get_naver_news_titles(company, month_list[month], month_end_list[month])\n",
        "        for i, (title, date, summary) in enumerate(news_data, 1):\n",
        "            print(f\"{i}. {title} ({date})\")\n",
        "            news_df.loc[len(news_df)] = [company, title, date, summary]\n",
        "    news_df.to_csv(f'뉴스크롤링{month}.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "news_df.to_csv('뉴스크롤링.csv', index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
